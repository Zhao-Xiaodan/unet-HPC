=======================================================================
U-NET HYPERPARAMETER OPTIMIZATION - GRID SEARCH
=======================================================================
Testing Learning Rates: [1e-4, 5e-4, 1e-3, 5e-3]
Testing Batch Sizes: [8, 16, 32]
Testing Architectures: [UNet, Attention_UNet, Attention_ResUNet]
Epochs per experiment: 75 (increased for proper convergence)
Total experiments: 4 LR Ã— 3 BS Ã— 3 Arch = 36 experiments
Estimated total time: 36-48 hours

Job started on Fri Sep 26 04:50:36 PM +08 2025
Running on node: GN-A40-007
Job ID: 229354.stdct-mgmt-02
Available GPUs: GPU-84085022-b1ef-6183-8c83-3620ed65464b
Memory: 503Gi, CPUs: 36

TensorFlow Container: /app1/common/singularity-img/hopper/tensorflow/tensorflow_2.16.1-cuda_12.5.0_24.06.sif

Main results directory: hyperparameter_optimization_20250926_165036

Creating hyperparameter training script...
âœ“ Hyperparameter training script created

ðŸš€ STARTING HYPERPARAMETER GRID SEARCH
======================================
Total experiments to run: 36


ðŸ”¬ EXPERIMENT 1/36
================================================
Architecture: UNet
Learning Rate: 1e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758876677.519503 1006088 service.cc:145] XLA service 0x145f6a11bdb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758876677.519530 1006088 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758876677.656728 1006088 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 8:55 - loss: 0.3266 - accuracy: 0.4468 - jacard_coef: 0.0944 2/17 [==>...........................] - ETA: 53s - loss: 0.3095 - accuracy: 0.3496 - jacard_coef: 0.0674  3/17 [====>.........................] - ETA: 32s - loss: 0.2836 - accuracy: 0.3035 - jacard_coef: 0.0806 4/17 [======>.......................] - ETA: 24s - loss: 0.2679 - accuracy: 0.2670 - jacard_coef: 0.0808 5/17 [=======>......................] - ETA: 18s - loss: 0.2576 - accuracy: 0.2369 - jacard_coef: 0.0778 6/17 [=========>....................] - ETA: 13s - loss: 0.2505 - accuracy: 0.2197 - jacard_coef: 0.0766 7/17 [===========>..................] - ETA: 10s - loss: 0.2430 - accuracy: 0.2231 - jacard_coef: 0.0738 8/17 [=============>................] - ETA: 8s - loss: 0.2377 - accuracy: 0.2140 - jacard_coef: 0.0721  9/17 [==============>...............] - ETA: 6s - loss: 0.2327 - accuracy: 0.2229 - jacard_coef: 0.075010/17 [================>.............] - ETA: 5s - loss: 0.2283 - accuracy: 0.2213 - jacard_coef: 0.072211/17 [==================>...........] - ETA: 4s - loss: 0.2248 - accuracy: 0.2289 - jacard_coef: 0.078712/17 [====================>.........] - ETA: 3s - loss: 0.2215 - accuracy: 0.2344 - jacard_coef: 0.080613/17 [=====================>........] - ETA: 2s - loss: 0.2185 - accuracy: 0.2409 - jacard_coef: 0.081214/17 [=======================>......] - ETA: 1s - loss: 0.2165 - accuracy: 0.2424 - jacard_coef: 0.081215/17 [=========================>....] - ETA: 1s - loss: 0.2191 - accuracy: 0.2483 - jacard_coef: 0.080916/17 [===========================>..] - ETA: 0s - loss: 0.2177 - accuracy: 0.2440 - jacard_coef: 0.080617/17 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.2438 - jacard_coef: 0.086117/17 [==============================] - 47s 846ms/step - loss: 0.2175 - accuracy: 0.2438 - jacard_coef: 0.0861 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1985 - accuracy: 0.1142 - jacard_coef: 0.0289 2/17 [==>...........................] - ETA: 1s - loss: 0.1949 - accuracy: 0.1458 - jacard_coef: 0.0703 3/17 [====>.........................] - ETA: 1s - loss: 0.1941 - accuracy: 0.1422 - jacard_coef: 0.0802 4/17 [======>.......................] - ETA: 1s - loss: 0.1949 - accuracy: 0.1409 - jacard_coef: 0.0793 5/17 [=======>......................] - ETA: 1s - loss: 0.1933 - accuracy: 0.1423 - jacard_coef: 0.0791 6/17 [=========>....................] - ETA: 1s - loss: 0.1954 - accuracy: 0.1432 - jacard_coef: 0.0801 7/17 [===========>..................] - ETA: 1s - loss: 0.1945 - accuracy: 0.1411 - jacard_coef: 0.0792 8/17 [=============>................] - ETA: 1s - loss: 0.1939 - accuracy: 0.1450 - jacard_coef: 0.0823 9/17 [==============>...............] - ETA: 1s - loss: 0.1926 - accuracy: 0.1505 - jacard_coef: 0.080110/17 [================>.............] - ETA: 0s - loss: 0.1919 - accuracy: 0.1631 - jacard_coef: 0.083311/17 [==================>...........] - ETA: 0s - loss: 0.1943 - accuracy: 0.1705 - jacard_coef: 0.081312/17 [====================>.........] - ETA: 0s - loss: 0.1944 - accuracy: 0.1718 - jacard_coef: 0.081313/17 [=====================>........] - ETA: 0s - loss: 0.1957 - accuracy: 0.1699 - jacard_coef: 0.079614/17 [=======================>......] - ETA: 0s - loss: 0.1978 - accuracy: 0.1682 - jacard_coef: 0.078915/17 [=========================>....] - ETA: 0s - loss: 0.1988 - accuracy: 0.1714 - jacard_coef: 0.078216/17 [===========================>..] - ETA: 0s - loss: 0.1988 - accuracy: 0.1844 - jacard_coef: 0.079117/17 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.1857 - jacard_coef: 0.079917/17 [==============================] - 2s 133ms/step - loss: 0.1990 - accuracy: 0.1857 - jacard_coef: 0.0799 - val_loss: 1.1014 - val_accuracy: 0.9222 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1880 - accuracy: 0.5201 - jacard_coef: 0.0890 2/17 [==>...........................] - ETA: 1s - loss: 0.1943 - accuracy: 0.5138 - jacard_coef: 0.0745 3/17 [====>.........................] - ETA: 1s - loss: 0.1929 - accuracy: 0.5010 - jacard_coef: 0.0847 4/17 [======>.......................] - ETA: 1s - loss: 0.1905 - accuracy: 0.4831 - jacard_coef: 0.0824 5/17 [=======>......................] - ETA: 1s - loss: 0.1903 - accuracy: 0.4603 - jacard_coef: 0.0876 6/17 [=========>....................] - ETA: 1s - loss: 0.1884 - accuracy: 0.4436 - jacard_coef: 0.0905 7/17 [===========>..................] - ETA: 1s - loss: 0.1876 - accuracy: 0.4252 - jacard_coef: 0.0881 8/17 [=============>................] - ETA: 1s - loss: 0.1862 - accuracy: 0.4209 - jacard_coef: 0.0839 9/17 [==============>...............] - ETA: 1s - loss: 0.1855 - accuracy: 0.4027 - jacard_coef: 0.082310/17 [================>.............] - ETA: 0s - loss: 0.1847 - accuracy: 0.3898 - jacard_coef: 0.080711/17 [==================>...........] - ETA: 0s - loss: 0.1839 - accuracy: 0.3939 - jacard_coef: 0.078912/17 [====================>.........] - ETA: 0s - loss: 0.1833 - accuracy: 0.3937 - jacard_coef: 0.078213/17 [=====================>........] - ETA: 0s - loss: 0.1826 - accuracy: 0.3994 - jacard_coef: 0.076714/17 [=======================>......] - ETA: 0s - loss: 0.1821 - accuracy: 0.4009 - jacard_coef: 0.076115/17 [=========================>....] - ETA: 0s - loss: 0.1815 - accuracy: 0.4134 - jacard_coef: 0.076016/17 [===========================>..] - ETA: 0s - loss: 0.1810 - accuracy: 0.4122 - jacard_coef: 0.076717/17 [==============================] - 2s 130ms/step - loss: 0.1810 - accuracy: 0.4115 - jacard_coef: 0.0759 - val_loss: 1.0180 - val_accuracy: 0.9269 - val_jacard_coef: 0.0064 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1746 - accuracy: 0.5272 - jacard_coef: 0.0680 2/17 [==>...........................] - ETA: 1s - loss: 0.1746 - accuracy: 0.5770 - jacard_coef: 0.0605 3/17 [====>.........................] - ETA: 1s - loss: 0.1744 - accuracy: 0.5800 - jacard_coef: 0.0752 4/17 [======>.......................] - ETA: 1s - loss: 0.1772 - accuracy: 0.5363 - jacard_coef: 0.0793 5/17 [=======>......................] - ETA: 1s - loss: 0.1764 - accuracy: 0.5190 - jacard_coef: 0.0795 6/17 [=========>....................] - ETA: 1s - loss: 0.1756 - accuracy: 0.5115 - jacard_coef: 0.0782 7/17 [===========>..................] - ETA: 1s - loss: 0.1757 - accuracy: 0.5069 - jacard_coef: 0.0769 8/17 [=============>................] - ETA: 1s - loss: 0.1756 - accuracy: 0.4928 - jacard_coef: 0.0778 9/17 [==============>...............] - ETA: 1s - loss: 0.1750 - accuracy: 0.4838 - jacard_coef: 0.073210/17 [================>.............] - ETA: 0s - loss: 0.1746 - accuracy: 0.5055 - jacard_coef: 0.073911/17 [==================>...........] - ETA: 0s - loss: 0.1741 - accuracy: 0.5311 - jacard_coef: 0.071912/17 [====================>.........] - ETA: 0s - loss: 0.1735 - accuracy: 0.5579 - jacard_coef: 0.069113/17 [=====================>........] - ETA: 0s - loss: 0.1732 - accuracy: 0.5661 - jacard_coef: 0.071314/17 [=======================>......] - ETA: 0s - loss: 0.1727 - accuracy: 0.5797 - jacard_coef: 0.071915/17 [=========================>....] - ETA: 0s - loss: 0.1722 - accuracy: 0.5954 - jacard_coef: 0.067816/17 [===========================>..] - ETA: 0s - loss: 0.1724 - accuracy: 0.6076 - jacard_coef: 0.065817/17 [==============================] - 2s 130ms/step - loss: 0.1725 - accuracy: 0.6063 - jacard_coef: 0.0716 - val_loss: 1.0617 - val_accuracy: 0.8968 - val_jacard_coef: 0.0359 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8688 - jacard_coef: 0.0241 2/17 [==>...........................] - ETA: 1s - loss: 0.1683 - accuracy: 0.8737 - jacard_coef: 0.0266 3/17 [====>.........................] - ETA: 1s - loss: 0.1704 - accuracy: 0.8612 - jacard_coef: 0.0358 4/17 [======>.......................] - ETA: 1s - loss: 0.1696 - accuracy: 0.8541 - jacard_coef: 0.0385 5/17 [=======>......................] - ETA: 1s - loss: 0.1714 - accuracy: 0.8319 - jacard_coef: 0.0417 6/17 [=========>....................] - ETA: 1s - loss: 0.1718 - accuracy: 0.8244 - jacard_coef: 0.0478 7/17 [===========>..................] - ETA: 1s - loss: 0.1711 - accuracy: 0.7823 - jacard_coef: 0.0498 8/17 [=============>................] - ETA: 1s - loss: 0.1712 - accuracy: 0.7693 - jacard_coef: 0.0520 9/17 [==============>...............] - ETA: 1s - loss: 0.1702 - accuracy: 0.7772 - jacard_coef: 0.050410/17 [================>.............] - ETA: 0s - loss: 0.1696 - accuracy: 0.7725 - jacard_coef: 0.048311/17 [==================>...........] - ETA: 0s - loss: 0.1689 - accuracy: 0.7690 - jacard_coef: 0.051012/17 [====================>.........] - ETA: 0s - loss: 0.1685 - accuracy: 0.7617 - jacard_coef: 0.055313/17 [=====================>........] - ETA: 0s - loss: 0.1681 - accuracy: 0.7550 - jacard_coef: 0.054814/17 [=======================>......] - ETA: 0s - loss: 0.1677 - accuracy: 0.7516 - jacard_coef: 0.057115/17 [=========================>....] - ETA: 0s - loss: 0.1687 - accuracy: 0.7311 - jacard_coef: 0.061116/17 [===========================>..] - ETA: 0s - loss: 0.1683 - accuracy: 0.7363 - jacard_coef: 0.060517/17 [==============================] - 2s 130ms/step - loss: 0.1686 - accuracy: 0.7371 - jacard_coef: 0.0600 - val_loss: 0.1817 - val_accuracy: 0.4574 - val_jacard_coef: 0.0531 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1678 - accuracy: 0.8735 - jacard_coef: 0.0221 2/17 [==>...........................] - ETA: 1s - loss: 0.1665 - accuracy: 0.8527 - jacard_coef: 0.0490 3/17 [====>.........................] - ETA: 1s - loss: 0.1669 - accuracy: 0.8363 - jacard_coef: 0.0463 4/17 [======>.......................] - ETA: 1s - loss: 0.1673 - accuracy: 0.8457 - jacard_coef: 0.0411 5/17 [=======>......................] - ETA: 1s - loss: 0.1670 - accuracy: 0.8303 - jacard_coef: 0.0429 6/17 [=========>....................] - ETA: 1s - loss: 0.1657 - accuracy: 0.8242 - jacard_coef: 0.0484 7/17 [===========>..................] - ETA: 1s - loss: 0.1660 - accuracy: 0.7972 - jacard_coef: 0.0523 8/17 [=============>................] - ETA: 1s - loss: 0.1656 - accuracy: 0.7902 - jacard_coef: 0.0601 9/17 [==============>...............] - ETA: 1s - loss: 0.1649 - accuracy: 0.7842 - jacard_coef: 0.062510/17 [================>.............] - ETA: 0s - loss: 0.1645 - accuracy: 0.7744 - jacard_coef: 0.062611/17 [==================>...........] - ETA: 0s - loss: 0.1645 - accuracy: 0.7699 - jacard_coef: 0.063212/17 [====================>.........] - ETA: 0s - loss: 0.1639 - accuracy: 0.7744 - jacard_coef: 0.064813/17 [=====================>........] - ETA: 0s - loss: 0.1637 - accuracy: 0.7766 - jacard_coef: 0.062214/17 [=======================>......] - ETA: 0s - loss: 0.1633 - accuracy: 0.7782 - jacard_coef: 0.062715/17 [=========================>....] - ETA: 0s - loss: 0.1630 - accuracy: 0.7774 - jacard_coef: 0.062216/17 [===========================>..] - ETA: 0s - loss: 0.1626 - accuracy: 0.7791 - jacard_coef: 0.061517/17 [==============================] - 2s 130ms/step - loss: 0.1627 - accuracy: 0.7776 - jacard_coef: 0.0632 - val_loss: 0.1751 - val_accuracy: 0.4348 - val_jacard_coef: 0.0599 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1590 - accuracy: 0.9169 - jacard_coef: 0.0193 2/17 [==>...........................] - ETA: 1s - loss: 0.1580 - accuracy: 0.8951 - jacard_coef: 0.0266 3/17 [====>.........................] - ETA: 1s - loss: 0.1597 - accuracy: 0.8584 - jacard_coef: 0.0474 4/17 [======>.......................] - ETA: 1s - loss: 0.1600 - accuracy: 0.8527 - jacard_coef: 0.0441 5/17 [=======>......................] - ETA: 1s - loss: 0.1600 - accuracy: 0.8460 - jacard_coef: 0.0486 6/17 [=========>....................] - ETA: 1s - loss: 0.1598 - accuracy: 0.8461 - jacard_coef: 0.0479 7/17 [===========>..................] - ETA: 1s - loss: 0.1599 - accuracy: 0.8367 - jacard_coef: 0.0611 8/17 [=============>................] - ETA: 1s - loss: 0.1597 - accuracy: 0.8318 - jacard_coef: 0.0611 9/17 [==============>...............] - ETA: 1s - loss: 0.1597 - accuracy: 0.8251 - jacard_coef: 0.064010/17 [================>.............] - ETA: 0s - loss: 0.1597 - accuracy: 0.8248 - jacard_coef: 0.062011/17 [==================>...........] - ETA: 0s - loss: 0.1594 - accuracy: 0.8236 - jacard_coef: 0.062212/17 [====================>.........] - ETA: 0s - loss: 0.1592 - accuracy: 0.8243 - jacard_coef: 0.062713/17 [=====================>........] - ETA: 0s - loss: 0.1588 - accuracy: 0.8259 - jacard_coef: 0.061914/17 [=======================>......] - ETA: 0s - loss: 0.1585 - accuracy: 0.8263 - jacard_coef: 0.060815/17 [=========================>....] - ETA: 0s - loss: 0.1583 - accuracy: 0.8266 - jacard_coef: 0.060016/17 [===========================>..] - ETA: 0s - loss: 0.1584 - accuracy: 0.8252 - jacard_coef: 0.060817/17 [==============================] - 2s 130ms/step - loss: 0.1584 - accuracy: 0.8252 - jacard_coef: 0.0579 - val_loss: 0.2021 - val_accuracy: 0.2289 - val_jacard_coef: 0.0649 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1554 - accuracy: 0.8220 - jacard_coef: 0.0613 2/17 [==>...........................] - ETA: 1s - loss: 0.1555 - accuracy: 0.8185 - jacard_coef: 0.0525 3/17 [====>.........................] - ETA: 1s - loss: 0.1549 - accuracy: 0.8342 - jacard_coef: 0.0447 4/17 [======>.......................] - ETA: 1s - loss: 0.1544 - accuracy: 0.8543 - jacard_coef: 0.0379 5/17 [=======>......................] - ETA: 1s - loss: 0.1540 - accuracy: 0.8665 - jacard_coef: 0.0327 6/17 [=========>....................] - ETA: 1s - loss: 0.1538 - accuracy: 0.8740 - jacard_coef: 0.0279 7/17 [===========>..................] - ETA: 1s - loss: 0.1540 - accuracy: 0.8807 - jacard_coef: 0.0243 8/17 [=============>................] - ETA: 1s - loss: 0.1539 - accuracy: 0.8818 - jacard_coef: 0.0220 9/17 [==============>...............] - ETA: 1s - loss: 0.1537 - accuracy: 0.8862 - jacard_coef: 0.019810/17 [================>.............] - ETA: 0s - loss: 0.1534 - accuracy: 0.8905 - jacard_coef: 0.018111/17 [==================>...........] - ETA: 0s - loss: 0.1533 - accuracy: 0.8907 - jacard_coef: 0.017312/17 [====================>.........] - ETA: 0s - loss: 0.1534 - accuracy: 0.8878 - jacard_coef: 0.017013/17 [=====================>........] - ETA: 0s - loss: 0.1533 - accuracy: 0.8883 - jacard_coef: 0.017014/17 [=======================>......] - ETA: 0s - loss: 0.1532 - accuracy: 0.8892 - jacard_coef: 0.016515/17 [=========================>....] - ETA: 0s - loss: 0.1530 - accuracy: 0.8902 - jacard_coef: 0.017716/17 [===========================>..] - ETA: 0s - loss: 0.1528 - accuracy: 0.8924 - jacard_coef: 0.017617/17 [==============================] - 2s 130ms/step - loss: 0.1528 - accuracy: 0.8905 - jacard_coef: 0.0251 - val_loss: 0.2428 - val_accuracy: 0.1590 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1489 - accuracy: 0.9274 - jacard_coef: 0.0133 2/17 [==>...........................] - ETA: 1s - loss: 0.1504 - accuracy: 0.9077 - jacard_coef: 0.0102 3/17 [====>.........................] - ETA: 1s - loss: 0.1511 - accuracy: 0.8969 - jacard_coef: 0.0072 4/17 [======>.......................] - ETA: 1s - loss: 0.1509 - accuracy: 0.8985 - jacard_coef: 0.0063 5/17 [=======>......................] - ETA: 1s - loss: 0.1506 - accuracy: 0.9046 - jacard_coef: 0.0060 6/17 [=========>....................] - ETA: 1s - loss: 0.1502 - accuracy: 0.9094 - jacard_coef: 0.0055 7/17 [===========>..................] - ETA: 1s - loss: 0.1507 - accuracy: 0.9066 - jacard_coef: 0.0050 8/17 [=============>................] - ETA: 1s - loss: 0.1507 - accuracy: 0.9102 - jacard_coef: 0.0044 9/17 [==============>...............] - ETA: 1s - loss: 0.1506 - accuracy: 0.9084 - jacard_coef: 0.003910/17 [================>.............] - ETA: 0s - loss: 0.1503 - accuracy: 0.9103 - jacard_coef: 0.003611/17 [==================>...........] - ETA: 0s - loss: 0.1501 - accuracy: 0.9119 - jacard_coef: 0.003412/17 [====================>.........] - ETA: 0s - loss: 0.1500 - accuracy: 0.9115 - jacard_coef: 0.003313/17 [=====================>........] - ETA: 0s - loss: 0.1497 - accuracy: 0.9141 - jacard_coef: 0.003214/17 [=======================>......] - ETA: 0s - loss: 0.1495 - accuracy: 0.9144 - jacard_coef: 0.003315/17 [=========================>....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9153 - jacard_coef: 0.003416/17 [===========================>..] - ETA: 0s - loss: 0.1492 - accuracy: 0.9137 - jacard_coef: 0.003417/17 [==============================] - 2s 128ms/step - loss: 0.1492 - accuracy: 0.9132 - jacard_coef: 0.0032 - val_loss: 0.1615 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1507 - accuracy: 0.8754 - jacard_coef: 0.0081 2/17 [==>...........................] - ETA: 1s - loss: 0.1476 - accuracy: 0.9105 - jacard_coef: 0.0092 3/17 [====>.........................] - ETA: 1s - loss: 0.1468 - accuracy: 0.9169 - jacard_coef: 0.0070 4/17 [======>.......................] - ETA: 1s - loss: 0.1465 - accuracy: 0.9190 - jacard_coef: 0.0056 5/17 [=======>......................] - ETA: 1s - loss: 0.1465 - accuracy: 0.9187 - jacard_coef: 0.0050 6/17 [=========>....................] - ETA: 1s - loss: 0.1467 - accuracy: 0.9114 - jacard_coef: 0.0043 7/17 [===========>..................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9144 - jacard_coef: 0.0040 8/17 [=============>................] - ETA: 1s - loss: 0.1462 - accuracy: 0.9166 - jacard_coef: 0.0038 9/17 [==============>...............] - ETA: 1s - loss: 0.1459 - accuracy: 0.9179 - jacard_coef: 0.003510/17 [================>.............] - ETA: 0s - loss: 0.1457 - accuracy: 0.9168 - jacard_coef: 0.003311/17 [==================>...........] - ETA: 0s - loss: 0.1457 - accuracy: 0.9147 - jacard_coef: 0.003112/17 [====================>.........] - ETA: 0s - loss: 0.1454 - accuracy: 0.9162 - jacard_coef: 0.003013/17 [=====================>........] - ETA: 0s - loss: 0.1451 - accuracy: 0.9177 - jacard_coef: 0.002714/17 [=======================>......] - ETA: 0s - loss: 0.1450 - accuracy: 0.9169 - jacard_coef: 0.002815/17 [=========================>....] - ETA: 0s - loss: 0.1449 - accuracy: 0.9161 - jacard_coef: 0.002816/17 [===========================>..] - ETA: 0s - loss: 0.1448 - accuracy: 0.9156 - jacard_coef: 0.002717/17 [==============================] - 2s 128ms/step - loss: 0.1449 - accuracy: 0.9149 - jacard_coef: 0.0026 - val_loss: 0.1618 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1440 - accuracy: 0.9039 - jacard_coef: 0.0020 2/17 [==>...........................] - ETA: 1s - loss: 0.1443 - accuracy: 0.8906 - jacard_coef: 0.0011 3/17 [====>.........................] - ETA: 1s - loss: 0.1427 - accuracy: 0.9108 - jacard_coef: 7.3425e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1418 - accuracy: 0.9219 - jacard_coef: 6.0372e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1417 - accuracy: 0.9208 - jacard_coef: 0.0043     6/17 [=========>....................] - ETA: 1s - loss: 0.1411 - accuracy: 0.9259 - jacard_coef: 0.0097 7/17 [===========>..................] - ETA: 1s - loss: 0.1410 - accuracy: 0.9251 - jacard_coef: 0.0126 8/17 [=============>................] - ETA: 1s - loss: 0.1410 - accuracy: 0.9234 - jacard_coef: 0.0118 9/17 [==============>...............] - ETA: 1s - loss: 0.1412 - accuracy: 0.9190 - jacard_coef: 0.011210/17 [================>.............] - ETA: 0s - loss: 0.1409 - accuracy: 0.9211 - jacard_coef: 0.010111/17 [==================>...........] - ETA: 0s - loss: 0.1408 - accuracy: 0.9221 - jacard_coef: 0.009212/17 [====================>.........] - ETA: 0s - loss: 0.1410 - accuracy: 0.9193 - jacard_coef: 0.008713/17 [=====================>........] - ETA: 0s - loss: 0.1412 - accuracy: 0.9160 - jacard_coef: 0.008014/17 [=======================>......] - ETA: 0s - loss: 0.1412 - accuracy: 0.9145 - jacard_coef: 0.007515/17 [=========================>....] - ETA: 0s - loss: 0.1411 - accuracy: 0.9149 - jacard_coef: 0.007016/17 [===========================>..] - ETA: 0s - loss: 0.1412 - accuracy: 0.9134 - jacard_coef: 0.006517/17 [==============================] - 2s 128ms/step - loss: 0.1412 - accuracy: 0.9133 - jacard_coef: 0.0061 - val_loss: 0.1571 - val_accuracy: 0.9294 - val_jacard_coef: 0.0018 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1381 - accuracy: 0.9230 - jacard_coef: 2.4775e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9449 - jacard_coef: 4.1138e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1383 - accuracy: 0.9224 - jacard_coef: 7.7752e-05 4/17 [======>.......................] - ETA: 1s - loss: 0.1384 - accuracy: 0.9203 - jacard_coef: 5.8314e-05 5/17 [=======>......................] - ETA: 1s - loss: 0.1379 - accuracy: 0.9249 - jacard_coef: 4.6651e-05 6/17 [=========>....................] - ETA: 1s - loss: 0.1383 - accuracy: 0.9194 - jacard_coef: 3.8876e-05 7/17 [===========>..................] - ETA: 1s - loss: 0.1386 - accuracy: 0.9166 - jacard_coef: 3.3322e-05 8/17 [=============>................] - ETA: 1s - loss: 0.1385 - accuracy: 0.9187 - jacard_coef: 2.9157e-05 9/17 [==============>...............] - ETA: 1s - loss: 0.1399 - accuracy: 0.8956 - jacard_coef: 0.0062    10/17 [================>.............] - ETA: 0s - loss: 0.1398 - accuracy: 0.8966 - jacard_coef: 0.005611/17 [==================>...........] - ETA: 0s - loss: 0.1400 - accuracy: 0.8957 - jacard_coef: 0.005112/17 [====================>.........] - ETA: 0s - loss: 0.1401 - accuracy: 0.8973 - jacard_coef: 0.004913/17 [=====================>........] - ETA: 0s - loss: 0.1401 - accuracy: 0.8972 - jacard_coef: 0.004714/17 [=======================>......] - ETA: 0s - loss: 0.1400 - accuracy: 0.8983 - jacard_coef: 0.005115/17 [=========================>....] - ETA: 0s - loss: 0.1403 - accuracy: 0.8962 - jacard_coef: 0.005716/17 [===========================>..] - ETA: 0s - loss: 0.1400 - accuracy: 0.8995 - jacard_coef: 0.005817/17 [==============================] - 2s 128ms/step - loss: 0.1400 - accuracy: 0.8989 - jacard_coef: 0.0087 - val_loss: 0.1541 - val_accuracy: 0.7392 - val_jacard_coef: 0.0685 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1403 - accuracy: 0.9166 - jacard_coef: 0.0206 2/17 [==>...........................] - ETA: 1s - loss: 0.1408 - accuracy: 0.8985 - jacard_coef: 0.0121 3/17 [====>.........................] - ETA: 1s - loss: 0.1409 - accuracy: 0.8933 - jacard_coef: 0.0119 4/17 [======>.......................] - ETA: 1s - loss: 0.1409 - accuracy: 0.8932 - jacard_coef: 0.0152 5/17 [=======>......................] - ETA: 1s - loss: 0.1402 - accuracy: 0.9054 - jacard_coef: 0.0157 6/17 [=========>....................] - ETA: 1s - loss: 0.1402 - accuracy: 0.9082 - jacard_coef: 0.0145 7/17 [===========>..................] - ETA: 1s - loss: 0.1400 - accuracy: 0.9087 - jacard_coef: 0.0132 8/17 [=============>................] - ETA: 1s - loss: 0.1400 - accuracy: 0.9066 - jacard_coef: 0.0122 9/17 [==============>...............] - ETA: 1s - loss: 0.1401 - accuracy: 0.9053 - jacard_coef: 0.011010/17 [================>.............] - ETA: 0s - loss: 0.1402 - accuracy: 0.9031 - jacard_coef: 0.010711/17 [==================>...........] - ETA: 0s - loss: 0.1401 - accuracy: 0.9028 - jacard_coef: 0.009812/17 [====================>.........] - ETA: 0s - loss: 0.1399 - accuracy: 0.9031 - jacard_coef: 0.009313/17 [=====================>........] - ETA: 0s - loss: 0.1399 - accuracy: 0.9022 - jacard_coef: 0.008914/17 [=======================>......] - ETA: 0s - loss: 0.1397 - accuracy: 0.9035 - jacard_coef: 0.008415/17 [=========================>....] - ETA: 0s - loss: 0.1393 - accuracy: 0.9062 - jacard_coef: 0.008016/17 [===========================>..] - ETA: 0s - loss: 0.1394 - accuracy: 0.9053 - jacard_coef: 0.007717/17 [==============================] - 2s 128ms/step - loss: 0.1394 - accuracy: 0.9052 - jacard_coef: 0.0072 - val_loss: 0.1496 - val_accuracy: 0.9302 - val_jacard_coef: 5.2965e-05 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1331 - accuracy: 0.9421 - jacard_coef: 9.8645e-04 2/17 [==>...........................] - ETA: 1s - loss: 0.1342 - accuracy: 0.9317 - jacard_coef: 0.0050     3/17 [====>.........................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9289 - jacard_coef: 0.0047 4/17 [======>.......................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9280 - jacard_coef: 0.0040 5/17 [=======>......................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9300 - jacard_coef: 0.0035 6/17 [=========>....................] - ETA: 1s - loss: 0.1343 - accuracy: 0.9221 - jacard_coef: 0.0032 7/17 [===========>..................] - ETA: 1s - loss: 0.1347 - accuracy: 0.9187 - jacard_coef: 0.0028 8/17 [=============>................] - ETA: 1s - loss: 0.1347 - accuracy: 0.9184 - jacard_coef: 0.0026 9/17 [==============>...............] - ETA: 1s - loss: 0.1346 - accuracy: 0.9200 - jacard_coef: 0.002310/17 [================>.............] - ETA: 0s - loss: 0.1344 - accuracy: 0.9215 - jacard_coef: 0.002211/17 [==================>...........] - ETA: 0s - loss: 0.1346 - accuracy: 0.9189 - jacard_coef: 0.002012/17 [====================>.........] - ETA: 0s - loss: 0.1344 - accuracy: 0.9201 - jacard_coef: 0.001813/17 [=====================>........] - ETA: 0s - loss: 0.1348 - accuracy: 0.9171 - jacard_coef: 0.001714/17 [=======================>......] - ETA: 0s - loss: 0.1346 - accuracy: 0.9180 - jacard_coef: 0.001615/17 [=========================>....] - ETA: 0s - loss: 0.1344 - accuracy: 0.9194 - jacard_coef: 0.001516/17 [===========================>..] - ETA: 0s - loss: 0.1348 - accuracy: 0.9148 - jacard_coef: 0.001417/17 [==============================] - 2s 128ms/step - loss: 0.1348 - accuracy: 0.9137 - jacard_coef: 0.0016 - val_loss: 0.1460 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1336 - accuracy: 0.9232 - jacard_coef: 3.4770e-04 2/17 [==>...........................] - ETA: 1s - loss: 0.1345 - accuracy: 0.9163 - jacard_coef: 5.3134e-04 3/17 [====>.........................] - ETA: 1s - loss: 0.1369 - accuracy: 0.8978 - jacard_coef: 3.5423e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1364 - accuracy: 0.9012 - jacard_coef: 3.2488e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9117 - jacard_coef: 0.0018     6/17 [=========>....................] - ETA: 1s - loss: 0.1361 - accuracy: 0.9083 - jacard_coef: 0.0015 7/17 [===========>..................] - ETA: 1s - loss: 0.1358 - accuracy: 0.9129 - jacard_coef: 0.0017 8/17 [=============>................] - ETA: 1s - loss: 0.1358 - accuracy: 0.9117 - jacard_coef: 0.0015 9/17 [==============>...............] - ETA: 1s - loss: 0.1356 - accuracy: 0.9131 - jacard_coef: 0.001410/17 [================>.............] - ETA: 0s - loss: 0.1351 - accuracy: 0.9172 - jacard_coef: 0.001311/17 [==================>...........] - ETA: 0s - loss: 0.1350 - accuracy: 0.9178 - jacard_coef: 0.001212/17 [====================>.........] - ETA: 0s - loss: 0.1352 - accuracy: 0.9165 - jacard_coef: 0.001213/17 [=====================>........] - ETA: 0s - loss: 0.1352 - accuracy: 0.9161 - jacard_coef: 0.001114/17 [=======================>......] - ETA: 0s - loss: 0.1356 - accuracy: 0.9131 - jacard_coef: 9.9805e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1357 - accuracy: 0.9131 - jacard_coef: 9.3151e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1353 - accuracy: 0.9162 - jacard_coef: 8.7329e-0417/17 [==============================] - 2s 128ms/step - loss: 0.1353 - accuracy: 0.9164 - jacard_coef: 8.2192e-04 - val_loss: 0.1427 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1373 - accuracy: 0.9041 - jacard_coef: 1.9894e-05 2/17 [==>...........................] - ETA: 1s - loss: 0.1363 - accuracy: 0.9069 - jacard_coef: 4.1650e-05 3/17 [====>.........................] - ETA: 1s - loss: 0.1346 - accuracy: 0.9168 - jacard_coef: 2.7767e-05 4/17 [======>.......................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9224 - jacard_coef: 3.6504e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1337 - accuracy: 0.9216 - jacard_coef: 2.9203e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9198 - jacard_coef: 2.4692e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9179 - jacard_coef: 6.4515e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1337 - accuracy: 0.9168 - jacard_coef: 5.9348e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1336 - accuracy: 0.9184 - jacard_coef: 9.2945e-0410/17 [================>.............] - ETA: 0s - loss: 0.1336 - accuracy: 0.9175 - jacard_coef: 0.0010    11/17 [==================>...........] - ETA: 0s - loss: 0.1335 - accuracy: 0.9175 - jacard_coef: 9.2787e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1334 - accuracy: 0.9185 - jacard_coef: 0.0010    13/17 [=====================>........] - ETA: 0s - loss: 0.1335 - accuracy: 0.9178 - jacard_coef: 0.001114/17 [=======================>......] - ETA: 0s - loss: 0.1332 - accuracy: 0.9199 - jacard_coef: 0.001615/17 [=========================>....] - ETA: 0s - loss: 0.1332 - accuracy: 0.9185 - jacard_coef: 0.001516/17 [===========================>..] - ETA: 0s - loss: 0.1333 - accuracy: 0.9166 - jacard_coef: 0.001417/17 [==============================] - 2s 128ms/step - loss: 0.1333 - accuracy: 0.9162 - jacard_coef: 0.0013 - val_loss: 0.1433 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1289 - accuracy: 0.9345 - jacard_coef: 2.9121e-05 2/17 [==>...........................] - ETA: 1s - loss: 0.1291 - accuracy: 0.9336 - jacard_coef: 1.4560e-05 3/17 [====>.........................] - ETA: 1s - loss: 0.1297 - accuracy: 0.9314 - jacard_coef: 1.4883e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1300 - accuracy: 0.9298 - jacard_coef: 1.6244e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1298 - accuracy: 0.9310 - jacard_coef: 1.4183e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1301 - accuracy: 0.9281 - jacard_coef: 1.1819e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1306 - accuracy: 0.9228 - jacard_coef: 1.0131e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1309 - accuracy: 0.9210 - jacard_coef: 9.1240e-05 9/17 [==============>...............] - ETA: 1s - loss: 0.1309 - accuracy: 0.9194 - jacard_coef: 2.4262e-0410/17 [================>.............] - ETA: 0s - loss: 0.1306 - accuracy: 0.9219 - jacard_coef: 3.2452e-0411/17 [==================>...........] - ETA: 0s - loss: 0.1311 - accuracy: 0.9181 - jacard_coef: 2.9935e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1308 - accuracy: 0.9208 - jacard_coef: 8.1206e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1311 - accuracy: 0.9171 - jacard_coef: 8.5150e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1310 - accuracy: 0.9176 - jacard_coef: 8.2624e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1310 - accuracy: 0.9180 - jacard_coef: 0.0013    16/17 [===========================>..] - ETA: 0s - loss: 0.1310 - accuracy: 0.9166 - jacard_coef: 0.001417/17 [==============================] - 2s 128ms/step - loss: 0.1311 - accuracy: 0.9162 - jacard_coef: 0.0014 - val_loss: 0.1397 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1288 - accuracy: 0.9209 - jacard_coef: 5.7810e-04 2/17 [==>...........................] - ETA: 1s - loss: 0.1282 - accuracy: 0.9231 - jacard_coef: 9.1464e-04 3/17 [====>.........................] - ETA: 1s - loss: 0.1290 - accuracy: 0.9205 - jacard_coef: 7.8999e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1298 - accuracy: 0.9141 - jacard_coef: 7.9617e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1294 - accuracy: 0.9155 - jacard_coef: 6.3693e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1290 - accuracy: 0.9179 - jacard_coef: 0.0015     7/17 [===========>..................] - ETA: 1s - loss: 0.1292 - accuracy: 0.9155 - jacard_coef: 0.0017 8/17 [=============>................] - ETA: 1s - loss: 0.1291 - accuracy: 0.9173 - jacard_coef: 0.0017 9/17 [==============>...............] - ETA: 1s - loss: 0.1289 - accuracy: 0.9185 - jacard_coef: 0.001610/17 [================>.............] - ETA: 0s - loss: 0.1287 - accuracy: 0.9189 - jacard_coef: 0.002511/17 [==================>...........] - ETA: 0s - loss: 0.1289 - accuracy: 0.9172 - jacard_coef: 0.002612/17 [====================>.........] - ETA: 0s - loss: 0.1289 - accuracy: 0.9167 - jacard_coef: 0.002413/17 [=====================>........] - ETA: 0s - loss: 0.1288 - accuracy: 0.9165 - jacard_coef: 0.002414/17 [=======================>......] - ETA: 0s - loss: 0.1288 - accuracy: 0.9171 - jacard_coef: 0.002415/17 [=========================>....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9148 - jacard_coef: 0.002216/17 [===========================>..] - ETA: 0s - loss: 0.1289 - accuracy: 0.9160 - jacard_coef: 0.002217/17 [==============================] - 2s 128ms/step - loss: 0.1289 - accuracy: 0.9154 - jacard_coef: 0.0031 - val_loss: 0.1364 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0698 (epoch 8)
  Final Val Loss: 0.1364
  Training Time: 0:01:24.751752
  Stability (std): 0.0086

Results saved to: hyperparameter_optimization_20250926_165036/exp_1_UNet_lr1e-4_bs8/UNet_lr0.0001_bs8_results.json

Experiment 1 completed in 102s
Progress: 1/36 completed
Estimated remaining time: 59 minutes

ðŸ”¬ EXPERIMENT 2/36
================================================
Architecture: UNet
Learning Rate: 1e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758876778.093802 1013869 service.cc:145] XLA service 0x150209b665e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758876778.093826 1013869 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758876778.230681 1013869 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 4:51 - loss: 0.3436 - accuracy: 0.4998 - jacard_coef: 0.09322/9 [=====>........................] - ETA: 47s - loss: 0.3100 - accuracy: 0.4170 - jacard_coef: 0.0872 3/9 [=========>....................] - ETA: 31s - loss: 0.2842 - accuracy: 0.3601 - jacard_coef: 0.09144/9 [============>.................] - ETA: 22s - loss: 0.2666 - accuracy: 0.3108 - jacard_coef: 0.09035/9 [===============>..............] - ETA: 14s - loss: 0.2569 - accuracy: 0.2827 - jacard_coef: 0.09066/9 [===================>..........] - ETA: 9s - loss: 0.2488 - accuracy: 0.2551 - jacard_coef: 0.0819 7/9 [======================>.......] - ETA: 5s - loss: 0.2428 - accuracy: 0.2387 - jacard_coef: 0.08118/9 [=========================>....] - ETA: 2s - loss: 0.2371 - accuracy: 0.2311 - jacard_coef: 0.08189/9 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.2308 - jacard_coef: 0.08259/9 [==============================] - 58s 3s/step - loss: 0.2375 - accuracy: 0.2308 - jacard_coef: 0.0825 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.2144 - accuracy: 0.3307 - jacard_coef: 0.09902/9 [=====>........................] - ETA: 1s - loss: 0.2103 - accuracy: 0.3041 - jacard_coef: 0.08373/9 [=========>....................] - ETA: 1s - loss: 0.2020 - accuracy: 0.3447 - jacard_coef: 0.08664/9 [============>.................] - ETA: 1s - loss: 0.1999 - accuracy: 0.3456 - jacard_coef: 0.08355/9 [===============>..............] - ETA: 0s - loss: 0.1979 - accuracy: 0.3556 - jacard_coef: 0.08236/9 [===================>..........] - ETA: 0s - loss: 0.1945 - accuracy: 0.3704 - jacard_coef: 0.08417/9 [======================>.......] - ETA: 0s - loss: 0.1925 - accuracy: 0.3695 - jacard_coef: 0.08338/9 [=========================>....] - ETA: 0s - loss: 0.1908 - accuracy: 0.3742 - jacard_coef: 0.08379/9 [==============================] - 2s 234ms/step - loss: 0.1907 - accuracy: 0.3741 - jacard_coef: 0.0913 - val_loss: 1.0667 - val_accuracy: 0.9303 - val_jacard_coef: 1.4600e-05 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1773 - accuracy: 0.4202 - jacard_coef: 0.08472/9 [=====>........................] - ETA: 1s - loss: 0.1769 - accuracy: 0.4201 - jacard_coef: 0.08353/9 [=========>....................] - ETA: 1s - loss: 0.1762 - accuracy: 0.4362 - jacard_coef: 0.08154/9 [============>.................] - ETA: 1s - loss: 0.1767 - accuracy: 0.4505 - jacard_coef: 0.07655/9 [===============>..............] - ETA: 0s - loss: 0.1763 - accuracy: 0.4514 - jacard_coef: 0.07576/9 [===================>..........] - ETA: 0s - loss: 0.1759 - accuracy: 0.4507 - jacard_coef: 0.07727/9 [======================>.......] - ETA: 0s - loss: 0.1755 - accuracy: 0.4537 - jacard_coef: 0.08058/9 [=========================>....] - ETA: 0s - loss: 0.1754 - accuracy: 0.4562 - jacard_coef: 0.07999/9 [==============================] - 2s 234ms/step - loss: 0.1756 - accuracy: 0.4557 - jacard_coef: 0.0814 - val_loss: 0.1691 - val_accuracy: 0.7269 - val_jacard_coef: 0.0561 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1764 - accuracy: 0.5201 - jacard_coef: 0.09782/9 [=====>........................] - ETA: 1s - loss: 0.1770 - accuracy: 0.5226 - jacard_coef: 0.08993/9 [=========>....................] - ETA: 1s - loss: 0.1769 - accuracy: 0.5350 - jacard_coef: 0.07324/9 [============>.................] - ETA: 1s - loss: 0.1774 - accuracy: 0.4998 - jacard_coef: 0.07715/9 [===============>..............] - ETA: 0s - loss: 0.1801 - accuracy: 0.4656 - jacard_coef: 0.07666/9 [===================>..........] - ETA: 0s - loss: 0.1794 - accuracy: 0.4617 - jacard_coef: 0.07927/9 [======================>.......] - ETA: 0s - loss: 0.1794 - accuracy: 0.4461 - jacard_coef: 0.07468/9 [=========================>....] - ETA: 0s - loss: 0.1796 - accuracy: 0.4378 - jacard_coef: 0.07709/9 [==============================] - 2s 229ms/step - loss: 0.1797 - accuracy: 0.4354 - jacard_coef: 0.0724 - val_loss: 0.2647 - val_accuracy: 0.8981 - val_jacard_coef: 0.0215 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1744 - accuracy: 0.5153 - jacard_coef: 0.06972/9 [=====>........................] - ETA: 1s - loss: 0.1738 - accuracy: 0.5214 - jacard_coef: 0.08553/9 [=========>....................] - ETA: 1s - loss: 0.1759 - accuracy: 0.4819 - jacard_coef: 0.08884/9 [============>.................] - ETA: 1s - loss: 0.1756 - accuracy: 0.4775 - jacard_coef: 0.08835/9 [===============>..............] - ETA: 0s - loss: 0.1764 - accuracy: 0.4581 - jacard_coef: 0.08946/9 [===================>..........] - ETA: 0s - loss: 0.1753 - accuracy: 0.4817 - jacard_coef: 0.08387/9 [======================>.......] - ETA: 0s - loss: 0.1747 - accuracy: 0.4956 - jacard_coef: 0.07918/9 [=========================>....] - ETA: 0s - loss: 0.1741 - accuracy: 0.5117 - jacard_coef: 0.07659/9 [==============================] - 2s 233ms/step - loss: 0.1742 - accuracy: 0.5112 - jacard_coef: 0.0687 - val_loss: 0.1689 - val_accuracy: 0.4263 - val_jacard_coef: 0.0598 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1700 - accuracy: 0.6307 - jacard_coef: 0.07692/9 [=====>........................] - ETA: 1s - loss: 0.1702 - accuracy: 0.6198 - jacard_coef: 0.08263/9 [=========>....................] - ETA: 1s - loss: 0.1711 - accuracy: 0.6207 - jacard_coef: 0.07964/9 [============>.................] - ETA: 1s - loss: 0.1706 - accuracy: 0.6121 - jacard_coef: 0.08145/9 [===============>..............] - ETA: 0s - loss: 0.1701 - accuracy: 0.6110 - jacard_coef: 0.08076/9 [===================>..........] - ETA: 0s - loss: 0.1695 - accuracy: 0.6185 - jacard_coef: 0.07887/9 [======================>.......] - ETA: 0s - loss: 0.1693 - accuracy: 0.6223 - jacard_coef: 0.07738/9 [=========================>....] - ETA: 0s - loss: 0.1690 - accuracy: 0.6166 - jacard_coef: 0.07889/9 [==============================] - 2s 229ms/step - loss: 0.1694 - accuracy: 0.6143 - jacard_coef: 0.0730 - val_loss: 0.1575 - val_accuracy: 0.8245 - val_jacard_coef: 0.0439 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1705 - accuracy: 0.5559 - jacard_coef: 0.05142/9 [=====>........................] - ETA: 1s - loss: 0.1736 - accuracy: 0.5391 - jacard_coef: 0.06383/9 [=========>....................] - ETA: 1s - loss: 0.1744 - accuracy: 0.5294 - jacard_coef: 0.06984/9 [============>.................] - ETA: 1s - loss: 0.1754 - accuracy: 0.5172 - jacard_coef: 0.06755/9 [===============>..............] - ETA: 0s - loss: 0.1752 - accuracy: 0.5157 - jacard_coef: 0.06746/9 [===================>..........] - ETA: 0s - loss: 0.1751 - accuracy: 0.5140 - jacard_coef: 0.06987/9 [======================>.......] - ETA: 0s - loss: 0.1776 - accuracy: 0.5141 - jacard_coef: 0.06958/9 [=========================>....] - ETA: 0s - loss: 0.1768 - accuracy: 0.5198 - jacard_coef: 0.07219/9 [==============================] - 2s 230ms/step - loss: 0.1768 - accuracy: 0.5198 - jacard_coef: 0.0659 - val_loss: 0.2836 - val_accuracy: 0.8879 - val_jacard_coef: 0.0269 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1751 - accuracy: 0.6742 - jacard_coef: 0.06882/9 [=====>........................] - ETA: 1s - loss: 0.1715 - accuracy: 0.6791 - jacard_coef: 0.06593/9 [=========>....................] - ETA: 1s - loss: 0.1711 - accuracy: 0.6786 - jacard_coef: 0.06714/9 [============>.................] - ETA: 1s - loss: 0.1697 - accuracy: 0.6903 - jacard_coef: 0.06395/9 [===============>..............] - ETA: 0s - loss: 0.1694 - accuracy: 0.6915 - jacard_coef: 0.06506/9 [===================>..........] - ETA: 0s - loss: 0.1685 - accuracy: 0.7044 - jacard_coef: 0.06187/9 [======================>.......] - ETA: 0s - loss: 0.1678 - accuracy: 0.7151 - jacard_coef: 0.06158/9 [=========================>....] - ETA: 0s - loss: 0.1673 - accuracy: 0.7214 - jacard_coef: 0.06189/9 [==============================] - 2s 229ms/step - loss: 0.1676 - accuracy: 0.7179 - jacard_coef: 0.0582 - val_loss: 0.1503 - val_accuracy: 0.7728 - val_jacard_coef: 0.0483 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1988 - accuracy: 0.1941 - jacard_coef: 0.08662/9 [=====>........................] - ETA: 1s - loss: 0.1845 - accuracy: 0.4708 - jacard_coef: 0.07593/9 [=========>....................] - ETA: 1s - loss: 0.1803 - accuracy: 0.5258 - jacard_coef: 0.07294/9 [============>.................] - ETA: 1s - loss: 0.1801 - accuracy: 0.5616 - jacard_coef: 0.07705/9 [===============>..............] - ETA: 0s - loss: 0.1787 - accuracy: 0.5637 - jacard_coef: 0.07356/9 [===================>..........] - ETA: 0s - loss: 0.1763 - accuracy: 0.6083 - jacard_coef: 0.06897/9 [======================>.......] - ETA: 0s - loss: 0.1743 - accuracy: 0.6414 - jacard_coef: 0.06358/9 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.6677 - jacard_coef: 0.06149/9 [==============================] - 2s 229ms/step - loss: 0.1730 - accuracy: 0.6643 - jacard_coef: 0.0552 - val_loss: 1.0589 - val_accuracy: 0.9253 - val_jacard_coef: 1.3621e-05 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1611 - accuracy: 0.8526 - jacard_coef: 0.03992/9 [=====>........................] - ETA: 1s - loss: 0.1621 - accuracy: 0.8550 - jacard_coef: 0.04583/9 [=========>....................] - ETA: 1s - loss: 0.1616 - accuracy: 0.8619 - jacard_coef: 0.04204/9 [============>.................] - ETA: 1s - loss: 0.1625 - accuracy: 0.8489 - jacard_coef: 0.04645/9 [===============>..............] - ETA: 0s - loss: 0.1623 - accuracy: 0.8485 - jacard_coef: 0.04516/9 [===================>..........] - ETA: 0s - loss: 0.1621 - accuracy: 0.8456 - jacard_coef: 0.04477/9 [======================>.......] - ETA: 0s - loss: 0.1619 - accuracy: 0.8429 - jacard_coef: 0.04648/9 [=========================>....] - ETA: 0s - loss: 0.1616 - accuracy: 0.8367 - jacard_coef: 0.04939/9 [==============================] - 2s 229ms/step - loss: 0.1617 - accuracy: 0.8352 - jacard_coef: 0.0553 - val_loss: 0.9566 - val_accuracy: 0.9189 - val_jacard_coef: 0.0043 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1596 - accuracy: 0.7655 - jacard_coef: 0.08582/9 [=====>........................] - ETA: 1s - loss: 0.1597 - accuracy: 0.7641 - jacard_coef: 0.07643/9 [=========>....................] - ETA: 1s - loss: 0.1593 - accuracy: 0.7786 - jacard_coef: 0.06524/9 [============>.................] - ETA: 1s - loss: 0.1589 - accuracy: 0.7994 - jacard_coef: 0.05825/9 [===============>..............] - ETA: 0s - loss: 0.1585 - accuracy: 0.8201 - jacard_coef: 0.04996/9 [===================>..........] - ETA: 0s - loss: 0.1583 - accuracy: 0.8356 - jacard_coef: 0.04437/9 [======================>.......] - ETA: 0s - loss: 0.1584 - accuracy: 0.8419 - jacard_coef: 0.03958/9 [=========================>....] - ETA: 0s - loss: 0.1582 - accuracy: 0.8516 - jacard_coef: 0.03609/9 [==============================] - 2s 229ms/step - loss: 0.1585 - accuracy: 0.8461 - jacard_coef: 0.0351 - val_loss: 0.2009 - val_accuracy: 0.6621 - val_jacard_coef: 0.0494 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1603 - accuracy: 0.8779 - jacard_coef: 0.02162/9 [=====>........................] - ETA: 1s - loss: 0.1596 - accuracy: 0.8839 - jacard_coef: 0.01773/9 [=========>....................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8949 - jacard_coef: 0.02034/9 [============>.................] - ETA: 1s - loss: 0.1585 - accuracy: 0.8912 - jacard_coef: 0.01965/9 [===============>..............] - ETA: 0s - loss: 0.1584 - accuracy: 0.8942 - jacard_coef: 0.02216/9 [===================>..........] - ETA: 0s - loss: 0.1584 - accuracy: 0.8902 - jacard_coef: 0.02127/9 [======================>.......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8928 - jacard_coef: 0.02098/9 [=========================>....] - ETA: 0s - loss: 0.1594 - accuracy: 0.8729 - jacard_coef: 0.02709/9 [==============================] - 2s 229ms/step - loss: 0.1595 - accuracy: 0.8719 - jacard_coef: 0.0300 - val_loss: 0.1691 - val_accuracy: 0.8771 - val_jacard_coef: 0.0327 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1580 - accuracy: 0.8959 - jacard_coef: 0.01222/9 [=====>........................] - ETA: 1s - loss: 0.1588 - accuracy: 0.8863 - jacard_coef: 0.02283/9 [=========>....................] - ETA: 1s - loss: 0.1589 - accuracy: 0.8900 - jacard_coef: 0.01954/9 [============>.................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8926 - jacard_coef: 0.02195/9 [===============>..............] - ETA: 0s - loss: 0.1580 - accuracy: 0.8923 - jacard_coef: 0.02086/9 [===================>..........] - ETA: 0s - loss: 0.1583 - accuracy: 0.8838 - jacard_coef: 0.02437/9 [======================>.......] - ETA: 0s - loss: 0.1579 - accuracy: 0.8872 - jacard_coef: 0.02508/9 [=========================>....] - ETA: 0s - loss: 0.1586 - accuracy: 0.8880 - jacard_coef: 0.02589/9 [==============================] - 2s 234ms/step - loss: 0.1586 - accuracy: 0.8860 - jacard_coef: 0.0299 - val_loss: 0.1686 - val_accuracy: 0.7624 - val_jacard_coef: 0.0685 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1551 - accuracy: 0.8802 - jacard_coef: 0.02592/9 [=====>........................] - ETA: 1s - loss: 0.1554 - accuracy: 0.8963 - jacard_coef: 0.02973/9 [=========>....................] - ETA: 1s - loss: 0.1552 - accuracy: 0.8952 - jacard_coef: 0.02964/9 [============>.................] - ETA: 1s - loss: 0.1554 - accuracy: 0.8901 - jacard_coef: 0.02595/9 [===============>..............] - ETA: 0s - loss: 0.1558 - accuracy: 0.8908 - jacard_coef: 0.02416/9 [===================>..........] - ETA: 0s - loss: 0.1558 - accuracy: 0.8901 - jacard_coef: 0.02047/9 [======================>.......] - ETA: 0s - loss: 0.1553 - accuracy: 0.8984 - jacard_coef: 0.01778/9 [=========================>....] - ETA: 0s - loss: 0.1563 - accuracy: 0.8958 - jacard_coef: 0.02019/9 [==============================] - 2s 233ms/step - loss: 0.1564 - accuracy: 0.8943 - jacard_coef: 0.0301 - val_loss: 0.1685 - val_accuracy: 0.6855 - val_jacard_coef: 0.0763 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1545 - accuracy: 0.8946 - jacard_coef: 0.01442/9 [=====>........................] - ETA: 1s - loss: 0.1537 - accuracy: 0.9027 - jacard_coef: 0.01453/9 [=========>....................] - ETA: 1s - loss: 0.1539 - accuracy: 0.8975 - jacard_coef: 0.02444/9 [============>.................] - ETA: 1s - loss: 0.1544 - accuracy: 0.8816 - jacard_coef: 0.02975/9 [===============>..............] - ETA: 0s - loss: 0.1551 - accuracy: 0.8789 - jacard_coef: 0.03286/9 [===================>..........] - ETA: 0s - loss: 0.1548 - accuracy: 0.8850 - jacard_coef: 0.03017/9 [======================>.......] - ETA: 0s - loss: 0.1546 - accuracy: 0.8862 - jacard_coef: 0.02668/9 [=========================>....] - ETA: 0s - loss: 0.1542 - accuracy: 0.8911 - jacard_coef: 0.02489/9 [==============================] - 2s 229ms/step - loss: 0.1544 - accuracy: 0.8876 - jacard_coef: 0.0377 - val_loss: 0.1680 - val_accuracy: 0.6655 - val_jacard_coef: 0.0740 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1518 - accuracy: 0.9191 - jacard_coef: 0.02302/9 [=====>........................] - ETA: 1s - loss: 0.1526 - accuracy: 0.8807 - jacard_coef: 0.03123/9 [=========>....................] - ETA: 1s - loss: 0.1526 - accuracy: 0.8654 - jacard_coef: 0.03504/9 [============>.................] - ETA: 1s - loss: 0.1530 - accuracy: 0.8510 - jacard_coef: 0.04185/9 [===============>..............] - ETA: 0s - loss: 0.1533 - accuracy: 0.8476 - jacard_coef: 0.04706/9 [===================>..........] - ETA: 0s - loss: 0.1536 - accuracy: 0.8426 - jacard_coef: 0.04507/9 [======================>.......] - ETA: 0s - loss: 0.1534 - accuracy: 0.8449 - jacard_coef: 0.04478/9 [=========================>....] - ETA: 0s - loss: 0.1537 - accuracy: 0.8443 - jacard_coef: 0.04479/9 [==============================] - 2s 229ms/step - loss: 0.1538 - accuracy: 0.8419 - jacard_coef: 0.0545 - val_loss: 0.1586 - val_accuracy: 0.7735 - val_jacard_coef: 0.0505 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1539 - accuracy: 0.8734 - jacard_coef: 0.01652/9 [=====>........................] - ETA: 1s - loss: 0.1542 - accuracy: 0.8899 - jacard_coef: 0.01883/9 [=========>....................] - ETA: 1s - loss: 0.1544 - accuracy: 0.8894 - jacard_coef: 0.01914/9 [============>.................] - ETA: 1s - loss: 0.1542 - accuracy: 0.8926 - jacard_coef: 0.01815/9 [===============>..............] - ETA: 0s - loss: 0.1541 - accuracy: 0.8914 - jacard_coef: 0.01656/9 [===================>..........] - ETA: 0s - loss: 0.1540 - accuracy: 0.8912 - jacard_coef: 0.01497/9 [======================>.......] - ETA: 0s - loss: 0.1535 - accuracy: 0.8989 - jacard_coef: 0.01358/9 [=========================>....] - ETA: 0s - loss: 0.1541 - accuracy: 0.9000 - jacard_coef: 0.01229/9 [==============================] - 2s 229ms/step - loss: 0.1542 - accuracy: 0.8984 - jacard_coef: 0.0218 - val_loss: 0.1478 - val_accuracy: 0.8905 - val_jacard_coef: 0.0429 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1517 - accuracy: 0.9227 - jacard_coef: 0.00302/9 [=====>........................] - ETA: 1s - loss: 0.1517 - accuracy: 0.9246 - jacard_coef: 0.00183/9 [=========>....................] - ETA: 1s - loss: 0.1523 - accuracy: 0.9148 - jacard_coef: 0.00254/9 [============>.................] - ETA: 1s - loss: 0.1521 - accuracy: 0.9131 - jacard_coef: 0.00295/9 [===============>..............] - ETA: 0s - loss: 0.1518 - accuracy: 0.9148 - jacard_coef: 0.00286/9 [===================>..........] - ETA: 0s - loss: 0.1515 - accuracy: 0.9152 - jacard_coef: 0.00297/9 [======================>.......] - ETA: 0s - loss: 0.1518 - accuracy: 0.9149 - jacard_coef: 0.00348/9 [=========================>....] - ETA: 0s - loss: 0.1518 - accuracy: 0.9126 - jacard_coef: 0.00369/9 [==============================] - 2s 229ms/step - loss: 0.1518 - accuracy: 0.9110 - jacard_coef: 0.0075 - val_loss: 0.1477 - val_accuracy: 0.9114 - val_jacard_coef: 0.0089 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1489 - accuracy: 0.9360 - jacard_coef: 9.0835e-042/9 [=====>........................] - ETA: 1s - loss: 0.1500 - accuracy: 0.9334 - jacard_coef: 0.0048    3/9 [=========>....................] - ETA: 1s - loss: 0.1505 - accuracy: 0.9262 - jacard_coef: 0.00384/9 [============>.................] - ETA: 1s - loss: 0.1507 - accuracy: 0.9222 - jacard_coef: 0.00335/9 [===============>..............] - ETA: 0s - loss: 0.1509 - accuracy: 0.9168 - jacard_coef: 0.00276/9 [===================>..........] - ETA: 0s - loss: 0.1510 - accuracy: 0.9129 - jacard_coef: 0.00277/9 [======================>.......] - ETA: 0s - loss: 0.1506 - accuracy: 0.9150 - jacard_coef: 0.00268/9 [=========================>....] - ETA: 0s - loss: 0.1505 - accuracy: 0.9153 - jacard_coef: 0.00239/9 [==============================] - 2s 229ms/step - loss: 0.1506 - accuracy: 0.9135 - jacard_coef: 0.0188 - val_loss: 0.1666 - val_accuracy: 0.8533 - val_jacard_coef: 0.0273 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1567 - accuracy: 0.8701 - jacard_coef: 0.03052/9 [=====>........................] - ETA: 1s - loss: 0.1526 - accuracy: 0.8946 - jacard_coef: 0.01523/9 [=========>....................] - ETA: 1s - loss: 0.1510 - accuracy: 0.9064 - jacard_coef: 0.01054/9 [============>.................] - ETA: 1s - loss: 0.1510 - accuracy: 0.9066 - jacard_coef: 0.00815/9 [===============>..............] - ETA: 0s - loss: 0.1503 - accuracy: 0.9127 - jacard_coef: 0.00666/9 [===================>..........] - ETA: 0s - loss: 0.1503 - accuracy: 0.9128 - jacard_coef: 0.00557/9 [======================>.......] - ETA: 0s - loss: 0.1508 - accuracy: 0.9116 - jacard_coef: 0.00658/9 [=========================>....] - ETA: 0s - loss: 0.1507 - accuracy: 0.9101 - jacard_coef: 0.00589/9 [==============================] - 2s 229ms/step - loss: 0.1508 - accuracy: 0.9090 - jacard_coef: 0.0088 - val_loss: 0.1682 - val_accuracy: 0.8102 - val_jacard_coef: 0.0364 - lr: 2.5000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1493 - accuracy: 0.9294 - jacard_coef: 0.00182/9 [=====>........................] - ETA: 1s - loss: 0.1495 - accuracy: 0.9149 - jacard_coef: 0.00243/9 [=========>....................] - ETA: 1s - loss: 0.1484 - accuracy: 0.9240 - jacard_coef: 0.00264/9 [============>.................] - ETA: 1s - loss: 0.1481 - accuracy: 0.9239 - jacard_coef: 0.00255/9 [===============>..............] - ETA: 0s - loss: 0.1481 - accuracy: 0.9220 - jacard_coef: 0.00226/9 [===================>..........] - ETA: 0s - loss: 0.1490 - accuracy: 0.9201 - jacard_coef: 0.00407/9 [======================>.......] - ETA: 0s - loss: 0.1491 - accuracy: 0.9164 - jacard_coef: 0.00498/9 [=========================>....] - ETA: 0s - loss: 0.1491 - accuracy: 0.9145 - jacard_coef: 0.00449/9 [==============================] - 2s 229ms/step - loss: 0.1492 - accuracy: 0.9133 - jacard_coef: 0.0092 - val_loss: 0.1665 - val_accuracy: 0.8556 - val_jacard_coef: 0.0317 - lr: 2.5000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1489 - accuracy: 0.8997 - jacard_coef: 0.00212/9 [=====>........................] - ETA: 1s - loss: 0.1478 - accuracy: 0.9164 - jacard_coef: 0.00613/9 [=========>....................] - ETA: 1s - loss: 0.1482 - accuracy: 0.9120 - jacard_coef: 0.00594/9 [============>.................] - ETA: 1s - loss: 0.1482 - accuracy: 0.9124 - jacard_coef: 0.00555/9 [===============>..............] - ETA: 0s - loss: 0.1479 - accuracy: 0.9153 - jacard_coef: 0.00456/9 [===================>..........] - ETA: 0s - loss: 0.1477 - accuracy: 0.9169 - jacard_coef: 0.00377/9 [======================>.......] - ETA: 0s - loss: 0.1477 - accuracy: 0.9180 - jacard_coef: 0.00328/9 [=========================>....] - ETA: 0s - loss: 0.1483 - accuracy: 0.9149 - jacard_coef: 0.00389/9 [==============================] - 2s 229ms/step - loss: 0.1484 - accuracy: 0.9129 - jacard_coef: 0.0155 - val_loss: 0.1653 - val_accuracy: 0.9200 - val_jacard_coef: 0.0152 - lr: 2.5000e-04
Epoch 23/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1542 - accuracy: 0.8787 - jacard_coef: 0.00922/9 [=====>........................] - ETA: 1s - loss: 0.1507 - accuracy: 0.8922 - jacard_coef: 0.00523/9 [=========>....................] - ETA: 1s - loss: 0.1497 - accuracy: 0.8997 - jacard_coef: 0.00574/9 [============>.................] - ETA: 1s - loss: 0.1491 - accuracy: 0.9008 - jacard_coef: 0.00515/9 [===============>..............] - ETA: 0s - loss: 0.1483 - accuracy: 0.9076 - jacard_coef: 0.00546/9 [===================>..........] - ETA: 0s - loss: 0.1478 - accuracy: 0.9115 - jacard_coef: 0.00547/9 [======================>.......] - ETA: 0s - loss: 0.1478 - accuracy: 0.9101 - jacard_coef: 0.00518/9 [=========================>....] - ETA: 0s - loss: 0.1474 - accuracy: 0.9133 - jacard_coef: 0.00509/9 [==============================] - 2s 229ms/step - loss: 0.1475 - accuracy: 0.9126 - jacard_coef: 0.0082 - val_loss: 0.1644 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 2.5000e-04
Epoch 24/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1477 - accuracy: 0.9023 - jacard_coef: 3.0240e-042/9 [=====>........................] - ETA: 1s - loss: 0.1494 - accuracy: 0.9115 - jacard_coef: 0.0073    3/9 [=========>....................] - ETA: 1s - loss: 0.1491 - accuracy: 0.9053 - jacard_coef: 0.00564/9 [============>.................] - ETA: 1s - loss: 0.1487 - accuracy: 0.9063 - jacard_coef: 0.00475/9 [===============>..............] - ETA: 0s - loss: 0.1478 - accuracy: 0.9139 - jacard_coef: 0.00436/9 [===================>..........] - ETA: 0s - loss: 0.1475 - accuracy: 0.9155 - jacard_coef: 0.00367/9 [======================>.......] - ETA: 0s - loss: 0.1473 - accuracy: 0.9158 - jacard_coef: 0.00468/9 [=========================>....] - ETA: 0s - loss: 0.1471 - accuracy: 0.9150 - jacard_coef: 0.00419/9 [==============================] - 2s 229ms/step - loss: 0.1472 - accuracy: 0.9143 - jacard_coef: 0.0091 - val_loss: 0.1662 - val_accuracy: 0.9138 - val_jacard_coef: 0.0463 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0763 (epoch 14)
  Final Val Loss: 0.1662
  Training Time: 0:01:46.401900
  Stability (std): 0.0075

Results saved to: hyperparameter_optimization_20250926_165036/exp_2_UNet_lr1e-4_bs16/UNet_lr0.0001_bs16_results.json

Experiment 2 completed in 121s
Progress: 2/36 completed
Estimated remaining time: 68 minutes

ðŸ”¬ EXPERIMENT 3/36
================================================
Architecture: UNet
Learning Rate: 1e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758876903.776010 1024132 service.cc:145] XLA service 0x14ee85c5e650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758876903.776036 1024132 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758876903.913663 1024132 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 2:59 - loss: 0.3303 - accuracy: 0.4777 - jacard_coef: 0.07152/5 [===========>..................] - ETA: 40s - loss: 0.3016 - accuracy: 0.3680 - jacard_coef: 0.0737 3/5 [=================>............] - ETA: 17s - loss: 0.2765 - accuracy: 0.3030 - jacard_coef: 0.07944/5 [=======================>......] - ETA: 6s - loss: 0.2658 - accuracy: 0.2651 - jacard_coef: 0.0808 5/5 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.2646 - jacard_coef: 0.09155/5 [==============================] - 72s 7s/step - loss: 0.2655 - accuracy: 0.2646 - jacard_coef: 0.0915 - val_loss: 1.1241 - val_accuracy: 0.9300 - val_jacard_coef: 1.4542e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2024 - accuracy: 0.1862 - jacard_coef: 0.08382/5 [===========>..................] - ETA: 1s - loss: 0.2004 - accuracy: 0.1805 - jacard_coef: 0.08863/5 [=================>............] - ETA: 0s - loss: 0.2000 - accuracy: 0.1723 - jacard_coef: 0.08374/5 [=======================>......] - ETA: 0s - loss: 0.1986 - accuracy: 0.1699 - jacard_coef: 0.08335/5 [==============================] - 2s 402ms/step - loss: 0.1985 - accuracy: 0.1694 - jacard_coef: 0.0677 - val_loss: 1.1238 - val_accuracy: 0.9303 - val_jacard_coef: 1.4590e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1893 - accuracy: 0.1702 - jacard_coef: 0.10312/5 [===========>..................] - ETA: 1s - loss: 0.1888 - accuracy: 0.1634 - jacard_coef: 0.09063/5 [=================>............] - ETA: 0s - loss: 0.1886 - accuracy: 0.1704 - jacard_coef: 0.08804/5 [=======================>......] - ETA: 0s - loss: 0.1886 - accuracy: 0.1748 - jacard_coef: 0.08445/5 [==============================] - 2s 396ms/step - loss: 0.1887 - accuracy: 0.1743 - jacard_coef: 0.0684 - val_loss: 14.2244 - val_accuracy: 0.1170 - val_jacard_coef: 0.0718 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1868 - accuracy: 0.1727 - jacard_coef: 0.08132/5 [===========>..................] - ETA: 1s - loss: 0.1858 - accuracy: 0.1790 - jacard_coef: 0.08923/5 [=================>............] - ETA: 0s - loss: 0.1858 - accuracy: 0.1758 - jacard_coef: 0.08674/5 [=======================>......] - ETA: 0s - loss: 0.1855 - accuracy: 0.1719 - jacard_coef: 0.08385/5 [==============================] - 2s 387ms/step - loss: 0.1856 - accuracy: 0.1714 - jacard_coef: 0.0678 - val_loss: 2.1616 - val_accuracy: 0.8651 - val_jacard_coef: 0.0351 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1839 - accuracy: 0.1993 - jacard_coef: 0.07572/5 [===========>..................] - ETA: 1s - loss: 0.1835 - accuracy: 0.2123 - jacard_coef: 0.07483/5 [=================>............] - ETA: 0s - loss: 0.1827 - accuracy: 0.2232 - jacard_coef: 0.08224/5 [=======================>......] - ETA: 0s - loss: 0.1822 - accuracy: 0.2266 - jacard_coef: 0.08305/5 [==============================] - 2s 387ms/step - loss: 0.1822 - accuracy: 0.2264 - jacard_coef: 0.0758 - val_loss: 14.2862 - val_accuracy: 0.1121 - val_jacard_coef: 0.0708 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1787 - accuracy: 0.2523 - jacard_coef: 0.09442/5 [===========>..................] - ETA: 1s - loss: 0.1793 - accuracy: 0.2513 - jacard_coef: 0.08383/5 [=================>............] - ETA: 0s - loss: 0.1786 - accuracy: 0.2679 - jacard_coef: 0.08474/5 [=======================>......] - ETA: 0s - loss: 0.1781 - accuracy: 0.2905 - jacard_coef: 0.08295/5 [==============================] - 2s 387ms/step - loss: 0.1780 - accuracy: 0.2913 - jacard_coef: 0.0932 - val_loss: 14.8746 - val_accuracy: 0.0771 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1769 - accuracy: 0.5074 - jacard_coef: 0.05872/5 [===========>..................] - ETA: 1s - loss: 0.1757 - accuracy: 0.5294 - jacard_coef: 0.07473/5 [=================>............] - ETA: 0s - loss: 0.1754 - accuracy: 0.5343 - jacard_coef: 0.07854/5 [=======================>......] - ETA: 0s - loss: 0.1749 - accuracy: 0.5305 - jacard_coef: 0.07835/5 [==============================] - 2s 387ms/step - loss: 0.1749 - accuracy: 0.5288 - jacard_coef: 0.0644 - val_loss: 14.8157 - val_accuracy: 0.0772 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1730 - accuracy: 0.6327 - jacard_coef: 0.06972/5 [===========>..................] - ETA: 1s - loss: 0.1737 - accuracy: 0.6507 - jacard_coef: 0.07493/5 [=================>............] - ETA: 0s - loss: 0.1737 - accuracy: 0.6521 - jacard_coef: 0.06974/5 [=======================>......] - ETA: 0s - loss: 0.1735 - accuracy: 0.6491 - jacard_coef: 0.06795/5 [==============================] - 2s 387ms/step - loss: 0.1735 - accuracy: 0.6487 - jacard_coef: 0.0660 - val_loss: 1.0894 - val_accuracy: 0.8717 - val_jacard_coef: 0.0314 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1724 - accuracy: 0.5987 - jacard_coef: 0.06402/5 [===========>..................] - ETA: 1s - loss: 0.1726 - accuracy: 0.5777 - jacard_coef: 0.07083/5 [=================>............] - ETA: 0s - loss: 0.1726 - accuracy: 0.5629 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 0s - loss: 0.1724 - accuracy: 0.5622 - jacard_coef: 0.07485/5 [==============================] - 2s 386ms/step - loss: 0.1724 - accuracy: 0.5615 - jacard_coef: 0.0890 - val_loss: 1.1237 - val_accuracy: 0.9211 - val_jacard_coef: 0.0035 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1715 - accuracy: 0.5465 - jacard_coef: 0.08372/5 [===========>..................] - ETA: 1s - loss: 0.1718 - accuracy: 0.5407 - jacard_coef: 0.07553/5 [=================>............] - ETA: 0s - loss: 0.1714 - accuracy: 0.5608 - jacard_coef: 0.07934/5 [=======================>......] - ETA: 0s - loss: 0.1709 - accuracy: 0.5788 - jacard_coef: 0.07985/5 [==============================] - 2s 387ms/step - loss: 0.1709 - accuracy: 0.5786 - jacard_coef: 0.0689 - val_loss: 0.7223 - val_accuracy: 0.9084 - val_jacard_coef: 0.0081 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1702 - accuracy: 0.7322 - jacard_coef: 0.07072/5 [===========>..................] - ETA: 1s - loss: 0.1694 - accuracy: 0.7433 - jacard_coef: 0.06563/5 [=================>............] - ETA: 0s - loss: 0.1691 - accuracy: 0.7566 - jacard_coef: 0.06124/5 [=======================>......] - ETA: 0s - loss: 0.1691 - accuracy: 0.7603 - jacard_coef: 0.06435/5 [==============================] - 2s 387ms/step - loss: 0.1692 - accuracy: 0.7582 - jacard_coef: 0.0618 - val_loss: 0.2075 - val_accuracy: 0.7679 - val_jacard_coef: 0.0657 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1692 - accuracy: 0.6612 - jacard_coef: 0.06302/5 [===========>..................] - ETA: 1s - loss: 0.1693 - accuracy: 0.6230 - jacard_coef: 0.07793/5 [=================>............] - ETA: 0s - loss: 0.1692 - accuracy: 0.5886 - jacard_coef: 0.07874/5 [=======================>......] - ETA: 0s - loss: 0.1695 - accuracy: 0.5630 - jacard_coef: 0.07945/5 [==============================] - 2s 387ms/step - loss: 0.1696 - accuracy: 0.5616 - jacard_coef: 0.0899 - val_loss: 0.1623 - val_accuracy: 0.8518 - val_jacard_coef: 0.0227 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1698 - accuracy: 0.4679 - jacard_coef: 0.06612/5 [===========>..................] - ETA: 1s - loss: 0.1697 - accuracy: 0.4926 - jacard_coef: 0.07043/5 [=================>............] - ETA: 0s - loss: 0.1696 - accuracy: 0.5220 - jacard_coef: 0.07624/5 [=======================>......] - ETA: 0s - loss: 0.1696 - accuracy: 0.5749 - jacard_coef: 0.07405/5 [==============================] - 2s 387ms/step - loss: 0.1696 - accuracy: 0.5753 - jacard_coef: 0.0857 - val_loss: 0.1739 - val_accuracy: 0.4658 - val_jacard_coef: 0.0527 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0718 (epoch 3)
  Final Val Loss: 0.1739
  Training Time: 0:01:36.714054
  Stability (std): 6.3750

Results saved to: hyperparameter_optimization_20250926_165036/exp_3_UNet_lr1e-4_bs32/UNet_lr0.0001_bs32_results.json

Experiment 3 completed in 112s
Progress: 3/36 completed
Estimated remaining time: 61 minutes

ðŸ”¬ EXPERIMENT 4/36
================================================
Architecture: UNet
Learning Rate: 5e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877007.910028 1035153 service.cc:145] XLA service 0x14edb1b63db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877007.910053 1035153 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877008.047200 1035153 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 8:38 - loss: 0.3458 - accuracy: 0.5009 - jacard_coef: 0.0837 2/17 [==>...........................] - ETA: 53s - loss: 0.3285 - accuracy: 0.4808 - jacard_coef: 0.0967  3/17 [====>.........................] - ETA: 33s - loss: 0.2991 - accuracy: 0.4096 - jacard_coef: 0.0950 4/17 [======>.......................] - ETA: 25s - loss: 0.2880 - accuracy: 0.3600 - jacard_coef: 0.0912 5/17 [=======>......................] - ETA: 18s - loss: 0.2729 - accuracy: 0.3197 - jacard_coef: 0.0901 6/17 [=========>....................] - ETA: 14s - loss: 0.2600 - accuracy: 0.2981 - jacard_coef: 0.0849 7/17 [===========>..................] - ETA: 11s - loss: 0.2516 - accuracy: 0.2898 - jacard_coef: 0.0847 8/17 [=============>................] - ETA: 8s - loss: 0.2450 - accuracy: 0.2785 - jacard_coef: 0.0837  9/17 [==============>...............] - ETA: 7s - loss: 0.2387 - accuracy: 0.2720 - jacard_coef: 0.084610/17 [================>.............] - ETA: 5s - loss: 0.2339 - accuracy: 0.2670 - jacard_coef: 0.084811/17 [==================>...........] - ETA: 4s - loss: 0.2304 - accuracy: 0.2571 - jacard_coef: 0.083712/17 [====================>.........] - ETA: 3s - loss: 0.2302 - accuracy: 0.2502 - jacard_coef: 0.082713/17 [=====================>........] - ETA: 2s - loss: 0.2273 - accuracy: 0.2467 - jacard_coef: 0.081214/17 [=======================>......] - ETA: 1s - loss: 0.2246 - accuracy: 0.2448 - jacard_coef: 0.082715/17 [=========================>....] - ETA: 1s - loss: 0.2221 - accuracy: 0.2447 - jacard_coef: 0.081116/17 [===========================>..] - ETA: 0s - loss: 0.2199 - accuracy: 0.2432 - jacard_coef: 0.080517/17 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.2438 - jacard_coef: 0.088417/17 [==============================] - 46s 867ms/step - loss: 0.2196 - accuracy: 0.2438 - jacard_coef: 0.0884 - val_loss: 0.3903 - val_accuracy: 0.9296 - val_jacard_coef: 1.4291e-04 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1862 - accuracy: 0.2270 - jacard_coef: 0.1233 2/17 [==>...........................] - ETA: 1s - loss: 0.1859 - accuracy: 0.2071 - jacard_coef: 0.0922 3/17 [====>.........................] - ETA: 1s - loss: 0.1850 - accuracy: 0.2209 - jacard_coef: 0.0772 4/17 [======>.......................] - ETA: 1s - loss: 0.1884 - accuracy: 0.2314 - jacard_coef: 0.0758 5/17 [=======>......................] - ETA: 1s - loss: 0.1873 - accuracy: 0.2348 - jacard_coef: 0.0769 6/17 [=========>....................] - ETA: 1s - loss: 0.1870 - accuracy: 0.2509 - jacard_coef: 0.0786 7/17 [===========>..................] - ETA: 1s - loss: 0.1858 - accuracy: 0.2671 - jacard_coef: 0.0805 8/17 [=============>................] - ETA: 1s - loss: 0.1852 - accuracy: 0.2702 - jacard_coef: 0.0835 9/17 [==============>...............] - ETA: 1s - loss: 0.1844 - accuracy: 0.2742 - jacard_coef: 0.079610/17 [================>.............] - ETA: 0s - loss: 0.1836 - accuracy: 0.2847 - jacard_coef: 0.081111/17 [==================>...........] - ETA: 0s - loss: 0.1830 - accuracy: 0.2914 - jacard_coef: 0.083112/17 [====================>.........] - ETA: 0s - loss: 0.1823 - accuracy: 0.3024 - jacard_coef: 0.082813/17 [=====================>........] - ETA: 0s - loss: 0.1817 - accuracy: 0.3148 - jacard_coef: 0.080014/17 [=======================>......] - ETA: 0s - loss: 0.1829 - accuracy: 0.3208 - jacard_coef: 0.079815/17 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.3335 - jacard_coef: 0.077516/17 [===========================>..] - ETA: 0s - loss: 0.1830 - accuracy: 0.3452 - jacard_coef: 0.077517/17 [==============================] - 2s 131ms/step - loss: 0.1831 - accuracy: 0.3440 - jacard_coef: 0.0821 - val_loss: 0.6800 - val_accuracy: 0.7966 - val_jacard_coef: 0.0330 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1834 - accuracy: 0.2565 - jacard_coef: 0.1192 2/17 [==>...........................] - ETA: 1s - loss: 0.1848 - accuracy: 0.2017 - jacard_coef: 0.1078 3/17 [====>.........................] - ETA: 1s - loss: 0.1849 - accuracy: 0.1971 - jacard_coef: 0.0999 4/17 [======>.......................] - ETA: 1s - loss: 0.1841 - accuracy: 0.1924 - jacard_coef: 0.0856 5/17 [=======>......................] - ETA: 1s - loss: 0.1847 - accuracy: 0.1883 - jacard_coef: 0.0852 6/17 [=========>....................] - ETA: 1s - loss: 0.1837 - accuracy: 0.1976 - jacard_coef: 0.0816 7/17 [===========>..................] - ETA: 1s - loss: 0.1850 - accuracy: 0.1959 - jacard_coef: 0.0810 8/17 [=============>................] - ETA: 1s - loss: 0.1845 - accuracy: 0.2052 - jacard_coef: 0.0862 9/17 [==============>...............] - ETA: 1s - loss: 0.1839 - accuracy: 0.2212 - jacard_coef: 0.084510/17 [================>.............] - ETA: 0s - loss: 0.1834 - accuracy: 0.2387 - jacard_coef: 0.084111/17 [==================>...........] - ETA: 0s - loss: 0.1830 - accuracy: 0.2499 - jacard_coef: 0.081812/17 [====================>.........] - ETA: 0s - loss: 0.1821 - accuracy: 0.2699 - jacard_coef: 0.079213/17 [=====================>........] - ETA: 0s - loss: 0.1816 - accuracy: 0.2807 - jacard_coef: 0.078514/17 [=======================>......] - ETA: 0s - loss: 0.1808 - accuracy: 0.3004 - jacard_coef: 0.076915/17 [=========================>....] - ETA: 0s - loss: 0.1801 - accuracy: 0.3154 - jacard_coef: 0.078016/17 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.3292 - jacard_coef: 0.079617/17 [==============================] - 2s 128ms/step - loss: 0.1799 - accuracy: 0.3290 - jacard_coef: 0.0777 - val_loss: 1.0501 - val_accuracy: 0.9296 - val_jacard_coef: 1.8871e-04 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1908 - accuracy: 0.3609 - jacard_coef: 0.0970 2/17 [==>...........................] - ETA: 1s - loss: 0.1861 - accuracy: 0.3005 - jacard_coef: 0.0776 3/17 [====>.........................] - ETA: 1s - loss: 0.1856 - accuracy: 0.2669 - jacard_coef: 0.0700 4/17 [======>.......................] - ETA: 1s - loss: 0.1837 - accuracy: 0.2676 - jacard_coef: 0.0649 5/17 [=======>......................] - ETA: 1s - loss: 0.1823 - accuracy: 0.2830 - jacard_coef: 0.0732 6/17 [=========>....................] - ETA: 1s - loss: 0.1815 - accuracy: 0.3015 - jacard_coef: 0.0797 7/17 [===========>..................] - ETA: 1s - loss: 0.1810 - accuracy: 0.3214 - jacard_coef: 0.0752 8/17 [=============>................] - ETA: 1s - loss: 0.1811 - accuracy: 0.3373 - jacard_coef: 0.0778 9/17 [==============>...............] - ETA: 1s - loss: 0.1805 - accuracy: 0.3525 - jacard_coef: 0.078910/17 [================>.............] - ETA: 0s - loss: 0.1803 - accuracy: 0.3549 - jacard_coef: 0.082211/17 [==================>...........] - ETA: 0s - loss: 0.1820 - accuracy: 0.3577 - jacard_coef: 0.081612/17 [====================>.........] - ETA: 0s - loss: 0.1813 - accuracy: 0.3644 - jacard_coef: 0.078313/17 [=====================>........] - ETA: 0s - loss: 0.1819 - accuracy: 0.3664 - jacard_coef: 0.076514/17 [=======================>......] - ETA: 0s - loss: 0.1813 - accuracy: 0.3727 - jacard_coef: 0.075915/17 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.3818 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1807 - accuracy: 0.3904 - jacard_coef: 0.078517/17 [==============================] - 2s 128ms/step - loss: 0.1807 - accuracy: 0.3911 - jacard_coef: 0.0763 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4108e-12 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4328 - jacard_coef: 0.0601 2/17 [==>...........................] - ETA: 1s - loss: 0.1767 - accuracy: 0.4243 - jacard_coef: 0.0995 3/17 [====>.........................] - ETA: 1s - loss: 0.1766 - accuracy: 0.4116 - jacard_coef: 0.0971 4/17 [======>.......................] - ETA: 1s - loss: 0.1824 - accuracy: 0.3842 - jacard_coef: 0.0874 5/17 [=======>......................] - ETA: 1s - loss: 0.1811 - accuracy: 0.3735 - jacard_coef: 0.0791 6/17 [=========>....................] - ETA: 1s - loss: 0.1800 - accuracy: 0.3730 - jacard_coef: 0.0796 7/17 [===========>..................] - ETA: 1s - loss: 0.1802 - accuracy: 0.3690 - jacard_coef: 0.0808 8/17 [=============>................] - ETA: 1s - loss: 0.1796 - accuracy: 0.3730 - jacard_coef: 0.0795 9/17 [==============>...............] - ETA: 1s - loss: 0.1803 - accuracy: 0.3597 - jacard_coef: 0.079610/17 [================>.............] - ETA: 0s - loss: 0.1802 - accuracy: 0.3698 - jacard_coef: 0.078911/17 [==================>...........] - ETA: 0s - loss: 0.1814 - accuracy: 0.3615 - jacard_coef: 0.077512/17 [====================>.........] - ETA: 0s - loss: 0.1804 - accuracy: 0.3844 - jacard_coef: 0.078213/17 [=====================>........] - ETA: 0s - loss: 0.1799 - accuracy: 0.3807 - jacard_coef: 0.077114/17 [=======================>......] - ETA: 0s - loss: 0.1808 - accuracy: 0.3780 - jacard_coef: 0.077815/17 [=========================>....] - ETA: 0s - loss: 0.1799 - accuracy: 0.3963 - jacard_coef: 0.076716/17 [===========================>..] - ETA: 0s - loss: 0.1792 - accuracy: 0.4045 - jacard_coef: 0.076717/17 [==============================] - 2s 128ms/step - loss: 0.1792 - accuracy: 0.4052 - jacard_coef: 0.0778 - val_loss: 0.4334 - val_accuracy: 0.9213 - val_jacard_coef: 0.0039 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1940 - accuracy: 0.5843 - jacard_coef: 0.0828 2/17 [==>...........................] - ETA: 1s - loss: 0.1810 - accuracy: 0.6363 - jacard_coef: 0.0831 3/17 [====>.........................] - ETA: 1s - loss: 0.1760 - accuracy: 0.6692 - jacard_coef: 0.0786 4/17 [======>.......................] - ETA: 1s - loss: 0.1733 - accuracy: 0.6801 - jacard_coef: 0.0743 5/17 [=======>......................] - ETA: 1s - loss: 0.1724 - accuracy: 0.6812 - jacard_coef: 0.0739 6/17 [=========>....................] - ETA: 1s - loss: 0.1743 - accuracy: 0.6665 - jacard_coef: 0.0707 7/17 [===========>..................] - ETA: 1s - loss: 0.1731 - accuracy: 0.6689 - jacard_coef: 0.0708 8/17 [=============>................] - ETA: 1s - loss: 0.1722 - accuracy: 0.6685 - jacard_coef: 0.0663 9/17 [==============>...............] - ETA: 1s - loss: 0.1714 - accuracy: 0.6709 - jacard_coef: 0.068010/17 [================>.............] - ETA: 0s - loss: 0.1708 - accuracy: 0.6715 - jacard_coef: 0.067511/17 [==================>...........] - ETA: 0s - loss: 0.1701 - accuracy: 0.6813 - jacard_coef: 0.065412/17 [====================>.........] - ETA: 0s - loss: 0.1693 - accuracy: 0.6928 - jacard_coef: 0.063813/17 [=====================>........] - ETA: 0s - loss: 0.1687 - accuracy: 0.6882 - jacard_coef: 0.062114/17 [=======================>......] - ETA: 0s - loss: 0.1681 - accuracy: 0.7036 - jacard_coef: 0.059515/17 [=========================>....] - ETA: 0s - loss: 0.1676 - accuracy: 0.7176 - jacard_coef: 0.056216/17 [===========================>..] - ETA: 0s - loss: 0.1671 - accuracy: 0.7271 - jacard_coef: 0.055017/17 [==============================] - 2s 130ms/step - loss: 0.1671 - accuracy: 0.7247 - jacard_coef: 0.0592 - val_loss: 0.1799 - val_accuracy: 0.2408 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1579 - accuracy: 0.9230 - jacard_coef: 0.0479 2/17 [==>...........................] - ETA: 1s - loss: 0.1592 - accuracy: 0.9169 - jacard_coef: 0.0343 3/17 [====>.........................] - ETA: 1s - loss: 0.1593 - accuracy: 0.8860 - jacard_coef: 0.0334 4/17 [======>.......................] - ETA: 1s - loss: 0.1598 - accuracy: 0.8708 - jacard_coef: 0.0363 5/17 [=======>......................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8245 - jacard_coef: 0.0427 6/17 [=========>....................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8298 - jacard_coef: 0.0380 7/17 [===========>..................] - ETA: 1s - loss: 0.1606 - accuracy: 0.8343 - jacard_coef: 0.0396 8/17 [=============>................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8424 - jacard_coef: 0.0396 9/17 [==============>...............] - ETA: 1s - loss: 0.1602 - accuracy: 0.8492 - jacard_coef: 0.036810/17 [================>.............] - ETA: 0s - loss: 0.1601 - accuracy: 0.8473 - jacard_coef: 0.041111/17 [==================>...........] - ETA: 0s - loss: 0.1599 - accuracy: 0.8475 - jacard_coef: 0.041412/17 [====================>.........] - ETA: 0s - loss: 0.1596 - accuracy: 0.8505 - jacard_coef: 0.040313/17 [=====================>........] - ETA: 0s - loss: 0.1593 - accuracy: 0.8537 - jacard_coef: 0.039214/17 [=======================>......] - ETA: 0s - loss: 0.1600 - accuracy: 0.8565 - jacard_coef: 0.038215/17 [=========================>....] - ETA: 0s - loss: 0.1596 - accuracy: 0.8609 - jacard_coef: 0.036416/17 [===========================>..] - ETA: 0s - loss: 0.1596 - accuracy: 0.8632 - jacard_coef: 0.035817/17 [==============================] - 2s 130ms/step - loss: 0.1596 - accuracy: 0.8604 - jacard_coef: 0.0376 - val_loss: 0.1721 - val_accuracy: 0.5462 - val_jacard_coef: 0.0781 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1539 - accuracy: 0.9202 - jacard_coef: 0.0060 2/17 [==>...........................] - ETA: 1s - loss: 0.1544 - accuracy: 0.9180 - jacard_coef: 0.0149 3/17 [====>.........................] - ETA: 1s - loss: 0.1559 - accuracy: 0.8864 - jacard_coef: 0.0203 4/17 [======>.......................] - ETA: 1s - loss: 0.1560 - accuracy: 0.8891 - jacard_coef: 0.0173 5/17 [=======>......................] - ETA: 1s - loss: 0.1561 - accuracy: 0.9010 - jacard_coef: 0.0165 6/17 [=========>....................] - ETA: 1s - loss: 0.1559 - accuracy: 0.8995 - jacard_coef: 0.0143 7/17 [===========>..................] - ETA: 1s - loss: 0.1557 - accuracy: 0.9009 - jacard_coef: 0.0123 8/17 [=============>................] - ETA: 1s - loss: 0.1556 - accuracy: 0.8998 - jacard_coef: 0.0113 9/17 [==============>...............] - ETA: 1s - loss: 0.1553 - accuracy: 0.9001 - jacard_coef: 0.010110/17 [================>.............] - ETA: 0s - loss: 0.1550 - accuracy: 0.9021 - jacard_coef: 0.009511/17 [==================>...........] - ETA: 0s - loss: 0.1546 - accuracy: 0.9050 - jacard_coef: 0.009112/17 [====================>.........] - ETA: 0s - loss: 0.1546 - accuracy: 0.9051 - jacard_coef: 0.009013/17 [=====================>........] - ETA: 0s - loss: 0.1541 - accuracy: 0.9063 - jacard_coef: 0.008514/17 [=======================>......] - ETA: 0s - loss: 0.1538 - accuracy: 0.9038 - jacard_coef: 0.011415/17 [=========================>....] - ETA: 0s - loss: 0.1536 - accuracy: 0.9035 - jacard_coef: 0.012516/17 [===========================>..] - ETA: 0s - loss: 0.1535 - accuracy: 0.9032 - jacard_coef: 0.013917/17 [==============================] - 2s 128ms/step - loss: 0.1535 - accuracy: 0.9013 - jacard_coef: 0.0132 - val_loss: 0.1696 - val_accuracy: 0.5464 - val_jacard_coef: 0.0516 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1550 - accuracy: 0.8873 - jacard_coef: 1.6929e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1539 - accuracy: 0.9179 - jacard_coef: 0.0018     3/17 [====>.........................] - ETA: 1s - loss: 0.1547 - accuracy: 0.9039 - jacard_coef: 0.0012 4/17 [======>.......................] - ETA: 1s - loss: 0.1556 - accuracy: 0.8912 - jacard_coef: 0.0027 5/17 [=======>......................] - ETA: 1s - loss: 0.1557 - accuracy: 0.8933 - jacard_coef: 0.0023 6/17 [=========>....................] - ETA: 1s - loss: 0.1555 - accuracy: 0.8966 - jacard_coef: 0.0022 7/17 [===========>..................] - ETA: 1s - loss: 0.1548 - accuracy: 0.9050 - jacard_coef: 0.0019 8/17 [=============>................] - ETA: 1s - loss: 0.1547 - accuracy: 0.9035 - jacard_coef: 0.0019 9/17 [==============>...............] - ETA: 1s - loss: 0.1549 - accuracy: 0.9022 - jacard_coef: 0.003810/17 [================>.............] - ETA: 0s - loss: 0.1547 - accuracy: 0.9025 - jacard_coef: 0.003911/17 [==================>...........] - ETA: 0s - loss: 0.1561 - accuracy: 0.9049 - jacard_coef: 0.004112/17 [====================>.........] - ETA: 0s - loss: 0.1557 - accuracy: 0.9065 - jacard_coef: 0.004413/17 [=====================>........] - ETA: 0s - loss: 0.1554 - accuracy: 0.9081 - jacard_coef: 0.005514/17 [=======================>......] - ETA: 0s - loss: 0.1552 - accuracy: 0.9074 - jacard_coef: 0.007815/17 [=========================>....] - ETA: 0s - loss: 0.1549 - accuracy: 0.9077 - jacard_coef: 0.008416/17 [===========================>..] - ETA: 0s - loss: 0.1550 - accuracy: 0.9059 - jacard_coef: 0.009817/17 [==============================] - 2s 128ms/step - loss: 0.1554 - accuracy: 0.9053 - jacard_coef: 0.0109 - val_loss: 0.1263 - val_accuracy: 0.7862 - val_jacard_coef: 0.0368 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1480 - accuracy: 0.9155 - jacard_coef: 0.0197 2/17 [==>...........................] - ETA: 1s - loss: 0.1520 - accuracy: 0.9027 - jacard_coef: 0.0202 3/17 [====>.........................] - ETA: 1s - loss: 0.1514 - accuracy: 0.8838 - jacard_coef: 0.0320 4/17 [======>.......................] - ETA: 1s - loss: 0.1515 - accuracy: 0.8800 - jacard_coef: 0.0309 5/17 [=======>......................] - ETA: 1s - loss: 0.1514 - accuracy: 0.8763 - jacard_coef: 0.0295 6/17 [=========>....................] - ETA: 1s - loss: 0.1512 - accuracy: 0.8790 - jacard_coef: 0.0261 7/17 [===========>..................] - ETA: 1s - loss: 0.1507 - accuracy: 0.8857 - jacard_coef: 0.0234 8/17 [=============>................] - ETA: 1s - loss: 0.1501 - accuracy: 0.8934 - jacard_coef: 0.0205 9/17 [==============>...............] - ETA: 1s - loss: 0.1499 - accuracy: 0.8937 - jacard_coef: 0.018310/17 [================>.............] - ETA: 0s - loss: 0.1500 - accuracy: 0.8889 - jacard_coef: 0.019811/17 [==================>...........] - ETA: 0s - loss: 0.1495 - accuracy: 0.8964 - jacard_coef: 0.018012/17 [====================>.........] - ETA: 0s - loss: 0.1493 - accuracy: 0.8976 - jacard_coef: 0.016513/17 [=====================>........] - ETA: 0s - loss: 0.1491 - accuracy: 0.8988 - jacard_coef: 0.015214/17 [=======================>......] - ETA: 0s - loss: 0.1489 - accuracy: 0.8980 - jacard_coef: 0.014215/17 [=========================>....] - ETA: 0s - loss: 0.1487 - accuracy: 0.9007 - jacard_coef: 0.013516/17 [===========================>..] - ETA: 0s - loss: 0.1484 - accuracy: 0.9013 - jacard_coef: 0.013017/17 [==============================] - 2s 128ms/step - loss: 0.1484 - accuracy: 0.9010 - jacard_coef: 0.0126 - val_loss: 0.1663 - val_accuracy: 0.6196 - val_jacard_coef: 0.0628 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1433 - accuracy: 0.9478 - jacard_coef: 0.0046 2/17 [==>...........................] - ETA: 1s - loss: 0.1445 - accuracy: 0.9260 - jacard_coef: 0.0025 3/17 [====>.........................] - ETA: 1s - loss: 0.1452 - accuracy: 0.9171 - jacard_coef: 0.0017 4/17 [======>.......................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9246 - jacard_coef: 0.0015 5/17 [=======>......................] - ETA: 1s - loss: 0.1447 - accuracy: 0.9305 - jacard_coef: 0.0035 6/17 [=========>....................] - ETA: 1s - loss: 0.1452 - accuracy: 0.9256 - jacard_coef: 0.0043 7/17 [===========>..................] - ETA: 1s - loss: 0.1454 - accuracy: 0.9211 - jacard_coef: 0.0041 8/17 [=============>................] - ETA: 1s - loss: 0.1454 - accuracy: 0.9148 - jacard_coef: 0.0072 9/17 [==============>...............] - ETA: 1s - loss: 0.1456 - accuracy: 0.9091 - jacard_coef: 0.010310/17 [================>.............] - ETA: 0s - loss: 0.1455 - accuracy: 0.9085 - jacard_coef: 0.009811/17 [==================>...........] - ETA: 0s - loss: 0.1454 - accuracy: 0.9077 - jacard_coef: 0.009012/17 [====================>.........] - ETA: 0s - loss: 0.1453 - accuracy: 0.9066 - jacard_coef: 0.008613/17 [=====================>........] - ETA: 0s - loss: 0.1452 - accuracy: 0.9074 - jacard_coef: 0.008214/17 [=======================>......] - ETA: 0s - loss: 0.1449 - accuracy: 0.9088 - jacard_coef: 0.007715/17 [=========================>....] - ETA: 0s - loss: 0.1447 - accuracy: 0.9082 - jacard_coef: 0.007316/17 [===========================>..] - ETA: 0s - loss: 0.1443 - accuracy: 0.9100 - jacard_coef: 0.006917/17 [==============================] - 2s 128ms/step - loss: 0.1443 - accuracy: 0.9106 - jacard_coef: 0.0065 - val_loss: 0.1527 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1424 - accuracy: 0.9003 - jacard_coef: 1.9129e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1414 - accuracy: 0.9095 - jacard_coef: 2.1307e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1412 - accuracy: 0.9052 - jacard_coef: 1.1064e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1407 - accuracy: 0.9106 - jacard_coef: 8.2983e-05 5/17 [=======>......................] - ETA: 1s - loss: 0.1412 - accuracy: 0.9062 - jacard_coef: 7.3217e-05 6/17 [=========>....................] - ETA: 1s - loss: 0.1408 - accuracy: 0.9115 - jacard_coef: 6.1014e-05 7/17 [===========>..................] - ETA: 1s - loss: 0.1404 - accuracy: 0.9141 - jacard_coef: 5.2298e-05 8/17 [=============>................] - ETA: 1s - loss: 0.1403 - accuracy: 0.9142 - jacard_coef: 1.5507e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1399 - accuracy: 0.9161 - jacard_coef: 2.2456e-0410/17 [================>.............] - ETA: 0s - loss: 0.1401 - accuracy: 0.9130 - jacard_coef: 0.0014    11/17 [==================>...........] - ETA: 0s - loss: 0.1399 - accuracy: 0.9127 - jacard_coef: 0.001612/17 [====================>.........] - ETA: 0s - loss: 0.1396 - accuracy: 0.9150 - jacard_coef: 0.001513/17 [=====================>........] - ETA: 0s - loss: 0.1391 - accuracy: 0.9176 - jacard_coef: 0.001314/17 [=======================>......] - ETA: 0s - loss: 0.1391 - accuracy: 0.9172 - jacard_coef: 0.001315/17 [=========================>....] - ETA: 0s - loss: 0.1389 - accuracy: 0.9177 - jacard_coef: 0.001216/17 [===========================>..] - ETA: 0s - loss: 0.1390 - accuracy: 0.9157 - jacard_coef: 0.001217/17 [==============================] - 2s 128ms/step - loss: 0.1392 - accuracy: 0.9159 - jacard_coef: 0.0015 - val_loss: 0.1521 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1393 - accuracy: 0.9218 - jacard_coef: 0.0227 2/17 [==>...........................] - ETA: 1s - loss: 0.1410 - accuracy: 0.8943 - jacard_coef: 0.0120 3/17 [====>.........................] - ETA: 1s - loss: 0.1403 - accuracy: 0.8956 - jacard_coef: 0.0105 4/17 [======>.......................] - ETA: 1s - loss: 0.1400 - accuracy: 0.8996 - jacard_coef: 0.0108 5/17 [=======>......................] - ETA: 1s - loss: 0.1392 - accuracy: 0.9059 - jacard_coef: 0.0105 6/17 [=========>....................] - ETA: 1s - loss: 0.1389 - accuracy: 0.9079 - jacard_coef: 0.0087 7/17 [===========>..................] - ETA: 1s - loss: 0.1392 - accuracy: 0.9040 - jacard_coef: 0.0104 8/17 [=============>................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9059 - jacard_coef: 0.0093 9/17 [==============>...............] - ETA: 1s - loss: 0.1388 - accuracy: 0.9050 - jacard_coef: 0.008310/17 [================>.............] - ETA: 0s - loss: 0.1380 - accuracy: 0.9119 - jacard_coef: 0.008111/17 [==================>...........] - ETA: 0s - loss: 0.1377 - accuracy: 0.9154 - jacard_coef: 0.007612/17 [====================>.........] - ETA: 0s - loss: 0.1375 - accuracy: 0.9164 - jacard_coef: 0.007013/17 [=====================>........] - ETA: 0s - loss: 0.1376 - accuracy: 0.9159 - jacard_coef: 0.006414/17 [=======================>......] - ETA: 0s - loss: 0.1377 - accuracy: 0.9158 - jacard_coef: 0.006015/17 [=========================>....] - ETA: 0s - loss: 0.1376 - accuracy: 0.9149 - jacard_coef: 0.005616/17 [===========================>..] - ETA: 0s - loss: 0.1383 - accuracy: 0.9128 - jacard_coef: 0.005217/17 [==============================] - 2s 128ms/step - loss: 0.1383 - accuracy: 0.9124 - jacard_coef: 0.0049 - val_loss: 0.1560 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1346 - accuracy: 0.9419 - jacard_coef: 3.2809e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1351 - accuracy: 0.9344 - jacard_coef: 2.9456e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1368 - accuracy: 0.9167 - jacard_coef: 2.4997e-12 4/17 [======>.......................] - ETA: 1s - loss: 0.1377 - accuracy: 0.9063 - jacard_coef: 2.2558e-12 5/17 [=======>......................] - ETA: 1s - loss: 0.1368 - accuracy: 0.9140 - jacard_coef: 2.4962e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1366 - accuracy: 0.9147 - jacard_coef: 2.4681e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1360 - accuracy: 0.9206 - jacard_coef: 6.2009e-06 8/17 [=============>................] - ETA: 1s - loss: 0.1358 - accuracy: 0.9221 - jacard_coef: 0.0013     9/17 [==============>...............] - ETA: 1s - loss: 0.1360 - accuracy: 0.9210 - jacard_coef: 0.001210/17 [================>.............] - ETA: 0s - loss: 0.1358 - accuracy: 0.9218 - jacard_coef: 0.001711/17 [==================>...........] - ETA: 0s - loss: 0.1359 - accuracy: 0.9202 - jacard_coef: 0.001512/17 [====================>.........] - ETA: 0s - loss: 0.1360 - accuracy: 0.9187 - jacard_coef: 0.002213/17 [=====================>........] - ETA: 0s - loss: 0.1360 - accuracy: 0.9182 - jacard_coef: 0.002514/17 [=======================>......] - ETA: 0s - loss: 0.1360 - accuracy: 0.9167 - jacard_coef: 0.002915/17 [=========================>....] - ETA: 0s - loss: 0.1358 - accuracy: 0.9175 - jacard_coef: 0.002716/17 [===========================>..] - ETA: 0s - loss: 0.1362 - accuracy: 0.9162 - jacard_coef: 0.002717/17 [==============================] - 2s 128ms/step - loss: 0.1362 - accuracy: 0.9153 - jacard_coef: 0.0040 - val_loss: 0.1518 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1320 - accuracy: 0.9319 - jacard_coef: 0.0080 2/17 [==>...........................] - ETA: 1s - loss: 0.1335 - accuracy: 0.9250 - jacard_coef: 0.0040 3/17 [====>.........................] - ETA: 1s - loss: 0.1337 - accuracy: 0.9214 - jacard_coef: 0.0027 4/17 [======>.......................] - ETA: 1s - loss: 0.1342 - accuracy: 0.9181 - jacard_coef: 0.0020 5/17 [=======>......................] - ETA: 1s - loss: 0.1337 - accuracy: 0.9234 - jacard_coef: 0.0017 6/17 [=========>....................] - ETA: 1s - loss: 0.1335 - accuracy: 0.9234 - jacard_coef: 0.0020 7/17 [===========>..................] - ETA: 1s - loss: 0.1333 - accuracy: 0.9267 - jacard_coef: 0.0017 8/17 [=============>................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9241 - jacard_coef: 0.0015 9/17 [==============>...............] - ETA: 1s - loss: 0.1337 - accuracy: 0.9250 - jacard_coef: 0.001410/17 [================>.............] - ETA: 0s - loss: 0.1334 - accuracy: 0.9264 - jacard_coef: 0.001211/17 [==================>...........] - ETA: 0s - loss: 0.1339 - accuracy: 0.9220 - jacard_coef: 0.001112/17 [====================>.........] - ETA: 0s - loss: 0.1338 - accuracy: 0.9223 - jacard_coef: 0.001013/17 [=====================>........] - ETA: 0s - loss: 0.1337 - accuracy: 0.9230 - jacard_coef: 9.5309e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1337 - accuracy: 0.9227 - jacard_coef: 9.7491e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1340 - accuracy: 0.9184 - jacard_coef: 0.0019    16/17 [===========================>..] - ETA: 0s - loss: 0.1340 - accuracy: 0.9171 - jacard_coef: 0.002017/17 [==============================] - 2s 128ms/step - loss: 0.1341 - accuracy: 0.9163 - jacard_coef: 0.0038 - val_loss: 0.1485 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1345 - accuracy: 0.8994 - jacard_coef: 1.3276e-04 2/17 [==>...........................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9153 - jacard_coef: 9.0929e-04 3/17 [====>.........................] - ETA: 1s - loss: 0.1348 - accuracy: 0.9064 - jacard_coef: 0.0018     4/17 [======>.......................] - ETA: 1s - loss: 0.1342 - accuracy: 0.9099 - jacard_coef: 0.0017 5/17 [=======>......................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9122 - jacard_coef: 0.0014 6/17 [=========>....................] - ETA: 1s - loss: 0.1337 - accuracy: 0.9106 - jacard_coef: 0.0022 7/17 [===========>..................] - ETA: 1s - loss: 0.1330 - accuracy: 0.9164 - jacard_coef: 0.0018 8/17 [=============>................] - ETA: 1s - loss: 0.1329 - accuracy: 0.9163 - jacard_coef: 0.0016 9/17 [==============>...............] - ETA: 1s - loss: 0.1329 - accuracy: 0.9158 - jacard_coef: 0.001610/17 [================>.............] - ETA: 0s - loss: 0.1328 - accuracy: 0.9159 - jacard_coef: 0.001411/17 [==================>...........] - ETA: 0s - loss: 0.1325 - accuracy: 0.9185 - jacard_coef: 0.001312/17 [====================>.........] - ETA: 0s - loss: 0.1324 - accuracy: 0.9196 - jacard_coef: 0.001213/17 [=====================>........] - ETA: 0s - loss: 0.1325 - accuracy: 0.9183 - jacard_coef: 0.001114/17 [=======================>......] - ETA: 0s - loss: 0.1329 - accuracy: 0.9141 - jacard_coef: 0.001015/17 [=========================>....] - ETA: 0s - loss: 0.1329 - accuracy: 0.9137 - jacard_coef: 9.5805e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9161 - jacard_coef: 9.1291e-0417/17 [==============================] - 2s 128ms/step - loss: 0.1326 - accuracy: 0.9149 - jacard_coef: 0.0038 - val_loss: 0.1454 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1281 - accuracy: 0.9509 - jacard_coef: 0.0027 2/17 [==>...........................] - ETA: 1s - loss: 0.1283 - accuracy: 0.9558 - jacard_coef: 0.0014 3/17 [====>.........................] - ETA: 1s - loss: 0.1298 - accuracy: 0.9459 - jacard_coef: 0.0013 4/17 [======>.......................] - ETA: 1s - loss: 0.1314 - accuracy: 0.9335 - jacard_coef: 0.0013 5/17 [=======>......................] - ETA: 1s - loss: 0.1320 - accuracy: 0.9330 - jacard_coef: 0.0014 6/17 [=========>....................] - ETA: 1s - loss: 0.1347 - accuracy: 0.9237 - jacard_coef: 0.0021 7/17 [===========>..................] - ETA: 1s - loss: 0.1346 - accuracy: 0.9243 - jacard_coef: 0.0018 8/17 [=============>................] - ETA: 1s - loss: 0.1356 - accuracy: 0.9224 - jacard_coef: 0.0041 9/17 [==============>...............] - ETA: 1s - loss: 0.1363 - accuracy: 0.9201 - jacard_coef: 0.004710/17 [================>.............] - ETA: 0s - loss: 0.1362 - accuracy: 0.9184 - jacard_coef: 0.004511/17 [==================>...........] - ETA: 0s - loss: 0.1365 - accuracy: 0.9161 - jacard_coef: 0.004212/17 [====================>.........] - ETA: 0s - loss: 0.1369 - accuracy: 0.9121 - jacard_coef: 0.004013/17 [=====================>........] - ETA: 0s - loss: 0.1381 - accuracy: 0.9119 - jacard_coef: 0.004114/17 [=======================>......] - ETA: 0s - loss: 0.1377 - accuracy: 0.9128 - jacard_coef: 0.003815/17 [=========================>....] - ETA: 0s - loss: 0.1378 - accuracy: 0.9130 - jacard_coef: 0.003616/17 [===========================>..] - ETA: 0s - loss: 0.1376 - accuracy: 0.9147 - jacard_coef: 0.003417/17 [==============================] - 2s 128ms/step - loss: 0.1376 - accuracy: 0.9139 - jacard_coef: 0.0032 - val_loss: 0.1384 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0781 (epoch 7)
  Final Val Loss: 0.1384
  Training Time: 0:01:21.688693
  Stability (std): 0.0119

Results saved to: hyperparameter_optimization_20250926_165036/exp_4_UNet_lr5e-4_bs8/UNet_lr0.0005_bs8_results.json

Experiment 4 completed in 96s
Progress: 4/36 completed
Estimated remaining time: 51 minutes

ðŸ”¬ EXPERIMENT 5/36
================================================
Architecture: UNet
Learning Rate: 5e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877105.906270 1039268 service.cc:145] XLA service 0x148669c5e390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877105.906294 1039268 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877106.043459 1039268 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 4:48 - loss: 0.3466 - accuracy: 0.5095 - jacard_coef: 0.06362/9 [=====>........................] - ETA: 42s - loss: 0.3218 - accuracy: 0.4689 - jacard_coef: 0.0632 3/9 [=========>....................] - ETA: 26s - loss: 0.3027 - accuracy: 0.4129 - jacard_coef: 0.06924/9 [============>.................] - ETA: 19s - loss: 0.2803 - accuracy: 0.3677 - jacard_coef: 0.07685/9 [===============>..............] - ETA: 12s - loss: 0.2657 - accuracy: 0.3323 - jacard_coef: 0.07766/9 [===================>..........] - ETA: 8s - loss: 0.2558 - accuracy: 0.3065 - jacard_coef: 0.0775 7/9 [======================>.......] - ETA: 4s - loss: 0.2487 - accuracy: 0.2874 - jacard_coef: 0.08188/9 [=========================>....] - ETA: 2s - loss: 0.2431 - accuracy: 0.2683 - jacard_coef: 0.08239/9 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.2681 - jacard_coef: 0.09489/9 [==============================] - 56s 3s/step - loss: 0.2427 - accuracy: 0.2681 - jacard_coef: 0.0948 - val_loss: 0.1445 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1993 - accuracy: 0.1243 - jacard_coef: 0.05572/9 [=====>........................] - ETA: 1s - loss: 0.1967 - accuracy: 0.1425 - jacard_coef: 0.07263/9 [=========>....................] - ETA: 1s - loss: 0.1960 - accuracy: 0.1475 - jacard_coef: 0.07424/9 [============>.................] - ETA: 1s - loss: 0.1949 - accuracy: 0.1587 - jacard_coef: 0.08305/9 [===============>..............] - ETA: 0s - loss: 0.1947 - accuracy: 0.1542 - jacard_coef: 0.07656/9 [===================>..........] - ETA: 0s - loss: 0.1938 - accuracy: 0.1534 - jacard_coef: 0.07677/9 [======================>.......] - ETA: 0s - loss: 0.1938 - accuracy: 0.1555 - jacard_coef: 0.08138/9 [=========================>....] - ETA: 0s - loss: 0.1933 - accuracy: 0.1560 - jacard_coef: 0.08339/9 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.1563 - jacard_coef: 0.09159/9 [==============================] - 2s 233ms/step - loss: 0.1932 - accuracy: 0.1563 - jacard_coef: 0.0915 - val_loss: 0.8863 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1861 - accuracy: 0.1549 - jacard_coef: 0.08642/9 [=====>........................] - ETA: 1s - loss: 0.1866 - accuracy: 0.1552 - jacard_coef: 0.08693/9 [=========>....................] - ETA: 1s - loss: 0.1872 - accuracy: 0.1530 - jacard_coef: 0.08134/9 [============>.................] - ETA: 1s - loss: 0.1861 - accuracy: 0.1595 - jacard_coef: 0.08515/9 [===============>..............] - ETA: 0s - loss: 0.1858 - accuracy: 0.1637 - jacard_coef: 0.08766/9 [===================>..........] - ETA: 0s - loss: 0.1858 - accuracy: 0.1694 - jacard_coef: 0.09047/9 [======================>.......] - ETA: 0s - loss: 0.1854 - accuracy: 0.1719 - jacard_coef: 0.08848/9 [=========================>....] - ETA: 0s - loss: 0.1855 - accuracy: 0.1719 - jacard_coef: 0.08529/9 [==============================] - 2s 228ms/step - loss: 0.1855 - accuracy: 0.1720 - jacard_coef: 0.0827 - val_loss: 1.0112 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1828 - accuracy: 0.1875 - jacard_coef: 0.07912/9 [=====>........................] - ETA: 1s - loss: 0.1816 - accuracy: 0.2046 - jacard_coef: 0.08453/9 [=========>....................] - ETA: 1s - loss: 0.1815 - accuracy: 0.2195 - jacard_coef: 0.09394/9 [============>.................] - ETA: 1s - loss: 0.1823 - accuracy: 0.2213 - jacard_coef: 0.08685/9 [===============>..............] - ETA: 0s - loss: 0.1824 - accuracy: 0.2288 - jacard_coef: 0.08626/9 [===================>..........] - ETA: 0s - loss: 0.1825 - accuracy: 0.2385 - jacard_coef: 0.08397/9 [======================>.......] - ETA: 0s - loss: 0.1823 - accuracy: 0.2480 - jacard_coef: 0.08368/9 [=========================>....] - ETA: 0s - loss: 0.1819 - accuracy: 0.2565 - jacard_coef: 0.08579/9 [==============================] - 2s 228ms/step - loss: 0.1818 - accuracy: 0.2573 - jacard_coef: 0.0975 - val_loss: 0.7690 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1775 - accuracy: 0.3411 - jacard_coef: 0.13222/9 [=====>........................] - ETA: 1s - loss: 0.1786 - accuracy: 0.3179 - jacard_coef: 0.10303/9 [=========>....................] - ETA: 1s - loss: 0.1783 - accuracy: 0.3262 - jacard_coef: 0.09824/9 [============>.................] - ETA: 1s - loss: 0.1784 - accuracy: 0.3381 - jacard_coef: 0.08995/9 [===============>..............] - ETA: 0s - loss: 0.1780 - accuracy: 0.3502 - jacard_coef: 0.09176/9 [===================>..........] - ETA: 0s - loss: 0.1775 - accuracy: 0.3683 - jacard_coef: 0.08597/9 [======================>.......] - ETA: 0s - loss: 0.1769 - accuracy: 0.3906 - jacard_coef: 0.08608/9 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.4125 - jacard_coef: 0.08719/9 [==============================] - 2s 234ms/step - loss: 0.1767 - accuracy: 0.4114 - jacard_coef: 0.0938 - val_loss: 0.4577 - val_accuracy: 0.9201 - val_jacard_coef: 0.0082 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1751 - accuracy: 0.5728 - jacard_coef: 0.09112/9 [=====>........................] - ETA: 1s - loss: 0.1751 - accuracy: 0.5833 - jacard_coef: 0.10473/9 [=========>....................] - ETA: 1s - loss: 0.1752 - accuracy: 0.5715 - jacard_coef: 0.08904/9 [============>.................] - ETA: 1s - loss: 0.1749 - accuracy: 0.5700 - jacard_coef: 0.09015/9 [===============>..............] - ETA: 0s - loss: 0.1754 - accuracy: 0.5623 - jacard_coef: 0.08916/9 [===================>..........] - ETA: 0s - loss: 0.1753 - accuracy: 0.5524 - jacard_coef: 0.08357/9 [======================>.......] - ETA: 0s - loss: 0.1754 - accuracy: 0.5443 - jacard_coef: 0.08418/9 [=========================>....] - ETA: 0s - loss: 0.1751 - accuracy: 0.5426 - jacard_coef: 0.08409/9 [==============================] - 2s 228ms/step - loss: 0.1751 - accuracy: 0.5422 - jacard_coef: 0.0831 - val_loss: 1.1122 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1729 - accuracy: 0.5196 - jacard_coef: 0.07452/9 [=====>........................] - ETA: 1s - loss: 0.1727 - accuracy: 0.5287 - jacard_coef: 0.07543/9 [=========>....................] - ETA: 1s - loss: 0.1719 - accuracy: 0.5488 - jacard_coef: 0.07374/9 [============>.................] - ETA: 1s - loss: 0.1716 - accuracy: 0.5646 - jacard_coef: 0.07335/9 [===============>..............] - ETA: 0s - loss: 0.1709 - accuracy: 0.5895 - jacard_coef: 0.07186/9 [===================>..........] - ETA: 0s - loss: 0.1702 - accuracy: 0.6235 - jacard_coef: 0.07117/9 [======================>.......] - ETA: 0s - loss: 0.1695 - accuracy: 0.6515 - jacard_coef: 0.06968/9 [=========================>....] - ETA: 0s - loss: 0.1689 - accuracy: 0.6694 - jacard_coef: 0.07049/9 [==============================] - 2s 228ms/step - loss: 0.1692 - accuracy: 0.6680 - jacard_coef: 0.0788 - val_loss: 1.1067 - val_accuracy: 0.9269 - val_jacard_coef: 1.3915e-12 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1646 - accuracy: 0.8602 - jacard_coef: 0.03572/9 [=====>........................] - ETA: 1s - loss: 0.1661 - accuracy: 0.7339 - jacard_coef: 0.04453/9 [=========>....................] - ETA: 1s - loss: 0.1666 - accuracy: 0.7120 - jacard_coef: 0.05264/9 [============>.................] - ETA: 1s - loss: 0.1673 - accuracy: 0.6901 - jacard_coef: 0.05675/9 [===============>..............] - ETA: 0s - loss: 0.1675 - accuracy: 0.6982 - jacard_coef: 0.05646/9 [===================>..........] - ETA: 0s - loss: 0.1676 - accuracy: 0.7044 - jacard_coef: 0.05757/9 [======================>.......] - ETA: 0s - loss: 0.1678 - accuracy: 0.7126 - jacard_coef: 0.05898/9 [=========================>....] - ETA: 0s - loss: 0.1677 - accuracy: 0.7173 - jacard_coef: 0.05819/9 [==============================] - 2s 228ms/step - loss: 0.1677 - accuracy: 0.7177 - jacard_coef: 0.0517 - val_loss: 1.1186 - val_accuracy: 0.9279 - val_jacard_coef: 1.4099e-12 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1668 - accuracy: 0.7498 - jacard_coef: 0.06802/9 [=====>........................] - ETA: 1s - loss: 0.1667 - accuracy: 0.7384 - jacard_coef: 0.06053/9 [=========>....................] - ETA: 1s - loss: 0.1664 - accuracy: 0.7211 - jacard_coef: 0.06074/9 [============>.................] - ETA: 1s - loss: 0.1666 - accuracy: 0.7332 - jacard_coef: 0.06445/9 [===============>..............] - ETA: 0s - loss: 0.1662 - accuracy: 0.7519 - jacard_coef: 0.06046/9 [===================>..........] - ETA: 0s - loss: 0.1658 - accuracy: 0.7688 - jacard_coef: 0.05677/9 [======================>.......] - ETA: 0s - loss: 0.1652 - accuracy: 0.7885 - jacard_coef: 0.05168/9 [=========================>....] - ETA: 0s - loss: 0.1648 - accuracy: 0.7974 - jacard_coef: 0.04749/9 [==============================] - 2s 233ms/step - loss: 0.1648 - accuracy: 0.7970 - jacard_coef: 0.0492 - val_loss: 14.0876 - val_accuracy: 0.0904 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1601 - accuracy: 0.8992 - jacard_coef: 0.02352/9 [=====>........................] - ETA: 1s - loss: 0.1601 - accuracy: 0.8966 - jacard_coef: 0.01703/9 [=========>....................] - ETA: 1s - loss: 0.1598 - accuracy: 0.9041 - jacard_coef: 0.01404/9 [============>.................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8714 - jacard_coef: 0.02655/9 [===============>..............] - ETA: 0s - loss: 0.1603 - accuracy: 0.8768 - jacard_coef: 0.02266/9 [===================>..........] - ETA: 0s - loss: 0.1602 - accuracy: 0.8813 - jacard_coef: 0.01977/9 [======================>.......] - ETA: 0s - loss: 0.1600 - accuracy: 0.8865 - jacard_coef: 0.01828/9 [=========================>....] - ETA: 0s - loss: 0.1600 - accuracy: 0.8866 - jacard_coef: 0.01729/9 [==============================] - 2s 228ms/step - loss: 0.1600 - accuracy: 0.8868 - jacard_coef: 0.0172 - val_loss: 1.0995 - val_accuracy: 0.9256 - val_jacard_coef: 0.0039 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1589 - accuracy: 0.9181 - jacard_coef: 0.01022/9 [=====>........................] - ETA: 1s - loss: 0.1587 - accuracy: 0.9104 - jacard_coef: 0.01183/9 [=========>....................] - ETA: 1s - loss: 0.1587 - accuracy: 0.9088 - jacard_coef: 0.00954/9 [============>.................] - ETA: 1s - loss: 0.1582 - accuracy: 0.9125 - jacard_coef: 0.00795/9 [===============>..............] - ETA: 0s - loss: 0.1581 - accuracy: 0.9131 - jacard_coef: 0.00656/9 [===================>..........] - ETA: 0s - loss: 0.1577 - accuracy: 0.9108 - jacard_coef: 0.00567/9 [======================>.......] - ETA: 0s - loss: 0.1573 - accuracy: 0.9128 - jacard_coef: 0.00498/9 [=========================>....] - ETA: 0s - loss: 0.1570 - accuracy: 0.9136 - jacard_coef: 0.00439/9 [==============================] - 2s 228ms/step - loss: 0.1571 - accuracy: 0.9129 - jacard_coef: 0.0039 - val_loss: 1.0487 - val_accuracy: 0.9304 - val_jacard_coef: 1.4606e-12 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1554 - accuracy: 0.8959 - jacard_coef: 3.6635e-042/9 [=====>........................] - ETA: 1s - loss: 0.1549 - accuracy: 0.9041 - jacard_coef: 5.6331e-043/9 [=========>....................] - ETA: 1s - loss: 0.1547 - accuracy: 0.9041 - jacard_coef: 0.0014    4/9 [============>.................] - ETA: 1s - loss: 0.1546 - accuracy: 0.9041 - jacard_coef: 0.00315/9 [===============>..............] - ETA: 0s - loss: 0.1542 - accuracy: 0.9107 - jacard_coef: 0.00286/9 [===================>..........] - ETA: 0s - loss: 0.1540 - accuracy: 0.9110 - jacard_coef: 0.00307/9 [======================>.......] - ETA: 0s - loss: 0.1541 - accuracy: 0.9076 - jacard_coef: 0.00768/9 [=========================>....] - ETA: 0s - loss: 0.1538 - accuracy: 0.9107 - jacard_coef: 0.00699/9 [==============================] - 2s 228ms/step - loss: 0.1544 - accuracy: 0.9073 - jacard_coef: 0.0237 - val_loss: 0.8888 - val_accuracy: 0.9270 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 13/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1562 - accuracy: 0.8959 - jacard_coef: 0.02202/9 [=====>........................] - ETA: 1s - loss: 0.1570 - accuracy: 0.8496 - jacard_coef: 0.03643/9 [=========>....................] - ETA: 1s - loss: 0.1567 - accuracy: 0.8504 - jacard_coef: 0.04014/9 [============>.................] - ETA: 1s - loss: 0.1561 - accuracy: 0.8437 - jacard_coef: 0.04285/9 [===============>..............] - ETA: 0s - loss: 0.1557 - accuracy: 0.8432 - jacard_coef: 0.04266/9 [===================>..........] - ETA: 0s - loss: 0.1554 - accuracy: 0.8411 - jacard_coef: 0.04417/9 [======================>.......] - ETA: 0s - loss: 0.1551 - accuracy: 0.8435 - jacard_coef: 0.04328/9 [=========================>....] - ETA: 0s - loss: 0.1548 - accuracy: 0.8504 - jacard_coef: 0.04009/9 [==============================] - 2s 228ms/step - loss: 0.1550 - accuracy: 0.8473 - jacard_coef: 0.0483 - val_loss: 0.9680 - val_accuracy: 0.9232 - val_jacard_coef: 0.0028 - lr: 0.0010
Epoch 14/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1528 - accuracy: 0.9136 - jacard_coef: 0.00562/9 [=====>........................] - ETA: 1s - loss: 0.1528 - accuracy: 0.9090 - jacard_coef: 0.01013/9 [=========>....................] - ETA: 1s - loss: 0.1537 - accuracy: 0.8893 - jacard_coef: 0.01904/9 [============>.................] - ETA: 1s - loss: 0.1537 - accuracy: 0.8802 - jacard_coef: 0.02315/9 [===============>..............] - ETA: 0s - loss: 0.1542 - accuracy: 0.8755 - jacard_coef: 0.02536/9 [===================>..........] - ETA: 0s - loss: 0.1543 - accuracy: 0.8767 - jacard_coef: 0.02437/9 [======================>.......] - ETA: 0s - loss: 0.1541 - accuracy: 0.8826 - jacard_coef: 0.02108/9 [=========================>....] - ETA: 0s - loss: 0.1540 - accuracy: 0.8845 - jacard_coef: 0.01889/9 [==============================] - 2s 229ms/step - loss: 0.1541 - accuracy: 0.8842 - jacard_coef: 0.0188 - val_loss: 0.9077 - val_accuracy: 0.9237 - val_jacard_coef: 0.0028 - lr: 0.0010
Epoch 15/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1516 - accuracy: 0.9114 - jacard_coef: 9.9972e-042/9 [=====>........................] - ETA: 1s - loss: 0.1523 - accuracy: 0.9004 - jacard_coef: 0.0032    3/9 [=========>....................] - ETA: 1s - loss: 0.1520 - accuracy: 0.9045 - jacard_coef: 0.00614/9 [============>.................] - ETA: 1s - loss: 0.1525 - accuracy: 0.8946 - jacard_coef: 0.00725/9 [===============>..............] - ETA: 0s - loss: 0.1523 - accuracy: 0.8963 - jacard_coef: 0.01026/9 [===================>..........] - ETA: 0s - loss: 0.1522 - accuracy: 0.9005 - jacard_coef: 0.01197/9 [======================>.......] - ETA: 0s - loss: 0.1520 - accuracy: 0.9023 - jacard_coef: 0.01138/9 [=========================>....] - ETA: 0s - loss: 0.1517 - accuracy: 0.9037 - jacard_coef: 0.01159/9 [==============================] - 2s 228ms/step - loss: 0.1518 - accuracy: 0.9031 - jacard_coef: 0.0114 - val_loss: 0.7093 - val_accuracy: 0.9265 - val_jacard_coef: 6.9196e-04 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1505 - accuracy: 0.9099 - jacard_coef: 0.00152/9 [=====>........................] - ETA: 1s - loss: 0.1498 - accuracy: 0.9249 - jacard_coef: 0.00153/9 [=========>....................] - ETA: 1s - loss: 0.1504 - accuracy: 0.9103 - jacard_coef: 0.00164/9 [============>.................] - ETA: 1s - loss: 0.1510 - accuracy: 0.9083 - jacard_coef: 0.00145/9 [===============>..............] - ETA: 0s - loss: 0.1504 - accuracy: 0.9144 - jacard_coef: 0.00156/9 [===================>..........] - ETA: 0s - loss: 0.1502 - accuracy: 0.9159 - jacard_coef: 0.00147/9 [======================>.......] - ETA: 0s - loss: 0.1500 - accuracy: 0.9159 - jacard_coef: 0.00138/9 [=========================>....] - ETA: 0s - loss: 0.1500 - accuracy: 0.9144 - jacard_coef: 0.00189/9 [==============================] - 2s 228ms/step - loss: 0.1500 - accuracy: 0.9139 - jacard_coef: 0.0016 - val_loss: 0.3655 - val_accuracy: 0.9196 - val_jacard_coef: 0.0065 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1493 - accuracy: 0.9079 - jacard_coef: 0.00292/9 [=====>........................] - ETA: 1s - loss: 0.1496 - accuracy: 0.9090 - jacard_coef: 0.00263/9 [=========>....................] - ETA: 1s - loss: 0.1491 - accuracy: 0.9183 - jacard_coef: 0.00644/9 [============>.................] - ETA: 1s - loss: 0.1487 - accuracy: 0.9192 - jacard_coef: 0.00505/9 [===============>..............] - ETA: 0s - loss: 0.1489 - accuracy: 0.9138 - jacard_coef: 0.00476/9 [===================>..........] - ETA: 0s - loss: 0.1488 - accuracy: 0.9117 - jacard_coef: 0.00407/9 [======================>.......] - ETA: 0s - loss: 0.1488 - accuracy: 0.9105 - jacard_coef: 0.00398/9 [=========================>....] - ETA: 0s - loss: 0.1487 - accuracy: 0.9108 - jacard_coef: 0.00379/9 [==============================] - 2s 228ms/step - loss: 0.1488 - accuracy: 0.9098 - jacard_coef: 0.0033 - val_loss: 0.1040 - val_accuracy: 0.9239 - val_jacard_coef: 0.0046 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1485 - accuracy: 0.8940 - jacard_coef: 0.00172/9 [=====>........................] - ETA: 1s - loss: 0.1478 - accuracy: 0.9081 - jacard_coef: 8.7173e-043/9 [=========>....................] - ETA: 1s - loss: 0.1477 - accuracy: 0.9095 - jacard_coef: 6.0293e-044/9 [============>.................] - ETA: 1s - loss: 0.1473 - accuracy: 0.9159 - jacard_coef: 8.0699e-045/9 [===============>..............] - ETA: 0s - loss: 0.1474 - accuracy: 0.9166 - jacard_coef: 7.6421e-046/9 [===================>..........] - ETA: 0s - loss: 0.1474 - accuracy: 0.9166 - jacard_coef: 6.3685e-047/9 [======================>.......] - ETA: 0s - loss: 0.1475 - accuracy: 0.9164 - jacard_coef: 5.4587e-048/9 [=========================>....] - ETA: 0s - loss: 0.1474 - accuracy: 0.9163 - jacard_coef: 5.4234e-049/9 [==============================] - 2s 228ms/step - loss: 0.1474 - accuracy: 0.9164 - jacard_coef: 4.8208e-04 - val_loss: 0.0877 - val_accuracy: 0.9289 - val_jacard_coef: 7.5736e-04 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1467 - accuracy: 0.9186 - jacard_coef: 0.00162/9 [=====>........................] - ETA: 1s - loss: 0.1468 - accuracy: 0.9104 - jacard_coef: 8.2598e-043/9 [=========>....................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9152 - jacard_coef: 0.0011    4/9 [============>.................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9111 - jacard_coef: 8.4667e-045/9 [===============>..............] - ETA: 0s - loss: 0.1460 - accuracy: 0.9171 - jacard_coef: 7.0973e-046/9 [===================>..........] - ETA: 0s - loss: 0.1462 - accuracy: 0.9159 - jacard_coef: 7.2188e-047/9 [======================>.......] - ETA: 0s - loss: 0.1461 - accuracy: 0.9159 - jacard_coef: 8.8268e-048/9 [=========================>....] - ETA: 0s - loss: 0.1460 - accuracy: 0.9166 - jacard_coef: 7.8294e-049/9 [==============================] - 2s 229ms/step - loss: 0.1462 - accuracy: 0.9138 - jacard_coef: 0.0089 - val_loss: 0.1065 - val_accuracy: 0.9177 - val_jacard_coef: 0.0091 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0701 (epoch 9)
  Final Val Loss: 0.1065
  Training Time: 0:01:34.201222
  Stability (std): 0.3969

Results saved to: hyperparameter_optimization_20250926_165036/exp_5_UNet_lr5e-4_bs16/UNet_lr0.0005_bs16_results.json

Experiment 5 completed in 108s
Progress: 5/36 completed
Estimated remaining time: 55 minutes

ðŸ”¬ EXPERIMENT 6/36
================================================
Architecture: UNet
Learning Rate: 5e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877219.674443 1043399 service.cc:145] XLA service 0x147831b452e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877219.674468 1043399 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877219.813095 1043399 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 2:59 - loss: 0.3430 - accuracy: 0.4908 - jacard_coef: 0.07712/5 [===========>..................] - ETA: 42s - loss: 0.3131 - accuracy: 0.4808 - jacard_coef: 0.0894 3/5 [=================>............] - ETA: 17s - loss: 0.2857 - accuracy: 0.4312 - jacard_coef: 0.08354/5 [=======================>......] - ETA: 6s - loss: 0.2656 - accuracy: 0.3728 - jacard_coef: 0.0821 5/5 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.3713 - jacard_coef: 0.07895/5 [==============================] - 71s 6s/step - loss: 0.2650 - accuracy: 0.3713 - jacard_coef: 0.0789 - val_loss: 0.1367 - val_accuracy: 0.9209 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1952 - accuracy: 0.1350 - jacard_coef: 0.07542/5 [===========>..................] - ETA: 1s - loss: 0.1939 - accuracy: 0.1409 - jacard_coef: 0.07963/5 [=================>............] - ETA: 0s - loss: 0.1933 - accuracy: 0.1449 - jacard_coef: 0.08054/5 [=======================>......] - ETA: 0s - loss: 0.1928 - accuracy: 0.1473 - jacard_coef: 0.08315/5 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.1471 - jacard_coef: 0.07535/5 [==============================] - 2s 422ms/step - loss: 0.1928 - accuracy: 0.1471 - jacard_coef: 0.0753 - val_loss: 0.1599 - val_accuracy: 0.7479 - val_jacard_coef: 0.0484 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1902 - accuracy: 0.1882 - jacard_coef: 0.08512/5 [===========>..................] - ETA: 1s - loss: 0.1903 - accuracy: 0.2139 - jacard_coef: 0.07873/5 [=================>............] - ETA: 0s - loss: 0.1895 - accuracy: 0.2239 - jacard_coef: 0.07984/5 [=======================>......] - ETA: 0s - loss: 0.1895 - accuracy: 0.2145 - jacard_coef: 0.08165/5 [==============================] - 2s 387ms/step - loss: 0.1895 - accuracy: 0.2145 - jacard_coef: 0.1005 - val_loss: 0.2496 - val_accuracy: 0.9226 - val_jacard_coef: 0.0072 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1861 - accuracy: 0.1649 - jacard_coef: 0.07982/5 [===========>..................] - ETA: 1s - loss: 0.1874 - accuracy: 0.1766 - jacard_coef: 0.07593/5 [=================>............] - ETA: 0s - loss: 0.1866 - accuracy: 0.1903 - jacard_coef: 0.07854/5 [=======================>......] - ETA: 0s - loss: 0.1865 - accuracy: 0.1989 - jacard_coef: 0.08255/5 [==============================] - 2s 387ms/step - loss: 0.1864 - accuracy: 0.1996 - jacard_coef: 0.0961 - val_loss: 0.3658 - val_accuracy: 0.9257 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1846 - accuracy: 0.1868 - jacard_coef: 0.06142/5 [===========>..................] - ETA: 1s - loss: 0.1847 - accuracy: 0.1911 - jacard_coef: 0.07393/5 [=================>............] - ETA: 0s - loss: 0.1847 - accuracy: 0.1889 - jacard_coef: 0.07764/5 [=======================>......] - ETA: 0s - loss: 0.1843 - accuracy: 0.1872 - jacard_coef: 0.08325/5 [==============================] - 2s 386ms/step - loss: 0.1843 - accuracy: 0.1864 - jacard_coef: 0.0683 - val_loss: 0.2325 - val_accuracy: 0.9192 - val_jacard_coef: 0.0071 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1815 - accuracy: 0.1457 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 1s - loss: 0.1815 - accuracy: 0.1541 - jacard_coef: 0.08713/5 [=================>............] - ETA: 0s - loss: 0.1812 - accuracy: 0.1653 - jacard_coef: 0.08454/5 [=======================>......] - ETA: 0s - loss: 0.1811 - accuracy: 0.1864 - jacard_coef: 0.08235/5 [==============================] - 2s 387ms/step - loss: 0.1811 - accuracy: 0.1878 - jacard_coef: 0.1014 - val_loss: 0.1189 - val_accuracy: 0.9075 - val_jacard_coef: 0.0237 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1791 - accuracy: 0.3496 - jacard_coef: 0.05842/5 [===========>..................] - ETA: 1s - loss: 0.1781 - accuracy: 0.3812 - jacard_coef: 0.07283/5 [=================>............] - ETA: 0s - loss: 0.1778 - accuracy: 0.4139 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1772 - accuracy: 0.4394 - jacard_coef: 0.07785/5 [==============================] - 2s 387ms/step - loss: 0.1772 - accuracy: 0.4399 - jacard_coef: 0.0763 - val_loss: 0.1136 - val_accuracy: 0.9125 - val_jacard_coef: 0.0217 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1745 - accuracy: 0.5919 - jacard_coef: 0.08572/5 [===========>..................] - ETA: 1s - loss: 0.1738 - accuracy: 0.6026 - jacard_coef: 0.08723/5 [=================>............] - ETA: 0s - loss: 0.1736 - accuracy: 0.6081 - jacard_coef: 0.07924/5 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.6164 - jacard_coef: 0.07985/5 [==============================] - 2s 398ms/step - loss: 0.1732 - accuracy: 0.6170 - jacard_coef: 0.0642 - val_loss: 0.1951 - val_accuracy: 0.2330 - val_jacard_coef: 0.0718 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1757 - accuracy: 0.3762 - jacard_coef: 0.08142/5 [===========>..................] - ETA: 1s - loss: 0.1734 - accuracy: 0.5269 - jacard_coef: 0.07613/5 [=================>............] - ETA: 0s - loss: 0.1730 - accuracy: 0.5848 - jacard_coef: 0.07424/5 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.6164 - jacard_coef: 0.07745/5 [==============================] - 2s 388ms/step - loss: 0.1729 - accuracy: 0.6167 - jacard_coef: 0.0853 - val_loss: 0.2073 - val_accuracy: 0.3072 - val_jacard_coef: 0.0711 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1735 - accuracy: 0.6480 - jacard_coef: 0.07522/5 [===========>..................] - ETA: 1s - loss: 0.1728 - accuracy: 0.6366 - jacard_coef: 0.07233/5 [=================>............] - ETA: 0s - loss: 0.1726 - accuracy: 0.6125 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1728 - accuracy: 0.5942 - jacard_coef: 0.07985/5 [==============================] - 2s 387ms/step - loss: 0.1729 - accuracy: 0.5935 - jacard_coef: 0.0868 - val_loss: 0.1512 - val_accuracy: 0.9066 - val_jacard_coef: 0.0328 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1727 - accuracy: 0.5504 - jacard_coef: 0.06632/5 [===========>..................] - ETA: 1s - loss: 0.1723 - accuracy: 0.5819 - jacard_coef: 0.07643/5 [=================>............] - ETA: 0s - loss: 0.1724 - accuracy: 0.6234 - jacard_coef: 0.07554/5 [=======================>......] - ETA: 0s - loss: 0.1720 - accuracy: 0.6517 - jacard_coef: 0.07725/5 [==============================] - 2s 387ms/step - loss: 0.1720 - accuracy: 0.6523 - jacard_coef: 0.0757 - val_loss: 0.1872 - val_accuracy: 0.6985 - val_jacard_coef: 0.0584 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1711 - accuracy: 0.7200 - jacard_coef: 0.08212/5 [===========>..................] - ETA: 1s - loss: 0.1708 - accuracy: 0.7271 - jacard_coef: 0.07803/5 [=================>............] - ETA: 0s - loss: 0.1706 - accuracy: 0.7180 - jacard_coef: 0.07984/5 [=======================>......] - ETA: 0s - loss: 0.1703 - accuracy: 0.7107 - jacard_coef: 0.07835/5 [==============================] - 2s 388ms/step - loss: 0.1703 - accuracy: 0.7097 - jacard_coef: 0.0631 - val_loss: 0.1601 - val_accuracy: 0.8918 - val_jacard_coef: 0.0244 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1688 - accuracy: 0.6768 - jacard_coef: 0.08692/5 [===========>..................] - ETA: 1s - loss: 0.1685 - accuracy: 0.6968 - jacard_coef: 0.07873/5 [=================>............] - ETA: 0s - loss: 0.1680 - accuracy: 0.7072 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 0s - loss: 0.1678 - accuracy: 0.7158 - jacard_coef: 0.07275/5 [==============================] - 2s 387ms/step - loss: 0.1680 - accuracy: 0.7130 - jacard_coef: 0.0653 - val_loss: 0.1723 - val_accuracy: 0.8183 - val_jacard_coef: 0.0510 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1661 - accuracy: 0.8298 - jacard_coef: 0.06082/5 [===========>..................] - ETA: 1s - loss: 0.1680 - accuracy: 0.7781 - jacard_coef: 0.07233/5 [=================>............] - ETA: 0s - loss: 0.1682 - accuracy: 0.7665 - jacard_coef: 0.07014/5 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.7364 - jacard_coef: 0.07305/5 [==============================] - 2s 387ms/step - loss: 0.1686 - accuracy: 0.7326 - jacard_coef: 0.0601 - val_loss: 0.1557 - val_accuracy: 0.9096 - val_jacard_coef: 0.0092 - lr: 2.5000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1683 - accuracy: 0.8292 - jacard_coef: 0.05532/5 [===========>..................] - ETA: 1s - loss: 0.1677 - accuracy: 0.8234 - jacard_coef: 0.05483/5 [=================>............] - ETA: 0s - loss: 0.1680 - accuracy: 0.8224 - jacard_coef: 0.06464/5 [=======================>......] - ETA: 0s - loss: 0.1678 - accuracy: 0.8321 - jacard_coef: 0.06105/5 [==============================] - 2s 388ms/step - loss: 0.1678 - accuracy: 0.8291 - jacard_coef: 0.0764 - val_loss: 0.1662 - val_accuracy: 0.8643 - val_jacard_coef: 0.0345 - lr: 2.5000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1659 - accuracy: 0.8817 - jacard_coef: 0.04322/5 [===========>..................] - ETA: 1s - loss: 0.1662 - accuracy: 0.8648 - jacard_coef: 0.04993/5 [=================>............] - ETA: 0s - loss: 0.1660 - accuracy: 0.8534 - jacard_coef: 0.04934/5 [=======================>......] - ETA: 0s - loss: 0.1660 - accuracy: 0.8387 - jacard_coef: 0.05215/5 [==============================] - 2s 396ms/step - loss: 0.1660 - accuracy: 0.8384 - jacard_coef: 0.0473 - val_loss: 0.1724 - val_accuracy: 0.5695 - val_jacard_coef: 0.0761 - lr: 2.5000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1655 - accuracy: 0.7809 - jacard_coef: 0.05602/5 [===========>..................] - ETA: 1s - loss: 0.1650 - accuracy: 0.7912 - jacard_coef: 0.06523/5 [=================>............] - ETA: 0s - loss: 0.1649 - accuracy: 0.7972 - jacard_coef: 0.06214/5 [=======================>......] - ETA: 0s - loss: 0.1651 - accuracy: 0.8010 - jacard_coef: 0.06145/5 [==============================] - 2s 387ms/step - loss: 0.1655 - accuracy: 0.7978 - jacard_coef: 0.0606 - val_loss: 0.1697 - val_accuracy: 0.8171 - val_jacard_coef: 0.0495 - lr: 2.5000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1648 - accuracy: 0.8468 - jacard_coef: 0.07102/5 [===========>..................] - ETA: 1s - loss: 0.1657 - accuracy: 0.8331 - jacard_coef: 0.06203/5 [=================>............] - ETA: 0s - loss: 0.1656 - accuracy: 0.8245 - jacard_coef: 0.06004/5 [=======================>......] - ETA: 0s - loss: 0.1656 - accuracy: 0.8109 - jacard_coef: 0.06655/5 [==============================] - 2s 387ms/step - loss: 0.1656 - accuracy: 0.8096 - jacard_coef: 0.0781 - val_loss: 0.1613 - val_accuracy: 0.9286 - val_jacard_coef: 1.4255e-12 - lr: 2.5000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1659 - accuracy: 0.7103 - jacard_coef: 0.08172/5 [===========>..................] - ETA: 1s - loss: 0.1662 - accuracy: 0.6968 - jacard_coef: 0.08153/5 [=================>............] - ETA: 0s - loss: 0.1658 - accuracy: 0.6958 - jacard_coef: 0.08224/5 [=======================>......] - ETA: 0s - loss: 0.1656 - accuracy: 0.6979 - jacard_coef: 0.07865/5 [==============================] - 2s 388ms/step - loss: 0.1656 - accuracy: 0.6975 - jacard_coef: 0.0636 - val_loss: 0.1578 - val_accuracy: 0.9293 - val_jacard_coef: 1.4386e-12 - lr: 2.5000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1658 - accuracy: 0.6900 - jacard_coef: 0.08022/5 [===========>..................] - ETA: 1s - loss: 0.1651 - accuracy: 0.6962 - jacard_coef: 0.07243/5 [=================>............] - ETA: 0s - loss: 0.1649 - accuracy: 0.7050 - jacard_coef: 0.07484/5 [=======================>......] - ETA: 0s - loss: 0.1649 - accuracy: 0.7127 - jacard_coef: 0.07565/5 [==============================] - 2s 387ms/step - loss: 0.1650 - accuracy: 0.7102 - jacard_coef: 0.0707 - val_loss: 0.1600 - val_accuracy: 0.9275 - val_jacard_coef: 0.0030 - lr: 2.5000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1641 - accuracy: 0.8268 - jacard_coef: 0.05712/5 [===========>..................] - ETA: 1s - loss: 0.1642 - accuracy: 0.8386 - jacard_coef: 0.05693/5 [=================>............] - ETA: 0s - loss: 0.1643 - accuracy: 0.8382 - jacard_coef: 0.05714/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.8431 - jacard_coef: 0.05835/5 [==============================] - 2s 388ms/step - loss: 0.1643 - accuracy: 0.8404 - jacard_coef: 0.0812 - val_loss: 0.1634 - val_accuracy: 0.9252 - val_jacard_coef: 0.0030 - lr: 2.5000e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1643 - accuracy: 0.8311 - jacard_coef: 0.05792/5 [===========>..................] - ETA: 1s - loss: 0.1642 - accuracy: 0.8334 - jacard_coef: 0.05743/5 [=================>............] - ETA: 0s - loss: 0.1641 - accuracy: 0.8309 - jacard_coef: 0.05784/5 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.8288 - jacard_coef: 0.06335/5 [==============================] - 2s 387ms/step - loss: 0.1640 - accuracy: 0.8285 - jacard_coef: 0.0554 - val_loss: 0.1633 - val_accuracy: 0.9269 - val_jacard_coef: 0.0023 - lr: 1.2500e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1635 - accuracy: 0.8371 - jacard_coef: 0.07602/5 [===========>..................] - ETA: 1s - loss: 0.1637 - accuracy: 0.8234 - jacard_coef: 0.06963/5 [=================>............] - ETA: 0s - loss: 0.1637 - accuracy: 0.8279 - jacard_coef: 0.06264/5 [=======================>......] - ETA: 0s - loss: 0.1637 - accuracy: 0.8289 - jacard_coef: 0.06145/5 [==============================] - 2s 388ms/step - loss: 0.1637 - accuracy: 0.8293 - jacard_coef: 0.0496 - val_loss: 0.1628 - val_accuracy: 0.9277 - val_jacard_coef: 0.0010 - lr: 1.2500e-04
Epoch 24/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1633 - accuracy: 0.8484 - jacard_coef: 0.07212/5 [===========>..................] - ETA: 1s - loss: 0.1633 - accuracy: 0.8455 - jacard_coef: 0.06673/5 [=================>............] - ETA: 0s - loss: 0.1634 - accuracy: 0.8469 - jacard_coef: 0.06324/5 [=======================>......] - ETA: 0s - loss: 0.1633 - accuracy: 0.8518 - jacard_coef: 0.05875/5 [==============================] - 2s 387ms/step - loss: 0.1633 - accuracy: 0.8522 - jacard_coef: 0.0475 - val_loss: 0.1621 - val_accuracy: 0.9274 - val_jacard_coef: 0.0014 - lr: 1.2500e-04
Epoch 25/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8714 - jacard_coef: 0.06592/5 [===========>..................] - ETA: 1s - loss: 0.1631 - accuracy: 0.8719 - jacard_coef: 0.05663/5 [=================>............] - ETA: 0s - loss: 0.1626 - accuracy: 0.8795 - jacard_coef: 0.06304/5 [=======================>......] - ETA: 0s - loss: 0.1629 - accuracy: 0.8798 - jacard_coef: 0.05385/5 [==============================] - 2s 387ms/step - loss: 0.1629 - accuracy: 0.8803 - jacard_coef: 0.0440 - val_loss: 0.1620 - val_accuracy: 0.9255 - val_jacard_coef: 0.0012 - lr: 1.2500e-04
Epoch 26/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1625 - accuracy: 0.8802 - jacard_coef: 0.04332/5 [===========>..................] - ETA: 1s - loss: 0.1624 - accuracy: 0.8909 - jacard_coef: 0.05873/5 [=================>............] - ETA: 0s - loss: 0.1622 - accuracy: 0.8942 - jacard_coef: 0.04974/5 [=======================>......] - ETA: 0s - loss: 0.1625 - accuracy: 0.8880 - jacard_coef: 0.04995/5 [==============================] - 2s 388ms/step - loss: 0.1625 - accuracy: 0.8863 - jacard_coef: 0.0697 - val_loss: 0.1624 - val_accuracy: 0.9249 - val_jacard_coef: 0.0028 - lr: 1.2500e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0761 (epoch 16)
  Final Val Loss: 0.1624
  Training Time: 0:02:01.987647
  Stability (std): 0.0029

Results saved to: hyperparameter_optimization_20250926_165036/exp_6_UNet_lr5e-4_bs32/UNet_lr0.0005_bs32_results.json

Experiment 6 completed in 136s
Progress: 6/36 completed
Estimated remaining time: 68 minutes

ðŸ”¬ EXPERIMENT 7/36
================================================
Architecture: UNet
Learning Rate: 1e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877348.517895 1047958 service.cc:145] XLA service 0x146629c20470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877348.517919 1047958 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877348.654818 1047958 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 8:34 - loss: 0.3451 - accuracy: 0.5031 - jacard_coef: 0.0648 2/17 [==>...........................] - ETA: 59s - loss: 0.3218 - accuracy: 0.4470 - jacard_coef: 0.0635  3/17 [====>.........................] - ETA: 35s - loss: 0.2874 - accuracy: 0.3872 - jacard_coef: 0.0826 4/17 [======>.......................] - ETA: 26s - loss: 0.2667 - accuracy: 0.3466 - jacard_coef: 0.0824 5/17 [=======>......................] - ETA: 18s - loss: 0.2562 - accuracy: 0.3151 - jacard_coef: 0.0856 6/17 [=========>....................] - ETA: 13s - loss: 0.2460 - accuracy: 0.2890 - jacard_coef: 0.0797 7/17 [===========>..................] - ETA: 10s - loss: 0.2398 - accuracy: 0.2657 - jacard_coef: 0.0784 8/17 [=============>................] - ETA: 8s - loss: 0.2366 - accuracy: 0.2620 - jacard_coef: 0.0819  9/17 [==============>...............] - ETA: 6s - loss: 0.2310 - accuracy: 0.2596 - jacard_coef: 0.079810/17 [================>.............] - ETA: 5s - loss: 0.2281 - accuracy: 0.2658 - jacard_coef: 0.080511/17 [==================>...........] - ETA: 4s - loss: 0.2278 - accuracy: 0.2581 - jacard_coef: 0.078912/17 [====================>.........] - ETA: 3s - loss: 0.2251 - accuracy: 0.2585 - jacard_coef: 0.082413/17 [=====================>........] - ETA: 2s - loss: 0.2222 - accuracy: 0.2593 - jacard_coef: 0.083814/17 [=======================>......] - ETA: 1s - loss: 0.2208 - accuracy: 0.2559 - jacard_coef: 0.084915/17 [=========================>....] - ETA: 1s - loss: 0.2198 - accuracy: 0.2501 - jacard_coef: 0.083216/17 [===========================>..] - ETA: 0s - loss: 0.2176 - accuracy: 0.2509 - jacard_coef: 0.081217/17 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.2506 - jacard_coef: 0.079817/17 [==============================] - 46s 855ms/step - loss: 0.2174 - accuracy: 0.2506 - jacard_coef: 0.0798 - val_loss: 1.1062 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1843 - accuracy: 0.3133 - jacard_coef: 0.0613 2/17 [==>...........................] - ETA: 1s - loss: 0.1854 - accuracy: 0.2918 - jacard_coef: 0.0735 3/17 [====>.........................] - ETA: 1s - loss: 0.1837 - accuracy: 0.3090 - jacard_coef: 0.0755 4/17 [======>.......................] - ETA: 1s - loss: 0.1824 - accuracy: 0.3158 - jacard_coef: 0.0827 5/17 [=======>......................] - ETA: 1s - loss: 0.1828 - accuracy: 0.2986 - jacard_coef: 0.0827 6/17 [=========>....................] - ETA: 1s - loss: 0.1824 - accuracy: 0.3070 - jacard_coef: 0.0824 7/17 [===========>..................] - ETA: 1s - loss: 0.1821 - accuracy: 0.3133 - jacard_coef: 0.0748 8/17 [=============>................] - ETA: 1s - loss: 0.1815 - accuracy: 0.3236 - jacard_coef: 0.0729 9/17 [==============>...............] - ETA: 1s - loss: 0.1806 - accuracy: 0.3403 - jacard_coef: 0.072010/17 [================>.............] - ETA: 0s - loss: 0.1801 - accuracy: 0.3498 - jacard_coef: 0.071011/17 [==================>...........] - ETA: 0s - loss: 0.1799 - accuracy: 0.3580 - jacard_coef: 0.073812/17 [====================>.........] - ETA: 0s - loss: 0.1805 - accuracy: 0.3809 - jacard_coef: 0.073913/17 [=====================>........] - ETA: 0s - loss: 0.1804 - accuracy: 0.3970 - jacard_coef: 0.073314/17 [=======================>......] - ETA: 0s - loss: 0.1802 - accuracy: 0.4092 - jacard_coef: 0.077215/17 [=========================>....] - ETA: 0s - loss: 0.1798 - accuracy: 0.4199 - jacard_coef: 0.077616/17 [===========================>..] - ETA: 0s - loss: 0.1794 - accuracy: 0.4223 - jacard_coef: 0.077617/17 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.4218 - jacard_coef: 0.081817/17 [==============================] - 2s 134ms/step - loss: 0.1796 - accuracy: 0.4218 - jacard_coef: 0.0818 - val_loss: 0.1199 - val_accuracy: 0.9195 - val_jacard_coef: 0.0181 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1781 - accuracy: 0.3206 - jacard_coef: 0.0464 2/17 [==>...........................] - ETA: 1s - loss: 0.1796 - accuracy: 0.3087 - jacard_coef: 0.0664 3/17 [====>.........................] - ETA: 1s - loss: 0.1830 - accuracy: 0.2918 - jacard_coef: 0.0756 4/17 [======>.......................] - ETA: 1s - loss: 0.1842 - accuracy: 0.2731 - jacard_coef: 0.0730 5/17 [=======>......................] - ETA: 1s - loss: 0.1839 - accuracy: 0.2814 - jacard_coef: 0.0735 6/17 [=========>....................] - ETA: 1s - loss: 0.1848 - accuracy: 0.2853 - jacard_coef: 0.0822 7/17 [===========>..................] - ETA: 1s - loss: 0.1854 - accuracy: 0.2831 - jacard_coef: 0.0807 8/17 [=============>................] - ETA: 1s - loss: 0.1855 - accuracy: 0.2716 - jacard_coef: 0.0813 9/17 [==============>...............] - ETA: 1s - loss: 0.1857 - accuracy: 0.2660 - jacard_coef: 0.083710/17 [================>.............] - ETA: 0s - loss: 0.1865 - accuracy: 0.2742 - jacard_coef: 0.083011/17 [==================>...........] - ETA: 0s - loss: 0.1860 - accuracy: 0.2927 - jacard_coef: 0.082112/17 [====================>.........] - ETA: 0s - loss: 0.1864 - accuracy: 0.2988 - jacard_coef: 0.082113/17 [=====================>........] - ETA: 0s - loss: 0.1859 - accuracy: 0.2988 - jacard_coef: 0.082314/17 [=======================>......] - ETA: 0s - loss: 0.1856 - accuracy: 0.3082 - jacard_coef: 0.082015/17 [=========================>....] - ETA: 0s - loss: 0.1848 - accuracy: 0.3212 - jacard_coef: 0.083716/17 [===========================>..] - ETA: 0s - loss: 0.1842 - accuracy: 0.3297 - jacard_coef: 0.081217/17 [==============================] - 2s 127ms/step - loss: 0.1842 - accuracy: 0.3302 - jacard_coef: 0.0765 - val_loss: 0.4437 - val_accuracy: 0.9302 - val_jacard_coef: 2.4789e-04 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1757 - accuracy: 0.4821 - jacard_coef: 0.0819 2/17 [==>...........................] - ETA: 1s - loss: 0.1758 - accuracy: 0.5123 - jacard_coef: 0.0679 3/17 [====>.........................] - ETA: 1s - loss: 0.1743 - accuracy: 0.5116 - jacard_coef: 0.0600 4/17 [======>.......................] - ETA: 1s - loss: 0.1743 - accuracy: 0.4936 - jacard_coef: 0.0582 5/17 [=======>......................] - ETA: 1s - loss: 0.1757 - accuracy: 0.5222 - jacard_coef: 0.0629 6/17 [=========>....................] - ETA: 1s - loss: 0.1747 - accuracy: 0.5410 - jacard_coef: 0.0632 7/17 [===========>..................] - ETA: 1s - loss: 0.1738 - accuracy: 0.5628 - jacard_coef: 0.0645 8/17 [=============>................] - ETA: 1s - loss: 0.1734 - accuracy: 0.5625 - jacard_coef: 0.0678 9/17 [==============>...............] - ETA: 1s - loss: 0.1728 - accuracy: 0.5686 - jacard_coef: 0.071210/17 [================>.............] - ETA: 0s - loss: 0.1724 - accuracy: 0.5663 - jacard_coef: 0.067711/17 [==================>...........] - ETA: 0s - loss: 0.1717 - accuracy: 0.5792 - jacard_coef: 0.070312/17 [====================>.........] - ETA: 0s - loss: 0.1717 - accuracy: 0.5749 - jacard_coef: 0.071313/17 [=====================>........] - ETA: 0s - loss: 0.1717 - accuracy: 0.5725 - jacard_coef: 0.074914/17 [=======================>......] - ETA: 0s - loss: 0.1714 - accuracy: 0.5745 - jacard_coef: 0.076515/17 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.5690 - jacard_coef: 0.077716/17 [===========================>..] - ETA: 0s - loss: 0.1711 - accuracy: 0.5711 - jacard_coef: 0.078317/17 [==============================] - 2s 128ms/step - loss: 0.1711 - accuracy: 0.5713 - jacard_coef: 0.0821 - val_loss: 0.1515 - val_accuracy: 0.9081 - val_jacard_coef: 0.0104 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1674 - accuracy: 0.6038 - jacard_coef: 0.0624 2/17 [==>...........................] - ETA: 1s - loss: 0.1666 - accuracy: 0.6794 - jacard_coef: 0.0841 3/17 [====>.........................] - ETA: 1s - loss: 0.1653 - accuracy: 0.7562 - jacard_coef: 0.0615 4/17 [======>.......................] - ETA: 1s - loss: 0.1670 - accuracy: 0.7441 - jacard_coef: 0.0658 5/17 [=======>......................] - ETA: 1s - loss: 0.1661 - accuracy: 0.7474 - jacard_coef: 0.0647 6/17 [=========>....................] - ETA: 1s - loss: 0.1654 - accuracy: 0.7409 - jacard_coef: 0.0644 7/17 [===========>..................] - ETA: 1s - loss: 0.1650 - accuracy: 0.7476 - jacard_coef: 0.0645 8/17 [=============>................] - ETA: 1s - loss: 0.1645 - accuracy: 0.7485 - jacard_coef: 0.0617 9/17 [==============>...............] - ETA: 1s - loss: 0.1642 - accuracy: 0.7478 - jacard_coef: 0.062710/17 [================>.............] - ETA: 0s - loss: 0.1652 - accuracy: 0.7260 - jacard_coef: 0.063411/17 [==================>...........] - ETA: 0s - loss: 0.1653 - accuracy: 0.7333 - jacard_coef: 0.062312/17 [====================>.........] - ETA: 0s - loss: 0.1657 - accuracy: 0.7302 - jacard_coef: 0.063913/17 [=====================>........] - ETA: 0s - loss: 0.1658 - accuracy: 0.7239 - jacard_coef: 0.068114/17 [=======================>......] - ETA: 0s - loss: 0.1658 - accuracy: 0.7172 - jacard_coef: 0.068515/17 [=========================>....] - ETA: 0s - loss: 0.1660 - accuracy: 0.7165 - jacard_coef: 0.067016/17 [===========================>..] - ETA: 0s - loss: 0.1661 - accuracy: 0.7042 - jacard_coef: 0.069017/17 [==============================] - 2s 127ms/step - loss: 0.1662 - accuracy: 0.7022 - jacard_coef: 0.0651 - val_loss: 0.0843 - val_accuracy: 0.9183 - val_jacard_coef: 0.0077 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1657 - accuracy: 0.6898 - jacard_coef: 0.0713 2/17 [==>...........................] - ETA: 1s - loss: 0.1735 - accuracy: 0.6785 - jacard_coef: 0.0670 3/17 [====>.........................] - ETA: 1s - loss: 0.1700 - accuracy: 0.6932 - jacard_coef: 0.0651 4/17 [======>.......................] - ETA: 1s - loss: 0.1681 - accuracy: 0.6995 - jacard_coef: 0.0615 5/17 [=======>......................] - ETA: 1s - loss: 0.1671 - accuracy: 0.7224 - jacard_coef: 0.0591 6/17 [=========>....................] - ETA: 1s - loss: 0.1670 - accuracy: 0.7305 - jacard_coef: 0.0591 7/17 [===========>..................] - ETA: 1s - loss: 0.1661 - accuracy: 0.7307 - jacard_coef: 0.0612 8/17 [=============>................] - ETA: 1s - loss: 0.1656 - accuracy: 0.7332 - jacard_coef: 0.0621 9/17 [==============>...............] - ETA: 1s - loss: 0.1651 - accuracy: 0.7335 - jacard_coef: 0.062610/17 [================>.............] - ETA: 0s - loss: 0.1647 - accuracy: 0.7263 - jacard_coef: 0.064011/17 [==================>...........] - ETA: 0s - loss: 0.1642 - accuracy: 0.7391 - jacard_coef: 0.062712/17 [====================>.........] - ETA: 0s - loss: 0.1638 - accuracy: 0.7461 - jacard_coef: 0.060813/17 [=====================>........] - ETA: 0s - loss: 0.1633 - accuracy: 0.7574 - jacard_coef: 0.057814/17 [=======================>......] - ETA: 0s - loss: 0.1630 - accuracy: 0.7494 - jacard_coef: 0.057515/17 [=========================>....] - ETA: 0s - loss: 0.1629 - accuracy: 0.7490 - jacard_coef: 0.058616/17 [===========================>..] - ETA: 0s - loss: 0.1632 - accuracy: 0.7393 - jacard_coef: 0.061017/17 [==============================] - 2s 128ms/step - loss: 0.1634 - accuracy: 0.7384 - jacard_coef: 0.0650 - val_loss: 0.0963 - val_accuracy: 0.9180 - val_jacard_coef: 0.0076 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1591 - accuracy: 0.5912 - jacard_coef: 0.0763 2/17 [==>...........................] - ETA: 1s - loss: 0.1588 - accuracy: 0.6807 - jacard_coef: 0.0730 3/17 [====>.........................] - ETA: 1s - loss: 0.1595 - accuracy: 0.6861 - jacard_coef: 0.0774 4/17 [======>.......................] - ETA: 1s - loss: 0.1591 - accuracy: 0.7021 - jacard_coef: 0.0708 5/17 [=======>......................] - ETA: 1s - loss: 0.1619 - accuracy: 0.6846 - jacard_coef: 0.0788 6/17 [=========>....................] - ETA: 1s - loss: 0.1627 - accuracy: 0.7155 - jacard_coef: 0.0717 7/17 [===========>..................] - ETA: 1s - loss: 0.1624 - accuracy: 0.7389 - jacard_coef: 0.0653 8/17 [=============>................] - ETA: 1s - loss: 0.1627 - accuracy: 0.7410 - jacard_coef: 0.0665 9/17 [==============>...............] - ETA: 1s - loss: 0.1627 - accuracy: 0.7449 - jacard_coef: 0.065610/17 [================>.............] - ETA: 0s - loss: 0.1631 - accuracy: 0.7280 - jacard_coef: 0.068211/17 [==================>...........] - ETA: 0s - loss: 0.1631 - accuracy: 0.7324 - jacard_coef: 0.066912/17 [====================>.........] - ETA: 0s - loss: 0.1633 - accuracy: 0.7228 - jacard_coef: 0.065513/17 [=====================>........] - ETA: 0s - loss: 0.1641 - accuracy: 0.7335 - jacard_coef: 0.063714/17 [=======================>......] - ETA: 0s - loss: 0.1653 - accuracy: 0.7349 - jacard_coef: 0.064615/17 [=========================>....] - ETA: 0s - loss: 0.1656 - accuracy: 0.7329 - jacard_coef: 0.063216/17 [===========================>..] - ETA: 0s - loss: 0.1652 - accuracy: 0.7370 - jacard_coef: 0.062217/17 [==============================] - 2s 130ms/step - loss: 0.1652 - accuracy: 0.7381 - jacard_coef: 0.0588 - val_loss: 0.1652 - val_accuracy: 0.5866 - val_jacard_coef: 0.0574 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1709 - accuracy: 0.8911 - jacard_coef: 0.0488 2/17 [==>...........................] - ETA: 1s - loss: 0.1641 - accuracy: 0.8595 - jacard_coef: 0.0401 3/17 [====>.........................] - ETA: 1s - loss: 0.1626 - accuracy: 0.8046 - jacard_coef: 0.0523 4/17 [======>.......................] - ETA: 1s - loss: 0.1606 - accuracy: 0.8139 - jacard_coef: 0.0529 5/17 [=======>......................] - ETA: 1s - loss: 0.1597 - accuracy: 0.8039 - jacard_coef: 0.0583 6/17 [=========>....................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8102 - jacard_coef: 0.0554 7/17 [===========>..................] - ETA: 1s - loss: 0.1583 - accuracy: 0.8050 - jacard_coef: 0.0557 8/17 [=============>................] - ETA: 1s - loss: 0.1578 - accuracy: 0.7960 - jacard_coef: 0.0589 9/17 [==============>...............] - ETA: 1s - loss: 0.1574 - accuracy: 0.7996 - jacard_coef: 0.060010/17 [================>.............] - ETA: 0s - loss: 0.1569 - accuracy: 0.8055 - jacard_coef: 0.056711/17 [==================>...........] - ETA: 0s - loss: 0.1564 - accuracy: 0.8162 - jacard_coef: 0.051812/17 [====================>.........] - ETA: 0s - loss: 0.1559 - accuracy: 0.8251 - jacard_coef: 0.048113/17 [=====================>........] - ETA: 0s - loss: 0.1556 - accuracy: 0.8319 - jacard_coef: 0.044414/17 [=======================>......] - ETA: 0s - loss: 0.1550 - accuracy: 0.8394 - jacard_coef: 0.041215/17 [=========================>....] - ETA: 0s - loss: 0.1546 - accuracy: 0.8462 - jacard_coef: 0.039216/17 [===========================>..] - ETA: 0s - loss: 0.1542 - accuracy: 0.8511 - jacard_coef: 0.036817/17 [==============================] - 2s 130ms/step - loss: 0.1543 - accuracy: 0.8489 - jacard_coef: 0.0414 - val_loss: 0.1781 - val_accuracy: 0.4047 - val_jacard_coef: 0.0755 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1504 - accuracy: 0.9187 - jacard_coef: 2.3470e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1575 - accuracy: 0.8801 - jacard_coef: 0.0313     3/17 [====>.........................] - ETA: 1s - loss: 0.1564 - accuracy: 0.8529 - jacard_coef: 0.0391 4/17 [======>.......................] - ETA: 1s - loss: 0.1565 - accuracy: 0.8329 - jacard_coef: 0.0477 5/17 [=======>......................] - ETA: 1s - loss: 0.1560 - accuracy: 0.8224 - jacard_coef: 0.0469 6/17 [=========>....................] - ETA: 1s - loss: 0.1566 - accuracy: 0.8061 - jacard_coef: 0.0458 7/17 [===========>..................] - ETA: 1s - loss: 0.1562 - accuracy: 0.8102 - jacard_coef: 0.0430 8/17 [=============>................] - ETA: 1s - loss: 0.1560 - accuracy: 0.8150 - jacard_coef: 0.0409 9/17 [==============>...............] - ETA: 1s - loss: 0.1559 - accuracy: 0.8266 - jacard_coef: 0.038110/17 [================>.............] - ETA: 0s - loss: 0.1571 - accuracy: 0.8129 - jacard_coef: 0.037311/17 [==================>...........] - ETA: 0s - loss: 0.1564 - accuracy: 0.8195 - jacard_coef: 0.036612/17 [====================>.........] - ETA: 0s - loss: 0.1562 - accuracy: 0.8232 - jacard_coef: 0.037313/17 [=====================>........] - ETA: 0s - loss: 0.1560 - accuracy: 0.8282 - jacard_coef: 0.036414/17 [=======================>......] - ETA: 0s - loss: 0.1562 - accuracy: 0.8303 - jacard_coef: 0.037215/17 [=========================>....] - ETA: 0s - loss: 0.1561 - accuracy: 0.8319 - jacard_coef: 0.036416/17 [===========================>..] - ETA: 0s - loss: 0.1560 - accuracy: 0.8343 - jacard_coef: 0.036117/17 [==============================] - 2s 128ms/step - loss: 0.1562 - accuracy: 0.8335 - jacard_coef: 0.0371 - val_loss: 0.1787 - val_accuracy: 0.2318 - val_jacard_coef: 0.0678 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1504 - accuracy: 0.8883 - jacard_coef: 0.0095 2/17 [==>...........................] - ETA: 1s - loss: 0.1501 - accuracy: 0.8870 - jacard_coef: 0.0122 3/17 [====>.........................] - ETA: 1s - loss: 0.1524 - accuracy: 0.8910 - jacard_coef: 0.0171 4/17 [======>.......................] - ETA: 1s - loss: 0.1523 - accuracy: 0.8829 - jacard_coef: 0.0190 5/17 [=======>......................] - ETA: 1s - loss: 0.1515 - accuracy: 0.8762 - jacard_coef: 0.0226 6/17 [=========>....................] - ETA: 1s - loss: 0.1510 - accuracy: 0.8745 - jacard_coef: 0.0242 7/17 [===========>..................] - ETA: 1s - loss: 0.1506 - accuracy: 0.8767 - jacard_coef: 0.0244 8/17 [=============>................] - ETA: 1s - loss: 0.1507 - accuracy: 0.8764 - jacard_coef: 0.0267 9/17 [==============>...............] - ETA: 1s - loss: 0.1504 - accuracy: 0.8741 - jacard_coef: 0.028510/17 [================>.............] - ETA: 0s - loss: 0.1507 - accuracy: 0.8642 - jacard_coef: 0.037811/17 [==================>...........] - ETA: 0s - loss: 0.1504 - accuracy: 0.8632 - jacard_coef: 0.037812/17 [====================>.........] - ETA: 0s - loss: 0.1502 - accuracy: 0.8597 - jacard_coef: 0.038113/17 [=====================>........] - ETA: 0s - loss: 0.1497 - accuracy: 0.8537 - jacard_coef: 0.035714/17 [=======================>......] - ETA: 0s - loss: 0.1496 - accuracy: 0.8497 - jacard_coef: 0.037615/17 [=========================>....] - ETA: 0s - loss: 0.1493 - accuracy: 0.8510 - jacard_coef: 0.038116/17 [===========================>..] - ETA: 0s - loss: 0.1493 - accuracy: 0.8544 - jacard_coef: 0.036317/17 [==============================] - 2s 128ms/step - loss: 0.1493 - accuracy: 0.8543 - jacard_coef: 0.0345 - val_loss: 0.1654 - val_accuracy: 0.8213 - val_jacard_coef: 0.0317 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1496 - accuracy: 0.8783 - jacard_coef: 0.0142 2/17 [==>...........................] - ETA: 1s - loss: 0.1474 - accuracy: 0.8325 - jacard_coef: 0.0394 3/17 [====>.........................] - ETA: 1s - loss: 0.1465 - accuracy: 0.8326 - jacard_coef: 0.0344 4/17 [======>.......................] - ETA: 1s - loss: 0.1456 - accuracy: 0.8504 - jacard_coef: 0.0339 5/17 [=======>......................] - ETA: 1s - loss: 0.1466 - accuracy: 0.8272 - jacard_coef: 0.0329 6/17 [=========>....................] - ETA: 1s - loss: 0.1459 - accuracy: 0.8438 - jacard_coef: 0.0274 7/17 [===========>..................] - ETA: 1s - loss: 0.1465 - accuracy: 0.8525 - jacard_coef: 0.0267 8/17 [=============>................] - ETA: 1s - loss: 0.1463 - accuracy: 0.8568 - jacard_coef: 0.0233 9/17 [==============>...............] - ETA: 1s - loss: 0.1459 - accuracy: 0.8639 - jacard_coef: 0.022010/17 [================>.............] - ETA: 0s - loss: 0.1459 - accuracy: 0.8652 - jacard_coef: 0.019811/17 [==================>...........] - ETA: 0s - loss: 0.1456 - accuracy: 0.8681 - jacard_coef: 0.018712/17 [====================>.........] - ETA: 0s - loss: 0.1451 - accuracy: 0.8740 - jacard_coef: 0.017113/17 [=====================>........] - ETA: 0s - loss: 0.1449 - accuracy: 0.8765 - jacard_coef: 0.015814/17 [=======================>......] - ETA: 0s - loss: 0.1450 - accuracy: 0.8766 - jacard_coef: 0.016915/17 [=========================>....] - ETA: 0s - loss: 0.1447 - accuracy: 0.8784 - jacard_coef: 0.015816/17 [===========================>..] - ETA: 0s - loss: 0.1445 - accuracy: 0.8811 - jacard_coef: 0.014817/17 [==============================] - 2s 128ms/step - loss: 0.1445 - accuracy: 0.8814 - jacard_coef: 0.0139 - val_loss: 0.1809 - val_accuracy: 0.1947 - val_jacard_coef: 0.0680 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1417 - accuracy: 0.9249 - jacard_coef: 2.5402e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1399 - accuracy: 0.9323 - jacard_coef: 2.8488e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1392 - accuracy: 0.9374 - jacard_coef: 3.1165e-12 4/17 [======>.......................] - ETA: 1s - loss: 0.1389 - accuracy: 0.9384 - jacard_coef: 3.1482e-12 5/17 [=======>......................] - ETA: 1s - loss: 0.1410 - accuracy: 0.9289 - jacard_coef: 3.8122e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1403 - accuracy: 0.9324 - jacard_coef: 3.1768e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1402 - accuracy: 0.9324 - jacard_coef: 0.0010     8/17 [=============>................] - ETA: 1s - loss: 0.1403 - accuracy: 0.9265 - jacard_coef: 8.9960e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1400 - accuracy: 0.9259 - jacard_coef: 7.9964e-0410/17 [================>.............] - ETA: 0s - loss: 0.1400 - accuracy: 0.9230 - jacard_coef: 0.0018    11/17 [==================>...........] - ETA: 0s - loss: 0.1398 - accuracy: 0.9219 - jacard_coef: 0.001712/17 [====================>.........] - ETA: 0s - loss: 0.1399 - accuracy: 0.9186 - jacard_coef: 0.001513/17 [=====================>........] - ETA: 0s - loss: 0.1397 - accuracy: 0.9188 - jacard_coef: 0.001414/17 [=======================>......] - ETA: 0s - loss: 0.1396 - accuracy: 0.9187 - jacard_coef: 0.001415/17 [=========================>....] - ETA: 0s - loss: 0.1397 - accuracy: 0.9151 - jacard_coef: 0.001316/17 [===========================>..] - ETA: 0s - loss: 0.1395 - accuracy: 0.9157 - jacard_coef: 0.001317/17 [==============================] - 2s 128ms/step - loss: 0.1395 - accuracy: 0.9154 - jacard_coef: 0.0018 - val_loss: 0.1854 - val_accuracy: 0.1616 - val_jacard_coef: 0.0686 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1336 - accuracy: 0.9497 - jacard_coef: 3.7887e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1346 - accuracy: 0.9361 - jacard_coef: 3.1271e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1364 - accuracy: 0.9209 - jacard_coef: 0.0029     4/17 [======>.......................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9248 - jacard_coef: 0.0022 5/17 [=======>......................] - ETA: 1s - loss: 0.1362 - accuracy: 0.9161 - jacard_coef: 0.0081 6/17 [=========>....................] - ETA: 1s - loss: 0.1363 - accuracy: 0.9114 - jacard_coef: 0.0088 7/17 [===========>..................] - ETA: 1s - loss: 0.1360 - accuracy: 0.9150 - jacard_coef: 0.0076 8/17 [=============>................] - ETA: 1s - loss: 0.1360 - accuracy: 0.9136 - jacard_coef: 0.0066 9/17 [==============>...............] - ETA: 1s - loss: 0.1361 - accuracy: 0.9120 - jacard_coef: 0.005910/17 [================>.............] - ETA: 0s - loss: 0.1367 - accuracy: 0.9063 - jacard_coef: 0.005311/17 [==================>...........] - ETA: 0s - loss: 0.1364 - accuracy: 0.9086 - jacard_coef: 0.004812/17 [====================>.........] - ETA: 0s - loss: 0.1364 - accuracy: 0.9114 - jacard_coef: 0.004513/17 [=====================>........] - ETA: 0s - loss: 0.1365 - accuracy: 0.9098 - jacard_coef: 0.004214/17 [=======================>......] - ETA: 0s - loss: 0.1362 - accuracy: 0.9112 - jacard_coef: 0.003915/17 [=========================>....] - ETA: 0s - loss: 0.1362 - accuracy: 0.9110 - jacard_coef: 0.003616/17 [===========================>..] - ETA: 0s - loss: 0.1359 - accuracy: 0.9134 - jacard_coef: 0.003417/17 [==============================] - 2s 128ms/step - loss: 0.1360 - accuracy: 0.9129 - jacard_coef: 0.0032 - val_loss: 0.1558 - val_accuracy: 0.9300 - val_jacard_coef: 2.5806e-04 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1341 - accuracy: 0.9263 - jacard_coef: 2.5867e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1328 - accuracy: 0.9346 - jacard_coef: 2.9632e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1332 - accuracy: 0.9270 - jacard_coef: 2.6979e-12 4/17 [======>.......................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9323 - jacard_coef: 2.9392e-12 5/17 [=======>......................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9310 - jacard_coef: 2.8662e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1329 - accuracy: 0.9273 - jacard_coef: 2.7362e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1333 - accuracy: 0.9239 - jacard_coef: 2.6280e-12 8/17 [=============>................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9202 - jacard_coef: 2.5250e-12 9/17 [==============>...............] - ETA: 1s - loss: 0.1338 - accuracy: 0.9212 - jacard_coef: 2.5454e-1210/17 [================>.............] - ETA: 0s - loss: 0.1341 - accuracy: 0.9186 - jacard_coef: 2.4718e-1211/17 [==================>...........] - ETA: 0s - loss: 0.1342 - accuracy: 0.9167 - jacard_coef: 2.4171e-1212/17 [====================>.........] - ETA: 0s - loss: 0.1339 - accuracy: 0.9181 - jacard_coef: 2.4539e-1213/17 [=====================>........] - ETA: 0s - loss: 0.1339 - accuracy: 0.9170 - jacard_coef: 1.9768e-0514/17 [=======================>......] - ETA: 0s - loss: 0.1341 - accuracy: 0.9149 - jacard_coef: 1.8356e-0515/17 [=========================>....] - ETA: 0s - loss: 0.1339 - accuracy: 0.9154 - jacard_coef: 2.0821e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1336 - accuracy: 0.9169 - jacard_coef: 1.9714e-0417/17 [==============================] - 2s 128ms/step - loss: 0.1341 - accuracy: 0.9141 - jacard_coef: 0.0032 - val_loss: 0.1571 - val_accuracy: 0.9304 - val_jacard_coef: 3.4099e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1307 - accuracy: 0.9434 - jacard_coef: 3.3714e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1357 - accuracy: 0.9176 - jacard_coef: 2.5663e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9224 - jacard_coef: 0.0030     4/17 [======>.......................] - ETA: 1s - loss: 0.1388 - accuracy: 0.9192 - jacard_coef: 0.0082 5/17 [=======>......................] - ETA: 1s - loss: 0.1399 - accuracy: 0.9170 - jacard_coef: 0.0170 6/17 [=========>....................] - ETA: 1s - loss: 0.1400 - accuracy: 0.9163 - jacard_coef: 0.0197 7/17 [===========>..................] - ETA: 1s - loss: 0.1421 - accuracy: 0.9113 - jacard_coef: 0.0178 8/17 [=============>................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9068 - jacard_coef: 0.0168 9/17 [==============>...............] - ETA: 1s - loss: 0.1487 - accuracy: 0.9055 - jacard_coef: 0.016410/17 [================>.............] - ETA: 0s - loss: 0.1485 - accuracy: 0.9036 - jacard_coef: 0.014911/17 [==================>...........] - ETA: 0s - loss: 0.1485 - accuracy: 0.9002 - jacard_coef: 0.013812/17 [====================>.........] - ETA: 0s - loss: 0.1483 - accuracy: 0.8988 - jacard_coef: 0.012813/17 [=====================>........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9009 - jacard_coef: 0.012314/17 [=======================>......] - ETA: 0s - loss: 0.1484 - accuracy: 0.9021 - jacard_coef: 0.011415/17 [=========================>....] - ETA: 0s - loss: 0.1480 - accuracy: 0.9016 - jacard_coef: 0.010616/17 [===========================>..] - ETA: 0s - loss: 0.1484 - accuracy: 0.9037 - jacard_coef: 0.010117/17 [==============================] - 2s 128ms/step - loss: 0.1483 - accuracy: 0.9040 - jacard_coef: 0.0095 - val_loss: 0.9688 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1391 - accuracy: 0.9651 - jacard_coef: 5.4726e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9389 - jacard_coef: 3.8281e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1499 - accuracy: 0.9243 - jacard_coef: 3.1582e-12 4/17 [======>.......................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9222 - jacard_coef: 2.9367e-12 5/17 [=======>......................] - ETA: 1s - loss: 0.1458 - accuracy: 0.9111 - jacard_coef: 2.6350e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1444 - accuracy: 0.9113 - jacard_coef: 1.4495e-05 7/17 [===========>..................] - ETA: 1s - loss: 0.1428 - accuracy: 0.9158 - jacard_coef: 1.2424e-05 8/17 [=============>................] - ETA: 1s - loss: 0.1414 - accuracy: 0.9201 - jacard_coef: 1.0871e-05 9/17 [==============>...............] - ETA: 1s - loss: 0.1409 - accuracy: 0.9190 - jacard_coef: 9.6631e-0610/17 [================>.............] - ETA: 0s - loss: 0.1402 - accuracy: 0.9193 - jacard_coef: 8.6968e-0611/17 [==================>...........] - ETA: 0s - loss: 0.1405 - accuracy: 0.9188 - jacard_coef: 1.2641e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1402 - accuracy: 0.9186 - jacard_coef: 1.1587e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1399 - accuracy: 0.9168 - jacard_coef: 1.0696e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1397 - accuracy: 0.9159 - jacard_coef: 1.2354e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1392 - accuracy: 0.9170 - jacard_coef: 1.1530e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.9177 - jacard_coef: 1.0810e-0417/17 [==============================] - 2s 128ms/step - loss: 0.1388 - accuracy: 0.9172 - jacard_coef: 1.1392e-04 - val_loss: 0.3073 - val_accuracy: 0.9304 - val_jacard_coef: 8.4830e-05 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1384 - accuracy: 0.8755 - jacard_coef: 1.5325e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1356 - accuracy: 0.8924 - jacard_coef: 1.8167e-12 3/17 [====>.........................] - ETA: 1s - loss: 0.1331 - accuracy: 0.9133 - jacard_coef: 2.6252e-12 4/17 [======>.......................] - ETA: 1s - loss: 0.1329 - accuracy: 0.9146 - jacard_coef: 5.8517e-06 5/17 [=======>......................] - ETA: 1s - loss: 0.1332 - accuracy: 0.9150 - jacard_coef: 4.6813e-06 6/17 [=========>....................] - ETA: 1s - loss: 0.1333 - accuracy: 0.9130 - jacard_coef: 3.9011e-06 7/17 [===========>..................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9096 - jacard_coef: 3.3438e-06 8/17 [=============>................] - ETA: 1s - loss: 0.1334 - accuracy: 0.9123 - jacard_coef: 2.9258e-06 9/17 [==============>...............] - ETA: 1s - loss: 0.1348 - accuracy: 0.9121 - jacard_coef: 1.2060e-0510/17 [================>.............] - ETA: 0s - loss: 0.1344 - accuracy: 0.9138 - jacard_coef: 1.0854e-0511/17 [==================>...........] - ETA: 0s - loss: 0.1336 - accuracy: 0.9187 - jacard_coef: 9.8673e-0612/17 [====================>.........] - ETA: 0s - loss: 0.1336 - accuracy: 0.9186 - jacard_coef: 9.0450e-0613/17 [=====================>........] - ETA: 0s - loss: 0.1331 - accuracy: 0.9205 - jacard_coef: 8.3492e-0614/17 [=======================>......] - ETA: 0s - loss: 0.1332 - accuracy: 0.9197 - jacard_coef: 5.1462e-0515/17 [=========================>....] - ETA: 0s - loss: 0.1334 - accuracy: 0.9169 - jacard_coef: 4.8031e-0516/17 [===========================>..] - ETA: 0s - loss: 0.1332 - accuracy: 0.9168 - jacard_coef: 4.5029e-0517/17 [==============================] - 2s 128ms/step - loss: 0.1332 - accuracy: 0.9168 - jacard_coef: 4.2381e-05 - val_loss: 0.1330 - val_accuracy: 0.9300 - val_jacard_coef: 4.5350e-04 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1408 - accuracy: 0.8916 - jacard_coef: 0.0012 2/17 [==>...........................] - ETA: 1s - loss: 0.1347 - accuracy: 0.9115 - jacard_coef: 5.8859e-04 3/17 [====>.........................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9135 - jacard_coef: 3.9239e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1327 - accuracy: 0.9199 - jacard_coef: 2.9429e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9176 - jacard_coef: 2.3543e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1324 - accuracy: 0.9171 - jacard_coef: 3.1881e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1325 - accuracy: 0.9164 - jacard_coef: 2.7327e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1328 - accuracy: 0.9133 - jacard_coef: 2.5896e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1323 - accuracy: 0.9165 - jacard_coef: 6.2010e-0410/17 [================>.............] - ETA: 0s - loss: 0.1321 - accuracy: 0.9174 - jacard_coef: 6.4445e-0411/17 [==================>...........] - ETA: 0s - loss: 0.1315 - accuracy: 0.9219 - jacard_coef: 5.8586e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1320 - accuracy: 0.9170 - jacard_coef: 5.3704e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1318 - accuracy: 0.9175 - jacard_coef: 5.5304e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1318 - accuracy: 0.9172 - jacard_coef: 5.6558e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1316 - accuracy: 0.9189 - jacard_coef: 5.2787e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1322 - accuracy: 0.9168 - jacard_coef: 5.3452e-0417/17 [==============================] - 2s 128ms/step - loss: 0.1322 - accuracy: 0.9166 - jacard_coef: 5.0307e-04 - val_loss: 0.1383 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0755 (epoch 8)
  Final Val Loss: 0.1383
  Training Time: 0:01:23.444804
  Stability (std): 0.2417

Results saved to: hyperparameter_optimization_20250926_165036/exp_7_UNet_lr1e-3_bs8/UNet_lr0.001_bs8_results.json

Experiment 7 completed in 98s
Progress: 7/36 completed
Estimated remaining time: 47 minutes

ðŸ”¬ EXPERIMENT 8/36
================================================
Architecture: UNet
Learning Rate: 1e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877448.074332 1051952 service.cc:145] XLA service 0x15496db9ab70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877448.074361 1051952 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877448.210771 1051952 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 4:49 - loss: 0.3448 - accuracy: 0.5123 - jacard_coef: 0.07582/9 [=====>........................] - ETA: 50s - loss: 0.3120 - accuracy: 0.4809 - jacard_coef: 0.0788 3/9 [=========>....................] - ETA: 28s - loss: 0.2888 - accuracy: 0.4550 - jacard_coef: 0.08064/9 [============>.................] - ETA: 19s - loss: 0.2729 - accuracy: 0.4562 - jacard_coef: 0.08235/9 [===============>..............] - ETA: 12s - loss: 0.2623 - accuracy: 0.4908 - jacard_coef: 0.08236/9 [===================>..........] - ETA: 7s - loss: 0.2544 - accuracy: 0.5310 - jacard_coef: 0.0779 7/9 [======================>.......] - ETA: 4s - loss: 0.2484 - accuracy: 0.5673 - jacard_coef: 0.07528/9 [=========================>....] - ETA: 1s - loss: 0.2432 - accuracy: 0.5968 - jacard_coef: 0.07109/9 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.5982 - jacard_coef: 0.07669/9 [==============================] - 56s 2s/step - loss: 0.2429 - accuracy: 0.5982 - jacard_coef: 0.0766 - val_loss: 3.0357 - val_accuracy: 0.1218 - val_jacard_coef: 0.0715 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1995 - accuracy: 0.7973 - jacard_coef: 0.03272/9 [=====>........................] - ETA: 1s - loss: 0.1972 - accuracy: 0.7971 - jacard_coef: 0.04093/9 [=========>....................] - ETA: 1s - loss: 0.1944 - accuracy: 0.8054 - jacard_coef: 0.04674/9 [============>.................] - ETA: 1s - loss: 0.1928 - accuracy: 0.8191 - jacard_coef: 0.04265/9 [===============>..............] - ETA: 0s - loss: 0.1908 - accuracy: 0.8095 - jacard_coef: 0.04616/9 [===================>..........] - ETA: 0s - loss: 0.1886 - accuracy: 0.7961 - jacard_coef: 0.04977/9 [======================>.......] - ETA: 0s - loss: 0.1863 - accuracy: 0.7814 - jacard_coef: 0.05118/9 [=========================>....] - ETA: 0s - loss: 0.1845 - accuracy: 0.7744 - jacard_coef: 0.05349/9 [==============================] - 2s 229ms/step - loss: 0.1850 - accuracy: 0.7741 - jacard_coef: 0.0551 - val_loss: 1.9189 - val_accuracy: 0.8793 - val_jacard_coef: 0.0338 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1917 - accuracy: 0.8119 - jacard_coef: 0.05052/9 [=====>........................] - ETA: 1s - loss: 0.1866 - accuracy: 0.7301 - jacard_coef: 0.05133/9 [=========>....................] - ETA: 1s - loss: 0.1857 - accuracy: 0.6798 - jacard_coef: 0.06084/9 [============>.................] - ETA: 1s - loss: 0.1834 - accuracy: 0.6522 - jacard_coef: 0.06725/9 [===============>..............] - ETA: 0s - loss: 0.1818 - accuracy: 0.6487 - jacard_coef: 0.06826/9 [===================>..........] - ETA: 0s - loss: 0.1807 - accuracy: 0.6639 - jacard_coef: 0.06837/9 [======================>.......] - ETA: 0s - loss: 0.1798 - accuracy: 0.6838 - jacard_coef: 0.06548/9 [=========================>....] - ETA: 0s - loss: 0.1796 - accuracy: 0.7049 - jacard_coef: 0.06139/9 [==============================] - 2s 229ms/step - loss: 0.1796 - accuracy: 0.7049 - jacard_coef: 0.0556 - val_loss: 14.9366 - val_accuracy: 0.0732 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1748 - accuracy: 0.8449 - jacard_coef: 0.04322/9 [=====>........................] - ETA: 1s - loss: 0.1754 - accuracy: 0.8287 - jacard_coef: 0.05513/9 [=========>....................] - ETA: 1s - loss: 0.1757 - accuracy: 0.7932 - jacard_coef: 0.05414/9 [============>.................] - ETA: 1s - loss: 0.1751 - accuracy: 0.7588 - jacard_coef: 0.05455/9 [===============>..............] - ETA: 0s - loss: 0.1747 - accuracy: 0.7566 - jacard_coef: 0.05666/9 [===================>..........] - ETA: 0s - loss: 0.1743 - accuracy: 0.7713 - jacard_coef: 0.05487/9 [======================>.......] - ETA: 0s - loss: 0.1737 - accuracy: 0.7781 - jacard_coef: 0.05488/9 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.7706 - jacard_coef: 0.05599/9 [==============================] - 2s 229ms/step - loss: 0.1730 - accuracy: 0.7709 - jacard_coef: 0.0560 - val_loss: 14.9402 - val_accuracy: 0.0731 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1671 - accuracy: 0.7007 - jacard_coef: 0.06222/9 [=====>........................] - ETA: 1s - loss: 0.1667 - accuracy: 0.7117 - jacard_coef: 0.07093/9 [=========>....................] - ETA: 1s - loss: 0.1661 - accuracy: 0.7543 - jacard_coef: 0.06144/9 [============>.................] - ETA: 1s - loss: 0.1656 - accuracy: 0.7444 - jacard_coef: 0.06695/9 [===============>..............] - ETA: 0s - loss: 0.1656 - accuracy: 0.7669 - jacard_coef: 0.06246/9 [===================>..........] - ETA: 0s - loss: 0.1654 - accuracy: 0.7794 - jacard_coef: 0.06367/9 [======================>.......] - ETA: 0s - loss: 0.1654 - accuracy: 0.7895 - jacard_coef: 0.06148/9 [=========================>....] - ETA: 0s - loss: 0.1653 - accuracy: 0.7966 - jacard_coef: 0.06189/9 [==============================] - 2s 229ms/step - loss: 0.1665 - accuracy: 0.7947 - jacard_coef: 0.0550 - val_loss: 14.8118 - val_accuracy: 0.0799 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1647 - accuracy: 0.6743 - jacard_coef: 0.07982/9 [=====>........................] - ETA: 1s - loss: 0.1681 - accuracy: 0.6539 - jacard_coef: 0.07813/9 [=========>....................] - ETA: 1s - loss: 0.1689 - accuracy: 0.6512 - jacard_coef: 0.07694/9 [============>.................] - ETA: 1s - loss: 0.1696 - accuracy: 0.6426 - jacard_coef: 0.07625/9 [===============>..............] - ETA: 0s - loss: 0.1695 - accuracy: 0.6451 - jacard_coef: 0.07126/9 [===================>..........] - ETA: 0s - loss: 0.1694 - accuracy: 0.6643 - jacard_coef: 0.06927/9 [======================>.......] - ETA: 0s - loss: 0.1690 - accuracy: 0.6781 - jacard_coef: 0.07068/9 [=========================>....] - ETA: 0s - loss: 0.1689 - accuracy: 0.6867 - jacard_coef: 0.06999/9 [==============================] - 2s 229ms/step - loss: 0.1689 - accuracy: 0.6876 - jacard_coef: 0.0914 - val_loss: 13.3378 - val_accuracy: 0.0757 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1646 - accuracy: 0.6323 - jacard_coef: 0.10612/9 [=====>........................] - ETA: 1s - loss: 0.1660 - accuracy: 0.6306 - jacard_coef: 0.09463/9 [=========>....................] - ETA: 1s - loss: 0.1663 - accuracy: 0.6242 - jacard_coef: 0.08604/9 [============>.................] - ETA: 1s - loss: 0.1670 - accuracy: 0.6124 - jacard_coef: 0.07805/9 [===============>..............] - ETA: 0s - loss: 0.1663 - accuracy: 0.6135 - jacard_coef: 0.08106/9 [===================>..........] - ETA: 0s - loss: 0.1657 - accuracy: 0.6239 - jacard_coef: 0.08617/9 [======================>.......] - ETA: 0s - loss: 0.1654 - accuracy: 0.6395 - jacard_coef: 0.08318/9 [=========================>....] - ETA: 0s - loss: 0.1653 - accuracy: 0.6625 - jacard_coef: 0.07859/9 [==============================] - 2s 235ms/step - loss: 0.1658 - accuracy: 0.6616 - jacard_coef: 0.0856 - val_loss: 0.8949 - val_accuracy: 0.2750 - val_jacard_coef: 0.0735 - lr: 5.0000e-04
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1666 - accuracy: 0.8787 - jacard_coef: 0.01502/9 [=====>........................] - ETA: 1s - loss: 0.1662 - accuracy: 0.9089 - jacard_coef: 0.00853/9 [=========>....................] - ETA: 1s - loss: 0.1676 - accuracy: 0.9041 - jacard_coef: 0.00704/9 [============>.................] - ETA: 1s - loss: 0.1671 - accuracy: 0.9102 - jacard_coef: 0.00675/9 [===============>..............] - ETA: 0s - loss: 0.1676 - accuracy: 0.9106 - jacard_coef: 0.00716/9 [===================>..........] - ETA: 0s - loss: 0.1681 - accuracy: 0.9143 - jacard_coef: 0.00727/9 [======================>.......] - ETA: 0s - loss: 0.1687 - accuracy: 0.9126 - jacard_coef: 0.00708/9 [=========================>....] - ETA: 0s - loss: 0.1692 - accuracy: 0.9099 - jacard_coef: 0.00729/9 [==============================] - 2s 229ms/step - loss: 0.1693 - accuracy: 0.9099 - jacard_coef: 0.0097 - val_loss: 0.2894 - val_accuracy: 0.8884 - val_jacard_coef: 0.0215 - lr: 5.0000e-04
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1694 - accuracy: 0.9089 - jacard_coef: 0.00892/9 [=====>........................] - ETA: 1s - loss: 0.1712 - accuracy: 0.9027 - jacard_coef: 0.00873/9 [=========>....................] - ETA: 1s - loss: 0.1714 - accuracy: 0.9131 - jacard_coef: 0.00844/9 [============>.................] - ETA: 1s - loss: 0.1715 - accuracy: 0.9074 - jacard_coef: 0.01005/9 [===============>..............] - ETA: 0s - loss: 0.1719 - accuracy: 0.9044 - jacard_coef: 0.01016/9 [===================>..........] - ETA: 0s - loss: 0.1721 - accuracy: 0.9026 - jacard_coef: 0.00997/9 [======================>.......] - ETA: 0s - loss: 0.1716 - accuracy: 0.9044 - jacard_coef: 0.01028/9 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.9044 - jacard_coef: 0.01119/9 [==============================] - 2s 229ms/step - loss: 0.1713 - accuracy: 0.9048 - jacard_coef: 0.0108 - val_loss: 0.2260 - val_accuracy: 0.8912 - val_jacard_coef: 0.0238 - lr: 5.0000e-04
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1682 - accuracy: 0.9125 - jacard_coef: 0.00882/9 [=====>........................] - ETA: 1s - loss: 0.1673 - accuracy: 0.9129 - jacard_coef: 0.01103/9 [=========>....................] - ETA: 1s - loss: 0.1686 - accuracy: 0.9087 - jacard_coef: 0.01004/9 [============>.................] - ETA: 1s - loss: 0.1685 - accuracy: 0.9104 - jacard_coef: 0.01005/9 [===============>..............] - ETA: 0s - loss: 0.1680 - accuracy: 0.9047 - jacard_coef: 0.01056/9 [===================>..........] - ETA: 0s - loss: 0.1675 - accuracy: 0.9044 - jacard_coef: 0.01147/9 [======================>.......] - ETA: 0s - loss: 0.1675 - accuracy: 0.9020 - jacard_coef: 0.01118/9 [=========================>....] - ETA: 0s - loss: 0.1673 - accuracy: 0.9043 - jacard_coef: 0.01029/9 [==============================] - 2s 229ms/step - loss: 0.1674 - accuracy: 0.9049 - jacard_coef: 0.0090 - val_loss: 0.1976 - val_accuracy: 0.9058 - val_jacard_coef: 0.0171 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1638 - accuracy: 0.8948 - jacard_coef: 0.00712/9 [=====>........................] - ETA: 1s - loss: 0.1648 - accuracy: 0.9027 - jacard_coef: 0.00703/9 [=========>....................] - ETA: 1s - loss: 0.1646 - accuracy: 0.9033 - jacard_coef: 0.00654/9 [============>.................] - ETA: 1s - loss: 0.1646 - accuracy: 0.9039 - jacard_coef: 0.00675/9 [===============>..............] - ETA: 0s - loss: 0.1643 - accuracy: 0.9057 - jacard_coef: 0.00676/9 [===================>..........] - ETA: 0s - loss: 0.1641 - accuracy: 0.9079 - jacard_coef: 0.00657/9 [======================>.......] - ETA: 0s - loss: 0.1638 - accuracy: 0.9106 - jacard_coef: 0.00658/9 [=========================>....] - ETA: 0s - loss: 0.1634 - accuracy: 0.9110 - jacard_coef: 0.00639/9 [==============================] - 2s 229ms/step - loss: 0.1635 - accuracy: 0.9106 - jacard_coef: 0.0068 - val_loss: 0.1677 - val_accuracy: 0.9232 - val_jacard_coef: 0.0058 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1588 - accuracy: 0.9502 - jacard_coef: 0.00532/9 [=====>........................] - ETA: 1s - loss: 0.1597 - accuracy: 0.9433 - jacard_coef: 0.00343/9 [=========>....................] - ETA: 1s - loss: 0.1606 - accuracy: 0.9349 - jacard_coef: 0.00364/9 [============>.................] - ETA: 1s - loss: 0.1607 - accuracy: 0.9242 - jacard_coef: 0.00365/9 [===============>..............] - ETA: 0s - loss: 0.1615 - accuracy: 0.9176 - jacard_coef: 0.00376/9 [===================>..........] - ETA: 0s - loss: 0.1610 - accuracy: 0.9180 - jacard_coef: 0.00387/9 [======================>.......] - ETA: 0s - loss: 0.1610 - accuracy: 0.9130 - jacard_coef: 0.00388/9 [=========================>....] - ETA: 0s - loss: 0.1609 - accuracy: 0.9126 - jacard_coef: 0.00409/9 [==============================] - 2s 229ms/step - loss: 0.1609 - accuracy: 0.9129 - jacard_coef: 0.0057 - val_loss: 0.1533 - val_accuracy: 0.9269 - val_jacard_coef: 0.0029 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1615 - accuracy: 0.8923 - jacard_coef: 0.00332/9 [=====>........................] - ETA: 1s - loss: 0.1610 - accuracy: 0.8941 - jacard_coef: 0.00483/9 [=========>....................] - ETA: 1s - loss: 0.1606 - accuracy: 0.8948 - jacard_coef: 0.00534/9 [============>.................] - ETA: 1s - loss: 0.1600 - accuracy: 0.8967 - jacard_coef: 0.00605/9 [===============>..............] - ETA: 0s - loss: 0.1599 - accuracy: 0.9055 - jacard_coef: 0.00586/9 [===================>..........] - ETA: 0s - loss: 0.1599 - accuracy: 0.9048 - jacard_coef: 0.00567/9 [======================>.......] - ETA: 0s - loss: 0.1594 - accuracy: 0.9096 - jacard_coef: 0.00568/9 [=========================>....] - ETA: 0s - loss: 0.1591 - accuracy: 0.9113 - jacard_coef: 0.00599/9 [==============================] - 2s 229ms/step - loss: 0.1591 - accuracy: 0.9116 - jacard_coef: 0.0086 - val_loss: 0.1513 - val_accuracy: 0.9270 - val_jacard_coef: 0.0029 - lr: 2.5000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1566 - accuracy: 0.9052 - jacard_coef: 0.00722/9 [=====>........................] - ETA: 1s - loss: 0.1566 - accuracy: 0.9170 - jacard_coef: 0.00693/9 [=========>....................] - ETA: 1s - loss: 0.1566 - accuracy: 0.9130 - jacard_coef: 0.00634/9 [============>.................] - ETA: 1s - loss: 0.1573 - accuracy: 0.9088 - jacard_coef: 0.00585/9 [===============>..............] - ETA: 0s - loss: 0.1572 - accuracy: 0.9119 - jacard_coef: 0.00566/9 [===================>..........] - ETA: 0s - loss: 0.1576 - accuracy: 0.9099 - jacard_coef: 0.00527/9 [======================>.......] - ETA: 0s - loss: 0.1576 - accuracy: 0.9112 - jacard_coef: 0.00498/9 [=========================>....] - ETA: 0s - loss: 0.1574 - accuracy: 0.9118 - jacard_coef: 0.00499/9 [==============================] - 2s 229ms/step - loss: 0.1580 - accuracy: 0.9101 - jacard_coef: 0.0045 - val_loss: 0.1477 - val_accuracy: 0.9270 - val_jacard_coef: 0.0029 - lr: 2.5000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1569 - accuracy: 0.9144 - jacard_coef: 0.00132/9 [=====>........................] - ETA: 1s - loss: 0.1570 - accuracy: 0.9111 - jacard_coef: 0.00243/9 [=========>....................] - ETA: 1s - loss: 0.1572 - accuracy: 0.9038 - jacard_coef: 0.00264/9 [============>.................] - ETA: 1s - loss: 0.1580 - accuracy: 0.9055 - jacard_coef: 0.00285/9 [===============>..............] - ETA: 0s - loss: 0.1582 - accuracy: 0.9100 - jacard_coef: 0.00326/9 [===================>..........] - ETA: 0s - loss: 0.1580 - accuracy: 0.9085 - jacard_coef: 0.00457/9 [======================>.......] - ETA: 0s - loss: 0.1579 - accuracy: 0.9124 - jacard_coef: 0.00428/9 [=========================>....] - ETA: 0s - loss: 0.1578 - accuracy: 0.9142 - jacard_coef: 0.00459/9 [==============================] - 2s 229ms/step - loss: 0.1580 - accuracy: 0.9130 - jacard_coef: 0.0149 - val_loss: 0.1372 - val_accuracy: 0.9272 - val_jacard_coef: 0.0027 - lr: 2.5000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1581 - accuracy: 0.9171 - jacard_coef: 0.00332/9 [=====>........................] - ETA: 1s - loss: 0.1589 - accuracy: 0.9112 - jacard_coef: 0.00753/9 [=========>....................] - ETA: 1s - loss: 0.1584 - accuracy: 0.9144 - jacard_coef: 0.00844/9 [============>.................] - ETA: 1s - loss: 0.1589 - accuracy: 0.9093 - jacard_coef: 0.00865/9 [===============>..............] - ETA: 0s - loss: 0.1587 - accuracy: 0.9077 - jacard_coef: 0.00776/9 [===================>..........] - ETA: 0s - loss: 0.1584 - accuracy: 0.9072 - jacard_coef: 0.00677/9 [======================>.......] - ETA: 0s - loss: 0.1579 - accuracy: 0.9116 - jacard_coef: 0.00648/9 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.9119 - jacard_coef: 0.00609/9 [==============================] - 2s 229ms/step - loss: 0.1581 - accuracy: 0.9122 - jacard_coef: 0.0053 - val_loss: 0.1349 - val_accuracy: 0.9279 - val_jacard_coef: 0.0017 - lr: 2.5000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1580 - accuracy: 0.9043 - jacard_coef: 0.00162/9 [=====>........................] - ETA: 1s - loss: 0.1560 - accuracy: 0.9164 - jacard_coef: 0.00213/9 [=========>....................] - ETA: 1s - loss: 0.1559 - accuracy: 0.9208 - jacard_coef: 0.00204/9 [============>.................] - ETA: 1s - loss: 0.1559 - accuracy: 0.9172 - jacard_coef: 0.00205/9 [===============>..............] - ETA: 0s - loss: 0.1559 - accuracy: 0.9151 - jacard_coef: 0.00216/9 [===================>..........] - ETA: 0s - loss: 0.1555 - accuracy: 0.9175 - jacard_coef: 0.00217/9 [======================>.......] - ETA: 0s - loss: 0.1552 - accuracy: 0.9188 - jacard_coef: 0.00238/9 [=========================>....] - ETA: 0s - loss: 0.1555 - accuracy: 0.9140 - jacard_coef: 0.00239/9 [==============================] - 2s 230ms/step - loss: 0.1556 - accuracy: 0.9143 - jacard_coef: 0.0028 - val_loss: 0.1360 - val_accuracy: 0.9303 - val_jacard_coef: 1.4596e-12 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0735 (epoch 7)
  Final Val Loss: 0.1360
  Training Time: 0:01:29.553995
  Stability (std): 0.0475

Results saved to: hyperparameter_optimization_20250926_165036/exp_8_UNet_lr1e-3_bs16/UNet_lr0.001_bs16_results.json

Experiment 8 completed in 103s
Progress: 8/36 completed
Estimated remaining time: 48 minutes

ðŸ”¬ EXPERIMENT 9/36
================================================
Architecture: UNet
Learning Rate: 1e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877556.699486 1056063 service.cc:145] XLA service 0x148b38584cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877556.699513 1056063 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877556.835966 1056063 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 2:58 - loss: 0.3518 - accuracy: 0.5183 - jacard_coef: 0.07802/5 [===========>..................] - ETA: 37s - loss: 0.3420 - accuracy: 0.5149 - jacard_coef: 0.0814 3/5 [=================>............] - ETA: 16s - loss: 0.3074 - accuracy: 0.4438 - jacard_coef: 0.08264/5 [=======================>......] - ETA: 7s - loss: 0.2853 - accuracy: 0.3877 - jacard_coef: 0.0811 5/5 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.3857 - jacard_coef: 0.06495/5 [==============================] - 72s 7s/step - loss: 0.2848 - accuracy: 0.3857 - jacard_coef: 0.0649 - val_loss: 0.1363 - val_accuracy: 0.9280 - val_jacard_coef: 0.0014 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 1s - loss: 0.2058 - accuracy: 0.1563 - jacard_coef: 0.09472/5 [===========>..................] - ETA: 1s - loss: 0.2043 - accuracy: 0.1431 - jacard_coef: 0.08053/5 [=================>............] - ETA: 0s - loss: 0.2035 - accuracy: 0.1424 - jacard_coef: 0.07944/5 [=======================>......] - ETA: 0s - loss: 0.2021 - accuracy: 0.1460 - jacard_coef: 0.08215/5 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.1467 - jacard_coef: 0.09735/5 [==============================] - 2s 418ms/step - loss: 0.2019 - accuracy: 0.1467 - jacard_coef: 0.0973 - val_loss: 0.0853 - val_accuracy: 0.9304 - val_jacard_coef: 1.4606e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1941 - accuracy: 0.1791 - jacard_coef: 0.09212/5 [===========>..................] - ETA: 1s - loss: 0.1914 - accuracy: 0.1864 - jacard_coef: 0.09873/5 [=================>............] - ETA: 0s - loss: 0.1910 - accuracy: 0.1795 - jacard_coef: 0.08844/5 [=======================>......] - ETA: 0s - loss: 0.1899 - accuracy: 0.1805 - jacard_coef: 0.08525/5 [==============================] - 2s 388ms/step - loss: 0.1899 - accuracy: 0.1801 - jacard_coef: 0.0692 - val_loss: 0.8470 - val_accuracy: 0.9304 - val_jacard_coef: 1.4614e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1838 - accuracy: 0.2305 - jacard_coef: 0.07932/5 [===========>..................] - ETA: 1s - loss: 0.1820 - accuracy: 0.2635 - jacard_coef: 0.07923/5 [=================>............] - ETA: 0s - loss: 0.1809 - accuracy: 0.2881 - jacard_coef: 0.08164/5 [=======================>......] - ETA: 0s - loss: 0.1818 - accuracy: 0.2858 - jacard_coef: 0.08285/5 [==============================] - 2s 387ms/step - loss: 0.1818 - accuracy: 0.2860 - jacard_coef: 0.0976 - val_loss: 0.2778 - val_accuracy: 0.9295 - val_jacard_coef: 0.0011 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1816 - accuracy: 0.3156 - jacard_coef: 0.07782/5 [===========>..................] - ETA: 1s - loss: 0.1802 - accuracy: 0.3300 - jacard_coef: 0.08523/5 [=================>............] - ETA: 0s - loss: 0.1802 - accuracy: 0.3193 - jacard_coef: 0.08594/5 [=======================>......] - ETA: 0s - loss: 0.1809 - accuracy: 0.3096 - jacard_coef: 0.08415/5 [==============================] - 2s 399ms/step - loss: 0.1809 - accuracy: 0.3097 - jacard_coef: 0.0674 - val_loss: 0.7628 - val_accuracy: 0.9302 - val_jacard_coef: 0.0016 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1790 - accuracy: 0.3601 - jacard_coef: 0.06782/5 [===========>..................] - ETA: 1s - loss: 0.1789 - accuracy: 0.3807 - jacard_coef: 0.07613/5 [=================>............] - ETA: 0s - loss: 0.1782 - accuracy: 0.4028 - jacard_coef: 0.08094/5 [=======================>......] - ETA: 0s - loss: 0.1776 - accuracy: 0.4150 - jacard_coef: 0.08245/5 [==============================] - 2s 387ms/step - loss: 0.1776 - accuracy: 0.4154 - jacard_coef: 0.0934 - val_loss: 0.9981 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1762 - accuracy: 0.4117 - jacard_coef: 0.07282/5 [===========>..................] - ETA: 1s - loss: 0.1751 - accuracy: 0.4285 - jacard_coef: 0.07863/5 [=================>............] - ETA: 0s - loss: 0.1747 - accuracy: 0.4441 - jacard_coef: 0.08134/5 [=======================>......] - ETA: 0s - loss: 0.1745 - accuracy: 0.4587 - jacard_coef: 0.08295/5 [==============================] - 2s 387ms/step - loss: 0.1746 - accuracy: 0.4587 - jacard_coef: 0.1007 - val_loss: 1.0219 - val_accuracy: 0.9302 - val_jacard_coef: 1.4569e-12 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1745 - accuracy: 0.4249 - jacard_coef: 0.07282/5 [===========>..................] - ETA: 1s - loss: 0.1761 - accuracy: 0.4087 - jacard_coef: 0.08513/5 [=================>............] - ETA: 0s - loss: 0.1755 - accuracy: 0.4164 - jacard_coef: 0.08294/5 [=======================>......] - ETA: 0s - loss: 0.1756 - accuracy: 0.4291 - jacard_coef: 0.08005/5 [==============================] - 2s 387ms/step - loss: 0.1757 - accuracy: 0.4295 - jacard_coef: 0.0891 - val_loss: 0.1310 - val_accuracy: 0.9295 - val_jacard_coef: 0.0014 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1765 - accuracy: 0.5027 - jacard_coef: 0.07542/5 [===========>..................] - ETA: 1s - loss: 0.1764 - accuracy: 0.5140 - jacard_coef: 0.07313/5 [=================>............] - ETA: 0s - loss: 0.1761 - accuracy: 0.5294 - jacard_coef: 0.07044/5 [=======================>......] - ETA: 0s - loss: 0.1757 - accuracy: 0.5412 - jacard_coef: 0.07095/5 [==============================] - 2s 398ms/step - loss: 0.1757 - accuracy: 0.5403 - jacard_coef: 0.0650 - val_loss: 0.1419 - val_accuracy: 0.8413 - val_jacard_coef: 0.0470 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1738 - accuracy: 0.4870 - jacard_coef: 0.07972/5 [===========>..................] - ETA: 1s - loss: 0.1741 - accuracy: 0.5062 - jacard_coef: 0.08073/5 [=================>............] - ETA: 0s - loss: 0.1736 - accuracy: 0.5280 - jacard_coef: 0.07674/5 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.5427 - jacard_coef: 0.08005/5 [==============================] - 2s 388ms/step - loss: 0.1732 - accuracy: 0.5429 - jacard_coef: 0.0868 - val_loss: 0.1146 - val_accuracy: 0.9303 - val_jacard_coef: 1.4596e-12 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1723 - accuracy: 0.5915 - jacard_coef: 0.06072/5 [===========>..................] - ETA: 1s - loss: 0.1709 - accuracy: 0.6236 - jacard_coef: 0.07423/5 [=================>............] - ETA: 0s - loss: 0.1701 - accuracy: 0.6491 - jacard_coef: 0.07214/5 [=======================>......] - ETA: 0s - loss: 0.1698 - accuracy: 0.6556 - jacard_coef: 0.07745/5 [==============================] - 2s 387ms/step - loss: 0.1698 - accuracy: 0.6553 - jacard_coef: 0.0949 - val_loss: 0.1056 - val_accuracy: 0.9303 - val_jacard_coef: 2.9188e-05 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1664 - accuracy: 0.6519 - jacard_coef: 0.07342/5 [===========>..................] - ETA: 1s - loss: 0.1661 - accuracy: 0.6343 - jacard_coef: 0.08403/5 [=================>............] - ETA: 0s - loss: 0.1659 - accuracy: 0.6277 - jacard_coef: 0.08304/5 [=======================>......] - ETA: 0s - loss: 0.1654 - accuracy: 0.6504 - jacard_coef: 0.08095/5 [==============================] - 2s 388ms/step - loss: 0.1654 - accuracy: 0.6512 - jacard_coef: 0.1163 - val_loss: 0.0827 - val_accuracy: 0.9303 - val_jacard_coef: 1.4603e-12 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1635 - accuracy: 0.6079 - jacard_coef: 0.10502/5 [===========>..................] - ETA: 1s - loss: 0.1651 - accuracy: 0.5725 - jacard_coef: 0.08743/5 [=================>............] - ETA: 0s - loss: 0.1651 - accuracy: 0.5739 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1647 - accuracy: 0.5884 - jacard_coef: 0.07865/5 [==============================] - 2s 388ms/step - loss: 0.1647 - accuracy: 0.5890 - jacard_coef: 0.0671 - val_loss: 0.2334 - val_accuracy: 0.9304 - val_jacard_coef: 1.4611e-05 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1629 - accuracy: 0.8260 - jacard_coef: 0.04692/5 [===========>..................] - ETA: 1s - loss: 0.1626 - accuracy: 0.8655 - jacard_coef: 0.02943/5 [=================>............] - ETA: 0s - loss: 0.1624 - accuracy: 0.8591 - jacard_coef: 0.03804/5 [=======================>......] - ETA: 0s - loss: 0.1621 - accuracy: 0.8356 - jacard_coef: 0.04225/5 [==============================] - 2s 388ms/step - loss: 0.1621 - accuracy: 0.8343 - jacard_coef: 0.0657 - val_loss: 0.3887 - val_accuracy: 0.9303 - val_jacard_coef: 1.4591e-12 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1612 - accuracy: 0.7009 - jacard_coef: 0.07362/5 [===========>..................] - ETA: 1s - loss: 0.1606 - accuracy: 0.7123 - jacard_coef: 0.07033/5 [=================>............] - ETA: 0s - loss: 0.1607 - accuracy: 0.7190 - jacard_coef: 0.06814/5 [=======================>......] - ETA: 0s - loss: 0.1605 - accuracy: 0.7316 - jacard_coef: 0.06825/5 [==============================] - 2s 388ms/step - loss: 0.1605 - accuracy: 0.7324 - jacard_coef: 0.0553 - val_loss: 0.2532 - val_accuracy: 0.9278 - val_jacard_coef: 1.1272e-04 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1593 - accuracy: 0.8127 - jacard_coef: 0.05382/5 [===========>..................] - ETA: 1s - loss: 0.1594 - accuracy: 0.8335 - jacard_coef: 0.04773/5 [=================>............] - ETA: 0s - loss: 0.1594 - accuracy: 0.8463 - jacard_coef: 0.04174/5 [=======================>......] - ETA: 0s - loss: 0.1593 - accuracy: 0.8564 - jacard_coef: 0.03885/5 [==============================] - 2s 387ms/step - loss: 0.1593 - accuracy: 0.8567 - jacard_coef: 0.0365 - val_loss: 0.1789 - val_accuracy: 0.9228 - val_jacard_coef: 5.9271e-04 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1587 - accuracy: 0.8761 - jacard_coef: 0.03542/5 [===========>..................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8751 - jacard_coef: 0.03513/5 [=================>............] - ETA: 0s - loss: 0.1585 - accuracy: 0.8750 - jacard_coef: 0.03854/5 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8713 - jacard_coef: 0.04125/5 [==============================] - 2s 388ms/step - loss: 0.1586 - accuracy: 0.8675 - jacard_coef: 0.0481 - val_loss: 0.0955 - val_accuracy: 0.9183 - val_jacard_coef: 0.0023 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1597 - accuracy: 0.9092 - jacard_coef: 0.00182/5 [===========>..................] - ETA: 1s - loss: 0.1610 - accuracy: 0.9043 - jacard_coef: 0.00513/5 [=================>............] - ETA: 0s - loss: 0.1620 - accuracy: 0.9068 - jacard_coef: 0.00594/5 [=======================>......] - ETA: 0s - loss: 0.1625 - accuracy: 0.9050 - jacard_coef: 0.00675/5 [==============================] - 2s 387ms/step - loss: 0.1626 - accuracy: 0.9032 - jacard_coef: 0.0201 - val_loss: 0.1576 - val_accuracy: 0.9244 - val_jacard_coef: 0.0049 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1651 - accuracy: 0.8443 - jacard_coef: 0.04112/5 [===========>..................] - ETA: 1s - loss: 0.1647 - accuracy: 0.8540 - jacard_coef: 0.04023/5 [=================>............] - ETA: 0s - loss: 0.1650 - accuracy: 0.8377 - jacard_coef: 0.04444/5 [=======================>......] - ETA: 0s - loss: 0.1651 - accuracy: 0.8337 - jacard_coef: 0.04815/5 [==============================] - 2s 388ms/step - loss: 0.1651 - accuracy: 0.8346 - jacard_coef: 0.0411 - val_loss: 0.1492 - val_accuracy: 0.9293 - val_jacard_coef: 0.0016 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0470 (epoch 9)
  Final Val Loss: 0.1492
  Training Time: 0:01:48.979806
  Stability (std): 0.0891

Results saved to: hyperparameter_optimization_20250926_165036/exp_9_UNet_lr1e-3_bs32/UNet_lr0.001_bs32_results.json

Experiment 9 completed in 124s
Progress: 9/36 completed
Estimated remaining time: 55 minutes

ðŸ”¬ EXPERIMENT 10/36
================================================
Architecture: UNet
Learning Rate: 5e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877672.757643 1060163 service.cc:145] XLA service 0x1523cdc866e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877672.757666 1060163 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877672.894405 1060163 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 8:32 - loss: 0.3419 - accuracy: 0.5166 - jacard_coef: 0.0996 2/17 [==>...........................] - ETA: 55s - loss: 0.3346 - accuracy: 0.5530 - jacard_coef: 0.0865  3/17 [====>.........................] - ETA: 38s - loss: 0.3185 - accuracy: 0.4977 - jacard_coef: 0.0860 4/17 [======>.......................] - ETA: 29s - loss: 0.3134 - accuracy: 0.5289 - jacard_coef: 0.0832 5/17 [=======>......................] - ETA: 20s - loss: 0.3025 - accuracy: 0.5037 - jacard_coef: 0.0831 6/17 [=========>....................] - ETA: 15s - loss: 0.2948 - accuracy: 0.4916 - jacard_coef: 0.0837 7/17 [===========>..................] - ETA: 12s - loss: 0.2891 - accuracy: 0.4891 - jacard_coef: 0.0809 8/17 [=============>................] - ETA: 9s - loss: 0.2817 - accuracy: 0.4646 - jacard_coef: 0.0833  9/17 [==============>...............] - ETA: 7s - loss: 0.2748 - accuracy: 0.4295 - jacard_coef: 0.078710/17 [================>.............] - ETA: 5s - loss: 0.2683 - accuracy: 0.4062 - jacard_coef: 0.082211/17 [==================>...........] - ETA: 4s - loss: 0.2637 - accuracy: 0.3862 - jacard_coef: 0.081912/17 [====================>.........] - ETA: 3s - loss: 0.2610 - accuracy: 0.3674 - jacard_coef: 0.083213/17 [=====================>........] - ETA: 2s - loss: 0.2577 - accuracy: 0.3508 - jacard_coef: 0.082014/17 [=======================>......] - ETA: 1s - loss: 0.2536 - accuracy: 0.3353 - jacard_coef: 0.080315/17 [=========================>....] - ETA: 1s - loss: 0.2500 - accuracy: 0.3214 - jacard_coef: 0.079216/17 [===========================>..] - ETA: 0s - loss: 0.2466 - accuracy: 0.3109 - jacard_coef: 0.080817/17 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.3090 - jacard_coef: 0.076617/17 [==============================] - 47s 919ms/step - loss: 0.2462 - accuracy: 0.3090 - jacard_coef: 0.0766 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1947 - accuracy: 0.1407 - jacard_coef: 0.0766 2/17 [==>...........................] - ETA: 1s - loss: 0.1955 - accuracy: 0.1270 - jacard_coef: 0.0656 3/17 [====>.........................] - ETA: 1s - loss: 0.1941 - accuracy: 0.1311 - jacard_coef: 0.0741 4/17 [======>.......................] - ETA: 1s - loss: 0.1930 - accuracy: 0.1303 - jacard_coef: 0.0750 5/17 [=======>......................] - ETA: 1s - loss: 0.1923 - accuracy: 0.1362 - jacard_coef: 0.0808 6/17 [=========>....................] - ETA: 1s - loss: 0.1922 - accuracy: 0.1314 - jacard_coef: 0.0736 7/17 [===========>..................] - ETA: 1s - loss: 0.1911 - accuracy: 0.1395 - jacard_coef: 0.0788 8/17 [=============>................] - ETA: 1s - loss: 0.1917 - accuracy: 0.1425 - jacard_coef: 0.0809 9/17 [==============>...............] - ETA: 1s - loss: 0.1910 - accuracy: 0.1483 - jacard_coef: 0.079110/17 [================>.............] - ETA: 0s - loss: 0.1902 - accuracy: 0.1596 - jacard_coef: 0.081111/17 [==================>...........] - ETA: 0s - loss: 0.1895 - accuracy: 0.1704 - jacard_coef: 0.083812/17 [====================>.........] - ETA: 0s - loss: 0.1886 - accuracy: 0.1774 - jacard_coef: 0.082313/17 [=====================>........] - ETA: 0s - loss: 0.1878 - accuracy: 0.1873 - jacard_coef: 0.081714/17 [=======================>......] - ETA: 0s - loss: 0.1872 - accuracy: 0.1982 - jacard_coef: 0.081215/17 [=========================>....] - ETA: 0s - loss: 0.1866 - accuracy: 0.2176 - jacard_coef: 0.081016/17 [===========================>..] - ETA: 0s - loss: 0.1862 - accuracy: 0.2383 - jacard_coef: 0.080917/17 [==============================] - 2s 131ms/step - loss: 0.1868 - accuracy: 0.2391 - jacard_coef: 0.0833 - val_loss: 1.1345 - val_accuracy: 0.9286 - val_jacard_coef: 0.0014 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1748 - accuracy: 0.5656 - jacard_coef: 0.1089 2/17 [==>...........................] - ETA: 1s - loss: 0.1810 - accuracy: 0.5472 - jacard_coef: 0.0845 3/17 [====>.........................] - ETA: 1s - loss: 0.1827 - accuracy: 0.5350 - jacard_coef: 0.0843 4/17 [======>.......................] - ETA: 1s - loss: 0.1834 - accuracy: 0.5230 - jacard_coef: 0.0880 5/17 [=======>......................] - ETA: 1s - loss: 0.1843 - accuracy: 0.5041 - jacard_coef: 0.0897 6/17 [=========>....................] - ETA: 1s - loss: 0.1839 - accuracy: 0.4920 - jacard_coef: 0.0844 7/17 [===========>..................] - ETA: 1s - loss: 0.1834 - accuracy: 0.4786 - jacard_coef: 0.0802 8/17 [=============>................] - ETA: 1s - loss: 0.1829 - accuracy: 0.4712 - jacard_coef: 0.0841 9/17 [==============>...............] - ETA: 1s - loss: 0.1825 - accuracy: 0.4669 - jacard_coef: 0.079510/17 [================>.............] - ETA: 0s - loss: 0.1820 - accuracy: 0.4590 - jacard_coef: 0.079711/17 [==================>...........] - ETA: 0s - loss: 0.1820 - accuracy: 0.4460 - jacard_coef: 0.081312/17 [====================>.........] - ETA: 0s - loss: 0.1818 - accuracy: 0.4339 - jacard_coef: 0.081413/17 [=====================>........] - ETA: 0s - loss: 0.1815 - accuracy: 0.4276 - jacard_coef: 0.081014/17 [=======================>......] - ETA: 0s - loss: 0.1813 - accuracy: 0.4205 - jacard_coef: 0.082115/17 [=========================>....] - ETA: 0s - loss: 0.1811 - accuracy: 0.4132 - jacard_coef: 0.082116/17 [===========================>..] - ETA: 0s - loss: 0.1808 - accuracy: 0.4071 - jacard_coef: 0.081617/17 [==============================] - 2s 128ms/step - loss: 0.1809 - accuracy: 0.4062 - jacard_coef: 0.0829 - val_loss: 0.6179 - val_accuracy: 0.9302 - val_jacard_coef: 3.3985e-12 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1748 - accuracy: 0.4301 - jacard_coef: 0.0686 2/17 [==>...........................] - ETA: 1s - loss: 0.1758 - accuracy: 0.4435 - jacard_coef: 0.0737 3/17 [====>.........................] - ETA: 1s - loss: 0.1743 - accuracy: 0.4518 - jacard_coef: 0.0664 4/17 [======>.......................] - ETA: 1s - loss: 0.1736 - accuracy: 0.4574 - jacard_coef: 0.0588 5/17 [=======>......................] - ETA: 1s - loss: 0.1732 - accuracy: 0.4586 - jacard_coef: 0.0595 6/17 [=========>....................] - ETA: 1s - loss: 0.1727 - accuracy: 0.4638 - jacard_coef: 0.0639 7/17 [===========>..................] - ETA: 1s - loss: 0.1726 - accuracy: 0.4646 - jacard_coef: 0.0617 8/17 [=============>................] - ETA: 1s - loss: 0.1721 - accuracy: 0.4979 - jacard_coef: 0.0619 9/17 [==============>...............] - ETA: 1s - loss: 0.1719 - accuracy: 0.5230 - jacard_coef: 0.063310/17 [================>.............] - ETA: 0s - loss: 0.1714 - accuracy: 0.5476 - jacard_coef: 0.061911/17 [==================>...........] - ETA: 0s - loss: 0.1711 - accuracy: 0.5581 - jacard_coef: 0.065212/17 [====================>.........] - ETA: 0s - loss: 0.1706 - accuracy: 0.5683 - jacard_coef: 0.066313/17 [=====================>........] - ETA: 0s - loss: 0.1702 - accuracy: 0.5756 - jacard_coef: 0.068114/17 [=======================>......] - ETA: 0s - loss: 0.1698 - accuracy: 0.5835 - jacard_coef: 0.074115/17 [=========================>....] - ETA: 0s - loss: 0.1696 - accuracy: 0.5858 - jacard_coef: 0.075216/17 [===========================>..] - ETA: 0s - loss: 0.1695 - accuracy: 0.5897 - jacard_coef: 0.078017/17 [==============================] - 2s 130ms/step - loss: 0.1697 - accuracy: 0.5879 - jacard_coef: 0.0799 - val_loss: 0.4143 - val_accuracy: 0.9266 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1702 - accuracy: 0.6786 - jacard_coef: 0.0672 2/17 [==>...........................] - ETA: 1s - loss: 0.1710 - accuracy: 0.7144 - jacard_coef: 0.0599 3/17 [====>.........................] - ETA: 1s - loss: 0.1702 - accuracy: 0.7290 - jacard_coef: 0.0722 4/17 [======>.......................] - ETA: 1s - loss: 0.1696 - accuracy: 0.7240 - jacard_coef: 0.0589 5/17 [=======>......................] - ETA: 1s - loss: 0.1688 - accuracy: 0.7177 - jacard_coef: 0.0592 6/17 [=========>....................] - ETA: 1s - loss: 0.1686 - accuracy: 0.7057 - jacard_coef: 0.0599 7/17 [===========>..................] - ETA: 1s - loss: 0.1688 - accuracy: 0.6822 - jacard_coef: 0.0673 8/17 [=============>................] - ETA: 1s - loss: 0.1695 - accuracy: 0.6500 - jacard_coef: 0.0714 9/17 [==============>...............] - ETA: 1s - loss: 0.1695 - accuracy: 0.6467 - jacard_coef: 0.072510/17 [================>.............] - ETA: 0s - loss: 0.1690 - accuracy: 0.6612 - jacard_coef: 0.072011/17 [==================>...........] - ETA: 0s - loss: 0.1686 - accuracy: 0.6732 - jacard_coef: 0.074412/17 [====================>.........] - ETA: 0s - loss: 0.1684 - accuracy: 0.6810 - jacard_coef: 0.073913/17 [=====================>........] - ETA: 0s - loss: 0.1683 - accuracy: 0.6841 - jacard_coef: 0.074314/17 [=======================>......] - ETA: 0s - loss: 0.1683 - accuracy: 0.6953 - jacard_coef: 0.071015/17 [=========================>....] - ETA: 0s - loss: 0.1681 - accuracy: 0.7003 - jacard_coef: 0.069116/17 [===========================>..] - ETA: 0s - loss: 0.1678 - accuracy: 0.7064 - jacard_coef: 0.068117/17 [==============================] - 2s 130ms/step - loss: 0.1678 - accuracy: 0.7079 - jacard_coef: 0.0641 - val_loss: 0.1413 - val_accuracy: 0.8707 - val_jacard_coef: 0.0251 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1624 - accuracy: 0.8075 - jacard_coef: 0.0513 2/17 [==>...........................] - ETA: 1s - loss: 0.1630 - accuracy: 0.8066 - jacard_coef: 0.0575 3/17 [====>.........................] - ETA: 1s - loss: 0.1625 - accuracy: 0.8223 - jacard_coef: 0.0497 4/17 [======>.......................] - ETA: 1s - loss: 0.1626 - accuracy: 0.8069 - jacard_coef: 0.0576 5/17 [=======>......................] - ETA: 1s - loss: 0.1627 - accuracy: 0.8184 - jacard_coef: 0.0583 6/17 [=========>....................] - ETA: 1s - loss: 0.1621 - accuracy: 0.8324 - jacard_coef: 0.0561 7/17 [===========>..................] - ETA: 1s - loss: 0.1621 - accuracy: 0.8333 - jacard_coef: 0.0524 8/17 [=============>................] - ETA: 1s - loss: 0.1619 - accuracy: 0.8366 - jacard_coef: 0.0517 9/17 [==============>...............] - ETA: 1s - loss: 0.1616 - accuracy: 0.8434 - jacard_coef: 0.047610/17 [================>.............] - ETA: 0s - loss: 0.1619 - accuracy: 0.8453 - jacard_coef: 0.044111/17 [==================>...........] - ETA: 0s - loss: 0.1615 - accuracy: 0.8519 - jacard_coef: 0.041412/17 [====================>.........] - ETA: 0s - loss: 0.1612 - accuracy: 0.8537 - jacard_coef: 0.042513/17 [=====================>........] - ETA: 0s - loss: 0.1609 - accuracy: 0.8593 - jacard_coef: 0.039614/17 [=======================>......] - ETA: 0s - loss: 0.1606 - accuracy: 0.8620 - jacard_coef: 0.036815/17 [=========================>....] - ETA: 0s - loss: 0.1604 - accuracy: 0.8661 - jacard_coef: 0.036316/17 [===========================>..] - ETA: 0s - loss: 0.1603 - accuracy: 0.8681 - jacard_coef: 0.034517/17 [==============================] - 2s 130ms/step - loss: 0.1604 - accuracy: 0.8657 - jacard_coef: 0.0356 - val_loss: 0.1850 - val_accuracy: 0.1935 - val_jacard_coef: 0.0672 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1580 - accuracy: 0.8329 - jacard_coef: 0.0463 2/17 [==>...........................] - ETA: 1s - loss: 0.1579 - accuracy: 0.8161 - jacard_coef: 0.0630 3/17 [====>.........................] - ETA: 1s - loss: 0.1578 - accuracy: 0.8005 - jacard_coef: 0.0660 4/17 [======>.......................] - ETA: 1s - loss: 0.1577 - accuracy: 0.7884 - jacard_coef: 0.0690 5/17 [=======>......................] - ETA: 1s - loss: 0.1581 - accuracy: 0.7721 - jacard_coef: 0.0623 6/17 [=========>....................] - ETA: 1s - loss: 0.1580 - accuracy: 0.7650 - jacard_coef: 0.0582 7/17 [===========>..................] - ETA: 1s - loss: 0.1578 - accuracy: 0.7688 - jacard_coef: 0.0661 8/17 [=============>................] - ETA: 1s - loss: 0.1578 - accuracy: 0.7819 - jacard_coef: 0.0607 9/17 [==============>...............] - ETA: 1s - loss: 0.1575 - accuracy: 0.7975 - jacard_coef: 0.054610/17 [================>.............] - ETA: 0s - loss: 0.1574 - accuracy: 0.8094 - jacard_coef: 0.049311/17 [==================>...........] - ETA: 0s - loss: 0.1572 - accuracy: 0.8175 - jacard_coef: 0.044912/17 [====================>.........] - ETA: 0s - loss: 0.1573 - accuracy: 0.8244 - jacard_coef: 0.041313/17 [=====================>........] - ETA: 0s - loss: 0.1570 - accuracy: 0.8319 - jacard_coef: 0.038414/17 [=======================>......] - ETA: 0s - loss: 0.1566 - accuracy: 0.8413 - jacard_coef: 0.035715/17 [=========================>....] - ETA: 0s - loss: 0.1563 - accuracy: 0.8475 - jacard_coef: 0.033416/17 [===========================>..] - ETA: 0s - loss: 0.1561 - accuracy: 0.8533 - jacard_coef: 0.031317/17 [==============================] - 2s 130ms/step - loss: 0.1562 - accuracy: 0.8534 - jacard_coef: 0.0299 - val_loss: 0.1737 - val_accuracy: 0.3298 - val_jacard_coef: 0.0702 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1514 - accuracy: 0.9510 - jacard_coef: 1.9459e-04 2/17 [==>...........................] - ETA: 1s - loss: 0.1526 - accuracy: 0.9213 - jacard_coef: 0.0026     3/17 [====>.........................] - ETA: 1s - loss: 0.1528 - accuracy: 0.9192 - jacard_coef: 0.0035 4/17 [======>.......................] - ETA: 1s - loss: 0.1528 - accuracy: 0.9182 - jacard_coef: 0.0037 5/17 [=======>......................] - ETA: 1s - loss: 0.1531 - accuracy: 0.9113 - jacard_coef: 0.0030 6/17 [=========>....................] - ETA: 1s - loss: 0.1532 - accuracy: 0.9125 - jacard_coef: 0.0031 7/17 [===========>..................] - ETA: 1s - loss: 0.1529 - accuracy: 0.9140 - jacard_coef: 0.0028 8/17 [=============>................] - ETA: 1s - loss: 0.1526 - accuracy: 0.9137 - jacard_coef: 0.0025 9/17 [==============>...............] - ETA: 1s - loss: 0.1524 - accuracy: 0.9126 - jacard_coef: 0.002210/17 [================>.............] - ETA: 0s - loss: 0.1520 - accuracy: 0.9139 - jacard_coef: 0.002611/17 [==================>...........] - ETA: 0s - loss: 0.1520 - accuracy: 0.9113 - jacard_coef: 0.005312/17 [====================>.........] - ETA: 0s - loss: 0.1518 - accuracy: 0.9112 - jacard_coef: 0.005213/17 [=====================>........] - ETA: 0s - loss: 0.1515 - accuracy: 0.9122 - jacard_coef: 0.004914/17 [=======================>......] - ETA: 0s - loss: 0.1513 - accuracy: 0.9131 - jacard_coef: 0.004615/17 [=========================>....] - ETA: 0s - loss: 0.1512 - accuracy: 0.9126 - jacard_coef: 0.004316/17 [===========================>..] - ETA: 0s - loss: 0.1511 - accuracy: 0.9132 - jacard_coef: 0.004217/17 [==============================] - 2s 130ms/step - loss: 0.1511 - accuracy: 0.9124 - jacard_coef: 0.0040 - val_loss: 0.1713 - val_accuracy: 0.4756 - val_jacard_coef: 0.0746 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1535 - accuracy: 0.8666 - jacard_coef: 0.0013 2/17 [==>...........................] - ETA: 1s - loss: 0.1507 - accuracy: 0.8951 - jacard_coef: 0.0040 3/17 [====>.........................] - ETA: 1s - loss: 0.1503 - accuracy: 0.9018 - jacard_coef: 0.0034 4/17 [======>.......................] - ETA: 1s - loss: 0.1541 - accuracy: 0.8147 - jacard_coef: 0.0220 5/17 [=======>......................] - ETA: 1s - loss: 0.1531 - accuracy: 0.8312 - jacard_coef: 0.0178 6/17 [=========>....................] - ETA: 1s - loss: 0.1525 - accuracy: 0.8441 - jacard_coef: 0.0154 7/17 [===========>..................] - ETA: 1s - loss: 0.1522 - accuracy: 0.8499 - jacard_coef: 0.0136 8/17 [=============>................] - ETA: 1s - loss: 0.1521 - accuracy: 0.8552 - jacard_coef: 0.0132 9/17 [==============>...............] - ETA: 1s - loss: 0.1518 - accuracy: 0.8604 - jacard_coef: 0.011810/17 [================>.............] - ETA: 0s - loss: 0.1514 - accuracy: 0.8680 - jacard_coef: 0.011511/17 [==================>...........] - ETA: 0s - loss: 0.1513 - accuracy: 0.8714 - jacard_coef: 0.010512/17 [====================>.........] - ETA: 0s - loss: 0.1514 - accuracy: 0.8727 - jacard_coef: 0.009813/17 [=====================>........] - ETA: 0s - loss: 0.1513 - accuracy: 0.8758 - jacard_coef: 0.009014/17 [=======================>......] - ETA: 0s - loss: 0.1511 - accuracy: 0.8805 - jacard_coef: 0.008815/17 [=========================>....] - ETA: 0s - loss: 0.1508 - accuracy: 0.8843 - jacard_coef: 0.008316/17 [===========================>..] - ETA: 0s - loss: 0.1508 - accuracy: 0.8850 - jacard_coef: 0.008117/17 [==============================] - 2s 128ms/step - loss: 0.1508 - accuracy: 0.8853 - jacard_coef: 0.0076 - val_loss: 0.1520 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1476 - accuracy: 0.9084 - jacard_coef: 0.0093 2/17 [==>...........................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9256 - jacard_coef: 0.0110 3/17 [====>.........................] - ETA: 1s - loss: 0.1470 - accuracy: 0.9197 - jacard_coef: 0.0112 4/17 [======>.......................] - ETA: 1s - loss: 0.1470 - accuracy: 0.9134 - jacard_coef: 0.0128 5/17 [=======>......................] - ETA: 1s - loss: 0.1470 - accuracy: 0.9106 - jacard_coef: 0.0165 6/17 [=========>....................] - ETA: 1s - loss: 0.1470 - accuracy: 0.9072 - jacard_coef: 0.0158 7/17 [===========>..................] - ETA: 1s - loss: 0.1468 - accuracy: 0.9096 - jacard_coef: 0.0136 8/17 [=============>................] - ETA: 1s - loss: 0.1467 - accuracy: 0.9103 - jacard_coef: 0.0119 9/17 [==============>...............] - ETA: 1s - loss: 0.1470 - accuracy: 0.9093 - jacard_coef: 0.010810/17 [================>.............] - ETA: 0s - loss: 0.1470 - accuracy: 0.9079 - jacard_coef: 0.009911/17 [==================>...........] - ETA: 0s - loss: 0.1468 - accuracy: 0.9089 - jacard_coef: 0.009012/17 [====================>.........] - ETA: 0s - loss: 0.1467 - accuracy: 0.9110 - jacard_coef: 0.008413/17 [=====================>........] - ETA: 0s - loss: 0.1468 - accuracy: 0.9095 - jacard_coef: 0.008014/17 [=======================>......] - ETA: 0s - loss: 0.1466 - accuracy: 0.9109 - jacard_coef: 0.007415/17 [=========================>....] - ETA: 0s - loss: 0.1466 - accuracy: 0.9106 - jacard_coef: 0.007016/17 [===========================>..] - ETA: 0s - loss: 0.1468 - accuracy: 0.9076 - jacard_coef: 0.006717/17 [==============================] - 2s 128ms/step - loss: 0.1468 - accuracy: 0.9078 - jacard_coef: 0.0063 - val_loss: 0.1631 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1455 - accuracy: 0.9193 - jacard_coef: 0.0070 2/17 [==>...........................] - ETA: 1s - loss: 0.1452 - accuracy: 0.9111 - jacard_coef: 0.0044 3/17 [====>.........................] - ETA: 1s - loss: 0.1443 - accuracy: 0.9157 - jacard_coef: 0.0032 4/17 [======>.......................] - ETA: 1s - loss: 0.1443 - accuracy: 0.9130 - jacard_coef: 0.0030 5/17 [=======>......................] - ETA: 1s - loss: 0.1447 - accuracy: 0.9087 - jacard_coef: 0.0025 6/17 [=========>....................] - ETA: 1s - loss: 0.1444 - accuracy: 0.9094 - jacard_coef: 0.0021 7/17 [===========>..................] - ETA: 1s - loss: 0.1444 - accuracy: 0.9057 - jacard_coef: 0.0018 8/17 [=============>................] - ETA: 1s - loss: 0.1443 - accuracy: 0.9068 - jacard_coef: 0.0016 9/17 [==============>...............] - ETA: 1s - loss: 0.1443 - accuracy: 0.9047 - jacard_coef: 0.001410/17 [================>.............] - ETA: 0s - loss: 0.1437 - accuracy: 0.9097 - jacard_coef: 0.001311/17 [==================>...........] - ETA: 0s - loss: 0.1434 - accuracy: 0.9111 - jacard_coef: 0.001212/17 [====================>.........] - ETA: 0s - loss: 0.1429 - accuracy: 0.9144 - jacard_coef: 0.001113/17 [=====================>........] - ETA: 0s - loss: 0.1428 - accuracy: 0.9143 - jacard_coef: 0.001014/17 [=======================>......] - ETA: 0s - loss: 0.1426 - accuracy: 0.9150 - jacard_coef: 9.8468e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1425 - accuracy: 0.9147 - jacard_coef: 9.2190e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1423 - accuracy: 0.9146 - jacard_coef: 8.6428e-0417/17 [==============================] - 2s 128ms/step - loss: 0.1422 - accuracy: 0.9152 - jacard_coef: 0.0010 - val_loss: 0.1672 - val_accuracy: 0.7981 - val_jacard_coef: 0.0504 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1387 - accuracy: 0.9289 - jacard_coef: 2.6822e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1394 - accuracy: 0.9185 - jacard_coef: 2.8019e-04 3/17 [====>.........................] - ETA: 1s - loss: 0.1401 - accuracy: 0.9190 - jacard_coef: 0.0013     4/17 [======>.......................] - ETA: 1s - loss: 0.1396 - accuracy: 0.9184 - jacard_coef: 0.0081 5/17 [=======>......................] - ETA: 1s - loss: 0.1392 - accuracy: 0.9197 - jacard_coef: 0.0075 6/17 [=========>....................] - ETA: 1s - loss: 0.1394 - accuracy: 0.9161 - jacard_coef: 0.0063 7/17 [===========>..................] - ETA: 1s - loss: 0.1394 - accuracy: 0.9133 - jacard_coef: 0.0055 8/17 [=============>................] - ETA: 1s - loss: 0.1391 - accuracy: 0.9138 - jacard_coef: 0.0048 9/17 [==============>...............] - ETA: 1s - loss: 0.1393 - accuracy: 0.9112 - jacard_coef: 0.004310/17 [================>.............] - ETA: 0s - loss: 0.1390 - accuracy: 0.9122 - jacard_coef: 0.004411/17 [==================>...........] - ETA: 0s - loss: 0.1388 - accuracy: 0.9132 - jacard_coef: 0.004012/17 [====================>.........] - ETA: 0s - loss: 0.1387 - accuracy: 0.9120 - jacard_coef: 0.003713/17 [=====================>........] - ETA: 0s - loss: 0.1386 - accuracy: 0.9120 - jacard_coef: 0.003514/17 [=======================>......] - ETA: 0s - loss: 0.1385 - accuracy: 0.9122 - jacard_coef: 0.003215/17 [=========================>....] - ETA: 0s - loss: 0.1384 - accuracy: 0.9124 - jacard_coef: 0.004116/17 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9136 - jacard_coef: 0.003917/17 [==============================] - 2s 128ms/step - loss: 0.1390 - accuracy: 0.9104 - jacard_coef: 0.0075 - val_loss: 0.1564 - val_accuracy: 0.9301 - val_jacard_coef: 1.0570e-05 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1415 - accuracy: 0.8964 - jacard_coef: 0.0067 2/17 [==>...........................] - ETA: 1s - loss: 0.1465 - accuracy: 0.7735 - jacard_coef: 0.0438 3/17 [====>.........................] - ETA: 1s - loss: 0.1513 - accuracy: 0.7038 - jacard_coef: 0.0600 4/17 [======>.......................] - ETA: 1s - loss: 0.1487 - accuracy: 0.7526 - jacard_coef: 0.0463 5/17 [=======>......................] - ETA: 1s - loss: 0.1468 - accuracy: 0.7828 - jacard_coef: 0.0370 6/17 [=========>....................] - ETA: 1s - loss: 0.1469 - accuracy: 0.7980 - jacard_coef: 0.0317 7/17 [===========>..................] - ETA: 1s - loss: 0.1454 - accuracy: 0.8168 - jacard_coef: 0.0271 8/17 [=============>................] - ETA: 1s - loss: 0.1447 - accuracy: 0.8276 - jacard_coef: 0.0238 9/17 [==============>...............] - ETA: 1s - loss: 0.1442 - accuracy: 0.8375 - jacard_coef: 0.022010/17 [================>.............] - ETA: 0s - loss: 0.1436 - accuracy: 0.8461 - jacard_coef: 0.020211/17 [==================>...........] - ETA: 0s - loss: 0.1429 - accuracy: 0.8543 - jacard_coef: 0.019812/17 [====================>.........] - ETA: 0s - loss: 0.1424 - accuracy: 0.8596 - jacard_coef: 0.019413/17 [=====================>........] - ETA: 0s - loss: 0.1422 - accuracy: 0.8630 - jacard_coef: 0.018814/17 [=======================>......] - ETA: 0s - loss: 0.1415 - accuracy: 0.8696 - jacard_coef: 0.017715/17 [=========================>....] - ETA: 0s - loss: 0.1416 - accuracy: 0.8695 - jacard_coef: 0.018216/17 [===========================>..] - ETA: 0s - loss: 0.1414 - accuracy: 0.8707 - jacard_coef: 0.017217/17 [==============================] - 2s 128ms/step - loss: 0.1414 - accuracy: 0.8705 - jacard_coef: 0.0162 - val_loss: 0.1506 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1381 - accuracy: 0.9039 - jacard_coef: 0.0012 2/17 [==>...........................] - ETA: 1s - loss: 0.1388 - accuracy: 0.8975 - jacard_coef: 8.5160e-04 3/17 [====>.........................] - ETA: 1s - loss: 0.1371 - accuracy: 0.9096 - jacard_coef: 5.6773e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1370 - accuracy: 0.9125 - jacard_coef: 5.3442e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1373 - accuracy: 0.9097 - jacard_coef: 6.7980e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1375 - accuracy: 0.9061 - jacard_coef: 5.9483e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1372 - accuracy: 0.9079 - jacard_coef: 0.0012     8/17 [=============>................] - ETA: 1s - loss: 0.1368 - accuracy: 0.9101 - jacard_coef: 0.0011 9/17 [==============>...............] - ETA: 1s - loss: 0.1364 - accuracy: 0.9133 - jacard_coef: 9.7688e-0410/17 [================>.............] - ETA: 0s - loss: 0.1364 - accuracy: 0.9115 - jacard_coef: 0.0031    11/17 [==================>...........] - ETA: 0s - loss: 0.1364 - accuracy: 0.9099 - jacard_coef: 0.002912/17 [====================>.........] - ETA: 0s - loss: 0.1363 - accuracy: 0.9097 - jacard_coef: 0.002813/17 [=====================>........] - ETA: 0s - loss: 0.1364 - accuracy: 0.9101 - jacard_coef: 0.002614/17 [=======================>......] - ETA: 0s - loss: 0.1361 - accuracy: 0.9123 - jacard_coef: 0.002515/17 [=========================>....] - ETA: 0s - loss: 0.1362 - accuracy: 0.9122 - jacard_coef: 0.002816/17 [===========================>..] - ETA: 0s - loss: 0.1361 - accuracy: 0.9128 - jacard_coef: 0.002717/17 [==============================] - 2s 128ms/step - loss: 0.1361 - accuracy: 0.9134 - jacard_coef: 0.0026 - val_loss: 0.1485 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1334 - accuracy: 0.9283 - jacard_coef: 0.0029 2/17 [==>...........................] - ETA: 1s - loss: 0.1321 - accuracy: 0.9365 - jacard_coef: 0.0015 3/17 [====>.........................] - ETA: 1s - loss: 0.1345 - accuracy: 0.9146 - jacard_coef: 0.0014 4/17 [======>.......................] - ETA: 1s - loss: 0.1341 - accuracy: 0.9167 - jacard_coef: 0.0011 5/17 [=======>......................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9206 - jacard_coef: 0.0012 6/17 [=========>....................] - ETA: 1s - loss: 0.1342 - accuracy: 0.9149 - jacard_coef: 0.0015 7/17 [===========>..................] - ETA: 1s - loss: 0.1338 - accuracy: 0.9173 - jacard_coef: 0.0016 8/17 [=============>................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9136 - jacard_coef: 0.0014 9/17 [==============>...............] - ETA: 1s - loss: 0.1337 - accuracy: 0.9160 - jacard_coef: 0.001410/17 [================>.............] - ETA: 0s - loss: 0.1336 - accuracy: 0.9169 - jacard_coef: 0.001511/17 [==================>...........] - ETA: 0s - loss: 0.1339 - accuracy: 0.9147 - jacard_coef: 0.002612/17 [====================>.........] - ETA: 0s - loss: 0.1341 - accuracy: 0.9126 - jacard_coef: 0.002413/17 [=====================>........] - ETA: 0s - loss: 0.1340 - accuracy: 0.9145 - jacard_coef: 0.002214/17 [=======================>......] - ETA: 0s - loss: 0.1340 - accuracy: 0.9148 - jacard_coef: 0.002215/17 [=========================>....] - ETA: 0s - loss: 0.1339 - accuracy: 0.9148 - jacard_coef: 0.002116/17 [===========================>..] - ETA: 0s - loss: 0.1337 - accuracy: 0.9157 - jacard_coef: 0.002117/17 [==============================] - 2s 128ms/step - loss: 0.1338 - accuracy: 0.9150 - jacard_coef: 0.0020 - val_loss: 0.1465 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1340 - accuracy: 0.9140 - jacard_coef: 0.0033 2/17 [==>...........................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9082 - jacard_coef: 0.0020 3/17 [====>.........................] - ETA: 1s - loss: 0.1335 - accuracy: 0.9116 - jacard_coef: 0.0019 4/17 [======>.......................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9195 - jacard_coef: 0.0015 5/17 [=======>......................] - ETA: 1s - loss: 0.1323 - accuracy: 0.9215 - jacard_coef: 0.0013 6/17 [=========>....................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9177 - jacard_coef: 0.0011 7/17 [===========>..................] - ETA: 1s - loss: 0.1319 - accuracy: 0.9218 - jacard_coef: 0.0015 8/17 [=============>................] - ETA: 1s - loss: 0.1317 - accuracy: 0.9221 - jacard_coef: 0.0013 9/17 [==============>...............] - ETA: 1s - loss: 0.1323 - accuracy: 0.9159 - jacard_coef: 0.001310/17 [================>.............] - ETA: 0s - loss: 0.1321 - accuracy: 0.9164 - jacard_coef: 0.001711/17 [==================>...........] - ETA: 0s - loss: 0.1319 - accuracy: 0.9163 - jacard_coef: 0.001612/17 [====================>.........] - ETA: 0s - loss: 0.1317 - accuracy: 0.9184 - jacard_coef: 0.002013/17 [=====================>........] - ETA: 0s - loss: 0.1316 - accuracy: 0.9188 - jacard_coef: 0.001914/17 [=======================>......] - ETA: 0s - loss: 0.1320 - accuracy: 0.9151 - jacard_coef: 0.001815/17 [=========================>....] - ETA: 0s - loss: 0.1318 - accuracy: 0.9158 - jacard_coef: 0.001816/17 [===========================>..] - ETA: 0s - loss: 0.1318 - accuracy: 0.9158 - jacard_coef: 0.001717/17 [==============================] - 2s 128ms/step - loss: 0.1321 - accuracy: 0.9129 - jacard_coef: 0.0060 - val_loss: 0.1456 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1310 - accuracy: 0.9164 - jacard_coef: 2.2819e-12 2/17 [==>...........................] - ETA: 1s - loss: 0.1292 - accuracy: 0.9370 - jacard_coef: 3.1394e-04 3/17 [====>.........................] - ETA: 1s - loss: 0.1308 - accuracy: 0.9250 - jacard_coef: 2.9282e-04 4/17 [======>.......................] - ETA: 1s - loss: 0.1319 - accuracy: 0.9188 - jacard_coef: 5.5840e-04 5/17 [=======>......................] - ETA: 1s - loss: 0.1316 - accuracy: 0.9221 - jacard_coef: 0.0030     6/17 [=========>....................] - ETA: 1s - loss: 0.1321 - accuracy: 0.9190 - jacard_coef: 0.0026 7/17 [===========>..................] - ETA: 1s - loss: 0.1320 - accuracy: 0.9190 - jacard_coef: 0.0040 8/17 [=============>................] - ETA: 1s - loss: 0.1320 - accuracy: 0.9176 - jacard_coef: 0.0036 9/17 [==============>...............] - ETA: 1s - loss: 0.1326 - accuracy: 0.9128 - jacard_coef: 0.006310/17 [================>.............] - ETA: 0s - loss: 0.1322 - accuracy: 0.9152 - jacard_coef: 0.009711/17 [==================>...........] - ETA: 0s - loss: 0.1324 - accuracy: 0.9135 - jacard_coef: 0.008912/17 [====================>.........] - ETA: 0s - loss: 0.1328 - accuracy: 0.9104 - jacard_coef: 0.008313/17 [=====================>........] - ETA: 0s - loss: 0.1327 - accuracy: 0.9108 - jacard_coef: 0.007714/17 [=======================>......] - ETA: 0s - loss: 0.1328 - accuracy: 0.9108 - jacard_coef: 0.007415/17 [=========================>....] - ETA: 0s - loss: 0.1329 - accuracy: 0.9106 - jacard_coef: 0.007116/17 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9120 - jacard_coef: 0.006717/17 [==============================] - 2s 128ms/step - loss: 0.1325 - accuracy: 0.9122 - jacard_coef: 0.0067 - val_loss: 0.1518 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1290 - accuracy: 0.9084 - jacard_coef: 0.0435 2/17 [==>...........................] - ETA: 1s - loss: 0.1311 - accuracy: 0.8961 - jacard_coef: 0.0221 3/17 [====>.........................] - ETA: 1s - loss: 0.1302 - accuracy: 0.9051 - jacard_coef: 0.0149 4/17 [======>.......................] - ETA: 1s - loss: 0.1290 - accuracy: 0.9155 - jacard_coef: 0.0112 5/17 [=======>......................] - ETA: 1s - loss: 0.1293 - accuracy: 0.9154 - jacard_coef: 0.0094 6/17 [=========>....................] - ETA: 1s - loss: 0.1292 - accuracy: 0.9178 - jacard_coef: 0.0078 7/17 [===========>..................] - ETA: 1s - loss: 0.1294 - accuracy: 0.9172 - jacard_coef: 0.0067 8/17 [=============>................] - ETA: 1s - loss: 0.1297 - accuracy: 0.9156 - jacard_coef: 0.0059 9/17 [==============>...............] - ETA: 1s - loss: 0.1300 - accuracy: 0.9146 - jacard_coef: 0.005310/17 [================>.............] - ETA: 0s - loss: 0.1305 - accuracy: 0.9110 - jacard_coef: 0.004811/17 [==================>...........] - ETA: 0s - loss: 0.1305 - accuracy: 0.9104 - jacard_coef: 0.004412/17 [====================>.........] - ETA: 0s - loss: 0.1303 - accuracy: 0.9127 - jacard_coef: 0.004013/17 [=====================>........] - ETA: 0s - loss: 0.1303 - accuracy: 0.9135 - jacard_coef: 0.003714/17 [=======================>......] - ETA: 0s - loss: 0.1301 - accuracy: 0.9152 - jacard_coef: 0.003415/17 [=========================>....] - ETA: 0s - loss: 0.1302 - accuracy: 0.9141 - jacard_coef: 0.003216/17 [===========================>..] - ETA: 0s - loss: 0.1304 - accuracy: 0.9135 - jacard_coef: 0.003017/17 [==============================] - 2s 128ms/step - loss: 0.1305 - accuracy: 0.9126 - jacard_coef: 0.0029 - val_loss: 0.1325 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0746 (epoch 8)
  Final Val Loss: 0.1325
  Training Time: 0:01:24.441209
  Stability (std): 0.0092

Results saved to: hyperparameter_optimization_20250926_165036/exp_10_UNet_lr5e-3_bs8/UNet_lr0.005_bs8_results.json

Experiment 10 completed in 99s
Progress: 10/36 completed
Estimated remaining time: 42 minutes

ðŸ”¬ EXPERIMENT 11/36
================================================
Architecture: UNet
Learning Rate: 5e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877773.706106 1064192 service.cc:145] XLA service 0x152849d28650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877773.706128 1064192 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877773.844512 1064192 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 4:48 - loss: 0.3420 - accuracy: 0.5078 - jacard_coef: 0.09042/9 [=====>........................] - ETA: 54s - loss: 0.3283 - accuracy: 0.4893 - jacard_coef: 0.0788 3/9 [=========>....................] - ETA: 32s - loss: 0.3041 - accuracy: 0.4063 - jacard_coef: 0.07524/9 [============>.................] - ETA: 21s - loss: 0.2868 - accuracy: 0.3625 - jacard_coef: 0.07775/9 [===============>..............] - ETA: 13s - loss: 0.2701 - accuracy: 0.3296 - jacard_coef: 0.07706/9 [===================>..........] - ETA: 8s - loss: 0.2634 - accuracy: 0.3004 - jacard_coef: 0.0826 7/9 [======================>.......] - ETA: 5s - loss: 0.2552 - accuracy: 0.2758 - jacard_coef: 0.08188/9 [=========================>....] - ETA: 2s - loss: 0.2487 - accuracy: 0.2580 - jacard_coef: 0.08169/9 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.2577 - jacard_coef: 0.09059/9 [==============================] - 58s 3s/step - loss: 0.2484 - accuracy: 0.2577 - jacard_coef: 0.0905 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1890 - accuracy: 0.1583 - jacard_coef: 0.06842/9 [=====>........................] - ETA: 1s - loss: 0.1914 - accuracy: 0.1547 - jacard_coef: 0.07643/9 [=========>....................] - ETA: 1s - loss: 0.1917 - accuracy: 0.1809 - jacard_coef: 0.08854/9 [============>.................] - ETA: 1s - loss: 0.1960 - accuracy: 0.1948 - jacard_coef: 0.08085/9 [===============>..............] - ETA: 0s - loss: 0.1948 - accuracy: 0.2172 - jacard_coef: 0.08096/9 [===================>..........] - ETA: 0s - loss: 0.1927 - accuracy: 0.2506 - jacard_coef: 0.08327/9 [======================>.......] - ETA: 0s - loss: 0.1916 - accuracy: 0.2582 - jacard_coef: 0.08058/9 [=========================>....] - ETA: 0s - loss: 0.1907 - accuracy: 0.2622 - jacard_coef: 0.08089/9 [==============================] - 2s 229ms/step - loss: 0.1909 - accuracy: 0.2627 - jacard_coef: 0.0855 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1902 - accuracy: 0.1261 - jacard_coef: 0.08392/9 [=====>........................] - ETA: 1s - loss: 0.1914 - accuracy: 0.1355 - jacard_coef: 0.07313/9 [=========>....................] - ETA: 1s - loss: 0.1905 - accuracy: 0.1479 - jacard_coef: 0.07334/9 [============>.................] - ETA: 1s - loss: 0.1930 - accuracy: 0.1520 - jacard_coef: 0.07865/9 [===============>..............] - ETA: 0s - loss: 0.1951 - accuracy: 0.1507 - jacard_coef: 0.07946/9 [===================>..........] - ETA: 0s - loss: 0.1985 - accuracy: 0.1531 - jacard_coef: 0.08357/9 [======================>.......] - ETA: 0s - loss: 0.1980 - accuracy: 0.1547 - jacard_coef: 0.08648/9 [=========================>....] - ETA: 0s - loss: 0.1981 - accuracy: 0.1536 - jacard_coef: 0.08279/9 [==============================] - 2s 228ms/step - loss: 0.1982 - accuracy: 0.1530 - jacard_coef: 0.0743 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1961 - accuracy: 0.1818 - jacard_coef: 0.08602/9 [=====>........................] - ETA: 1s - loss: 0.2006 - accuracy: 0.1853 - jacard_coef: 0.08453/9 [=========>....................] - ETA: 1s - loss: 0.2009 - accuracy: 0.1895 - jacard_coef: 0.08104/9 [============>.................] - ETA: 1s - loss: 0.1975 - accuracy: 0.1859 - jacard_coef: 0.08165/9 [===============>..............] - ETA: 0s - loss: 0.1953 - accuracy: 0.1853 - jacard_coef: 0.08386/9 [===================>..........] - ETA: 0s - loss: 0.1938 - accuracy: 0.1884 - jacard_coef: 0.08497/9 [======================>.......] - ETA: 0s - loss: 0.1930 - accuracy: 0.1926 - jacard_coef: 0.08358/9 [=========================>....] - ETA: 0s - loss: 0.1919 - accuracy: 0.1989 - jacard_coef: 0.08339/9 [==============================] - 2s 228ms/step - loss: 0.1925 - accuracy: 0.1983 - jacard_coef: 0.0761 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1847 - accuracy: 0.2239 - jacard_coef: 0.07592/9 [=====>........................] - ETA: 1s - loss: 0.1822 - accuracy: 0.2747 - jacard_coef: 0.07973/9 [=========>....................] - ETA: 1s - loss: 0.1813 - accuracy: 0.2914 - jacard_coef: 0.07814/9 [============>.................] - ETA: 1s - loss: 0.1806 - accuracy: 0.3028 - jacard_coef: 0.07905/9 [===============>..............] - ETA: 0s - loss: 0.1803 - accuracy: 0.3026 - jacard_coef: 0.07566/9 [===================>..........] - ETA: 0s - loss: 0.1797 - accuracy: 0.3126 - jacard_coef: 0.07797/9 [======================>.......] - ETA: 0s - loss: 0.1817 - accuracy: 0.3201 - jacard_coef: 0.07948/9 [=========================>....] - ETA: 0s - loss: 0.1810 - accuracy: 0.3290 - jacard_coef: 0.08319/9 [==============================] - 2s 228ms/step - loss: 0.1814 - accuracy: 0.3291 - jacard_coef: 0.0816 - val_loss: 1.1199 - val_accuracy: 0.9304 - val_jacard_coef: 1.4614e-12 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1809 - accuracy: 0.5316 - jacard_coef: 0.10062/9 [=====>........................] - ETA: 1s - loss: 0.1831 - accuracy: 0.5098 - jacard_coef: 0.09013/9 [=========>....................] - ETA: 1s - loss: 0.1867 - accuracy: 0.4955 - jacard_coef: 0.08724/9 [============>.................] - ETA: 1s - loss: 0.1851 - accuracy: 0.4924 - jacard_coef: 0.08395/9 [===============>..............] - ETA: 0s - loss: 0.1835 - accuracy: 0.4953 - jacard_coef: 0.08416/9 [===================>..........] - ETA: 0s - loss: 0.1823 - accuracy: 0.4939 - jacard_coef: 0.08407/9 [======================>.......] - ETA: 0s - loss: 0.1833 - accuracy: 0.4968 - jacard_coef: 0.08178/9 [=========================>....] - ETA: 0s - loss: 0.1823 - accuracy: 0.4974 - jacard_coef: 0.07799/9 [==============================] - 2s 228ms/step - loss: 0.1827 - accuracy: 0.4949 - jacard_coef: 0.0749 - val_loss: 1.0774 - val_accuracy: 0.9302 - val_jacard_coef: 1.4567e-12 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1820 - accuracy: 0.2871 - jacard_coef: 0.07662/9 [=====>........................] - ETA: 1s - loss: 0.1988 - accuracy: 0.2244 - jacard_coef: 0.07453/9 [=========>....................] - ETA: 1s - loss: 0.1957 - accuracy: 0.2493 - jacard_coef: 0.06714/9 [============>.................] - ETA: 1s - loss: 0.1900 - accuracy: 0.3098 - jacard_coef: 0.06855/9 [===============>..............] - ETA: 0s - loss: 0.1894 - accuracy: 0.3287 - jacard_coef: 0.07136/9 [===================>..........] - ETA: 0s - loss: 0.1871 - accuracy: 0.3409 - jacard_coef: 0.07657/9 [======================>.......] - ETA: 0s - loss: 0.1857 - accuracy: 0.3458 - jacard_coef: 0.07788/9 [=========================>....] - ETA: 0s - loss: 0.1844 - accuracy: 0.3524 - jacard_coef: 0.07769/9 [==============================] - 2s 228ms/step - loss: 0.1844 - accuracy: 0.3509 - jacard_coef: 0.0716 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4614e-12 - lr: 5.0000e-04
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1887 - accuracy: 0.4197 - jacard_coef: 0.08232/9 [=====>........................] - ETA: 1s - loss: 0.1828 - accuracy: 0.4129 - jacard_coef: 0.07193/9 [=========>....................] - ETA: 1s - loss: 0.1800 - accuracy: 0.4258 - jacard_coef: 0.07074/9 [============>.................] - ETA: 1s - loss: 0.1797 - accuracy: 0.4243 - jacard_coef: 0.07525/9 [===============>..............] - ETA: 0s - loss: 0.1831 - accuracy: 0.4254 - jacard_coef: 0.08096/9 [===================>..........] - ETA: 0s - loss: 0.1814 - accuracy: 0.4432 - jacard_coef: 0.08367/9 [======================>.......] - ETA: 0s - loss: 0.1801 - accuracy: 0.4615 - jacard_coef: 0.08168/9 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.4785 - jacard_coef: 0.07839/9 [==============================] - 2s 229ms/step - loss: 0.1790 - accuracy: 0.4774 - jacard_coef: 0.0776 - val_loss: 1.1195 - val_accuracy: 0.9302 - val_jacard_coef: 1.4576e-12 - lr: 5.0000e-04
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1681 - accuracy: 0.6332 - jacard_coef: 0.06572/9 [=====>........................] - ETA: 1s - loss: 0.1690 - accuracy: 0.6188 - jacard_coef: 0.07873/9 [=========>....................] - ETA: 1s - loss: 0.1746 - accuracy: 0.6003 - jacard_coef: 0.08244/9 [============>.................] - ETA: 1s - loss: 0.1735 - accuracy: 0.5870 - jacard_coef: 0.07825/9 [===============>..............] - ETA: 0s - loss: 0.1727 - accuracy: 0.5854 - jacard_coef: 0.07816/9 [===================>..........] - ETA: 0s - loss: 0.1718 - accuracy: 0.5972 - jacard_coef: 0.07837/9 [======================>.......] - ETA: 0s - loss: 0.1714 - accuracy: 0.6021 - jacard_coef: 0.07568/9 [=========================>....] - ETA: 0s - loss: 0.1719 - accuracy: 0.5932 - jacard_coef: 0.07849/9 [==============================] - 2s 234ms/step - loss: 0.1719 - accuracy: 0.5916 - jacard_coef: 0.0701 - val_loss: 1.0243 - val_accuracy: 0.9291 - val_jacard_coef: 8.1661e-04 - lr: 5.0000e-04
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1700 - accuracy: 0.6679 - jacard_coef: 0.06542/9 [=====>........................] - ETA: 1s - loss: 0.1689 - accuracy: 0.6963 - jacard_coef: 0.06203/9 [=========>....................] - ETA: 1s - loss: 0.1683 - accuracy: 0.6902 - jacard_coef: 0.06814/9 [============>.................] - ETA: 1s - loss: 0.1704 - accuracy: 0.6915 - jacard_coef: 0.06925/9 [===============>..............] - ETA: 0s - loss: 0.1704 - accuracy: 0.6539 - jacard_coef: 0.07056/9 [===================>..........] - ETA: 0s - loss: 0.1707 - accuracy: 0.6444 - jacard_coef: 0.07447/9 [======================>.......] - ETA: 0s - loss: 0.1698 - accuracy: 0.6609 - jacard_coef: 0.07008/9 [=========================>....] - ETA: 0s - loss: 0.1693 - accuracy: 0.6713 - jacard_coef: 0.07189/9 [==============================] - 2s 233ms/step - loss: 0.1696 - accuracy: 0.6703 - jacard_coef: 0.0703 - val_loss: 0.4039 - val_accuracy: 0.9221 - val_jacard_coef: 0.0034 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1636 - accuracy: 0.7599 - jacard_coef: 0.04982/9 [=====>........................] - ETA: 1s - loss: 0.1638 - accuracy: 0.7720 - jacard_coef: 0.06313/9 [=========>....................] - ETA: 1s - loss: 0.1634 - accuracy: 0.7768 - jacard_coef: 0.05814/9 [============>.................] - ETA: 1s - loss: 0.1632 - accuracy: 0.7642 - jacard_coef: 0.06555/9 [===============>..............] - ETA: 0s - loss: 0.1633 - accuracy: 0.7375 - jacard_coef: 0.07026/9 [===================>..........] - ETA: 0s - loss: 0.1632 - accuracy: 0.7215 - jacard_coef: 0.07157/9 [======================>.......] - ETA: 0s - loss: 0.1638 - accuracy: 0.7206 - jacard_coef: 0.07268/9 [=========================>....] - ETA: 0s - loss: 0.1636 - accuracy: 0.7289 - jacard_coef: 0.07029/9 [==============================] - 2s 233ms/step - loss: 0.1637 - accuracy: 0.7276 - jacard_coef: 0.0680 - val_loss: 0.2370 - val_accuracy: 0.9067 - val_jacard_coef: 0.0092 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1627 - accuracy: 0.8310 - jacard_coef: 0.05952/9 [=====>........................] - ETA: 1s - loss: 0.1621 - accuracy: 0.8532 - jacard_coef: 0.04263/9 [=========>....................] - ETA: 1s - loss: 0.1636 - accuracy: 0.8119 - jacard_coef: 0.06314/9 [============>.................] - ETA: 1s - loss: 0.1644 - accuracy: 0.7754 - jacard_coef: 0.06245/9 [===============>..............] - ETA: 0s - loss: 0.1641 - accuracy: 0.7670 - jacard_coef: 0.06296/9 [===================>..........] - ETA: 0s - loss: 0.1638 - accuracy: 0.7602 - jacard_coef: 0.07007/9 [======================>.......] - ETA: 0s - loss: 0.1638 - accuracy: 0.7401 - jacard_coef: 0.06988/9 [=========================>....] - ETA: 0s - loss: 0.1639 - accuracy: 0.7474 - jacard_coef: 0.06729/9 [==============================] - 2s 232ms/step - loss: 0.1639 - accuracy: 0.7454 - jacard_coef: 0.0722 - val_loss: 0.0919 - val_accuracy: 0.8489 - val_jacard_coef: 0.0268 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1605 - accuracy: 0.7889 - jacard_coef: 0.05772/9 [=====>........................] - ETA: 1s - loss: 0.1616 - accuracy: 0.7453 - jacard_coef: 0.05943/9 [=========>....................] - ETA: 1s - loss: 0.1619 - accuracy: 0.7404 - jacard_coef: 0.06064/9 [============>.................] - ETA: 1s - loss: 0.1624 - accuracy: 0.7347 - jacard_coef: 0.06005/9 [===============>..............] - ETA: 0s - loss: 0.1622 - accuracy: 0.7380 - jacard_coef: 0.06206/9 [===================>..........] - ETA: 0s - loss: 0.1624 - accuracy: 0.7326 - jacard_coef: 0.06117/9 [======================>.......] - ETA: 0s - loss: 0.1631 - accuracy: 0.7311 - jacard_coef: 0.06208/9 [=========================>....] - ETA: 0s - loss: 0.1631 - accuracy: 0.7291 - jacard_coef: 0.06429/9 [==============================] - 2s 233ms/step - loss: 0.1632 - accuracy: 0.7275 - jacard_coef: 0.0718 - val_loss: 0.1768 - val_accuracy: 0.4120 - val_jacard_coef: 0.0784 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8884 - jacard_coef: 0.03952/9 [=====>........................] - ETA: 1s - loss: 0.1609 - accuracy: 0.8639 - jacard_coef: 0.04783/9 [=========>....................] - ETA: 1s - loss: 0.1603 - accuracy: 0.8447 - jacard_coef: 0.04554/9 [============>.................] - ETA: 1s - loss: 0.1602 - accuracy: 0.8271 - jacard_coef: 0.04875/9 [===============>..............] - ETA: 0s - loss: 0.1604 - accuracy: 0.8117 - jacard_coef: 0.05496/9 [===================>..........] - ETA: 0s - loss: 0.1601 - accuracy: 0.8107 - jacard_coef: 0.05787/9 [======================>.......] - ETA: 0s - loss: 0.1599 - accuracy: 0.8098 - jacard_coef: 0.05738/9 [=========================>....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8084 - jacard_coef: 0.05389/9 [==============================] - 2s 229ms/step - loss: 0.1599 - accuracy: 0.8074 - jacard_coef: 0.0579 - val_loss: 0.2022 - val_accuracy: 0.1427 - val_jacard_coef: 0.0696 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8836 - jacard_coef: 0.03352/9 [=====>........................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8864 - jacard_coef: 0.03143/9 [=========>....................] - ETA: 1s - loss: 0.1601 - accuracy: 0.7975 - jacard_coef: 0.03894/9 [============>.................] - ETA: 1s - loss: 0.1607 - accuracy: 0.8177 - jacard_coef: 0.03115/9 [===============>..............] - ETA: 0s - loss: 0.1608 - accuracy: 0.8383 - jacard_coef: 0.02636/9 [===================>..........] - ETA: 0s - loss: 0.1605 - accuracy: 0.8499 - jacard_coef: 0.02217/9 [======================>.......] - ETA: 0s - loss: 0.1605 - accuracy: 0.8584 - jacard_coef: 0.02068/9 [=========================>....] - ETA: 0s - loss: 0.1605 - accuracy: 0.8640 - jacard_coef: 0.01929/9 [==============================] - 2s 229ms/step - loss: 0.1606 - accuracy: 0.8640 - jacard_coef: 0.0178 - val_loss: 0.1628 - val_accuracy: 0.8462 - val_jacard_coef: 0.0265 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8783 - jacard_coef: 0.02352/9 [=====>........................] - ETA: 1s - loss: 0.1598 - accuracy: 0.8883 - jacard_coef: 0.01913/9 [=========>....................] - ETA: 1s - loss: 0.1595 - accuracy: 0.8798 - jacard_coef: 0.02694/9 [============>.................] - ETA: 1s - loss: 0.1591 - accuracy: 0.8649 - jacard_coef: 0.03455/9 [===============>..............] - ETA: 0s - loss: 0.1598 - accuracy: 0.8428 - jacard_coef: 0.03876/9 [===================>..........] - ETA: 0s - loss: 0.1598 - accuracy: 0.8289 - jacard_coef: 0.04197/9 [======================>.......] - ETA: 0s - loss: 0.1594 - accuracy: 0.8314 - jacard_coef: 0.04468/9 [=========================>....] - ETA: 0s - loss: 0.1591 - accuracy: 0.8322 - jacard_coef: 0.04459/9 [==============================] - 2s 233ms/step - loss: 0.1592 - accuracy: 0.8304 - jacard_coef: 0.0557 - val_loss: 0.1719 - val_accuracy: 0.5880 - val_jacard_coef: 0.0808 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1582 - accuracy: 0.7660 - jacard_coef: 0.07422/9 [=====>........................] - ETA: 1s - loss: 0.1588 - accuracy: 0.7436 - jacard_coef: 0.06323/9 [=========>....................] - ETA: 1s - loss: 0.1585 - accuracy: 0.7253 - jacard_coef: 0.06984/9 [============>.................] - ETA: 1s - loss: 0.1584 - accuracy: 0.7280 - jacard_coef: 0.06855/9 [===============>..............] - ETA: 0s - loss: 0.1580 - accuracy: 0.7446 - jacard_coef: 0.06726/9 [===================>..........] - ETA: 0s - loss: 0.1577 - accuracy: 0.7702 - jacard_coef: 0.05777/9 [======================>.......] - ETA: 0s - loss: 0.1573 - accuracy: 0.7912 - jacard_coef: 0.05048/9 [=========================>....] - ETA: 0s - loss: 0.1572 - accuracy: 0.8068 - jacard_coef: 0.04499/9 [==============================] - 2s 229ms/step - loss: 0.1572 - accuracy: 0.8062 - jacard_coef: 0.0545 - val_loss: 0.1759 - val_accuracy: 0.3905 - val_jacard_coef: 0.0742 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1552 - accuracy: 0.9093 - jacard_coef: 0.00462/9 [=====>........................] - ETA: 1s - loss: 0.1550 - accuracy: 0.9145 - jacard_coef: 0.00323/9 [=========>....................] - ETA: 1s - loss: 0.1554 - accuracy: 0.9111 - jacard_coef: 0.00704/9 [============>.................] - ETA: 1s - loss: 0.1552 - accuracy: 0.9125 - jacard_coef: 0.00595/9 [===============>..............] - ETA: 0s - loss: 0.1549 - accuracy: 0.9131 - jacard_coef: 0.00566/9 [===================>..........] - ETA: 0s - loss: 0.1548 - accuracy: 0.9133 - jacard_coef: 0.00517/9 [======================>.......] - ETA: 0s - loss: 0.1546 - accuracy: 0.9146 - jacard_coef: 0.00528/9 [=========================>....] - ETA: 0s - loss: 0.1547 - accuracy: 0.9135 - jacard_coef: 0.00659/9 [==============================] - 2s 229ms/step - loss: 0.1548 - accuracy: 0.9105 - jacard_coef: 0.0113 - val_loss: 0.1820 - val_accuracy: 0.0699 - val_jacard_coef: 0.0696 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1574 - accuracy: 0.8959 - jacard_coef: 0.01252/9 [=====>........................] - ETA: 1s - loss: 0.1592 - accuracy: 0.8890 - jacard_coef: 0.01513/9 [=========>....................] - ETA: 1s - loss: 0.1599 - accuracy: 0.8890 - jacard_coef: 0.01244/9 [============>.................] - ETA: 1s - loss: 0.1604 - accuracy: 0.8886 - jacard_coef: 0.01565/9 [===============>..............] - ETA: 0s - loss: 0.1626 - accuracy: 0.8859 - jacard_coef: 0.01786/9 [===================>..........] - ETA: 0s - loss: 0.1627 - accuracy: 0.8848 - jacard_coef: 0.01837/9 [======================>.......] - ETA: 0s - loss: 0.1636 - accuracy: 0.8819 - jacard_coef: 0.02078/9 [=========================>....] - ETA: 0s - loss: 0.1634 - accuracy: 0.8827 - jacard_coef: 0.02259/9 [==============================] - 2s 229ms/step - loss: 0.1635 - accuracy: 0.8797 - jacard_coef: 0.0326 - val_loss: 0.1926 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1598 - accuracy: 0.9100 - jacard_coef: 0.02242/9 [=====>........................] - ETA: 1s - loss: 0.1613 - accuracy: 0.8923 - jacard_coef: 0.02663/9 [=========>....................] - ETA: 1s - loss: 0.1634 - accuracy: 0.8920 - jacard_coef: 0.02824/9 [============>.................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8902 - jacard_coef: 0.02685/9 [===============>..............] - ETA: 0s - loss: 0.1623 - accuracy: 0.8914 - jacard_coef: 0.02316/9 [===================>..........] - ETA: 0s - loss: 0.1638 - accuracy: 0.8929 - jacard_coef: 0.02097/9 [======================>.......] - ETA: 0s - loss: 0.1630 - accuracy: 0.8929 - jacard_coef: 0.01938/9 [=========================>....] - ETA: 0s - loss: 0.1622 - accuracy: 0.8960 - jacard_coef: 0.01709/9 [==============================] - 2s 229ms/step - loss: 0.1623 - accuracy: 0.8956 - jacard_coef: 0.0188 - val_loss: 0.1860 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 5.0000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1571 - accuracy: 0.8800 - jacard_coef: 0.02592/9 [=====>........................] - ETA: 1s - loss: 0.1553 - accuracy: 0.9004 - jacard_coef: 0.03223/9 [=========>....................] - ETA: 1s - loss: 0.1551 - accuracy: 0.8964 - jacard_coef: 0.02394/9 [============>.................] - ETA: 1s - loss: 0.1560 - accuracy: 0.8976 - jacard_coef: 0.02475/9 [===============>..............] - ETA: 0s - loss: 0.1556 - accuracy: 0.9005 - jacard_coef: 0.02546/9 [===================>..........] - ETA: 0s - loss: 0.1555 - accuracy: 0.8981 - jacard_coef: 0.02497/9 [======================>.......] - ETA: 0s - loss: 0.1554 - accuracy: 0.8973 - jacard_coef: 0.02208/9 [=========================>....] - ETA: 0s - loss: 0.1558 - accuracy: 0.8993 - jacard_coef: 0.02269/9 [==============================] - 2s 229ms/step - loss: 0.1558 - accuracy: 0.8983 - jacard_coef: 0.0230 - val_loss: 0.1833 - val_accuracy: 0.0866 - val_jacard_coef: 0.0691 - lr: 5.0000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1513 - accuracy: 0.9318 - jacard_coef: 0.00772/9 [=====>........................] - ETA: 1s - loss: 0.1531 - accuracy: 0.9243 - jacard_coef: 0.00403/9 [=========>....................] - ETA: 1s - loss: 0.1531 - accuracy: 0.9214 - jacard_coef: 0.00284/9 [============>.................] - ETA: 1s - loss: 0.1531 - accuracy: 0.9179 - jacard_coef: 0.00435/9 [===============>..............] - ETA: 0s - loss: 0.1544 - accuracy: 0.9162 - jacard_coef: 0.00496/9 [===================>..........] - ETA: 0s - loss: 0.1546 - accuracy: 0.9097 - jacard_coef: 0.00587/9 [======================>.......] - ETA: 0s - loss: 0.1544 - accuracy: 0.9090 - jacard_coef: 0.00508/9 [=========================>....] - ETA: 0s - loss: 0.1539 - accuracy: 0.9114 - jacard_coef: 0.00459/9 [==============================] - 2s 229ms/step - loss: 0.1540 - accuracy: 0.9116 - jacard_coef: 0.0082 - val_loss: 0.1819 - val_accuracy: 0.1126 - val_jacard_coef: 0.0689 - lr: 2.5000e-04
Epoch 23/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1528 - accuracy: 0.9082 - jacard_coef: 0.01032/9 [=====>........................] - ETA: 1s - loss: 0.1549 - accuracy: 0.9105 - jacard_coef: 0.01023/9 [=========>....................] - ETA: 1s - loss: 0.1540 - accuracy: 0.9045 - jacard_coef: 0.00724/9 [============>.................] - ETA: 1s - loss: 0.1534 - accuracy: 0.8991 - jacard_coef: 0.00835/9 [===============>..............] - ETA: 0s - loss: 0.1536 - accuracy: 0.8970 - jacard_coef: 0.00966/9 [===================>..........] - ETA: 0s - loss: 0.1535 - accuracy: 0.8993 - jacard_coef: 0.00867/9 [======================>.......] - ETA: 0s - loss: 0.1532 - accuracy: 0.9022 - jacard_coef: 0.00928/9 [=========================>....] - ETA: 0s - loss: 0.1528 - accuracy: 0.9071 - jacard_coef: 0.00859/9 [==============================] - 2s 229ms/step - loss: 0.1532 - accuracy: 0.9062 - jacard_coef: 0.0158 - val_loss: 0.1814 - val_accuracy: 0.1295 - val_jacard_coef: 0.0689 - lr: 2.5000e-04
Epoch 24/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1529 - accuracy: 0.9017 - jacard_coef: 0.00262/9 [=====>........................] - ETA: 1s - loss: 0.1521 - accuracy: 0.9131 - jacard_coef: 0.00173/9 [=========>....................] - ETA: 1s - loss: 0.1527 - accuracy: 0.9076 - jacard_coef: 0.00124/9 [============>.................] - ETA: 1s - loss: 0.1527 - accuracy: 0.9090 - jacard_coef: 0.00125/9 [===============>..............] - ETA: 0s - loss: 0.1522 - accuracy: 0.9144 - jacard_coef: 0.00356/9 [===================>..........] - ETA: 0s - loss: 0.1521 - accuracy: 0.9136 - jacard_coef: 0.00297/9 [======================>.......] - ETA: 0s - loss: 0.1527 - accuracy: 0.9141 - jacard_coef: 0.00418/9 [=========================>....] - ETA: 0s - loss: 0.1527 - accuracy: 0.9131 - jacard_coef: 0.00399/9 [==============================] - 2s 229ms/step - loss: 0.1527 - accuracy: 0.9131 - jacard_coef: 0.0081 - val_loss: 0.1804 - val_accuracy: 0.1631 - val_jacard_coef: 0.0689 - lr: 2.5000e-04
Epoch 25/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1513 - accuracy: 0.9035 - jacard_coef: 1.4814e-042/9 [=====>........................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9105 - jacard_coef: 3.9202e-043/9 [=========>....................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9186 - jacard_coef: 0.0012    4/9 [============>.................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9200 - jacard_coef: 0.00255/9 [===============>..............] - ETA: 0s - loss: 0.1507 - accuracy: 0.9230 - jacard_coef: 0.00216/9 [===================>..........] - ETA: 0s - loss: 0.1511 - accuracy: 0.9188 - jacard_coef: 0.00187/9 [======================>.......] - ETA: 0s - loss: 0.1515 - accuracy: 0.9186 - jacard_coef: 0.00188/9 [=========================>....] - ETA: 0s - loss: 0.1514 - accuracy: 0.9161 - jacard_coef: 0.00199/9 [==============================] - 2s 229ms/step - loss: 0.1515 - accuracy: 0.9153 - jacard_coef: 0.0024 - val_loss: 0.1812 - val_accuracy: 0.1777 - val_jacard_coef: 0.0689 - lr: 2.5000e-04
Epoch 26/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1534 - accuracy: 0.8731 - jacard_coef: 0.00522/9 [=====>........................] - ETA: 1s - loss: 0.1513 - accuracy: 0.8950 - jacard_coef: 0.00773/9 [=========>....................] - ETA: 1s - loss: 0.1505 - accuracy: 0.9060 - jacard_coef: 0.00524/9 [============>.................] - ETA: 1s - loss: 0.1523 - accuracy: 0.8779 - jacard_coef: 0.01955/9 [===============>..............] - ETA: 0s - loss: 0.1516 - accuracy: 0.8860 - jacard_coef: 0.01626/9 [===================>..........] - ETA: 0s - loss: 0.1511 - accuracy: 0.8935 - jacard_coef: 0.01367/9 [======================>.......] - ETA: 0s - loss: 0.1514 - accuracy: 0.8968 - jacard_coef: 0.01298/9 [=========================>....] - ETA: 0s - loss: 0.1510 - accuracy: 0.9005 - jacard_coef: 0.01139/9 [==============================] - 2s 229ms/step - loss: 0.1513 - accuracy: 0.8998 - jacard_coef: 0.0129 - val_loss: 0.1795 - val_accuracy: 0.1953 - val_jacard_coef: 0.0690 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0808 (epoch 16)
  Final Val Loss: 0.1795
  Training Time: 0:01:50.102906
  Stability (std): 0.0042

Results saved to: hyperparameter_optimization_20250926_165036/exp_11_UNet_lr5e-3_bs16/UNet_lr0.005_bs16_results.json

Experiment 11 completed in 124s
Progress: 11/36 completed
Estimated remaining time: 51 minutes

ðŸ”¬ EXPERIMENT 12/36
================================================
Architecture: UNet
Learning Rate: 5e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877902.829403 1068619 service.cc:145] XLA service 0x14865dcadea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877902.829429 1068619 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877902.966911 1068619 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 2:57 - loss: 0.3416 - accuracy: 0.4951 - jacard_coef: 0.07852/5 [===========>..................] - ETA: 38s - loss: 0.3022 - accuracy: 0.3959 - jacard_coef: 0.0751 3/5 [=================>............] - ETA: 16s - loss: 0.2733 - accuracy: 0.3523 - jacard_coef: 0.07464/5 [=======================>......] - ETA: 6s - loss: 0.2578 - accuracy: 0.3398 - jacard_coef: 0.0795 5/5 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.3386 - jacard_coef: 0.08465/5 [==============================] - 71s 7s/step - loss: 0.2574 - accuracy: 0.3386 - jacard_coef: 0.0846 - val_loss: 0.9988 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 6s - loss: 0.1891 - accuracy: 0.3674 - jacard_coef: 0.08192/5 [===========>..................] - ETA: 1s - loss: 0.1926 - accuracy: 0.3374 - jacard_coef: 0.08693/5 [=================>............] - ETA: 0s - loss: 0.1941 - accuracy: 0.3348 - jacard_coef: 0.08494/5 [=======================>......] - ETA: 0s - loss: 0.1928 - accuracy: 0.2990 - jacard_coef: 0.08225/5 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.2986 - jacard_coef: 0.09635/5 [==============================] - 3s 413ms/step - loss: 0.1928 - accuracy: 0.2986 - jacard_coef: 0.0963 - val_loss: 1.1185 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1933 - accuracy: 0.1679 - jacard_coef: 0.08392/5 [===========>..................] - ETA: 1s - loss: 0.1899 - accuracy: 0.1997 - jacard_coef: 0.07973/5 [=================>............] - ETA: 0s - loss: 0.1886 - accuracy: 0.2209 - jacard_coef: 0.08634/5 [=======================>......] - ETA: 0s - loss: 0.1872 - accuracy: 0.2354 - jacard_coef: 0.08245/5 [==============================] - 2s 387ms/step - loss: 0.1874 - accuracy: 0.2350 - jacard_coef: 0.0750 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1832 - accuracy: 0.2163 - jacard_coef: 0.07982/5 [===========>..................] - ETA: 1s - loss: 0.1837 - accuracy: 0.1955 - jacard_coef: 0.08133/5 [=================>............] - ETA: 0s - loss: 0.1829 - accuracy: 0.2010 - jacard_coef: 0.08624/5 [=======================>......] - ETA: 0s - loss: 0.1839 - accuracy: 0.1964 - jacard_coef: 0.08295/5 [==============================] - 2s 387ms/step - loss: 0.1840 - accuracy: 0.1952 - jacard_coef: 0.0674 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1793 - accuracy: 0.2794 - jacard_coef: 0.07652/5 [===========>..................] - ETA: 1s - loss: 0.1807 - accuracy: 0.2439 - jacard_coef: 0.07433/5 [=================>............] - ETA: 0s - loss: 0.1800 - accuracy: 0.2583 - jacard_coef: 0.08014/5 [=======================>......] - ETA: 0s - loss: 0.1795 - accuracy: 0.2995 - jacard_coef: 0.08055/5 [==============================] - 2s 387ms/step - loss: 0.1796 - accuracy: 0.2987 - jacard_coef: 0.0877 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1775 - accuracy: 0.3391 - jacard_coef: 0.09542/5 [===========>..................] - ETA: 1s - loss: 0.1775 - accuracy: 0.3518 - jacard_coef: 0.08503/5 [=================>............] - ETA: 0s - loss: 0.1765 - accuracy: 0.3939 - jacard_coef: 0.07954/5 [=======================>......] - ETA: 0s - loss: 0.1757 - accuracy: 0.4207 - jacard_coef: 0.07615/5 [==============================] - 2s 387ms/step - loss: 0.1764 - accuracy: 0.4195 - jacard_coef: 0.0830 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.2020 - accuracy: 0.2653 - jacard_coef: 0.08002/5 [===========>..................] - ETA: 1s - loss: 0.2012 - accuracy: 0.2855 - jacard_coef: 0.07823/5 [=================>............] - ETA: 0s - loss: 0.1992 - accuracy: 0.2955 - jacard_coef: 0.07684/5 [=======================>......] - ETA: 0s - loss: 0.1965 - accuracy: 0.3167 - jacard_coef: 0.08105/5 [==============================] - 2s 387ms/step - loss: 0.1964 - accuracy: 0.3167 - jacard_coef: 0.0724 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1878 - accuracy: 0.3361 - jacard_coef: 0.08422/5 [===========>..................] - ETA: 1s - loss: 0.1909 - accuracy: 0.3367 - jacard_coef: 0.07913/5 [=================>............] - ETA: 0s - loss: 0.1889 - accuracy: 0.3477 - jacard_coef: 0.07994/5 [=======================>......] - ETA: 0s - loss: 0.1889 - accuracy: 0.3501 - jacard_coef: 0.08025/5 [==============================] - 2s 387ms/step - loss: 0.1891 - accuracy: 0.3500 - jacard_coef: 0.0924 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1838 - accuracy: 0.4129 - jacard_coef: 0.08492/5 [===========>..................] - ETA: 1s - loss: 0.1904 - accuracy: 0.4153 - jacard_coef: 0.08923/5 [=================>............] - ETA: 0s - loss: 0.1883 - accuracy: 0.4188 - jacard_coef: 0.08394/5 [=======================>......] - ETA: 0s - loss: 0.1879 - accuracy: 0.4057 - jacard_coef: 0.07915/5 [==============================] - 2s 387ms/step - loss: 0.1880 - accuracy: 0.4052 - jacard_coef: 0.0941 - val_loss: 1.1166 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1834 - accuracy: 0.3904 - jacard_coef: 0.06272/5 [===========>..................] - ETA: 1s - loss: 0.1827 - accuracy: 0.3591 - jacard_coef: 0.07093/5 [=================>............] - ETA: 0s - loss: 0.1858 - accuracy: 0.3450 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1858 - accuracy: 0.3320 - jacard_coef: 0.07945/5 [==============================] - 2s 387ms/step - loss: 0.1858 - accuracy: 0.3318 - jacard_coef: 0.0900 - val_loss: 1.1145 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1803 - accuracy: 0.3279 - jacard_coef: 0.05752/5 [===========>..................] - ETA: 1s - loss: 0.1804 - accuracy: 0.3494 - jacard_coef: 0.06023/5 [=================>............] - ETA: 0s - loss: 0.1794 - accuracy: 0.3993 - jacard_coef: 0.07454/5 [=======================>......] - ETA: 0s - loss: 0.1800 - accuracy: 0.4171 - jacard_coef: 0.08125/5 [==============================] - 2s 387ms/step - loss: 0.1800 - accuracy: 0.4169 - jacard_coef: 0.0781 - val_loss: 1.1007 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0000 (epoch 1)
  Final Val Loss: 1.1007
  Training Time: 0:01:32.602475
  Stability (std): 0.0063

Results saved to: hyperparameter_optimization_20250926_165036/exp_12_UNet_lr5e-3_bs32/UNet_lr0.005_bs32_results.json

Experiment 12 completed in 107s
Progress: 12/36 completed
Estimated remaining time: 42 minutes

ðŸ”¬ EXPERIMENT 13/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878012.879644 1072361 service.cc:145] XLA service 0x14eedd592c50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878012.879667 1072361 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878013.017323 1072361 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 12:03 - loss: 0.3407 - accuracy: 0.5017 - jacard_coef: 0.0878 2/17 [==>...........................] - ETA: 1:11 - loss: 0.3009 - accuracy: 0.3998 - jacard_coef: 0.0656  3/17 [====>.........................] - ETA: 34s - loss: 0.2748 - accuracy: 0.3785 - jacard_coef: 0.0751  4/17 [======>.......................] - ETA: 22s - loss: 0.2567 - accuracy: 0.3739 - jacard_coef: 0.0742 5/17 [=======>......................] - ETA: 15s - loss: 0.2442 - accuracy: 0.3478 - jacard_coef: 0.0802 6/17 [=========>....................] - ETA: 12s - loss: 0.2354 - accuracy: 0.3082 - jacard_coef: 0.0738 7/17 [===========>..................] - ETA: 9s - loss: 0.2306 - accuracy: 0.3018 - jacard_coef: 0.0714  8/17 [=============>................] - ETA: 7s - loss: 0.2266 - accuracy: 0.2887 - jacard_coef: 0.0705 9/17 [==============>...............] - ETA: 6s - loss: 0.2227 - accuracy: 0.2741 - jacard_coef: 0.072310/17 [================>.............] - ETA: 4s - loss: 0.2199 - accuracy: 0.2581 - jacard_coef: 0.072411/17 [==================>...........] - ETA: 3s - loss: 0.2171 - accuracy: 0.2444 - jacard_coef: 0.073712/17 [====================>.........] - ETA: 2s - loss: 0.2147 - accuracy: 0.2352 - jacard_coef: 0.077013/17 [=====================>........] - ETA: 2s - loss: 0.2126 - accuracy: 0.2291 - jacard_coef: 0.079414/17 [=======================>......] - ETA: 1s - loss: 0.2103 - accuracy: 0.2261 - jacard_coef: 0.080915/17 [=========================>....] - ETA: 1s - loss: 0.2084 - accuracy: 0.2310 - jacard_coef: 0.083916/17 [===========================>..] - ETA: 0s - loss: 0.2066 - accuracy: 0.2369 - jacard_coef: 0.080817/17 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.2376 - jacard_coef: 0.076517/17 [==============================] - 59s 871ms/step - loss: 0.2064 - accuracy: 0.2376 - jacard_coef: 0.0765 - val_loss: 0.2493 - val_accuracy: 0.9046 - val_jacard_coef: 0.0162 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1781 - accuracy: 0.4230 - jacard_coef: 0.1143 2/17 [==>...........................] - ETA: 2s - loss: 0.1772 - accuracy: 0.4277 - jacard_coef: 0.1087 3/17 [====>.........................] - ETA: 2s - loss: 0.1759 - accuracy: 0.4535 - jacard_coef: 0.0959 4/17 [======>.......................] - ETA: 2s - loss: 0.1769 - accuracy: 0.4392 - jacard_coef: 0.0855 5/17 [=======>......................] - ETA: 2s - loss: 0.1765 - accuracy: 0.4528 - jacard_coef: 0.0873 6/17 [=========>....................] - ETA: 1s - loss: 0.1764 - accuracy: 0.4612 - jacard_coef: 0.0772 7/17 [===========>..................] - ETA: 1s - loss: 0.1763 - accuracy: 0.4715 - jacard_coef: 0.0760 8/17 [=============>................] - ETA: 1s - loss: 0.1763 - accuracy: 0.4674 - jacard_coef: 0.0761 9/17 [==============>...............] - ETA: 1s - loss: 0.1764 - accuracy: 0.4612 - jacard_coef: 0.076910/17 [================>.............] - ETA: 1s - loss: 0.1764 - accuracy: 0.4510 - jacard_coef: 0.077511/17 [==================>...........] - ETA: 1s - loss: 0.1762 - accuracy: 0.4485 - jacard_coef: 0.076812/17 [====================>.........] - ETA: 0s - loss: 0.1761 - accuracy: 0.4308 - jacard_coef: 0.079713/17 [=====================>........] - ETA: 0s - loss: 0.1761 - accuracy: 0.4127 - jacard_coef: 0.077814/17 [=======================>......] - ETA: 0s - loss: 0.1760 - accuracy: 0.4009 - jacard_coef: 0.078415/17 [=========================>....] - ETA: 0s - loss: 0.1758 - accuracy: 0.4056 - jacard_coef: 0.076416/17 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.4202 - jacard_coef: 0.078817/17 [==============================] - 3s 180ms/step - loss: 0.1755 - accuracy: 0.4201 - jacard_coef: 0.0764 - val_loss: 0.7565 - val_accuracy: 0.9302 - val_jacard_coef: 2.3436e-05 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1730 - accuracy: 0.7287 - jacard_coef: 0.0504 2/17 [==>...........................] - ETA: 2s - loss: 0.1740 - accuracy: 0.7351 - jacard_coef: 0.0758 3/17 [====>.........................] - ETA: 2s - loss: 0.1743 - accuracy: 0.7153 - jacard_coef: 0.0747 4/17 [======>.......................] - ETA: 2s - loss: 0.1735 - accuracy: 0.7403 - jacard_coef: 0.0651 5/17 [=======>......................] - ETA: 2s - loss: 0.1732 - accuracy: 0.7429 - jacard_coef: 0.0632 6/17 [=========>....................] - ETA: 1s - loss: 0.1734 - accuracy: 0.7410 - jacard_coef: 0.0650 7/17 [===========>..................] - ETA: 1s - loss: 0.1733 - accuracy: 0.7388 - jacard_coef: 0.0716 8/17 [=============>................] - ETA: 1s - loss: 0.1729 - accuracy: 0.7429 - jacard_coef: 0.0694 9/17 [==============>...............] - ETA: 1s - loss: 0.1727 - accuracy: 0.7504 - jacard_coef: 0.072310/17 [================>.............] - ETA: 1s - loss: 0.1726 - accuracy: 0.7512 - jacard_coef: 0.075511/17 [==================>...........] - ETA: 1s - loss: 0.1723 - accuracy: 0.7557 - jacard_coef: 0.073212/17 [====================>.........] - ETA: 0s - loss: 0.1720 - accuracy: 0.7578 - jacard_coef: 0.070813/17 [=====================>........] - ETA: 0s - loss: 0.1717 - accuracy: 0.7605 - jacard_coef: 0.066914/17 [=======================>......] - ETA: 0s - loss: 0.1714 - accuracy: 0.7601 - jacard_coef: 0.064415/17 [=========================>....] - ETA: 0s - loss: 0.1712 - accuracy: 0.7577 - jacard_coef: 0.066916/17 [===========================>..] - ETA: 0s - loss: 0.1709 - accuracy: 0.7547 - jacard_coef: 0.068017/17 [==============================] - 3s 179ms/step - loss: 0.1709 - accuracy: 0.7539 - jacard_coef: 0.0640 - val_loss: 0.2367 - val_accuracy: 0.9263 - val_jacard_coef: 0.0016 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1645 - accuracy: 0.7257 - jacard_coef: 0.0962 2/17 [==>...........................] - ETA: 2s - loss: 0.1665 - accuracy: 0.6705 - jacard_coef: 0.0772 3/17 [====>.........................] - ETA: 2s - loss: 0.1654 - accuracy: 0.6947 - jacard_coef: 0.0688 4/17 [======>.......................] - ETA: 2s - loss: 0.1652 - accuracy: 0.7359 - jacard_coef: 0.0562 5/17 [=======>......................] - ETA: 2s - loss: 0.1649 - accuracy: 0.7713 - jacard_coef: 0.0473 6/17 [=========>....................] - ETA: 1s - loss: 0.1649 - accuracy: 0.7905 - jacard_coef: 0.0421 7/17 [===========>..................] - ETA: 1s - loss: 0.1648 - accuracy: 0.8050 - jacard_coef: 0.0376 8/17 [=============>................] - ETA: 1s - loss: 0.1648 - accuracy: 0.8014 - jacard_coef: 0.0365 9/17 [==============>...............] - ETA: 1s - loss: 0.1646 - accuracy: 0.8101 - jacard_coef: 0.039310/17 [================>.............] - ETA: 1s - loss: 0.1644 - accuracy: 0.8142 - jacard_coef: 0.038411/17 [==================>...........] - ETA: 1s - loss: 0.1642 - accuracy: 0.8203 - jacard_coef: 0.036612/17 [====================>.........] - ETA: 0s - loss: 0.1640 - accuracy: 0.8237 - jacard_coef: 0.037613/17 [=====================>........] - ETA: 0s - loss: 0.1639 - accuracy: 0.8244 - jacard_coef: 0.039914/17 [=======================>......] - ETA: 0s - loss: 0.1637 - accuracy: 0.8275 - jacard_coef: 0.039115/17 [=========================>....] - ETA: 0s - loss: 0.1636 - accuracy: 0.8295 - jacard_coef: 0.037716/17 [===========================>..] - ETA: 0s - loss: 0.1635 - accuracy: 0.8218 - jacard_coef: 0.039817/17 [==============================] - 3s 180ms/step - loss: 0.1644 - accuracy: 0.8190 - jacard_coef: 0.0376 - val_loss: 0.1340 - val_accuracy: 0.9252 - val_jacard_coef: 0.0058 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1825 - accuracy: 0.1600 - jacard_coef: 0.0478 2/17 [==>...........................] - ETA: 2s - loss: 0.1764 - accuracy: 0.1989 - jacard_coef: 0.0514 3/17 [====>.........................] - ETA: 2s - loss: 0.1744 - accuracy: 0.3288 - jacard_coef: 0.0700 4/17 [======>.......................] - ETA: 2s - loss: 0.1729 - accuracy: 0.4701 - jacard_coef: 0.0591 5/17 [=======>......................] - ETA: 2s - loss: 0.1714 - accuracy: 0.5607 - jacard_coef: 0.0494 6/17 [=========>....................] - ETA: 1s - loss: 0.1713 - accuracy: 0.6121 - jacard_coef: 0.0461 7/17 [===========>..................] - ETA: 1s - loss: 0.1705 - accuracy: 0.6533 - jacard_coef: 0.0424 8/17 [=============>................] - ETA: 1s - loss: 0.1698 - accuracy: 0.6828 - jacard_coef: 0.0416 9/17 [==============>...............] - ETA: 1s - loss: 0.1697 - accuracy: 0.7022 - jacard_coef: 0.041410/17 [================>.............] - ETA: 1s - loss: 0.1698 - accuracy: 0.7146 - jacard_coef: 0.044911/17 [==================>...........] - ETA: 1s - loss: 0.1692 - accuracy: 0.7301 - jacard_coef: 0.045312/17 [====================>.........] - ETA: 0s - loss: 0.1690 - accuracy: 0.7368 - jacard_coef: 0.048113/17 [=====================>........] - ETA: 0s - loss: 0.1687 - accuracy: 0.7438 - jacard_coef: 0.049214/17 [=======================>......] - ETA: 0s - loss: 0.1686 - accuracy: 0.7492 - jacard_coef: 0.049515/17 [=========================>....] - ETA: 0s - loss: 0.1683 - accuracy: 0.7565 - jacard_coef: 0.049816/17 [===========================>..] - ETA: 0s - loss: 0.1679 - accuracy: 0.7657 - jacard_coef: 0.053617/17 [==============================] - 3s 179ms/step - loss: 0.1679 - accuracy: 0.7659 - jacard_coef: 0.0605 - val_loss: 1.0627 - val_accuracy: 0.9234 - val_jacard_coef: 0.0064 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1607 - accuracy: 0.8470 - jacard_coef: 0.0726 2/17 [==>...........................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8339 - jacard_coef: 0.0645 3/17 [====>.........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.8407 - jacard_coef: 0.0779 4/17 [======>.......................] - ETA: 2s - loss: 0.1613 - accuracy: 0.8367 - jacard_coef: 0.0789 5/17 [=======>......................] - ETA: 2s - loss: 0.1610 - accuracy: 0.8379 - jacard_coef: 0.0822 6/17 [=========>....................] - ETA: 1s - loss: 0.1613 - accuracy: 0.8372 - jacard_coef: 0.0739 7/17 [===========>..................] - ETA: 1s - loss: 0.1615 - accuracy: 0.8354 - jacard_coef: 0.0750 8/17 [=============>................] - ETA: 1s - loss: 0.1613 - accuracy: 0.8383 - jacard_coef: 0.0682 9/17 [==============>...............] - ETA: 1s - loss: 0.1607 - accuracy: 0.8445 - jacard_coef: 0.071510/17 [================>.............] - ETA: 1s - loss: 0.1604 - accuracy: 0.8474 - jacard_coef: 0.066911/17 [==================>...........] - ETA: 1s - loss: 0.1602 - accuracy: 0.8482 - jacard_coef: 0.065412/17 [====================>.........] - ETA: 0s - loss: 0.1608 - accuracy: 0.8252 - jacard_coef: 0.069113/17 [=====================>........] - ETA: 0s - loss: 0.1606 - accuracy: 0.8312 - jacard_coef: 0.064414/17 [=======================>......] - ETA: 0s - loss: 0.1605 - accuracy: 0.8333 - jacard_coef: 0.064315/17 [=========================>....] - ETA: 0s - loss: 0.1604 - accuracy: 0.8343 - jacard_coef: 0.062316/17 [===========================>..] - ETA: 0s - loss: 0.1604 - accuracy: 0.8349 - jacard_coef: 0.060017/17 [==============================] - 3s 180ms/step - loss: 0.1604 - accuracy: 0.8347 - jacard_coef: 0.0619 - val_loss: 0.4208 - val_accuracy: 0.9231 - val_jacard_coef: 0.0064 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1583 - accuracy: 0.8380 - jacard_coef: 0.0353 2/17 [==>...........................] - ETA: 2s - loss: 0.1588 - accuracy: 0.8477 - jacard_coef: 0.0375 3/17 [====>.........................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8570 - jacard_coef: 0.0380 4/17 [======>.......................] - ETA: 2s - loss: 0.1584 - accuracy: 0.8597 - jacard_coef: 0.0361 5/17 [=======>......................] - ETA: 2s - loss: 0.1580 - accuracy: 0.8670 - jacard_coef: 0.0332 6/17 [=========>....................] - ETA: 1s - loss: 0.1581 - accuracy: 0.8694 - jacard_coef: 0.0364 7/17 [===========>..................] - ETA: 1s - loss: 0.1577 - accuracy: 0.8750 - jacard_coef: 0.0357 8/17 [=============>................] - ETA: 1s - loss: 0.1576 - accuracy: 0.8791 - jacard_coef: 0.0356 9/17 [==============>...............] - ETA: 1s - loss: 0.1571 - accuracy: 0.8852 - jacard_coef: 0.060110/17 [================>.............] - ETA: 1s - loss: 0.1569 - accuracy: 0.8876 - jacard_coef: 0.061911/17 [==================>...........] - ETA: 1s - loss: 0.1570 - accuracy: 0.8858 - jacard_coef: 0.056712/17 [====================>.........] - ETA: 0s - loss: 0.1570 - accuracy: 0.8833 - jacard_coef: 0.055713/17 [=====================>........] - ETA: 0s - loss: 0.1572 - accuracy: 0.8804 - jacard_coef: 0.053614/17 [=======================>......] - ETA: 0s - loss: 0.1570 - accuracy: 0.8809 - jacard_coef: 0.052915/17 [=========================>....] - ETA: 0s - loss: 0.1568 - accuracy: 0.8814 - jacard_coef: 0.052516/17 [===========================>..] - ETA: 0s - loss: 0.1568 - accuracy: 0.8812 - jacard_coef: 0.052817/17 [==============================] - 3s 180ms/step - loss: 0.1568 - accuracy: 0.8813 - jacard_coef: 0.0665 - val_loss: 0.1415 - val_accuracy: 0.9262 - val_jacard_coef: 0.0032 - lr: 5.0000e-04
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1543 - accuracy: 0.8965 - jacard_coef: 0.0656 2/17 [==>...........................] - ETA: 2s - loss: 0.1544 - accuracy: 0.8953 - jacard_coef: 0.0385 3/17 [====>.........................] - ETA: 2s - loss: 0.1560 - accuracy: 0.8743 - jacard_coef: 0.0319 4/17 [======>.......................] - ETA: 2s - loss: 0.1561 - accuracy: 0.8661 - jacard_coef: 0.0342 5/17 [=======>......................] - ETA: 2s - loss: 0.1553 - accuracy: 0.8674 - jacard_coef: 0.0419 6/17 [=========>....................] - ETA: 1s - loss: 0.1552 - accuracy: 0.8688 - jacard_coef: 0.0627 7/17 [===========>..................] - ETA: 1s - loss: 0.1549 - accuracy: 0.8701 - jacard_coef: 0.0652 8/17 [=============>................] - ETA: 1s - loss: 0.1545 - accuracy: 0.8734 - jacard_coef: 0.0766 9/17 [==============>...............] - ETA: 1s - loss: 0.1544 - accuracy: 0.8747 - jacard_coef: 0.082010/17 [================>.............] - ETA: 1s - loss: 0.1542 - accuracy: 0.8732 - jacard_coef: 0.081411/17 [==================>...........] - ETA: 1s - loss: 0.1542 - accuracy: 0.8721 - jacard_coef: 0.078312/17 [====================>.........] - ETA: 0s - loss: 0.1540 - accuracy: 0.8736 - jacard_coef: 0.080713/17 [=====================>........] - ETA: 0s - loss: 0.1540 - accuracy: 0.8708 - jacard_coef: 0.076514/17 [=======================>......] - ETA: 0s - loss: 0.1541 - accuracy: 0.8689 - jacard_coef: 0.073315/17 [=========================>....] - ETA: 0s - loss: 0.1542 - accuracy: 0.8658 - jacard_coef: 0.071016/17 [===========================>..] - ETA: 0s - loss: 0.1540 - accuracy: 0.8669 - jacard_coef: 0.070917/17 [==============================] - 3s 180ms/step - loss: 0.1540 - accuracy: 0.8670 - jacard_coef: 0.0667 - val_loss: 0.0835 - val_accuracy: 0.9240 - val_jacard_coef: 0.0062 - lr: 5.0000e-04
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1526 - accuracy: 0.8610 - jacard_coef: 0.0817 2/17 [==>...........................] - ETA: 2s - loss: 0.1533 - accuracy: 0.8631 - jacard_coef: 0.0460 3/17 [====>.........................] - ETA: 2s - loss: 0.1525 - accuracy: 0.8725 - jacard_coef: 0.0427 4/17 [======>.......................] - ETA: 2s - loss: 0.1518 - accuracy: 0.8839 - jacard_coef: 0.0454 5/17 [=======>......................] - ETA: 2s - loss: 0.1522 - accuracy: 0.8815 - jacard_coef: 0.0460 6/17 [=========>....................] - ETA: 1s - loss: 0.1520 - accuracy: 0.8857 - jacard_coef: 0.0431 7/17 [===========>..................] - ETA: 1s - loss: 0.1516 - accuracy: 0.8918 - jacard_coef: 0.0402 8/17 [=============>................] - ETA: 1s - loss: 0.1519 - accuracy: 0.8874 - jacard_coef: 0.0365 9/17 [==============>...............] - ETA: 1s - loss: 0.1518 - accuracy: 0.8876 - jacard_coef: 0.037610/17 [================>.............] - ETA: 1s - loss: 0.1515 - accuracy: 0.8927 - jacard_coef: 0.033811/17 [==================>...........] - ETA: 1s - loss: 0.1516 - accuracy: 0.8923 - jacard_coef: 0.031512/17 [====================>.........] - ETA: 0s - loss: 0.1513 - accuracy: 0.8969 - jacard_coef: 0.030313/17 [=====================>........] - ETA: 0s - loss: 0.1512 - accuracy: 0.8976 - jacard_coef: 0.028714/17 [=======================>......] - ETA: 0s - loss: 0.1510 - accuracy: 0.8991 - jacard_coef: 0.027415/17 [=========================>....] - ETA: 0s - loss: 0.1510 - accuracy: 0.8991 - jacard_coef: 0.026116/17 [===========================>..] - ETA: 0s - loss: 0.1509 - accuracy: 0.8998 - jacard_coef: 0.024517/17 [==============================] - 3s 180ms/step - loss: 0.1509 - accuracy: 0.8991 - jacard_coef: 0.0301 - val_loss: 0.0966 - val_accuracy: 0.9292 - val_jacard_coef: 0.0019 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1501 - accuracy: 0.8281 - jacard_coef: 0.0607 2/17 [==>...........................] - ETA: 2s - loss: 0.1494 - accuracy: 0.8737 - jacard_coef: 0.0337 3/17 [====>.........................] - ETA: 2s - loss: 0.1502 - accuracy: 0.8789 - jacard_coef: 0.0257 4/17 [======>.......................] - ETA: 2s - loss: 0.1502 - accuracy: 0.8917 - jacard_coef: 0.0216 5/17 [=======>......................] - ETA: 2s - loss: 0.1505 - accuracy: 0.8918 - jacard_coef: 0.0192 6/17 [=========>....................] - ETA: 1s - loss: 0.1502 - accuracy: 0.8988 - jacard_coef: 0.0161 7/17 [===========>..................] - ETA: 1s - loss: 0.1500 - accuracy: 0.9036 - jacard_coef: 0.0141 8/17 [=============>................] - ETA: 1s - loss: 0.1500 - accuracy: 0.9034 - jacard_coef: 0.0131 9/17 [==============>...............] - ETA: 1s - loss: 0.1497 - accuracy: 0.9071 - jacard_coef: 0.011610/17 [================>.............] - ETA: 1s - loss: 0.1504 - accuracy: 0.9004 - jacard_coef: 0.011911/17 [==================>...........] - ETA: 1s - loss: 0.1503 - accuracy: 0.9021 - jacard_coef: 0.011312/17 [====================>.........] - ETA: 0s - loss: 0.1504 - accuracy: 0.9028 - jacard_coef: 0.010913/17 [=====================>........] - ETA: 0s - loss: 0.1502 - accuracy: 0.9050 - jacard_coef: 0.010214/17 [=======================>......] - ETA: 0s - loss: 0.1502 - accuracy: 0.9054 - jacard_coef: 0.009615/17 [=========================>....] - ETA: 0s - loss: 0.1502 - accuracy: 0.9048 - jacard_coef: 0.009116/17 [===========================>..] - ETA: 0s - loss: 0.1499 - accuracy: 0.9069 - jacard_coef: 0.008617/17 [==============================] - 3s 180ms/step - loss: 0.1499 - accuracy: 0.9074 - jacard_coef: 0.0081 - val_loss: 0.0890 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1502 - accuracy: 0.8862 - jacard_coef: 2.1779e-04 2/17 [==>...........................] - ETA: 2s - loss: 0.1482 - accuracy: 0.9207 - jacard_coef: 1.7271e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1487 - accuracy: 0.9106 - jacard_coef: 1.6738e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1480 - accuracy: 0.9220 - jacard_coef: 3.3179e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1482 - accuracy: 0.9196 - jacard_coef: 3.9681e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1484 - accuracy: 0.9147 - jacard_coef: 4.4086e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1481 - accuracy: 0.9167 - jacard_coef: 3.7788e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1483 - accuracy: 0.9168 - jacard_coef: 4.7309e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1484 - accuracy: 0.9134 - jacard_coef: 5.9236e-0410/17 [================>.............] - ETA: 1s - loss: 0.1486 - accuracy: 0.9120 - jacard_coef: 6.7935e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1486 - accuracy: 0.9114 - jacard_coef: 0.0011    12/17 [====================>.........] - ETA: 0s - loss: 0.1483 - accuracy: 0.9135 - jacard_coef: 0.001113/17 [=====================>........] - ETA: 0s - loss: 0.1483 - accuracy: 0.9134 - jacard_coef: 0.002014/17 [=======================>......] - ETA: 0s - loss: 0.1481 - accuracy: 0.9147 - jacard_coef: 0.002015/17 [=========================>....] - ETA: 0s - loss: 0.1482 - accuracy: 0.9136 - jacard_coef: 0.002816/17 [===========================>..] - ETA: 0s - loss: 0.1479 - accuracy: 0.9156 - jacard_coef: 0.002717/17 [==============================] - 3s 180ms/step - loss: 0.1481 - accuracy: 0.9136 - jacard_coef: 0.0027 - val_loss: 0.1454 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0162 (epoch 1)
  Final Val Loss: 0.1454
  Training Time: 0:01:30.178351
  Stability (std): 0.3186

Results saved to: hyperparameter_optimization_20250926_165036/exp_13_Attention_UNet_lr1e-4_bs8/Attention_UNet_lr0.0001_bs8_results.json

Experiment 13 completed in 105s
Progress: 13/36 completed
Estimated remaining time: 40 minutes

ðŸ”¬ EXPERIMENT 14/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878121.171496 1078426 service.cc:145] XLA service 0x150d991828e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878121.171519 1078426 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878121.308377 1078426 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 6:42 - loss: 0.3370 - accuracy: 0.4948 - jacard_coef: 0.08962/9 [=====>........................] - ETA: 58s - loss: 0.3025 - accuracy: 0.4124 - jacard_coef: 0.0731 3/9 [=========>....................] - ETA: 26s - loss: 0.2761 - accuracy: 0.3677 - jacard_coef: 0.09414/9 [============>.................] - ETA: 15s - loss: 0.2594 - accuracy: 0.3474 - jacard_coef: 0.09235/9 [===============>..............] - ETA: 9s - loss: 0.2474 - accuracy: 0.3395 - jacard_coef: 0.0877 6/9 [===================>..........] - ETA: 5s - loss: 0.2380 - accuracy: 0.3252 - jacard_coef: 0.08777/9 [======================>.......] - ETA: 3s - loss: 0.2313 - accuracy: 0.3121 - jacard_coef: 0.08138/9 [=========================>....] - ETA: 1s - loss: 0.2256 - accuracy: 0.3047 - jacard_coef: 0.08009/9 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.3049 - jacard_coef: 0.07229/9 [==============================] - 69s 2s/step - loss: 0.2254 - accuracy: 0.3049 - jacard_coef: 0.0722 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1856 - accuracy: 0.1287 - jacard_coef: 0.07472/9 [=====>........................] - ETA: 2s - loss: 0.1862 - accuracy: 0.1136 - jacard_coef: 0.06893/9 [=========>....................] - ETA: 2s - loss: 0.1871 - accuracy: 0.1063 - jacard_coef: 0.06544/9 [============>.................] - ETA: 1s - loss: 0.1874 - accuracy: 0.1164 - jacard_coef: 0.07675/9 [===============>..............] - ETA: 1s - loss: 0.1879 - accuracy: 0.1243 - jacard_coef: 0.08186/9 [===================>..........] - ETA: 1s - loss: 0.1878 - accuracy: 0.1267 - jacard_coef: 0.08047/9 [======================>.......] - ETA: 0s - loss: 0.1875 - accuracy: 0.1316 - jacard_coef: 0.08118/9 [=========================>....] - ETA: 0s - loss: 0.1874 - accuracy: 0.1353 - jacard_coef: 0.08259/9 [==============================] - 3s 333ms/step - loss: 0.1873 - accuracy: 0.1358 - jacard_coef: 0.0889 - val_loss: 1.0995 - val_accuracy: 0.9287 - val_jacard_coef: 2.8522e-04 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1853 - accuracy: 0.1027 - jacard_coef: 0.06842/9 [=====>........................] - ETA: 2s - loss: 0.1848 - accuracy: 0.1175 - jacard_coef: 0.08473/9 [=========>....................] - ETA: 2s - loss: 0.1847 - accuracy: 0.1154 - jacard_coef: 0.08274/9 [============>.................] - ETA: 1s - loss: 0.1847 - accuracy: 0.1211 - jacard_coef: 0.08755/9 [===============>..............] - ETA: 1s - loss: 0.1843 - accuracy: 0.1169 - jacard_coef: 0.08156/9 [===================>..........] - ETA: 1s - loss: 0.1841 - accuracy: 0.1220 - jacard_coef: 0.08377/9 [======================>.......] - ETA: 0s - loss: 0.1836 - accuracy: 0.1325 - jacard_coef: 0.08638/9 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.1428 - jacard_coef: 0.08399/9 [==============================] - 3s 326ms/step - loss: 0.1828 - accuracy: 0.1431 - jacard_coef: 0.0748 - val_loss: 1.0919 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1736 - accuracy: 0.3628 - jacard_coef: 0.08372/9 [=====>........................] - ETA: 2s - loss: 0.1786 - accuracy: 0.3444 - jacard_coef: 0.08653/9 [=========>....................] - ETA: 2s - loss: 0.1777 - accuracy: 0.3440 - jacard_coef: 0.08334/9 [============>.................] - ETA: 1s - loss: 0.1783 - accuracy: 0.4045 - jacard_coef: 0.07545/9 [===============>..............] - ETA: 1s - loss: 0.1783 - accuracy: 0.4496 - jacard_coef: 0.07016/9 [===================>..........] - ETA: 1s - loss: 0.1787 - accuracy: 0.4720 - jacard_coef: 0.06687/9 [======================>.......] - ETA: 0s - loss: 0.1784 - accuracy: 0.4925 - jacard_coef: 0.07128/9 [=========================>....] - ETA: 0s - loss: 0.1786 - accuracy: 0.4906 - jacard_coef: 0.07489/9 [==============================] - 3s 326ms/step - loss: 0.1786 - accuracy: 0.4903 - jacard_coef: 0.0684 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1804 - accuracy: 0.2459 - jacard_coef: 0.10332/9 [=====>........................] - ETA: 2s - loss: 0.1786 - accuracy: 0.2695 - jacard_coef: 0.07543/9 [=========>....................] - ETA: 2s - loss: 0.1771 - accuracy: 0.3077 - jacard_coef: 0.07534/9 [============>.................] - ETA: 1s - loss: 0.1767 - accuracy: 0.3333 - jacard_coef: 0.08215/9 [===============>..............] - ETA: 1s - loss: 0.1758 - accuracy: 0.3608 - jacard_coef: 0.08296/9 [===================>..........] - ETA: 1s - loss: 0.1755 - accuracy: 0.3789 - jacard_coef: 0.08327/9 [======================>.......] - ETA: 0s - loss: 0.1749 - accuracy: 0.3972 - jacard_coef: 0.08278/9 [=========================>....] - ETA: 0s - loss: 0.1744 - accuracy: 0.4125 - jacard_coef: 0.08239/9 [==============================] - 3s 334ms/step - loss: 0.1744 - accuracy: 0.4140 - jacard_coef: 0.0883 - val_loss: 1.1175 - val_accuracy: 0.9297 - val_jacard_coef: 0.0010 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1687 - accuracy: 0.5658 - jacard_coef: 0.04222/9 [=====>........................] - ETA: 2s - loss: 0.1675 - accuracy: 0.6042 - jacard_coef: 0.05553/9 [=========>....................] - ETA: 2s - loss: 0.1672 - accuracy: 0.5919 - jacard_coef: 0.06714/9 [============>.................] - ETA: 1s - loss: 0.1668 - accuracy: 0.5957 - jacard_coef: 0.06665/9 [===============>..............] - ETA: 1s - loss: 0.1665 - accuracy: 0.6117 - jacard_coef: 0.06746/9 [===================>..........] - ETA: 1s - loss: 0.1663 - accuracy: 0.6506 - jacard_coef: 0.05877/9 [======================>.......] - ETA: 0s - loss: 0.1661 - accuracy: 0.6732 - jacard_coef: 0.05838/9 [=========================>....] - ETA: 0s - loss: 0.1659 - accuracy: 0.6961 - jacard_coef: 0.05319/9 [==============================] - 3s 326ms/step - loss: 0.1659 - accuracy: 0.6951 - jacard_coef: 0.0616 - val_loss: 1.1153 - val_accuracy: 0.9304 - val_jacard_coef: 1.4608e-12 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1646 - accuracy: 0.8775 - jacard_coef: 0.02192/9 [=====>........................] - ETA: 2s - loss: 0.1653 - accuracy: 0.8803 - jacard_coef: 0.02573/9 [=========>....................] - ETA: 2s - loss: 0.1663 - accuracy: 0.8747 - jacard_coef: 0.03394/9 [============>.................] - ETA: 1s - loss: 0.1663 - accuracy: 0.8770 - jacard_coef: 0.03315/9 [===============>..............] - ETA: 1s - loss: 0.1668 - accuracy: 0.8739 - jacard_coef: 0.03486/9 [===================>..........] - ETA: 1s - loss: 0.1673 - accuracy: 0.8686 - jacard_coef: 0.03357/9 [======================>.......] - ETA: 0s - loss: 0.1675 - accuracy: 0.8651 - jacard_coef: 0.03478/9 [=========================>....] - ETA: 0s - loss: 0.1676 - accuracy: 0.8635 - jacard_coef: 0.03349/9 [==============================] - 3s 331ms/step - loss: 0.1676 - accuracy: 0.8639 - jacard_coef: 0.0297 - val_loss: 0.9762 - val_accuracy: 0.9232 - val_jacard_coef: 0.0054 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1699 - accuracy: 0.8352 - jacard_coef: 0.04632/9 [=====>........................] - ETA: 2s - loss: 0.1682 - accuracy: 0.8492 - jacard_coef: 0.04483/9 [=========>....................] - ETA: 2s - loss: 0.1673 - accuracy: 0.8600 - jacard_coef: 0.03624/9 [============>.................] - ETA: 1s - loss: 0.1669 - accuracy: 0.8681 - jacard_coef: 0.03055/9 [===============>..............] - ETA: 1s - loss: 0.1663 - accuracy: 0.8776 - jacard_coef: 0.02646/9 [===================>..........] - ETA: 1s - loss: 0.1663 - accuracy: 0.8827 - jacard_coef: 0.02287/9 [======================>.......] - ETA: 0s - loss: 0.1660 - accuracy: 0.8895 - jacard_coef: 0.02058/9 [=========================>....] - ETA: 0s - loss: 0.1656 - accuracy: 0.8927 - jacard_coef: 0.01859/9 [==============================] - 3s 326ms/step - loss: 0.1656 - accuracy: 0.8929 - jacard_coef: 0.0178 - val_loss: 0.4689 - val_accuracy: 0.9266 - val_jacard_coef: 0.0051 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1627 - accuracy: 0.9052 - jacard_coef: 0.00402/9 [=====>........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.9206 - jacard_coef: 0.00413/9 [=========>....................] - ETA: 2s - loss: 0.1606 - accuracy: 0.9145 - jacard_coef: 0.00374/9 [============>.................] - ETA: 1s - loss: 0.1601 - accuracy: 0.8883 - jacard_coef: 0.01665/9 [===============>..............] - ETA: 1s - loss: 0.1603 - accuracy: 0.8892 - jacard_coef: 0.01436/9 [===================>..........] - ETA: 1s - loss: 0.1600 - accuracy: 0.8939 - jacard_coef: 0.01277/9 [======================>.......] - ETA: 0s - loss: 0.1601 - accuracy: 0.8918 - jacard_coef: 0.01438/9 [=========================>....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8963 - jacard_coef: 0.01259/9 [==============================] - 3s 327ms/step - loss: 0.1598 - accuracy: 0.8956 - jacard_coef: 0.0112 - val_loss: 0.1554 - val_accuracy: 0.9255 - val_jacard_coef: 0.0030 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1583 - accuracy: 0.8950 - jacard_coef: 0.00222/9 [=====>........................] - ETA: 2s - loss: 0.1577 - accuracy: 0.9102 - jacard_coef: 0.00303/9 [=========>....................] - ETA: 2s - loss: 0.1582 - accuracy: 0.9068 - jacard_coef: 0.00324/9 [============>.................] - ETA: 1s - loss: 0.1622 - accuracy: 0.7897 - jacard_coef: 0.01955/9 [===============>..............] - ETA: 1s - loss: 0.1615 - accuracy: 0.8149 - jacard_coef: 0.02246/9 [===================>..........] - ETA: 1s - loss: 0.1617 - accuracy: 0.8246 - jacard_coef: 0.02357/9 [======================>.......] - ETA: 0s - loss: 0.1617 - accuracy: 0.8321 - jacard_coef: 0.02598/9 [=========================>....] - ETA: 0s - loss: 0.1619 - accuracy: 0.8345 - jacard_coef: 0.02559/9 [==============================] - 3s 327ms/step - loss: 0.1619 - accuracy: 0.8338 - jacard_coef: 0.0298 - val_loss: 1.0188 - val_accuracy: 0.9264 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1620 - accuracy: 0.8835 - jacard_coef: 0.02952/9 [=====>........................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8754 - jacard_coef: 0.03553/9 [=========>....................] - ETA: 2s - loss: 0.1614 - accuracy: 0.8716 - jacard_coef: 0.03324/9 [============>.................] - ETA: 1s - loss: 0.1615 - accuracy: 0.8705 - jacard_coef: 0.03295/9 [===============>..............] - ETA: 1s - loss: 0.1610 - accuracy: 0.8763 - jacard_coef: 0.02986/9 [===================>..........] - ETA: 1s - loss: 0.1614 - accuracy: 0.8809 - jacard_coef: 0.02667/9 [======================>.......] - ETA: 0s - loss: 0.1612 - accuracy: 0.8856 - jacard_coef: 0.02308/9 [=========================>....] - ETA: 0s - loss: 0.1609 - accuracy: 0.8905 - jacard_coef: 0.02029/9 [==============================] - 3s 331ms/step - loss: 0.1610 - accuracy: 0.8898 - jacard_coef: 0.0183 - val_loss: 0.9447 - val_accuracy: 0.9072 - val_jacard_coef: 0.0192 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1584 - accuracy: 0.9335 - jacard_coef: 0.00152/9 [=====>........................] - ETA: 2s - loss: 0.1586 - accuracy: 0.9324 - jacard_coef: 0.00143/9 [=========>....................] - ETA: 2s - loss: 0.1587 - accuracy: 0.9245 - jacard_coef: 0.00144/9 [============>.................] - ETA: 1s - loss: 0.1585 - accuracy: 0.9259 - jacard_coef: 0.00155/9 [===============>..............] - ETA: 1s - loss: 0.1584 - accuracy: 0.9189 - jacard_coef: 0.00136/9 [===================>..........] - ETA: 1s - loss: 0.1585 - accuracy: 0.9147 - jacard_coef: 0.00147/9 [======================>.......] - ETA: 0s - loss: 0.1586 - accuracy: 0.9148 - jacard_coef: 0.00158/9 [=========================>....] - ETA: 0s - loss: 0.1584 - accuracy: 0.9159 - jacard_coef: 0.00159/9 [==============================] - 3s 332ms/step - loss: 0.1584 - accuracy: 0.9155 - jacard_coef: 0.0014 - val_loss: 12.9955 - val_accuracy: 0.0862 - val_jacard_coef: 0.0707 - lr: 0.0010
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1561 - accuracy: 0.8895 - jacard_coef: 0.00822/9 [=====>........................] - ETA: 2s - loss: 0.1560 - accuracy: 0.8955 - jacard_coef: 0.02443/9 [=========>....................] - ETA: 2s - loss: 0.1566 - accuracy: 0.8934 - jacard_coef: 0.02574/9 [============>.................] - ETA: 1s - loss: 0.1562 - accuracy: 0.8940 - jacard_coef: 0.02625/9 [===============>..............] - ETA: 1s - loss: 0.1559 - accuracy: 0.8980 - jacard_coef: 0.02686/9 [===================>..........] - ETA: 1s - loss: 0.1553 - accuracy: 0.9022 - jacard_coef: 0.02487/9 [======================>.......] - ETA: 0s - loss: 0.1549 - accuracy: 0.9040 - jacard_coef: 0.02138/9 [=========================>....] - ETA: 0s - loss: 0.1549 - accuracy: 0.9036 - jacard_coef: 0.01889/9 [==============================] - 3s 327ms/step - loss: 0.1560 - accuracy: 0.9011 - jacard_coef: 0.0173 - val_loss: 0.3199 - val_accuracy: 0.9241 - val_jacard_coef: 0.0053 - lr: 0.0010
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1566 - accuracy: 0.8933 - jacard_coef: 6.1621e-042/9 [=====>........................] - ETA: 2s - loss: 0.1558 - accuracy: 0.8901 - jacard_coef: 0.0155    3/9 [=========>....................] - ETA: 2s - loss: 0.1564 - accuracy: 0.8384 - jacard_coef: 0.03084/9 [============>.................] - ETA: 1s - loss: 0.1563 - accuracy: 0.8152 - jacard_coef: 0.03915/9 [===============>..............] - ETA: 1s - loss: 0.1558 - accuracy: 0.8084 - jacard_coef: 0.04276/9 [===================>..........] - ETA: 1s - loss: 0.1556 - accuracy: 0.8169 - jacard_coef: 0.03967/9 [======================>.......] - ETA: 0s - loss: 0.1551 - accuracy: 0.8287 - jacard_coef: 0.03578/9 [=========================>....] - ETA: 0s - loss: 0.1546 - accuracy: 0.8400 - jacard_coef: 0.03139/9 [==============================] - 3s 327ms/step - loss: 0.1546 - accuracy: 0.8407 - jacard_coef: 0.0279 - val_loss: 0.1002 - val_accuracy: 0.9104 - val_jacard_coef: 0.0150 - lr: 0.0010
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1509 - accuracy: 0.8815 - jacard_coef: 0.01872/9 [=====>........................] - ETA: 2s - loss: 0.1527 - accuracy: 0.8589 - jacard_coef: 0.03113/9 [=========>....................] - ETA: 2s - loss: 0.1517 - accuracy: 0.8658 - jacard_coef: 0.03264/9 [============>.................] - ETA: 1s - loss: 0.1514 - accuracy: 0.8697 - jacard_coef: 0.02725/9 [===============>..............] - ETA: 1s - loss: 0.1515 - accuracy: 0.8698 - jacard_coef: 0.02696/9 [===================>..........] - ETA: 1s - loss: 0.1512 - accuracy: 0.8769 - jacard_coef: 0.02267/9 [======================>.......] - ETA: 0s - loss: 0.1508 - accuracy: 0.8812 - jacard_coef: 0.02008/9 [=========================>....] - ETA: 0s - loss: 0.1503 - accuracy: 0.8868 - jacard_coef: 0.01839/9 [==============================] - 3s 326ms/step - loss: 0.1503 - accuracy: 0.8876 - jacard_coef: 0.0163 - val_loss: 0.0914 - val_accuracy: 0.9132 - val_jacard_coef: 0.0135 - lr: 0.0010
Epoch 16/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1497 - accuracy: 0.8955 - jacard_coef: 2.9182e-042/9 [=====>........................] - ETA: 2s - loss: 0.1490 - accuracy: 0.9048 - jacard_coef: 3.0151e-043/9 [=========>....................] - ETA: 2s - loss: 0.1496 - accuracy: 0.9072 - jacard_coef: 5.2584e-044/9 [============>.................] - ETA: 1s - loss: 0.1487 - accuracy: 0.9136 - jacard_coef: 4.6182e-045/9 [===============>..............] - ETA: 1s - loss: 0.1484 - accuracy: 0.9116 - jacard_coef: 4.3459e-046/9 [===================>..........] - ETA: 1s - loss: 0.1484 - accuracy: 0.9065 - jacard_coef: 0.0080    7/9 [======================>.......] - ETA: 0s - loss: 0.1481 - accuracy: 0.9058 - jacard_coef: 0.00898/9 [=========================>....] - ETA: 0s - loss: 0.1481 - accuracy: 0.9013 - jacard_coef: 0.00949/9 [==============================] - 3s 327ms/step - loss: 0.1482 - accuracy: 0.9006 - jacard_coef: 0.0086 - val_loss: 0.0901 - val_accuracy: 0.9230 - val_jacard_coef: 0.0064 - lr: 0.0010
Epoch 17/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1453 - accuracy: 0.9222 - jacard_coef: 0.01432/9 [=====>........................] - ETA: 2s - loss: 0.1461 - accuracy: 0.9156 - jacard_coef: 0.00833/9 [=========>....................] - ETA: 2s - loss: 0.1461 - accuracy: 0.9176 - jacard_coef: 0.00554/9 [============>.................] - ETA: 1s - loss: 0.1463 - accuracy: 0.9186 - jacard_coef: 0.00425/9 [===============>..............] - ETA: 1s - loss: 0.1464 - accuracy: 0.9204 - jacard_coef: 0.00336/9 [===================>..........] - ETA: 1s - loss: 0.1466 - accuracy: 0.9169 - jacard_coef: 0.00287/9 [======================>.......] - ETA: 0s - loss: 0.1465 - accuracy: 0.9173 - jacard_coef: 0.00248/9 [=========================>....] - ETA: 0s - loss: 0.1465 - accuracy: 0.9157 - jacard_coef: 0.00219/9 [==============================] - 3s 327ms/step - loss: 0.1465 - accuracy: 0.9155 - jacard_coef: 0.0019 - val_loss: 0.1592 - val_accuracy: 0.8840 - val_jacard_coef: 0.0324 - lr: 0.0010
Epoch 18/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1429 - accuracy: 0.9273 - jacard_coef: 0.00162/9 [=====>........................] - ETA: 2s - loss: 0.1442 - accuracy: 0.9161 - jacard_coef: 0.00973/9 [=========>....................] - ETA: 2s - loss: 0.1447 - accuracy: 0.9091 - jacard_coef: 0.00674/9 [============>.................] - ETA: 1s - loss: 0.1442 - accuracy: 0.9144 - jacard_coef: 0.00505/9 [===============>..............] - ETA: 1s - loss: 0.1440 - accuracy: 0.9156 - jacard_coef: 0.00426/9 [===================>..........] - ETA: 1s - loss: 0.1440 - accuracy: 0.9144 - jacard_coef: 0.00367/9 [======================>.......] - ETA: 0s - loss: 0.1443 - accuracy: 0.9115 - jacard_coef: 0.00328/9 [=========================>....] - ETA: 0s - loss: 0.1443 - accuracy: 0.9114 - jacard_coef: 0.00669/9 [==============================] - 3s 332ms/step - loss: 0.1450 - accuracy: 0.9071 - jacard_coef: 0.0086 - val_loss: 0.1671 - val_accuracy: 0.7320 - val_jacard_coef: 0.0751 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1442 - accuracy: 0.9108 - jacard_coef: 3.8493e-042/9 [=====>........................] - ETA: 2s - loss: 0.1445 - accuracy: 0.9119 - jacard_coef: 0.0031    3/9 [=========>....................] - ETA: 2s - loss: 0.1449 - accuracy: 0.9001 - jacard_coef: 0.00554/9 [============>.................] - ETA: 1s - loss: 0.1448 - accuracy: 0.8963 - jacard_coef: 0.00915/9 [===============>..............] - ETA: 1s - loss: 0.1452 - accuracy: 0.8884 - jacard_coef: 0.01186/9 [===================>..........] - ETA: 1s - loss: 0.1449 - accuracy: 0.8863 - jacard_coef: 0.01377/9 [======================>.......] - ETA: 0s - loss: 0.1446 - accuracy: 0.8855 - jacard_coef: 0.01658/9 [=========================>....] - ETA: 0s - loss: 0.1446 - accuracy: 0.8831 - jacard_coef: 0.01929/9 [==============================] - 3s 327ms/step - loss: 0.1451 - accuracy: 0.8803 - jacard_coef: 0.0179 - val_loss: 0.2778 - val_accuracy: 0.0766 - val_jacard_coef: 0.0700 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1431 - accuracy: 0.8921 - jacard_coef: 0.01912/9 [=====>........................] - ETA: 2s - loss: 0.1432 - accuracy: 0.8894 - jacard_coef: 0.01723/9 [=========>....................] - ETA: 2s - loss: 0.1434 - accuracy: 0.8889 - jacard_coef: 0.01654/9 [============>.................] - ETA: 1s - loss: 0.1430 - accuracy: 0.9013 - jacard_coef: 0.01275/9 [===============>..............] - ETA: 1s - loss: 0.1428 - accuracy: 0.9053 - jacard_coef: 0.01016/9 [===================>..........] - ETA: 1s - loss: 0.1424 - accuracy: 0.9099 - jacard_coef: 0.00847/9 [======================>.......] - ETA: 0s - loss: 0.1425 - accuracy: 0.9087 - jacard_coef: 0.01158/9 [=========================>....] - ETA: 0s - loss: 0.1424 - accuracy: 0.9088 - jacard_coef: 0.01019/9 [==============================] - 3s 327ms/step - loss: 0.1427 - accuracy: 0.9050 - jacard_coef: 0.0106 - val_loss: 0.2122 - val_accuracy: 0.1061 - val_jacard_coef: 0.0709 - lr: 5.0000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1422 - accuracy: 0.9032 - jacard_coef: 0.01992/9 [=====>........................] - ETA: 2s - loss: 0.1419 - accuracy: 0.9047 - jacard_coef: 0.00993/9 [=========>....................] - ETA: 2s - loss: 0.1421 - accuracy: 0.8998 - jacard_coef: 0.02164/9 [============>.................] - ETA: 1s - loss: 0.1417 - accuracy: 0.9020 - jacard_coef: 0.01885/9 [===============>..............] - ETA: 1s - loss: 0.1416 - accuracy: 0.9025 - jacard_coef: 0.01906/9 [===================>..........] - ETA: 1s - loss: 0.1408 - accuracy: 0.9127 - jacard_coef: 0.01587/9 [======================>.......] - ETA: 0s - loss: 0.1414 - accuracy: 0.9065 - jacard_coef: 0.01368/9 [=========================>....] - ETA: 0s - loss: 0.1413 - accuracy: 0.9064 - jacard_coef: 0.01489/9 [==============================] - 3s 326ms/step - loss: 0.1415 - accuracy: 0.9036 - jacard_coef: 0.0169 - val_loss: 0.1703 - val_accuracy: 0.6407 - val_jacard_coef: 0.0516 - lr: 5.0000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1414 - accuracy: 0.9014 - jacard_coef: 0.00182/9 [=====>........................] - ETA: 2s - loss: 0.1415 - accuracy: 0.9032 - jacard_coef: 8.9374e-043/9 [=========>....................] - ETA: 2s - loss: 0.1417 - accuracy: 0.9008 - jacard_coef: 0.0058    4/9 [============>.................] - ETA: 1s - loss: 0.1413 - accuracy: 0.9054 - jacard_coef: 0.00715/9 [===============>..............] - ETA: 1s - loss: 0.1419 - accuracy: 0.9018 - jacard_coef: 0.00576/9 [===================>..........] - ETA: 1s - loss: 0.1418 - accuracy: 0.9032 - jacard_coef: 0.00487/9 [======================>.......] - ETA: 0s - loss: 0.1412 - accuracy: 0.9097 - jacard_coef: 0.00418/9 [=========================>....] - ETA: 0s - loss: 0.1409 - accuracy: 0.9123 - jacard_coef: 0.00369/9 [==============================] - 3s 327ms/step - loss: 0.1409 - accuracy: 0.9122 - jacard_coef: 0.0032 - val_loss: 0.1548 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 23/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1411 - accuracy: 0.9043 - jacard_coef: 2.9889e-052/9 [=====>........................] - ETA: 2s - loss: 0.1417 - accuracy: 0.9027 - jacard_coef: 2.4628e-043/9 [=========>....................] - ETA: 2s - loss: 0.1408 - accuracy: 0.9088 - jacard_coef: 1.7625e-044/9 [============>.................] - ETA: 1s - loss: 0.1405 - accuracy: 0.9131 - jacard_coef: 1.4829e-045/9 [===============>..............] - ETA: 1s - loss: 0.1405 - accuracy: 0.9125 - jacard_coef: 1.3563e-046/9 [===================>..........] - ETA: 1s - loss: 0.1404 - accuracy: 0.9132 - jacard_coef: 1.1303e-047/9 [======================>.......] - ETA: 0s - loss: 0.1403 - accuracy: 0.9142 - jacard_coef: 9.6881e-058/9 [=========================>....] - ETA: 0s - loss: 0.1399 - accuracy: 0.9171 - jacard_coef: 8.4771e-059/9 [==============================] - 3s 327ms/step - loss: 0.1401 - accuracy: 0.9132 - jacard_coef: 0.0026 - val_loss: 0.1500 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 24/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1416 - accuracy: 0.8885 - jacard_coef: 8.5547e-132/9 [=====>........................] - ETA: 2s - loss: 0.1398 - accuracy: 0.9093 - jacard_coef: 1.2278e-043/9 [=========>....................] - ETA: 2s - loss: 0.1395 - accuracy: 0.9152 - jacard_coef: 8.1855e-054/9 [============>.................] - ETA: 1s - loss: 0.1396 - accuracy: 0.9129 - jacard_coef: 6.6480e-055/9 [===============>..............] - ETA: 1s - loss: 0.1394 - accuracy: 0.9152 - jacard_coef: 5.3184e-056/9 [===================>..........] - ETA: 1s - loss: 0.1397 - accuracy: 0.9144 - jacard_coef: 4.4320e-057/9 [======================>.......] - ETA: 0s - loss: 0.1393 - accuracy: 0.9179 - jacard_coef: 3.7989e-058/9 [=========================>....] - ETA: 0s - loss: 0.1453 - accuracy: 0.9020 - jacard_coef: 0.0073    9/9 [==============================] - 3s 326ms/step - loss: 0.1452 - accuracy: 0.9028 - jacard_coef: 0.0065 - val_loss: 0.1463 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
Epoch 25/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1382 - accuracy: 0.9220 - jacard_coef: 0.03232/9 [=====>........................] - ETA: 2s - loss: 0.1382 - accuracy: 0.9195 - jacard_coef: 0.01613/9 [=========>....................] - ETA: 2s - loss: 0.1384 - accuracy: 0.9171 - jacard_coef: 0.01084/9 [============>.................] - ETA: 1s - loss: 0.1383 - accuracy: 0.9181 - jacard_coef: 0.00815/9 [===============>..............] - ETA: 1s - loss: 0.1387 - accuracy: 0.9129 - jacard_coef: 0.00676/9 [===================>..........] - ETA: 1s - loss: 0.1384 - accuracy: 0.9156 - jacard_coef: 0.00897/9 [======================>.......] - ETA: 0s - loss: 0.1386 - accuracy: 0.9139 - jacard_coef: 0.00798/9 [=========================>....] - ETA: 0s - loss: 0.1386 - accuracy: 0.9134 - jacard_coef: 0.00709/9 [==============================] - 3s 327ms/step - loss: 0.1388 - accuracy: 0.9109 - jacard_coef: 0.0170 - val_loss: 0.1424 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
Epoch 26/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1390 - accuracy: 0.9073 - jacard_coef: 1.3367e-042/9 [=====>........................] - ETA: 2s - loss: 0.1425 - accuracy: 0.9163 - jacard_coef: 6.5975e-043/9 [=========>....................] - ETA: 2s - loss: 0.1412 - accuracy: 0.9154 - jacard_coef: 0.0024    4/9 [============>.................] - ETA: 1s - loss: 0.1408 - accuracy: 0.9135 - jacard_coef: 0.00725/9 [===============>..............] - ETA: 1s - loss: 0.1403 - accuracy: 0.9145 - jacard_coef: 0.00586/9 [===================>..........] - ETA: 1s - loss: 0.1405 - accuracy: 0.9099 - jacard_coef: 0.01017/9 [======================>.......] - ETA: 0s - loss: 0.1400 - accuracy: 0.9126 - jacard_coef: 0.00908/9 [=========================>....] - ETA: 0s - loss: 0.1399 - accuracy: 0.9125 - jacard_coef: 0.00799/9 [==============================] - 3s 327ms/step - loss: 0.1400 - accuracy: 0.9111 - jacard_coef: 0.0071 - val_loss: 0.1431 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
Epoch 27/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1425 - accuracy: 0.8866 - jacard_coef: 0.01452/9 [=====>........................] - ETA: 2s - loss: 0.1400 - accuracy: 0.9072 - jacard_coef: 0.01873/9 [=========>....................] - ETA: 2s - loss: 0.1401 - accuracy: 0.9037 - jacard_coef: 0.01254/9 [============>.................] - ETA: 1s - loss: 0.1397 - accuracy: 0.9094 - jacard_coef: 0.01755/9 [===============>..............] - ETA: 1s - loss: 0.1397 - accuracy: 0.9089 - jacard_coef: 0.01536/9 [===================>..........] - ETA: 1s - loss: 0.1406 - accuracy: 0.9024 - jacard_coef: 0.01287/9 [======================>.......] - ETA: 0s - loss: 0.1402 - accuracy: 0.9060 - jacard_coef: 0.01158/9 [=========================>....] - ETA: 0s - loss: 0.1401 - accuracy: 0.9080 - jacard_coef: 0.01319/9 [==============================] - 3s 327ms/step - loss: 0.1402 - accuracy: 0.9076 - jacard_coef: 0.0117 - val_loss: 0.1321 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
Epoch 28/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1415 - accuracy: 0.9170 - jacard_coef: 5.8550e-042/9 [=====>........................] - ETA: 2s - loss: 0.1395 - accuracy: 0.9308 - jacard_coef: 5.2475e-043/9 [=========>....................] - ETA: 2s - loss: 0.1406 - accuracy: 0.9197 - jacard_coef: 4.6768e-044/9 [============>.................] - ETA: 1s - loss: 0.1407 - accuracy: 0.9191 - jacard_coef: 4.1694e-045/9 [===============>..............] - ETA: 1s - loss: 0.1405 - accuracy: 0.9181 - jacard_coef: 3.3799e-046/9 [===================>..........] - ETA: 1s - loss: 0.1410 - accuracy: 0.9160 - jacard_coef: 3.2872e-047/9 [======================>.......] - ETA: 0s - loss: 0.1409 - accuracy: 0.9171 - jacard_coef: 3.1928e-048/9 [=========================>....] - ETA: 0s - loss: 0.1408 - accuracy: 0.9167 - jacard_coef: 2.9886e-049/9 [==============================] - 3s 326ms/step - loss: 0.1408 - accuracy: 0.9169 - jacard_coef: 2.9308e-04 - val_loss: 0.1262 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0751 (epoch 18)
  Final Val Loss: 0.1262
  Training Time: 0:02:29.624672
  Stability (std): 0.0439

Results saved to: hyperparameter_optimization_20250926_165036/exp_14_Attention_UNet_lr1e-4_bs16/Attention_UNet_lr0.0001_bs16_results.json

Experiment 14 completed in 167s
Progress: 14/36 completed
Estimated remaining time: 61 minutes

ðŸ”¬ EXPERIMENT 15/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878292.987658 1085220 service.cc:145] XLA service 0x14875456d230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878292.987682 1085220 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878293.126460 1085220 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:02 - loss: 0.3366 - accuracy: 0.4936 - jacard_coef: 0.08892/5 [===========>..................] - ETA: 46s - loss: 0.2957 - accuracy: 0.3688 - jacard_coef: 0.0785 3/5 [=================>............] - ETA: 16s - loss: 0.2722 - accuracy: 0.3131 - jacard_coef: 0.07994/5 [=======================>......] - ETA: 5s - loss: 0.2563 - accuracy: 0.2670 - jacard_coef: 0.0809 5/5 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.2667 - jacard_coef: 0.09935/5 [==============================] - 86s 6s/step - loss: 0.2558 - accuracy: 0.2667 - jacard_coef: 0.0993 - val_loss: 1.1135 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.2021 - accuracy: 0.1435 - jacard_coef: 0.06582/5 [===========>..................] - ETA: 2s - loss: 0.2000 - accuracy: 0.1520 - jacard_coef: 0.07413/5 [=================>............] - ETA: 1s - loss: 0.1981 - accuracy: 0.1530 - jacard_coef: 0.08244/5 [=======================>......] - ETA: 0s - loss: 0.1961 - accuracy: 0.1494 - jacard_coef: 0.08095/5 [==============================] - 3s 558ms/step - loss: 0.1960 - accuracy: 0.1498 - jacard_coef: 0.0949 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1844 - accuracy: 0.1744 - jacard_coef: 0.08052/5 [===========>..................] - ETA: 2s - loss: 0.1831 - accuracy: 0.1888 - jacard_coef: 0.08773/5 [=================>............] - ETA: 1s - loss: 0.1825 - accuracy: 0.1976 - jacard_coef: 0.08304/5 [=======================>......] - ETA: 0s - loss: 0.1810 - accuracy: 0.2304 - jacard_coef: 0.08235/5 [==============================] - 3s 557ms/step - loss: 0.1816 - accuracy: 0.2313 - jacard_coef: 0.0920 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1793 - accuracy: 0.3469 - jacard_coef: 0.08922/5 [===========>..................] - ETA: 2s - loss: 0.1805 - accuracy: 0.3103 - jacard_coef: 0.09003/5 [=================>............] - ETA: 1s - loss: 0.1813 - accuracy: 0.2741 - jacard_coef: 0.08534/5 [=======================>......] - ETA: 0s - loss: 0.1815 - accuracy: 0.2535 - jacard_coef: 0.08055/5 [==============================] - 3s 557ms/step - loss: 0.1816 - accuracy: 0.2522 - jacard_coef: 0.0649 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1830 - accuracy: 0.1894 - jacard_coef: 0.06752/5 [===========>..................] - ETA: 2s - loss: 0.1818 - accuracy: 0.2182 - jacard_coef: 0.07923/5 [=================>............] - ETA: 1s - loss: 0.1810 - accuracy: 0.2402 - jacard_coef: 0.08434/5 [=======================>......] - ETA: 0s - loss: 0.1803 - accuracy: 0.2701 - jacard_coef: 0.08105/5 [==============================] - 3s 556ms/step - loss: 0.1803 - accuracy: 0.2709 - jacard_coef: 0.0862 - val_loss: 1.1202 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1754 - accuracy: 0.5342 - jacard_coef: 0.06922/5 [===========>..................] - ETA: 2s - loss: 0.1745 - accuracy: 0.5558 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1751 - accuracy: 0.5131 - jacard_coef: 0.08054/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.5359 - jacard_coef: 0.07915/5 [==============================] - 3s 570ms/step - loss: 0.1740 - accuracy: 0.5383 - jacard_coef: 0.0872 - val_loss: 0.6372 - val_accuracy: 0.8724 - val_jacard_coef: 0.0468 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1733 - accuracy: 0.8247 - jacard_coef: 0.04862/5 [===========>..................] - ETA: 2s - loss: 0.1763 - accuracy: 0.6494 - jacard_coef: 0.07343/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.7146 - jacard_coef: 0.06034/5 [=======================>......] - ETA: 0s - loss: 0.1722 - accuracy: 0.7473 - jacard_coef: 0.05955/5 [==============================] - 3s 557ms/step - loss: 0.1721 - accuracy: 0.7476 - jacard_coef: 0.0483 - val_loss: 0.3185 - val_accuracy: 0.9226 - val_jacard_coef: 0.0063 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1683 - accuracy: 0.6996 - jacard_coef: 0.05462/5 [===========>..................] - ETA: 2s - loss: 0.1678 - accuracy: 0.7852 - jacard_coef: 0.03793/5 [=================>............] - ETA: 1s - loss: 0.1674 - accuracy: 0.8188 - jacard_coef: 0.03584/5 [=======================>......] - ETA: 0s - loss: 0.1674 - accuracy: 0.8307 - jacard_coef: 0.03605/5 [==============================] - 3s 558ms/step - loss: 0.1677 - accuracy: 0.8261 - jacard_coef: 0.0366 - val_loss: 0.2064 - val_accuracy: 0.8104 - val_jacard_coef: 0.0421 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.8316 - jacard_coef: 0.03922/5 [===========>..................] - ETA: 2s - loss: 0.1668 - accuracy: 0.8197 - jacard_coef: 0.04953/5 [=================>............] - ETA: 1s - loss: 0.1670 - accuracy: 0.7986 - jacard_coef: 0.05524/5 [=======================>......] - ETA: 0s - loss: 0.1670 - accuracy: 0.7979 - jacard_coef: 0.05975/5 [==============================] - 3s 558ms/step - loss: 0.1674 - accuracy: 0.7945 - jacard_coef: 0.0477 - val_loss: 1.0291 - val_accuracy: 0.9255 - val_jacard_coef: 0.0013 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1665 - accuracy: 0.8509 - jacard_coef: 0.04572/5 [===========>..................] - ETA: 2s - loss: 0.1664 - accuracy: 0.8613 - jacard_coef: 0.04423/5 [=================>............] - ETA: 1s - loss: 0.1665 - accuracy: 0.8595 - jacard_coef: 0.04054/5 [=======================>......] - ETA: 0s - loss: 0.1662 - accuracy: 0.8655 - jacard_coef: 0.03805/5 [==============================] - 3s 558ms/step - loss: 0.1663 - accuracy: 0.8644 - jacard_coef: 0.0416 - val_loss: 1.0956 - val_accuracy: 0.9302 - val_jacard_coef: 2.4757e-04 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1651 - accuracy: 0.8773 - jacard_coef: 0.04202/5 [===========>..................] - ETA: 2s - loss: 0.1655 - accuracy: 0.8692 - jacard_coef: 0.02883/5 [=================>............] - ETA: 1s - loss: 0.1655 - accuracy: 0.8777 - jacard_coef: 0.02564/5 [=======================>......] - ETA: 0s - loss: 0.1652 - accuracy: 0.8897 - jacard_coef: 0.02295/5 [==============================] - 3s 558ms/step - loss: 0.1652 - accuracy: 0.8901 - jacard_coef: 0.0241 - val_loss: 1.1079 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1645 - accuracy: 0.9029 - jacard_coef: 0.01042/5 [===========>..................] - ETA: 2s - loss: 0.1645 - accuracy: 0.8986 - jacard_coef: 0.01003/5 [=================>............] - ETA: 1s - loss: 0.1642 - accuracy: 0.9070 - jacard_coef: 0.01264/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.9054 - jacard_coef: 0.01245/5 [==============================] - 3s 558ms/step - loss: 0.1641 - accuracy: 0.9059 - jacard_coef: 0.0134 - val_loss: 1.0928 - val_accuracy: 0.9304 - val_jacard_coef: 1.4609e-12 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1634 - accuracy: 0.9003 - jacard_coef: 0.01372/5 [===========>..................] - ETA: 2s - loss: 0.1635 - accuracy: 0.8897 - jacard_coef: 0.01653/5 [=================>............] - ETA: 1s - loss: 0.1633 - accuracy: 0.8894 - jacard_coef: 0.01824/5 [=======================>......] - ETA: 0s - loss: 0.1630 - accuracy: 0.8923 - jacard_coef: 0.02285/5 [==============================] - 3s 557ms/step - loss: 0.1630 - accuracy: 0.8918 - jacard_coef: 0.0271 - val_loss: 0.9834 - val_accuracy: 0.9303 - val_jacard_coef: 1.4587e-12 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1621 - accuracy: 0.8858 - jacard_coef: 0.03472/5 [===========>..................] - ETA: 2s - loss: 0.1623 - accuracy: 0.8724 - jacard_coef: 0.03443/5 [=================>............] - ETA: 1s - loss: 0.1621 - accuracy: 0.8756 - jacard_coef: 0.03364/5 [=======================>......] - ETA: 0s - loss: 0.1621 - accuracy: 0.8739 - jacard_coef: 0.03315/5 [==============================] - 3s 558ms/step - loss: 0.1621 - accuracy: 0.8743 - jacard_coef: 0.0341 - val_loss: 0.6692 - val_accuracy: 0.9301 - val_jacard_coef: 1.4563e-12 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1617 - accuracy: 0.8739 - jacard_coef: 0.02982/5 [===========>..................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8792 - jacard_coef: 0.02533/5 [=================>............] - ETA: 1s - loss: 0.1612 - accuracy: 0.8888 - jacard_coef: 0.02714/5 [=======================>......] - ETA: 0s - loss: 0.1612 - accuracy: 0.8877 - jacard_coef: 0.02545/5 [==============================] - 3s 558ms/step - loss: 0.1613 - accuracy: 0.8872 - jacard_coef: 0.0203 - val_loss: 0.5243 - val_accuracy: 0.9301 - val_jacard_coef: 1.4559e-12 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1612 - accuracy: 0.8911 - jacard_coef: 0.00292/5 [===========>..................] - ETA: 2s - loss: 0.1608 - accuracy: 0.9048 - jacard_coef: 0.00423/5 [=================>............] - ETA: 1s - loss: 0.1607 - accuracy: 0.9082 - jacard_coef: 0.00434/5 [=======================>......] - ETA: 0s - loss: 0.1605 - accuracy: 0.9120 - jacard_coef: 0.00415/5 [==============================] - 3s 558ms/step - loss: 0.1605 - accuracy: 0.9119 - jacard_coef: 0.0051 - val_loss: 0.3181 - val_accuracy: 0.9299 - val_jacard_coef: 1.4514e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0468 (epoch 6)
  Final Val Loss: 0.3181
  Training Time: 0:02:09.985396
  Stability (std): 0.3494

Results saved to: hyperparameter_optimization_20250926_165036/exp_15_Attention_UNet_lr1e-4_bs32/Attention_UNet_lr0.0001_bs32_results.json

Experiment 15 completed in 146s
Progress: 15/36 completed
Estimated remaining time: 51 minutes

ðŸ”¬ EXPERIMENT 16/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878431.522024 1091526 service.cc:145] XLA service 0x1539514aeb60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878431.522047 1091526 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878431.658747 1091526 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 12:07 - loss: 0.3474 - accuracy: 0.5381 - jacard_coef: 0.0765 2/17 [==>...........................] - ETA: 1:09 - loss: 0.3366 - accuracy: 0.5507 - jacard_coef: 0.0869  3/17 [====>.........................] - ETA: 33s - loss: 0.3074 - accuracy: 0.5325 - jacard_coef: 0.0811  4/17 [======>.......................] - ETA: 21s - loss: 0.2835 - accuracy: 0.5564 - jacard_coef: 0.0791 5/17 [=======>......................] - ETA: 15s - loss: 0.2697 - accuracy: 0.5683 - jacard_coef: 0.0708 6/17 [=========>....................] - ETA: 11s - loss: 0.2603 - accuracy: 0.5836 - jacard_coef: 0.0740 7/17 [===========>..................] - ETA: 9s - loss: 0.2519 - accuracy: 0.5904 - jacard_coef: 0.0681  8/17 [=============>................] - ETA: 7s - loss: 0.2436 - accuracy: 0.5928 - jacard_coef: 0.0719 9/17 [==============>...............] - ETA: 5s - loss: 0.2365 - accuracy: 0.6031 - jacard_coef: 0.072210/17 [================>.............] - ETA: 4s - loss: 0.2302 - accuracy: 0.6114 - jacard_coef: 0.072311/17 [==================>...........] - ETA: 3s - loss: 0.2250 - accuracy: 0.6183 - jacard_coef: 0.069912/17 [====================>.........] - ETA: 2s - loss: 0.2206 - accuracy: 0.6273 - jacard_coef: 0.067513/17 [=====================>........] - ETA: 2s - loss: 0.2167 - accuracy: 0.6387 - jacard_coef: 0.067914/17 [=======================>......] - ETA: 1s - loss: 0.2135 - accuracy: 0.6436 - jacard_coef: 0.067815/17 [=========================>....] - ETA: 0s - loss: 0.2105 - accuracy: 0.6516 - jacard_coef: 0.071216/17 [===========================>..] - ETA: 0s - loss: 0.2079 - accuracy: 0.6580 - jacard_coef: 0.074617/17 [==============================] - ETA: 0s - loss: 0.2088 - accuracy: 0.6582 - jacard_coef: 0.071617/17 [==============================] - 59s 863ms/step - loss: 0.2088 - accuracy: 0.6582 - jacard_coef: 0.0716 - val_loss: 0.3769 - val_accuracy: 0.2220 - val_jacard_coef: 0.0599 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1747 - accuracy: 0.7809 - jacard_coef: 0.0509 2/17 [==>...........................] - ETA: 2s - loss: 0.1823 - accuracy: 0.7746 - jacard_coef: 0.0449 3/17 [====>.........................] - ETA: 2s - loss: 0.1888 - accuracy: 0.7794 - jacard_coef: 0.0452 4/17 [======>.......................] - ETA: 2s - loss: 0.1923 - accuracy: 0.7975 - jacard_coef: 0.0408 5/17 [=======>......................] - ETA: 2s - loss: 0.1929 - accuracy: 0.8131 - jacard_coef: 0.0362 6/17 [=========>....................] - ETA: 1s - loss: 0.1941 - accuracy: 0.8275 - jacard_coef: 0.0302 7/17 [===========>..................] - ETA: 1s - loss: 0.1940 - accuracy: 0.8381 - jacard_coef: 0.0266 8/17 [=============>................] - ETA: 1s - loss: 0.1929 - accuracy: 0.8466 - jacard_coef: 0.0264 9/17 [==============>...............] - ETA: 1s - loss: 0.1927 - accuracy: 0.8487 - jacard_coef: 0.028710/17 [================>.............] - ETA: 1s - loss: 0.1920 - accuracy: 0.8498 - jacard_coef: 0.031911/17 [==================>...........] - ETA: 1s - loss: 0.1914 - accuracy: 0.8509 - jacard_coef: 0.030912/17 [====================>.........] - ETA: 0s - loss: 0.1902 - accuracy: 0.8504 - jacard_coef: 0.030613/17 [=====================>........] - ETA: 0s - loss: 0.1885 - accuracy: 0.8512 - jacard_coef: 0.033814/17 [=======================>......] - ETA: 0s - loss: 0.1869 - accuracy: 0.8527 - jacard_coef: 0.034615/17 [=========================>....] - ETA: 0s - loss: 0.1911 - accuracy: 0.8472 - jacard_coef: 0.036416/17 [===========================>..] - ETA: 0s - loss: 0.1901 - accuracy: 0.8511 - jacard_coef: 0.034217/17 [==============================] - 3s 179ms/step - loss: 0.1899 - accuracy: 0.8518 - jacard_coef: 0.0325 - val_loss: 1.4748 - val_accuracy: 0.9052 - val_jacard_coef: 0.0083 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1801 - accuracy: 0.9155 - jacard_coef: 0.0124 2/17 [==>...........................] - ETA: 2s - loss: 0.1813 - accuracy: 0.8876 - jacard_coef: 0.0273 3/17 [====>.........................] - ETA: 2s - loss: 0.1816 - accuracy: 0.8938 - jacard_coef: 0.0290 4/17 [======>.......................] - ETA: 2s - loss: 0.1824 - accuracy: 0.8898 - jacard_coef: 0.0302 5/17 [=======>......................] - ETA: 2s - loss: 0.1827 - accuracy: 0.8853 - jacard_coef: 0.0293 6/17 [=========>....................] - ETA: 1s - loss: 0.1821 - accuracy: 0.8857 - jacard_coef: 0.0300 7/17 [===========>..................] - ETA: 1s - loss: 0.1822 - accuracy: 0.8803 - jacard_coef: 0.0295 8/17 [=============>................] - ETA: 1s - loss: 0.1822 - accuracy: 0.8804 - jacard_coef: 0.0290 9/17 [==============>...............] - ETA: 1s - loss: 0.1816 - accuracy: 0.8787 - jacard_coef: 0.030310/17 [================>.............] - ETA: 1s - loss: 0.1810 - accuracy: 0.8751 - jacard_coef: 0.032811/17 [==================>...........] - ETA: 1s - loss: 0.1802 - accuracy: 0.8722 - jacard_coef: 0.034512/17 [====================>.........] - ETA: 0s - loss: 0.1799 - accuracy: 0.8720 - jacard_coef: 0.034313/17 [=====================>........] - ETA: 0s - loss: 0.1795 - accuracy: 0.8712 - jacard_coef: 0.035114/17 [=======================>......] - ETA: 0s - loss: 0.1795 - accuracy: 0.8717 - jacard_coef: 0.034015/17 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.8731 - jacard_coef: 0.034516/17 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.8739 - jacard_coef: 0.033517/17 [==============================] - 3s 179ms/step - loss: 0.1784 - accuracy: 0.8746 - jacard_coef: 0.0316 - val_loss: 4.1621 - val_accuracy: 0.7219 - val_jacard_coef: 0.0260 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1662 - accuracy: 0.9124 - jacard_coef: 0.0408 2/17 [==>...........................] - ETA: 2s - loss: 0.1652 - accuracy: 0.8836 - jacard_coef: 0.0680 3/17 [====>.........................] - ETA: 2s - loss: 0.1666 - accuracy: 0.8072 - jacard_coef: 0.0755 4/17 [======>.......................] - ETA: 2s - loss: 0.1667 - accuracy: 0.7685 - jacard_coef: 0.0771 5/17 [=======>......................] - ETA: 2s - loss: 0.1673 - accuracy: 0.7330 - jacard_coef: 0.0821 6/17 [=========>....................] - ETA: 1s - loss: 0.1664 - accuracy: 0.7105 - jacard_coef: 0.0772 7/17 [===========>..................] - ETA: 1s - loss: 0.1657 - accuracy: 0.7005 - jacard_coef: 0.0749 8/17 [=============>................] - ETA: 1s - loss: 0.1652 - accuracy: 0.7158 - jacard_coef: 0.0734 9/17 [==============>...............] - ETA: 1s - loss: 0.1646 - accuracy: 0.7299 - jacard_coef: 0.067310/17 [================>.............] - ETA: 1s - loss: 0.1640 - accuracy: 0.7449 - jacard_coef: 0.064411/17 [==================>...........] - ETA: 1s - loss: 0.1636 - accuracy: 0.7529 - jacard_coef: 0.064012/17 [====================>.........] - ETA: 0s - loss: 0.1634 - accuracy: 0.7575 - jacard_coef: 0.062813/17 [=====================>........] - ETA: 0s - loss: 0.1630 - accuracy: 0.7605 - jacard_coef: 0.062314/17 [=======================>......] - ETA: 0s - loss: 0.1627 - accuracy: 0.7674 - jacard_coef: 0.062015/17 [=========================>....] - ETA: 0s - loss: 0.1622 - accuracy: 0.7759 - jacard_coef: 0.068016/17 [===========================>..] - ETA: 0s - loss: 0.1619 - accuracy: 0.7807 - jacard_coef: 0.068817/17 [==============================] - 3s 183ms/step - loss: 0.1629 - accuracy: 0.7805 - jacard_coef: 0.0703 - val_loss: 14.9057 - val_accuracy: 0.0697 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1666 - accuracy: 0.8376 - jacard_coef: 0.0363 2/17 [==>...........................] - ETA: 2s - loss: 0.1621 - accuracy: 0.8580 - jacard_coef: 0.0860 3/17 [====>.........................] - ETA: 2s - loss: 0.1632 - accuracy: 0.8601 - jacard_coef: 0.0793 4/17 [======>.......................] - ETA: 2s - loss: 0.1628 - accuracy: 0.8582 - jacard_coef: 0.0636 5/17 [=======>......................] - ETA: 2s - loss: 0.1616 - accuracy: 0.8655 - jacard_coef: 0.0525 6/17 [=========>....................] - ETA: 1s - loss: 0.1609 - accuracy: 0.8730 - jacard_coef: 0.0437 7/17 [===========>..................] - ETA: 1s - loss: 0.1603 - accuracy: 0.8818 - jacard_coef: 0.0376 8/17 [=============>................] - ETA: 1s - loss: 0.1599 - accuracy: 0.8844 - jacard_coef: 0.0331 9/17 [==============>...............] - ETA: 1s - loss: 0.1597 - accuracy: 0.8884 - jacard_coef: 0.029510/17 [================>.............] - ETA: 1s - loss: 0.1593 - accuracy: 0.8954 - jacard_coef: 0.026711/17 [==================>...........] - ETA: 1s - loss: 0.1590 - accuracy: 0.8965 - jacard_coef: 0.024412/17 [====================>.........] - ETA: 0s - loss: 0.1589 - accuracy: 0.8968 - jacard_coef: 0.022313/17 [=====================>........] - ETA: 0s - loss: 0.1585 - accuracy: 0.8978 - jacard_coef: 0.020614/17 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8986 - jacard_coef: 0.020215/17 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.9005 - jacard_coef: 0.019016/17 [===========================>..] - ETA: 0s - loss: 0.1578 - accuracy: 0.8990 - jacard_coef: 0.018617/17 [==============================] - 3s 182ms/step - loss: 0.1578 - accuracy: 0.8987 - jacard_coef: 0.0175 - val_loss: 13.4955 - val_accuracy: 0.0763 - val_jacard_coef: 0.0684 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1505 - accuracy: 0.9551 - jacard_coef: 0.0359 2/17 [==>...........................] - ETA: 2s - loss: 0.1520 - accuracy: 0.9253 - jacard_coef: 0.0367 3/17 [====>.........................] - ETA: 2s - loss: 0.1521 - accuracy: 0.9219 - jacard_coef: 0.0271 4/17 [======>.......................] - ETA: 2s - loss: 0.1518 - accuracy: 0.9244 - jacard_coef: 0.0305 5/17 [=======>......................] - ETA: 2s - loss: 0.1523 - accuracy: 0.9131 - jacard_coef: 0.0266 6/17 [=========>....................] - ETA: 1s - loss: 0.1524 - accuracy: 0.9085 - jacard_coef: 0.0268 7/17 [===========>..................] - ETA: 1s - loss: 0.1522 - accuracy: 0.9134 - jacard_coef: 0.0230 8/17 [=============>................] - ETA: 1s - loss: 0.1523 - accuracy: 0.9108 - jacard_coef: 0.0201 9/17 [==============>...............] - ETA: 1s - loss: 0.1524 - accuracy: 0.9095 - jacard_coef: 0.017910/17 [================>.............] - ETA: 1s - loss: 0.1526 - accuracy: 0.9062 - jacard_coef: 0.016111/17 [==================>...........] - ETA: 1s - loss: 0.1525 - accuracy: 0.9078 - jacard_coef: 0.014712/17 [====================>.........] - ETA: 0s - loss: 0.1525 - accuracy: 0.9067 - jacard_coef: 0.013413/17 [=====================>........] - ETA: 0s - loss: 0.1525 - accuracy: 0.9069 - jacard_coef: 0.012414/17 [=======================>......] - ETA: 0s - loss: 0.1526 - accuracy: 0.9051 - jacard_coef: 0.011515/17 [=========================>....] - ETA: 0s - loss: 0.1526 - accuracy: 0.9035 - jacard_coef: 0.010816/17 [===========================>..] - ETA: 0s - loss: 0.1525 - accuracy: 0.9044 - jacard_coef: 0.010117/17 [==============================] - 3s 183ms/step - loss: 0.1524 - accuracy: 0.9052 - jacard_coef: 0.0095 - val_loss: 3.8666 - val_accuracy: 0.1361 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1502 - accuracy: 0.9070 - jacard_coef: 0.0399 2/17 [==>...........................] - ETA: 2s - loss: 0.1499 - accuracy: 0.9140 - jacard_coef: 0.0199 3/17 [====>.........................] - ETA: 2s - loss: 0.1499 - accuracy: 0.9079 - jacard_coef: 0.0133 4/17 [======>.......................] - ETA: 2s - loss: 0.1497 - accuracy: 0.9120 - jacard_coef: 0.0100 5/17 [=======>......................] - ETA: 2s - loss: 0.1496 - accuracy: 0.9127 - jacard_coef: 0.0080 6/17 [=========>....................] - ETA: 1s - loss: 0.1495 - accuracy: 0.9129 - jacard_coef: 0.0088 7/17 [===========>..................] - ETA: 1s - loss: 0.1495 - accuracy: 0.9120 - jacard_coef: 0.0075 8/17 [=============>................] - ETA: 1s - loss: 0.1495 - accuracy: 0.9121 - jacard_coef: 0.0066 9/17 [==============>...............] - ETA: 1s - loss: 0.1493 - accuracy: 0.9133 - jacard_coef: 0.005810/17 [================>.............] - ETA: 1s - loss: 0.1490 - accuracy: 0.9169 - jacard_coef: 0.005311/17 [==================>...........] - ETA: 1s - loss: 0.1490 - accuracy: 0.9163 - jacard_coef: 0.004812/17 [====================>.........] - ETA: 0s - loss: 0.1490 - accuracy: 0.9144 - jacard_coef: 0.004413/17 [=====================>........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9151 - jacard_coef: 0.004014/17 [=======================>......] - ETA: 0s - loss: 0.1489 - accuracy: 0.9146 - jacard_coef: 0.003815/17 [=========================>....] - ETA: 0s - loss: 0.1489 - accuracy: 0.9137 - jacard_coef: 0.003516/17 [===========================>..] - ETA: 0s - loss: 0.1488 - accuracy: 0.9136 - jacard_coef: 0.003317/17 [==============================] - 3s 183ms/step - loss: 0.1488 - accuracy: 0.9141 - jacard_coef: 0.0031 - val_loss: 0.1755 - val_accuracy: 0.4308 - val_jacard_coef: 0.0725 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1457 - accuracy: 0.9320 - jacard_coef: 2.8044e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1457 - accuracy: 0.9311 - jacard_coef: 2.7677e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9213 - jacard_coef: 5.1819e-05 4/17 [======>.......................] - ETA: 2s - loss: 0.1468 - accuracy: 0.9137 - jacard_coef: 3.8864e-05 5/17 [=======>......................] - ETA: 2s - loss: 0.1464 - accuracy: 0.9173 - jacard_coef: 3.1092e-05 6/17 [=========>....................] - ETA: 1s - loss: 0.1463 - accuracy: 0.9179 - jacard_coef: 2.5910e-05 7/17 [===========>..................] - ETA: 1s - loss: 0.1460 - accuracy: 0.9215 - jacard_coef: 2.2208e-05 8/17 [=============>................] - ETA: 1s - loss: 0.1459 - accuracy: 0.9208 - jacard_coef: 1.9432e-05 9/17 [==============>...............] - ETA: 1s - loss: 0.1460 - accuracy: 0.9182 - jacard_coef: 1.7273e-0510/17 [================>.............] - ETA: 1s - loss: 0.1458 - accuracy: 0.9200 - jacard_coef: 1.5546e-0511/17 [==================>...........] - ETA: 1s - loss: 0.1457 - accuracy: 0.9203 - jacard_coef: 1.4133e-0512/17 [====================>.........] - ETA: 0s - loss: 0.1456 - accuracy: 0.9202 - jacard_coef: 1.2955e-0513/17 [=====================>........] - ETA: 0s - loss: 0.1457 - accuracy: 0.9183 - jacard_coef: 1.1958e-0514/17 [=======================>......] - ETA: 0s - loss: 0.1455 - accuracy: 0.9194 - jacard_coef: 1.1104e-0515/17 [=========================>....] - ETA: 0s - loss: 0.1455 - accuracy: 0.9180 - jacard_coef: 1.0364e-0516/17 [===========================>..] - ETA: 0s - loss: 0.1455 - accuracy: 0.9171 - jacard_coef: 9.7161e-0617/17 [==============================] - 3s 180ms/step - loss: 0.1455 - accuracy: 0.9163 - jacard_coef: 0.0071 - val_loss: 0.1514 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1446 - accuracy: 0.9074 - jacard_coef: 2.0600e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1446 - accuracy: 0.9089 - jacard_coef: 1.1711e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1444 - accuracy: 0.9110 - jacard_coef: 5.8715e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9185 - jacard_coef: 4.4036e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1441 - accuracy: 0.9157 - jacard_coef: 4.2412e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1439 - accuracy: 0.9171 - jacard_coef: 3.7022e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1438 - accuracy: 0.9164 - jacard_coef: 3.1734e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1435 - accuracy: 0.9183 - jacard_coef: 2.8116e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1435 - accuracy: 0.9173 - jacard_coef: 3.0359e-0410/17 [================>.............] - ETA: 1s - loss: 0.1435 - accuracy: 0.9154 - jacard_coef: 2.7323e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1433 - accuracy: 0.9159 - jacard_coef: 2.7244e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1431 - accuracy: 0.9162 - jacard_coef: 0.0013    13/17 [=====================>........] - ETA: 0s - loss: 0.1428 - accuracy: 0.9184 - jacard_coef: 0.001314/17 [=======================>......] - ETA: 0s - loss: 0.1430 - accuracy: 0.9159 - jacard_coef: 0.001215/17 [=========================>....] - ETA: 0s - loss: 0.1429 - accuracy: 0.9158 - jacard_coef: 0.001216/17 [===========================>..] - ETA: 0s - loss: 0.1428 - accuracy: 0.9164 - jacard_coef: 0.001117/17 [==============================] - 3s 180ms/step - loss: 0.1428 - accuracy: 0.9157 - jacard_coef: 0.0010 - val_loss: 0.1320 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1415 - accuracy: 0.9079 - jacard_coef: 8.2787e-05 2/17 [==>...........................] - ETA: 2s - loss: 0.1403 - accuracy: 0.9213 - jacard_coef: 7.0626e-05 3/17 [====>.........................] - ETA: 2s - loss: 0.1400 - accuracy: 0.9235 - jacard_coef: 5.5903e-05 4/17 [======>.......................] - ETA: 2s - loss: 0.1402 - accuracy: 0.9200 - jacard_coef: 4.1927e-05 5/17 [=======>......................] - ETA: 2s - loss: 0.1403 - accuracy: 0.9183 - jacard_coef: 3.3542e-05 6/17 [=========>....................] - ETA: 1s - loss: 0.1410 - accuracy: 0.9112 - jacard_coef: 2.7951e-05 7/17 [===========>..................] - ETA: 1s - loss: 0.1409 - accuracy: 0.9113 - jacard_coef: 2.3958e-05 8/17 [=============>................] - ETA: 1s - loss: 0.1404 - accuracy: 0.9154 - jacard_coef: 2.0964e-05 9/17 [==============>...............] - ETA: 1s - loss: 0.1405 - accuracy: 0.9139 - jacard_coef: 1.8634e-0510/17 [================>.............] - ETA: 1s - loss: 0.1405 - accuracy: 0.9129 - jacard_coef: 1.6771e-0511/17 [==================>...........] - ETA: 1s - loss: 0.1402 - accuracy: 0.9150 - jacard_coef: 1.5246e-0512/17 [====================>.........] - ETA: 0s - loss: 0.1399 - accuracy: 0.9174 - jacard_coef: 1.3976e-0513/17 [=====================>........] - ETA: 0s - loss: 0.1398 - accuracy: 0.9168 - jacard_coef: 1.6133e-0514/17 [=======================>......] - ETA: 0s - loss: 0.1399 - accuracy: 0.9154 - jacard_coef: 1.6296e-0515/17 [=========================>....] - ETA: 0s - loss: 0.1396 - accuracy: 0.9176 - jacard_coef: 1.5209e-0516/17 [===========================>..] - ETA: 0s - loss: 0.1395 - accuracy: 0.9178 - jacard_coef: 1.4259e-0517/17 [==============================] - 3s 180ms/step - loss: 0.1395 - accuracy: 0.9172 - jacard_coef: 1.3420e-05 - val_loss: 0.1291 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1369 - accuracy: 0.9244 - jacard_coef: 2.5242e-05 2/17 [==>...........................] - ETA: 2s - loss: 0.1359 - accuracy: 0.9339 - jacard_coef: 0.0204     3/17 [====>.........................] - ETA: 2s - loss: 0.1384 - accuracy: 0.9074 - jacard_coef: 0.0141 4/17 [======>.......................] - ETA: 2s - loss: 0.1389 - accuracy: 0.9013 - jacard_coef: 0.0115 5/17 [=======>......................] - ETA: 2s - loss: 0.1390 - accuracy: 0.9005 - jacard_coef: 0.0092 6/17 [=========>....................] - ETA: 1s - loss: 0.1384 - accuracy: 0.9060 - jacard_coef: 0.0109 7/17 [===========>..................] - ETA: 1s - loss: 0.1378 - accuracy: 0.9119 - jacard_coef: 0.0094 8/17 [=============>................] - ETA: 1s - loss: 0.1374 - accuracy: 0.9148 - jacard_coef: 0.0082 9/17 [==============>...............] - ETA: 1s - loss: 0.1375 - accuracy: 0.9142 - jacard_coef: 0.007310/17 [================>.............] - ETA: 1s - loss: 0.1371 - accuracy: 0.9166 - jacard_coef: 0.006611/17 [==================>...........] - ETA: 1s - loss: 0.1376 - accuracy: 0.9119 - jacard_coef: 0.006012/17 [====================>.........] - ETA: 0s - loss: 0.1375 - accuracy: 0.9121 - jacard_coef: 0.005513/17 [=====================>........] - ETA: 0s - loss: 0.1373 - accuracy: 0.9130 - jacard_coef: 0.005014/17 [=======================>......] - ETA: 0s - loss: 0.1373 - accuracy: 0.9130 - jacard_coef: 0.004715/17 [=========================>....] - ETA: 0s - loss: 0.1370 - accuracy: 0.9153 - jacard_coef: 0.004416/17 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9153 - jacard_coef: 0.004117/17 [==============================] - 3s 180ms/step - loss: 0.1368 - accuracy: 0.9159 - jacard_coef: 0.0039 - val_loss: 0.1264 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1373 - accuracy: 0.8980 - jacard_coef: 1.8701e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1381 - accuracy: 0.8893 - jacard_coef: 1.7341e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1370 - accuracy: 0.8996 - jacard_coef: 1.9525e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1356 - accuracy: 0.9115 - jacard_coef: 2.3689e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1353 - accuracy: 0.9140 - jacard_coef: 2.3966e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1352 - accuracy: 0.9139 - jacard_coef: 2.3648e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1352 - accuracy: 0.9129 - jacard_coef: 2.3192e-12 8/17 [=============>................] - ETA: 1s - loss: 0.1355 - accuracy: 0.9102 - jacard_coef: 2.2478e-12 9/17 [==============>...............] - ETA: 1s - loss: 0.1353 - accuracy: 0.9114 - jacard_coef: 2.2684e-1210/17 [================>.............] - ETA: 1s - loss: 0.1354 - accuracy: 0.9097 - jacard_coef: 2.2214e-1211/17 [==================>...........] - ETA: 1s - loss: 0.1353 - accuracy: 0.9096 - jacard_coef: 2.2097e-1212/17 [====================>.........] - ETA: 0s - loss: 0.1351 - accuracy: 0.9108 - jacard_coef: 2.2350e-1213/17 [=====================>........] - ETA: 0s - loss: 0.1348 - accuracy: 0.9126 - jacard_coef: 2.2865e-1214/17 [=======================>......] - ETA: 0s - loss: 0.1345 - accuracy: 0.9147 - jacard_coef: 2.3592e-1215/17 [=========================>....] - ETA: 0s - loss: 0.1344 - accuracy: 0.9154 - jacard_coef: 2.3719e-1216/17 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.9171 - jacard_coef: 2.4299e-1217/17 [==============================] - 3s 180ms/step - loss: 0.1341 - accuracy: 0.9173 - jacard_coef: 3.9142e-12 - val_loss: 0.1241 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1315 - accuracy: 0.9287 - jacard_coef: 2.6769e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1318 - accuracy: 0.9240 - jacard_coef: 2.5199e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1306 - accuracy: 0.9339 - jacard_coef: 3.0555e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1312 - accuracy: 0.9282 - jacard_coef: 2.8277e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1312 - accuracy: 0.9280 - jacard_coef: 2.7873e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1318 - accuracy: 0.9232 - jacard_coef: 2.6368e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1320 - accuracy: 0.9210 - jacard_coef: 2.5566e-12 8/17 [=============>................] - ETA: 1s - loss: 0.1319 - accuracy: 0.9212 - jacard_coef: 2.5456e-12 9/17 [==============>...............] - ETA: 1s - loss: 0.1315 - accuracy: 0.9243 - jacard_coef: 2.6763e-1210/17 [================>.............] - ETA: 1s - loss: 0.1315 - accuracy: 0.9237 - jacard_coef: 2.6426e-1211/17 [==================>...........] - ETA: 1s - loss: 0.1318 - accuracy: 0.9204 - jacard_coef: 2.5562e-1212/17 [====================>.........] - ETA: 0s - loss: 0.1321 - accuracy: 0.9179 - jacard_coef: 2.4876e-1213/17 [=====================>........] - ETA: 0s - loss: 0.1320 - accuracy: 0.9188 - jacard_coef: 2.5045e-1214/17 [=======================>......] - ETA: 0s - loss: 0.1321 - accuracy: 0.9173 - jacard_coef: 2.4601e-1215/17 [=========================>....] - ETA: 0s - loss: 0.1321 - accuracy: 0.9171 - jacard_coef: 2.4437e-1216/17 [===========================>..] - ETA: 0s - loss: 0.1320 - accuracy: 0.9178 - jacard_coef: 2.4570e-1217/17 [==============================] - 3s 180ms/step - loss: 0.1320 - accuracy: 0.9172 - jacard_coef: 2.9036e-12 - val_loss: 0.1242 - val_accuracy: 0.9304 - val_jacard_coef: 3.4108e-12 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1281 - accuracy: 0.9419 - jacard_coef: 3.2817e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1293 - accuracy: 0.9335 - jacard_coef: 2.9151e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1311 - accuracy: 0.9184 - jacard_coef: 2.5122e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1312 - accuracy: 0.9177 - jacard_coef: 2.4481e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1313 - accuracy: 0.9166 - jacard_coef: 2.3930e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1312 - accuracy: 0.9171 - jacard_coef: 2.3897e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1313 - accuracy: 0.9155 - jacard_coef: 2.3389e-12 8/17 [=============>................] - ETA: 1s - loss: 0.1315 - accuracy: 0.9139 - jacard_coef: 2.2902e-12 9/17 [==============>...............] - ETA: 1s - loss: 0.1311 - accuracy: 0.9176 - jacard_coef: 2.4393e-1210/17 [================>.............] - ETA: 1s - loss: 0.1311 - accuracy: 0.9168 - jacard_coef: 2.4074e-1211/17 [==================>...........] - ETA: 1s - loss: 0.1309 - accuracy: 0.9185 - jacard_coef: 2.4545e-1212/17 [====================>.........] - ETA: 0s - loss: 0.1309 - accuracy: 0.9180 - jacard_coef: 2.4333e-1213/17 [=====================>........] - ETA: 0s - loss: 0.1311 - accuracy: 0.9159 - jacard_coef: 2.3798e-1214/17 [=======================>......] - ETA: 0s - loss: 0.1310 - accuracy: 0.9165 - jacard_coef: 2.3893e-1215/17 [=========================>....] - ETA: 0s - loss: 0.1311 - accuracy: 0.9151 - jacard_coef: 2.3519e-1216/17 [===========================>..] - ETA: 0s - loss: 0.1308 - accuracy: 0.9173 - jacard_coef: 2.4434e-1217/17 [==============================] - 3s 180ms/step - loss: 0.1308 - accuracy: 0.9175 - jacard_coef: 3.8818e-12 - val_loss: 0.1250 - val_accuracy: 0.9304 - val_jacard_coef: 3.4103e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1275 - accuracy: 0.9385 - jacard_coef: 3.1000e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1291 - accuracy: 0.9261 - jacard_coef: 2.6548e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1292 - accuracy: 0.9251 - jacard_coef: 2.5975e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1303 - accuracy: 0.9162 - jacard_coef: 2.3791e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1305 - accuracy: 0.9136 - jacard_coef: 2.2974e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1306 - accuracy: 0.9122 - jacard_coef: 2.2506e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1300 - accuracy: 0.9165 - jacard_coef: 2.3987e-12 8/17 [=============>................] - ETA: 1s - loss: 0.1307 - accuracy: 0.9105 - jacard_coef: 2.2809e-12 9/17 [==============>...............] - ETA: 1s - loss: 0.1308 - accuracy: 0.9099 - jacard_coef: 2.2497e-1210/17 [================>.............] - ETA: 1s - loss: 0.1303 - accuracy: 0.9138 - jacard_coef: 2.4007e-1211/17 [==================>...........] - ETA: 1s - loss: 0.1301 - accuracy: 0.9149 - jacard_coef: 2.4170e-1212/17 [====================>.........] - ETA: 0s - loss: 0.1301 - accuracy: 0.9141 - jacard_coef: 2.3836e-1213/17 [=====================>........] - ETA: 0s - loss: 0.1300 - accuracy: 0.9147 - jacard_coef: 2.3865e-1214/17 [=======================>......] - ETA: 0s - loss: 0.1295 - accuracy: 0.9179 - jacard_coef: 2.5592e-1215/17 [=========================>....] - ETA: 0s - loss: 0.1295 - accuracy: 0.9181 - jacard_coef: 2.5468e-1216/17 [===========================>..] - ETA: 0s - loss: 0.1295 - accuracy: 0.9172 - jacard_coef: 2.5126e-1217/17 [==============================] - 3s 180ms/step - loss: 0.1295 - accuracy: 0.9174 - jacard_coef: 3.9011e-12 - val_loss: 0.1227 - val_accuracy: 0.9304 - val_jacard_coef: 3.4094e-12 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1286 - accuracy: 0.9190 - jacard_coef: 2.3551e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1291 - accuracy: 0.9142 - jacard_coef: 2.2302e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1282 - accuracy: 0.9213 - jacard_coef: 2.4739e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1273 - accuracy: 0.9282 - jacard_coef: 2.7851e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1277 - accuracy: 0.9248 - jacard_coef: 2.6578e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1279 - accuracy: 0.9230 - jacard_coef: 2.5835e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1281 - accuracy: 0.9214 - jacard_coef: 2.5247e-12 8/17 [=============>................] - ETA: 1s - loss: 0.1286 - accuracy: 0.9176 - jacard_coef: 2.4282e-12 9/17 [==============>...............] - ETA: 1s - loss: 0.1282 - accuracy: 0.9198 - jacard_coef: 2.4969e-1210/17 [================>.............] - ETA: 1s - loss: 0.1285 - accuracy: 0.9173 - jacard_coef: 2.4284e-1211/17 [==================>...........] - ETA: 1s - loss: 0.1287 - accuracy: 0.9160 - jacard_coef: 2.3863e-1212/17 [====================>.........] - ETA: 0s - loss: 0.1284 - accuracy: 0.9178 - jacard_coef: 2.4399e-1213/17 [=====================>........] - ETA: 0s - loss: 0.1282 - accuracy: 0.9194 - jacard_coef: 2.4931e-1214/17 [=======================>......] - ETA: 0s - loss: 0.1279 - accuracy: 0.9210 - jacard_coef: 2.5494e-1215/17 [=========================>....] - ETA: 0s - loss: 0.1281 - accuracy: 0.9191 - jacard_coef: 2.4979e-1216/17 [===========================>..] - ETA: 0s - loss: 0.1283 - accuracy: 0.9173 - jacard_coef: 2.4503e-1217/17 [==============================] - 3s 180ms/step - loss: 0.1283 - accuracy: 0.9175 - jacard_coef: 3.8285e-12 - val_loss: 0.1233 - val_accuracy: 0.9304 - val_jacard_coef: 3.4085e-12 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1262 - accuracy: 0.9291 - jacard_coef: 2.6892e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1277 - accuracy: 0.9171 - jacard_coef: 2.3491e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1273 - accuracy: 0.9201 - jacard_coef: 2.4273e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1276 - accuracy: 0.9166 - jacard_coef: 2.3292e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1275 - accuracy: 0.9167 - jacard_coef: 2.3235e-12 6/17 [=========>....................] - ETA: 1s - loss: 0.1271 - accuracy: 0.9194 - jacard_coef: 2.4102e-12 7/17 [===========>..................] - ETA: 1s - loss: 0.1274 - accuracy: 0.9168 - jacard_coef: 2.3415e-12 8/17 [=============>................] - ETA: 1s - loss: 0.1269 - accuracy: 0.9208 - jacard_coef: 2.5123e-12 9/17 [==============>...............] - ETA: 1s - loss: 0.1269 - accuracy: 0.9204 - jacard_coef: 2.4893e-1210/17 [================>.............] - ETA: 1s - loss: 0.1268 - accuracy: 0.9217 - jacard_coef: 2.5256e-1211/17 [==================>...........] - ETA: 1s - loss: 0.1265 - accuracy: 0.9230 - jacard_coef: 2.5676e-1212/17 [====================>.........] - ETA: 0s - loss: 0.1266 - accuracy: 0.9220 - jacard_coef: 2.5326e-1213/17 [=====================>........] - ETA: 0s - loss: 0.1272 - accuracy: 0.9181 - jacard_coef: 2.4518e-1214/17 [=======================>......] - ETA: 0s - loss: 0.1272 - accuracy: 0.9176 - jacard_coef: 2.4305e-1215/17 [=========================>....] - ETA: 0s - loss: 0.1268 - accuracy: 0.9205 - jacard_coef: 2.5937e-1216/17 [===========================>..] - ETA: 0s - loss: 0.1270 - accuracy: 0.9182 - jacard_coef: 2.5341e-1217/17 [==============================] - 3s 180ms/step - loss: 0.1271 - accuracy: 0.9175 - jacard_coef: 2.8966e-12 - val_loss: 0.1227 - val_accuracy: 0.9303 - val_jacard_coef: 3.4077e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0725 (epoch 7)
  Final Val Loss: 0.1227
  Training Time: 0:01:48.906025
  Stability (std): 0.0083

Results saved to: hyperparameter_optimization_20250926_165036/exp_16_Attention_UNet_lr5e-4_bs8/Attention_UNet_lr0.0005_bs8_results.json

Experiment 16 completed in 125s
Progress: 16/36 completed
Estimated remaining time: 41 minutes

ðŸ”¬ EXPERIMENT 17/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878559.228619 1097935 service.cc:145] XLA service 0x14cf9d2946f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878559.228643 1097935 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878559.366436 1097935 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 6:44 - loss: 0.3313 - accuracy: 0.4637 - jacard_coef: 0.07842/9 [=====>........................] - ETA: 59s - loss: 0.3012 - accuracy: 0.3743 - jacard_coef: 0.0842 3/9 [=========>....................] - ETA: 26s - loss: 0.2751 - accuracy: 0.3195 - jacard_coef: 0.07544/9 [============>.................] - ETA: 15s - loss: 0.2617 - accuracy: 0.2997 - jacard_coef: 0.08515/9 [===============>..............] - ETA: 9s - loss: 0.2563 - accuracy: 0.2770 - jacard_coef: 0.0853 6/9 [===================>..........] - ETA: 5s - loss: 0.2474 - accuracy: 0.2583 - jacard_coef: 0.08417/9 [======================>.......] - ETA: 3s - loss: 0.2407 - accuracy: 0.2394 - jacard_coef: 0.08178/9 [=========================>....] - ETA: 1s - loss: 0.2357 - accuracy: 0.2287 - jacard_coef: 0.08259/9 [==============================] - ETA: 0s - loss: 0.2355 - accuracy: 0.2275 - jacard_coef: 0.07389/9 [==============================] - 69s 2s/step - loss: 0.2355 - accuracy: 0.2275 - jacard_coef: 0.0738 - val_loss: 1.1113 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1887 - accuracy: 0.2532 - jacard_coef: 0.08752/9 [=====>........................] - ETA: 2s - loss: 0.1870 - accuracy: 0.2477 - jacard_coef: 0.08433/9 [=========>....................] - ETA: 2s - loss: 0.1877 - accuracy: 0.2596 - jacard_coef: 0.08024/9 [============>.................] - ETA: 1s - loss: 0.1864 - accuracy: 0.2639 - jacard_coef: 0.08095/9 [===============>..............] - ETA: 1s - loss: 0.1854 - accuracy: 0.2706 - jacard_coef: 0.08436/9 [===================>..........] - ETA: 1s - loss: 0.1849 - accuracy: 0.2920 - jacard_coef: 0.08137/9 [======================>.......] - ETA: 0s - loss: 0.1839 - accuracy: 0.3044 - jacard_coef: 0.08038/9 [=========================>....] - ETA: 0s - loss: 0.1944 - accuracy: 0.3114 - jacard_coef: 0.08199/9 [==============================] - 3s 333ms/step - loss: 0.1947 - accuracy: 0.3101 - jacard_coef: 0.0733 - val_loss: 1.1194 - val_accuracy: 0.9270 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.2101 - accuracy: 0.1764 - jacard_coef: 0.10782/9 [=====>........................] - ETA: 2s - loss: 0.2154 - accuracy: 0.2251 - jacard_coef: 0.09883/9 [=========>....................] - ETA: 2s - loss: 0.2177 - accuracy: 0.1905 - jacard_coef: 0.09484/9 [============>.................] - ETA: 1s - loss: 0.2165 - accuracy: 0.1703 - jacard_coef: 0.08905/9 [===============>..............] - ETA: 1s - loss: 0.2130 - accuracy: 0.1735 - jacard_coef: 0.08756/9 [===================>..........] - ETA: 1s - loss: 0.2098 - accuracy: 0.2280 - jacard_coef: 0.08117/9 [======================>.......] - ETA: 0s - loss: 0.2063 - accuracy: 0.2494 - jacard_coef: 0.08248/9 [=========================>....] - ETA: 0s - loss: 0.2081 - accuracy: 0.2449 - jacard_coef: 0.08319/9 [==============================] - 3s 332ms/step - loss: 0.2081 - accuracy: 0.2437 - jacard_coef: 0.0796 - val_loss: 1.1243 - val_accuracy: 0.9170 - val_jacard_coef: 0.0141 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.2382 - accuracy: 0.1484 - jacard_coef: 0.10552/9 [=====>........................] - ETA: 2s - loss: 0.2291 - accuracy: 0.1284 - jacard_coef: 0.08553/9 [=========>....................] - ETA: 2s - loss: 0.2236 - accuracy: 0.1295 - jacard_coef: 0.08784/9 [============>.................] - ETA: 1s - loss: 0.2199 - accuracy: 0.1246 - jacard_coef: 0.08055/9 [===============>..............] - ETA: 1s - loss: 0.2166 - accuracy: 0.1886 - jacard_coef: 0.07776/9 [===================>..........] - ETA: 1s - loss: 0.2126 - accuracy: 0.2678 - jacard_coef: 0.07717/9 [======================>.......] - ETA: 0s - loss: 0.2094 - accuracy: 0.3287 - jacard_coef: 0.07708/9 [=========================>....] - ETA: 0s - loss: 0.2060 - accuracy: 0.3524 - jacard_coef: 0.07519/9 [==============================] - 3s 326ms/step - loss: 0.2059 - accuracy: 0.3512 - jacard_coef: 0.0769 - val_loss: 1.1189 - val_accuracy: 0.9300 - val_jacard_coef: 6.6793e-04 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1788 - accuracy: 0.3396 - jacard_coef: 0.08492/9 [=====>........................] - ETA: 2s - loss: 0.1794 - accuracy: 0.3490 - jacard_coef: 0.08183/9 [=========>....................] - ETA: 2s - loss: 0.1769 - accuracy: 0.3848 - jacard_coef: 0.08354/9 [============>.................] - ETA: 1s - loss: 0.1752 - accuracy: 0.4209 - jacard_coef: 0.07965/9 [===============>..............] - ETA: 1s - loss: 0.1878 - accuracy: 0.3806 - jacard_coef: 0.07556/9 [===================>..........] - ETA: 1s - loss: 0.1899 - accuracy: 0.3424 - jacard_coef: 0.07257/9 [======================>.......] - ETA: 0s - loss: 0.1924 - accuracy: 0.3204 - jacard_coef: 0.07448/9 [=========================>....] - ETA: 0s - loss: 0.1923 - accuracy: 0.3240 - jacard_coef: 0.07549/9 [==============================] - 3s 332ms/step - loss: 0.1922 - accuracy: 0.3249 - jacard_coef: 0.0786 - val_loss: 13.1525 - val_accuracy: 0.1362 - val_jacard_coef: 0.0704 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1974 - accuracy: 0.1498 - jacard_coef: 0.07362/9 [=====>........................] - ETA: 2s - loss: 0.2000 - accuracy: 0.1760 - jacard_coef: 0.07633/9 [=========>....................] - ETA: 2s - loss: 0.1920 - accuracy: 0.2501 - jacard_coef: 0.07574/9 [============>.................] - ETA: 1s - loss: 0.1895 - accuracy: 0.3111 - jacard_coef: 0.07375/9 [===============>..............] - ETA: 1s - loss: 0.1860 - accuracy: 0.3650 - jacard_coef: 0.07556/9 [===================>..........] - ETA: 1s - loss: 0.1853 - accuracy: 0.3600 - jacard_coef: 0.07867/9 [======================>.......] - ETA: 0s - loss: 0.1858 - accuracy: 0.3427 - jacard_coef: 0.07838/9 [=========================>....] - ETA: 0s - loss: 0.1866 - accuracy: 0.3386 - jacard_coef: 0.07739/9 [==============================] - 3s 327ms/step - loss: 0.1869 - accuracy: 0.3400 - jacard_coef: 0.0887 - val_loss: 1.1206 - val_accuracy: 0.9304 - val_jacard_coef: 1.4610e-12 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1842 - accuracy: 0.6242 - jacard_coef: 0.08932/9 [=====>........................] - ETA: 2s - loss: 0.1839 - accuracy: 0.5818 - jacard_coef: 0.09213/9 [=========>....................] - ETA: 2s - loss: 0.1820 - accuracy: 0.5349 - jacard_coef: 0.07924/9 [============>.................] - ETA: 1s - loss: 0.1830 - accuracy: 0.4766 - jacard_coef: 0.08015/9 [===============>..............] - ETA: 1s - loss: 0.1825 - accuracy: 0.4484 - jacard_coef: 0.08086/9 [===================>..........] - ETA: 1s - loss: 0.1817 - accuracy: 0.4308 - jacard_coef: 0.07827/9 [======================>.......] - ETA: 0s - loss: 0.1809 - accuracy: 0.4192 - jacard_coef: 0.08178/9 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.4073 - jacard_coef: 0.08229/9 [==============================] - 3s 327ms/step - loss: 0.1817 - accuracy: 0.4062 - jacard_coef: 0.0819 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1713 - accuracy: 0.6623 - jacard_coef: 0.06642/9 [=====>........................] - ETA: 2s - loss: 0.1736 - accuracy: 0.6733 - jacard_coef: 0.05403/9 [=========>....................] - ETA: 2s - loss: 0.1727 - accuracy: 0.6761 - jacard_coef: 0.06184/9 [============>.................] - ETA: 1s - loss: 0.1727 - accuracy: 0.6816 - jacard_coef: 0.06425/9 [===============>..............] - ETA: 1s - loss: 0.1720 - accuracy: 0.6841 - jacard_coef: 0.06696/9 [===================>..........] - ETA: 1s - loss: 0.1714 - accuracy: 0.6795 - jacard_coef: 0.06867/9 [======================>.......] - ETA: 0s - loss: 0.1706 - accuracy: 0.6844 - jacard_coef: 0.06748/9 [=========================>....] - ETA: 0s - loss: 0.1702 - accuracy: 0.6723 - jacard_coef: 0.07069/9 [==============================] - 3s 327ms/step - loss: 0.1706 - accuracy: 0.6709 - jacard_coef: 0.0770 - val_loss: 1.0678 - val_accuracy: 0.9187 - val_jacard_coef: 0.0054 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1728 - accuracy: 0.3271 - jacard_coef: 0.06762/9 [=====>........................] - ETA: 2s - loss: 0.1799 - accuracy: 0.2773 - jacard_coef: 0.07563/9 [=========>....................] - ETA: 2s - loss: 0.1787 - accuracy: 0.2704 - jacard_coef: 0.08154/9 [============>.................] - ETA: 1s - loss: 0.1806 - accuracy: 0.2597 - jacard_coef: 0.08145/9 [===============>..............] - ETA: 1s - loss: 0.1804 - accuracy: 0.2553 - jacard_coef: 0.08266/9 [===================>..........] - ETA: 1s - loss: 0.1798 - accuracy: 0.2556 - jacard_coef: 0.08507/9 [======================>.......] - ETA: 0s - loss: 0.1802 - accuracy: 0.2487 - jacard_coef: 0.08768/9 [=========================>....] - ETA: 0s - loss: 0.1813 - accuracy: 0.2399 - jacard_coef: 0.08409/9 [==============================] - 3s 327ms/step - loss: 0.1813 - accuracy: 0.2389 - jacard_coef: 0.0751 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1786 - accuracy: 0.1931 - jacard_coef: 0.08192/9 [=====>........................] - ETA: 2s - loss: 0.1799 - accuracy: 0.1835 - jacard_coef: 0.07123/9 [=========>....................] - ETA: 2s - loss: 0.1813 - accuracy: 0.1771 - jacard_coef: 0.07244/9 [============>.................] - ETA: 1s - loss: 0.1853 - accuracy: 0.1843 - jacard_coef: 0.08345/9 [===============>..............] - ETA: 1s - loss: 0.1838 - accuracy: 0.1868 - jacard_coef: 0.08356/9 [===================>..........] - ETA: 1s - loss: 0.1818 - accuracy: 0.1975 - jacard_coef: 0.08487/9 [======================>.......] - ETA: 0s - loss: 0.1811 - accuracy: 0.2095 - jacard_coef: 0.08218/9 [=========================>....] - ETA: 0s - loss: 0.1795 - accuracy: 0.2951 - jacard_coef: 0.07339/9 [==============================] - 3s 326ms/step - loss: 0.1794 - accuracy: 0.2946 - jacard_coef: 0.0804 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1705 - accuracy: 0.8360 - jacard_coef: 0.04242/9 [=====>........................] - ETA: 2s - loss: 0.1692 - accuracy: 0.8665 - jacard_coef: 0.02263/9 [=========>....................] - ETA: 2s - loss: 0.1684 - accuracy: 0.8860 - jacard_coef: 0.01834/9 [============>.................] - ETA: 1s - loss: 0.1679 - accuracy: 0.8904 - jacard_coef: 0.01735/9 [===============>..............] - ETA: 1s - loss: 0.1709 - accuracy: 0.8965 - jacard_coef: 0.01926/9 [===================>..........] - ETA: 1s - loss: 0.1699 - accuracy: 0.8977 - jacard_coef: 0.01677/9 [======================>.......] - ETA: 0s - loss: 0.1694 - accuracy: 0.8989 - jacard_coef: 0.01448/9 [=========================>....] - ETA: 0s - loss: 0.1689 - accuracy: 0.8957 - jacard_coef: 0.01509/9 [==============================] - 3s 327ms/step - loss: 0.1690 - accuracy: 0.8905 - jacard_coef: 0.0142 - val_loss: 1.1211 - val_accuracy: 0.9300 - val_jacard_coef: 1.4529e-12 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1673 - accuracy: 0.8764 - jacard_coef: 0.01912/9 [=====>........................] - ETA: 2s - loss: 0.1729 - accuracy: 0.9013 - jacard_coef: 0.01933/9 [=========>....................] - ETA: 2s - loss: 0.1705 - accuracy: 0.9029 - jacard_coef: 0.02034/9 [============>.................] - ETA: 1s - loss: 0.1685 - accuracy: 0.9004 - jacard_coef: 0.02195/9 [===============>..............] - ETA: 1s - loss: 0.1672 - accuracy: 0.8952 - jacard_coef: 0.02016/9 [===================>..........] - ETA: 1s - loss: 0.1660 - accuracy: 0.8975 - jacard_coef: 0.01727/9 [======================>.......] - ETA: 0s - loss: 0.1657 - accuracy: 0.8929 - jacard_coef: 0.01738/9 [=========================>....] - ETA: 0s - loss: 0.1650 - accuracy: 0.8984 - jacard_coef: 0.01649/9 [==============================] - 3s 327ms/step - loss: 0.1650 - accuracy: 0.8962 - jacard_coef: 0.0146 - val_loss: 1.0575 - val_accuracy: 0.8506 - val_jacard_coef: 0.0207 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1602 - accuracy: 0.9129 - jacard_coef: 0.02392/9 [=====>........................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8943 - jacard_coef: 0.01933/9 [=========>....................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8980 - jacard_coef: 0.01534/9 [============>.................] - ETA: 1s - loss: 0.1645 - accuracy: 0.9000 - jacard_coef: 0.02245/9 [===============>..............] - ETA: 1s - loss: 0.1638 - accuracy: 0.9034 - jacard_coef: 0.02106/9 [===================>..........] - ETA: 1s - loss: 0.1641 - accuracy: 0.8988 - jacard_coef: 0.02117/9 [======================>.......] - ETA: 0s - loss: 0.1633 - accuracy: 0.9017 - jacard_coef: 0.01818/9 [=========================>....] - ETA: 0s - loss: 0.1625 - accuracy: 0.9048 - jacard_coef: 0.01649/9 [==============================] - 3s 327ms/step - loss: 0.1626 - accuracy: 0.9010 - jacard_coef: 0.0279 - val_loss: 0.4806 - val_accuracy: 0.8337 - val_jacard_coef: 0.0247 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1595 - accuracy: 0.8858 - jacard_coef: 8.3488e-132/9 [=====>........................] - ETA: 2s - loss: 0.1586 - accuracy: 0.9056 - jacard_coef: 0.0036    3/9 [=========>....................] - ETA: 2s - loss: 0.1588 - accuracy: 0.9045 - jacard_coef: 0.00354/9 [============>.................] - ETA: 1s - loss: 0.1585 - accuracy: 0.9095 - jacard_coef: 0.00275/9 [===============>..............] - ETA: 1s - loss: 0.1588 - accuracy: 0.9070 - jacard_coef: 0.00216/9 [===================>..........] - ETA: 1s - loss: 0.1591 - accuracy: 0.9099 - jacard_coef: 0.00247/9 [======================>.......] - ETA: 0s - loss: 0.1606 - accuracy: 0.9124 - jacard_coef: 0.00258/9 [=========================>....] - ETA: 0s - loss: 0.1605 - accuracy: 0.9134 - jacard_coef: 0.00239/9 [==============================] - 3s 326ms/step - loss: 0.1606 - accuracy: 0.9082 - jacard_coef: 0.0073 - val_loss: 0.1418 - val_accuracy: 0.8031 - val_jacard_coef: 0.0229 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1612 - accuracy: 0.9118 - jacard_coef: 1.0812e-122/9 [=====>........................] - ETA: 2s - loss: 0.1604 - accuracy: 0.9177 - jacard_coef: 6.7330e-043/9 [=========>....................] - ETA: 2s - loss: 0.1591 - accuracy: 0.9138 - jacard_coef: 4.6238e-044/9 [============>.................] - ETA: 1s - loss: 0.1610 - accuracy: 0.9165 - jacard_coef: 0.0020    5/9 [===============>..............] - ETA: 1s - loss: 0.1608 - accuracy: 0.9150 - jacard_coef: 0.00176/9 [===================>..........] - ETA: 1s - loss: 0.1604 - accuracy: 0.9113 - jacard_coef: 0.00197/9 [======================>.......] - ETA: 0s - loss: 0.1594 - accuracy: 0.9153 - jacard_coef: 0.00168/9 [=========================>....] - ETA: 0s - loss: 0.1589 - accuracy: 0.9153 - jacard_coef: 0.00199/9 [==============================] - 3s 326ms/step - loss: 0.1590 - accuracy: 0.9145 - jacard_coef: 0.0023 - val_loss: 0.1555 - val_accuracy: 0.6726 - val_jacard_coef: 0.0497 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0704 (epoch 5)
  Final Val Loss: 0.1555
  Training Time: 0:01:51.465932
  Stability (std): 0.3974

Results saved to: hyperparameter_optimization_20250926_165036/exp_17_Attention_UNet_lr5e-4_bs16/Attention_UNet_lr0.0005_bs16_results.json

Experiment 17 completed in 128s
Progress: 17/36 completed
Estimated remaining time: 40 minutes

ðŸ”¬ EXPERIMENT 18/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878692.385216 1104332 service.cc:145] XLA service 0x146a65057f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878692.385240 1104332 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878692.524315 1104332 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:02 - loss: 0.3455 - accuracy: 0.5490 - jacard_coef: 0.06782/5 [===========>..................] - ETA: 46s - loss: 0.3249 - accuracy: 0.5055 - jacard_coef: 0.0694 3/5 [=================>............] - ETA: 16s - loss: 0.3026 - accuracy: 0.4821 - jacard_coef: 0.06874/5 [=======================>......] - ETA: 5s - loss: 0.2891 - accuracy: 0.5110 - jacard_coef: 0.0723 5/5 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.5109 - jacard_coef: 0.08005/5 [==============================] - 86s 6s/step - loss: 0.2887 - accuracy: 0.5109 - jacard_coef: 0.0800 - val_loss: 1.1776 - val_accuracy: 0.9234 - val_jacard_coef: 0.0061 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.2065 - accuracy: 0.4541 - jacard_coef: 0.08572/5 [===========>..................] - ETA: 2s - loss: 0.2008 - accuracy: 0.4463 - jacard_coef: 0.07843/5 [=================>............] - ETA: 1s - loss: 0.1972 - accuracy: 0.4448 - jacard_coef: 0.07664/5 [=======================>......] - ETA: 0s - loss: 0.1944 - accuracy: 0.4378 - jacard_coef: 0.07895/5 [==============================] - 3s 572ms/step - loss: 0.1943 - accuracy: 0.4377 - jacard_coef: 0.0953 - val_loss: 3.4704 - val_accuracy: 0.7810 - val_jacard_coef: 0.0756 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1815 - accuracy: 0.5230 - jacard_coef: 0.08112/5 [===========>..................] - ETA: 2s - loss: 0.1797 - accuracy: 0.4905 - jacard_coef: 0.07983/5 [=================>............] - ETA: 1s - loss: 0.1787 - accuracy: 0.4776 - jacard_coef: 0.07954/5 [=======================>......] - ETA: 0s - loss: 0.1777 - accuracy: 0.5020 - jacard_coef: 0.07705/5 [==============================] - 3s 558ms/step - loss: 0.1777 - accuracy: 0.5028 - jacard_coef: 0.0763 - val_loss: 14.9366 - val_accuracy: 0.0733 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1745 - accuracy: 0.7174 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 2s - loss: 0.1746 - accuracy: 0.7280 - jacard_coef: 0.06423/5 [=================>............] - ETA: 1s - loss: 0.1752 - accuracy: 0.7675 - jacard_coef: 0.05364/5 [=======================>......] - ETA: 0s - loss: 0.1757 - accuracy: 0.7919 - jacard_coef: 0.04765/5 [==============================] - 3s 559ms/step - loss: 0.1757 - accuracy: 0.7926 - jacard_coef: 0.0396 - val_loss: 14.9963 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1744 - accuracy: 0.8834 - jacard_coef: 0.01452/5 [===========>..................] - ETA: 2s - loss: 0.1732 - accuracy: 0.8858 - jacard_coef: 0.01223/5 [=================>............] - ETA: 1s - loss: 0.1727 - accuracy: 0.8954 - jacard_coef: 0.01244/5 [=======================>......] - ETA: 0s - loss: 0.1723 - accuracy: 0.9035 - jacard_coef: 0.01055/5 [==============================] - 3s 557ms/step - loss: 0.1724 - accuracy: 0.9030 - jacard_coef: 0.0090 - val_loss: 14.9944 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1702 - accuracy: 0.9124 - jacard_coef: 0.00482/5 [===========>..................] - ETA: 2s - loss: 0.1697 - accuracy: 0.9100 - jacard_coef: 0.00443/5 [=================>............] - ETA: 1s - loss: 0.1690 - accuracy: 0.9121 - jacard_coef: 0.00464/5 [=======================>......] - ETA: 0s - loss: 0.1688 - accuracy: 0.9100 - jacard_coef: 0.00565/5 [==============================] - 3s 558ms/step - loss: 0.1688 - accuracy: 0.9099 - jacard_coef: 0.0062 - val_loss: 14.8759 - val_accuracy: 0.0766 - val_jacard_coef: 0.0699 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1677 - accuracy: 0.8780 - jacard_coef: 0.03162/5 [===========>..................] - ETA: 2s - loss: 0.1675 - accuracy: 0.8792 - jacard_coef: 0.03103/5 [=================>............] - ETA: 1s - loss: 0.1674 - accuracy: 0.8761 - jacard_coef: 0.03444/5 [=======================>......] - ETA: 0s - loss: 0.1677 - accuracy: 0.8771 - jacard_coef: 0.03345/5 [==============================] - 3s 557ms/step - loss: 0.1677 - accuracy: 0.8766 - jacard_coef: 0.0271 - val_loss: 14.9114 - val_accuracy: 0.0697 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1667 - accuracy: 0.7707 - jacard_coef: 0.07702/5 [===========>..................] - ETA: 2s - loss: 0.1664 - accuracy: 0.7045 - jacard_coef: 0.07203/5 [=================>............] - ETA: 1s - loss: 0.1660 - accuracy: 0.6815 - jacard_coef: 0.06884/5 [=======================>......] - ETA: 0s - loss: 0.1656 - accuracy: 0.6720 - jacard_coef: 0.07265/5 [==============================] - 3s 556ms/step - loss: 0.1656 - accuracy: 0.6723 - jacard_coef: 0.0741 - val_loss: 14.4599 - val_accuracy: 0.0701 - val_jacard_coef: 0.0696 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1647 - accuracy: 0.7032 - jacard_coef: 0.08442/5 [===========>..................] - ETA: 2s - loss: 0.1647 - accuracy: 0.7090 - jacard_coef: 0.07203/5 [=================>............] - ETA: 1s - loss: 0.1646 - accuracy: 0.7749 - jacard_coef: 0.05254/5 [=======================>......] - ETA: 0s - loss: 0.1647 - accuracy: 0.8075 - jacard_coef: 0.03955/5 [==============================] - 3s 558ms/step - loss: 0.1647 - accuracy: 0.8089 - jacard_coef: 0.0417 - val_loss: 10.1646 - val_accuracy: 0.0782 - val_jacard_coef: 0.0700 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1656 - accuracy: 0.9168 - jacard_coef: 6.2448e-042/5 [===========>..................] - ETA: 2s - loss: 0.1651 - accuracy: 0.9096 - jacard_coef: 6.6395e-043/5 [=================>............] - ETA: 1s - loss: 0.1651 - accuracy: 0.9101 - jacard_coef: 8.4886e-044/5 [=======================>......] - ETA: 0s - loss: 0.1650 - accuracy: 0.9156 - jacard_coef: 0.0016    5/5 [==============================] - 3s 556ms/step - loss: 0.1650 - accuracy: 0.9161 - jacard_coef: 0.0020 - val_loss: 4.3360 - val_accuracy: 0.1411 - val_jacard_coef: 0.0713 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1638 - accuracy: 0.8856 - jacard_coef: 0.01372/5 [===========>..................] - ETA: 2s - loss: 0.1647 - accuracy: 0.8987 - jacard_coef: 0.00883/5 [=================>............] - ETA: 1s - loss: 0.1649 - accuracy: 0.9019 - jacard_coef: 0.00634/5 [=======================>......] - ETA: 0s - loss: 0.1645 - accuracy: 0.9069 - jacard_coef: 0.00535/5 [==============================] - 3s 559ms/step - loss: 0.1658 - accuracy: 0.9043 - jacard_coef: 0.0056 - val_loss: 0.6677 - val_accuracy: 0.2165 - val_jacard_coef: 0.0731 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1644 - accuracy: 0.8902 - jacard_coef: 0.02322/5 [===========>..................] - ETA: 2s - loss: 0.1647 - accuracy: 0.8928 - jacard_coef: 0.01663/5 [=================>............] - ETA: 1s - loss: 0.1650 - accuracy: 0.8879 - jacard_coef: 0.01934/5 [=======================>......] - ETA: 0s - loss: 0.1653 - accuracy: 0.8832 - jacard_coef: 0.02185/5 [==============================] - 3s 557ms/step - loss: 0.1653 - accuracy: 0.8839 - jacard_coef: 0.0237 - val_loss: 0.2196 - val_accuracy: 0.1991 - val_jacard_coef: 0.0729 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0756 (epoch 2)
  Final Val Loss: 0.2196
  Training Time: 0:01:58.345981
  Stability (std): 5.9568

Results saved to: hyperparameter_optimization_20250926_165036/exp_18_Attention_UNet_lr5e-4_bs32/Attention_UNet_lr0.0005_bs32_results.json

Experiment 18 completed in 135s
Progress: 18/36 completed
Estimated remaining time: 40 minutes

ðŸ”¬ EXPERIMENT 19/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878818.860933 1110523 service.cc:145] XLA service 0x14525d0b57b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878818.860956 1110523 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878818.997930 1110523 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 12:04 - loss: 0.3408 - accuracy: 0.5002 - jacard_coef: 0.0620 2/17 [==>...........................] - ETA: 1:12 - loss: 0.3074 - accuracy: 0.4315 - jacard_coef: 0.0807  3/17 [====>.........................] - ETA: 34s - loss: 0.2790 - accuracy: 0.3606 - jacard_coef: 0.0830  4/17 [======>.......................] - ETA: 22s - loss: 0.2652 - accuracy: 0.3030 - jacard_coef: 0.0772 5/17 [=======>......................] - ETA: 16s - loss: 0.2529 - accuracy: 0.2853 - jacard_coef: 0.0788 6/17 [=========>....................] - ETA: 12s - loss: 0.2446 - accuracy: 0.2875 - jacard_coef: 0.0757 7/17 [===========>..................] - ETA: 9s - loss: 0.2394 - accuracy: 0.2831 - jacard_coef: 0.0751  8/17 [=============>................] - ETA: 7s - loss: 0.2343 - accuracy: 0.2664 - jacard_coef: 0.0810 9/17 [==============>...............] - ETA: 6s - loss: 0.2292 - accuracy: 0.2506 - jacard_coef: 0.082210/17 [================>.............] - ETA: 4s - loss: 0.2242 - accuracy: 0.2452 - jacard_coef: 0.084211/17 [==================>...........] - ETA: 3s - loss: 0.2211 - accuracy: 0.2461 - jacard_coef: 0.081712/17 [====================>.........] - ETA: 3s - loss: 0.2178 - accuracy: 0.2549 - jacard_coef: 0.082513/17 [=====================>........] - ETA: 2s - loss: 0.2150 - accuracy: 0.2509 - jacard_coef: 0.078814/17 [=======================>......] - ETA: 1s - loss: 0.2129 - accuracy: 0.2747 - jacard_coef: 0.078915/17 [=========================>....] - ETA: 1s - loss: 0.2107 - accuracy: 0.2987 - jacard_coef: 0.081016/17 [===========================>..] - ETA: 0s - loss: 0.2089 - accuracy: 0.2927 - jacard_coef: 0.081017/17 [==============================] - ETA: 0s - loss: 0.2088 - accuracy: 0.2908 - jacard_coef: 0.077617/17 [==============================] - 59s 869ms/step - loss: 0.2088 - accuracy: 0.2908 - jacard_coef: 0.0776 - val_loss: 1.1584 - val_accuracy: 0.9263 - val_jacard_coef: 0.0043 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1820 - accuracy: 0.1537 - jacard_coef: 0.0368 2/17 [==>...........................] - ETA: 2s - loss: 0.1807 - accuracy: 0.1443 - jacard_coef: 0.0674 3/17 [====>.........................] - ETA: 2s - loss: 0.1823 - accuracy: 0.1345 - jacard_coef: 0.0698 4/17 [======>.......................] - ETA: 2s - loss: 0.1811 - accuracy: 0.1580 - jacard_coef: 0.0722 5/17 [=======>......................] - ETA: 2s - loss: 0.1805 - accuracy: 0.2127 - jacard_coef: 0.0770 6/17 [=========>....................] - ETA: 1s - loss: 0.1790 - accuracy: 0.2831 - jacard_coef: 0.0783 7/17 [===========>..................] - ETA: 1s - loss: 0.1838 - accuracy: 0.3056 - jacard_coef: 0.0806 8/17 [=============>................] - ETA: 1s - loss: 0.1825 - accuracy: 0.3052 - jacard_coef: 0.0837 9/17 [==============>...............] - ETA: 1s - loss: 0.1816 - accuracy: 0.2960 - jacard_coef: 0.079710/17 [================>.............] - ETA: 1s - loss: 0.1810 - accuracy: 0.2923 - jacard_coef: 0.079011/17 [==================>...........] - ETA: 1s - loss: 0.1803 - accuracy: 0.2906 - jacard_coef: 0.080412/17 [====================>.........] - ETA: 0s - loss: 0.1798 - accuracy: 0.2938 - jacard_coef: 0.081113/17 [=====================>........] - ETA: 0s - loss: 0.1790 - accuracy: 0.2964 - jacard_coef: 0.082114/17 [=======================>......] - ETA: 0s - loss: 0.1785 - accuracy: 0.2959 - jacard_coef: 0.083615/17 [=========================>....] - ETA: 0s - loss: 0.1781 - accuracy: 0.2971 - jacard_coef: 0.082716/17 [===========================>..] - ETA: 0s - loss: 0.1779 - accuracy: 0.3302 - jacard_coef: 0.079717/17 [==============================] - 3s 183ms/step - loss: 0.1779 - accuracy: 0.3283 - jacard_coef: 0.0752 - val_loss: 14.6023 - val_accuracy: 0.0746 - val_jacard_coef: 0.0684 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1783 - accuracy: 0.7858 - jacard_coef: 0.0653 2/17 [==>...........................] - ETA: 2s - loss: 0.1814 - accuracy: 0.5139 - jacard_coef: 0.0724 3/17 [====>.........................] - ETA: 2s - loss: 0.1804 - accuracy: 0.3786 - jacard_coef: 0.0549 4/17 [======>.......................] - ETA: 2s - loss: 0.1790 - accuracy: 0.3303 - jacard_coef: 0.0621 5/17 [=======>......................] - ETA: 2s - loss: 0.1791 - accuracy: 0.3015 - jacard_coef: 0.0663 6/17 [=========>....................] - ETA: 1s - loss: 0.1800 - accuracy: 0.2730 - jacard_coef: 0.0646 7/17 [===========>..................] - ETA: 1s - loss: 0.1793 - accuracy: 0.2658 - jacard_coef: 0.0761 8/17 [=============>................] - ETA: 1s - loss: 0.1790 - accuracy: 0.2556 - jacard_coef: 0.0817 9/17 [==============>...............] - ETA: 1s - loss: 0.1784 - accuracy: 0.2444 - jacard_coef: 0.081010/17 [================>.............] - ETA: 1s - loss: 0.1775 - accuracy: 0.3132 - jacard_coef: 0.072911/17 [==================>...........] - ETA: 1s - loss: 0.1774 - accuracy: 0.3684 - jacard_coef: 0.069512/17 [====================>.........] - ETA: 0s - loss: 0.1767 - accuracy: 0.4100 - jacard_coef: 0.063713/17 [=====================>........] - ETA: 0s - loss: 0.1762 - accuracy: 0.4487 - jacard_coef: 0.060414/17 [=======================>......] - ETA: 0s - loss: 0.1756 - accuracy: 0.4807 - jacard_coef: 0.056115/17 [=========================>....] - ETA: 0s - loss: 0.1750 - accuracy: 0.5082 - jacard_coef: 0.052716/17 [===========================>..] - ETA: 0s - loss: 0.1745 - accuracy: 0.5337 - jacard_coef: 0.049917/17 [==============================] - 3s 179ms/step - loss: 0.1745 - accuracy: 0.5361 - jacard_coef: 0.0477 - val_loss: 3.9354 - val_accuracy: 0.1294 - val_jacard_coef: 0.0653 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1684 - accuracy: 0.9137 - jacard_coef: 2.2093e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.8922 - jacard_coef: 0.0038     3/17 [====>.........................] - ETA: 2s - loss: 0.1683 - accuracy: 0.8890 - jacard_coef: 0.0037 4/17 [======>.......................] - ETA: 2s - loss: 0.1684 - accuracy: 0.8952 - jacard_coef: 0.0055 5/17 [=======>......................] - ETA: 2s - loss: 0.1686 - accuracy: 0.8876 - jacard_coef: 0.0107 6/17 [=========>....................] - ETA: 1s - loss: 0.1696 - accuracy: 0.7614 - jacard_coef: 0.0204 7/17 [===========>..................] - ETA: 1s - loss: 0.1694 - accuracy: 0.7750 - jacard_coef: 0.0193 8/17 [=============>................] - ETA: 1s - loss: 0.1695 - accuracy: 0.7828 - jacard_coef: 0.0217 9/17 [==============>...............] - ETA: 1s - loss: 0.1695 - accuracy: 0.7948 - jacard_coef: 0.023210/17 [================>.............] - ETA: 1s - loss: 0.1693 - accuracy: 0.8065 - jacard_coef: 0.021611/17 [==================>...........] - ETA: 1s - loss: 0.1693 - accuracy: 0.8142 - jacard_coef: 0.027612/17 [====================>.........] - ETA: 0s - loss: 0.1693 - accuracy: 0.8190 - jacard_coef: 0.027113/17 [=====================>........] - ETA: 0s - loss: 0.1691 - accuracy: 0.8270 - jacard_coef: 0.025214/17 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.8298 - jacard_coef: 0.024715/17 [=========================>....] - ETA: 0s - loss: 0.1690 - accuracy: 0.8338 - jacard_coef: 0.024216/17 [===========================>..] - ETA: 0s - loss: 0.1691 - accuracy: 0.8350 - jacard_coef: 0.023717/17 [==============================] - 3s 179ms/step - loss: 0.1690 - accuracy: 0.8350 - jacard_coef: 0.0325 - val_loss: 0.1618 - val_accuracy: 0.9304 - val_jacard_coef: 5.7580e-05 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1665 - accuracy: 0.8459 - jacard_coef: 0.0317 2/17 [==>...........................] - ETA: 2s - loss: 0.1651 - accuracy: 0.8349 - jacard_coef: 0.0428 3/17 [====>.........................] - ETA: 2s - loss: 0.1673 - accuracy: 0.8264 - jacard_coef: 0.0524 4/17 [======>.......................] - ETA: 2s - loss: 0.1661 - accuracy: 0.8256 - jacard_coef: 0.0508 5/17 [=======>......................] - ETA: 2s - loss: 0.1657 - accuracy: 0.8218 - jacard_coef: 0.0454 6/17 [=========>....................] - ETA: 1s - loss: 0.1656 - accuracy: 0.8245 - jacard_coef: 0.0430 7/17 [===========>..................] - ETA: 1s - loss: 0.1651 - accuracy: 0.8331 - jacard_coef: 0.0401 8/17 [=============>................] - ETA: 1s - loss: 0.1653 - accuracy: 0.8432 - jacard_coef: 0.0430 9/17 [==============>...............] - ETA: 1s - loss: 0.1654 - accuracy: 0.8398 - jacard_coef: 0.044210/17 [================>.............] - ETA: 1s - loss: 0.1651 - accuracy: 0.8476 - jacard_coef: 0.041411/17 [==================>...........] - ETA: 1s - loss: 0.1648 - accuracy: 0.8506 - jacard_coef: 0.038112/17 [====================>.........] - ETA: 0s - loss: 0.1643 - accuracy: 0.8558 - jacard_coef: 0.035313/17 [=====================>........] - ETA: 0s - loss: 0.1642 - accuracy: 0.8583 - jacard_coef: 0.034314/17 [=======================>......] - ETA: 0s - loss: 0.1638 - accuracy: 0.8617 - jacard_coef: 0.032115/17 [=========================>....] - ETA: 0s - loss: 0.1635 - accuracy: 0.8640 - jacard_coef: 0.033716/17 [===========================>..] - ETA: 0s - loss: 0.1633 - accuracy: 0.8632 - jacard_coef: 0.034117/17 [==============================] - 3s 182ms/step - loss: 0.1637 - accuracy: 0.8621 - jacard_coef: 0.0345 - val_loss: 0.1758 - val_accuracy: 0.3090 - val_jacard_coef: 0.0731 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1628 - accuracy: 0.9027 - jacard_coef: 1.9607e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1709 - accuracy: 0.9133 - jacard_coef: 0.0136     3/17 [====>.........................] - ETA: 2s - loss: 0.1701 - accuracy: 0.9129 - jacard_coef: 0.0091 4/17 [======>.......................] - ETA: 2s - loss: 0.1734 - accuracy: 0.9078 - jacard_coef: 0.0095 5/17 [=======>......................] - ETA: 2s - loss: 0.1780 - accuracy: 0.7602 - jacard_coef: 0.0232 6/17 [=========>....................] - ETA: 1s - loss: 0.1769 - accuracy: 0.7838 - jacard_coef: 0.0269 7/17 [===========>..................] - ETA: 1s - loss: 0.1759 - accuracy: 0.7994 - jacard_coef: 0.0240 8/17 [=============>................] - ETA: 1s - loss: 0.1767 - accuracy: 0.8140 - jacard_coef: 0.0265 9/17 [==============>...............] - ETA: 1s - loss: 0.1758 - accuracy: 0.8211 - jacard_coef: 0.025910/17 [================>.............] - ETA: 1s - loss: 0.1749 - accuracy: 0.8337 - jacard_coef: 0.023911/17 [==================>...........] - ETA: 1s - loss: 0.1766 - accuracy: 0.8394 - jacard_coef: 0.024912/17 [====================>.........] - ETA: 0s - loss: 0.1757 - accuracy: 0.8450 - jacard_coef: 0.023513/17 [=====================>........] - ETA: 0s - loss: 0.1751 - accuracy: 0.8087 - jacard_coef: 0.026714/17 [=======================>......] - ETA: 0s - loss: 0.1754 - accuracy: 0.8142 - jacard_coef: 0.025915/17 [=========================>....] - ETA: 0s - loss: 0.1749 - accuracy: 0.8181 - jacard_coef: 0.024616/17 [===========================>..] - ETA: 0s - loss: 0.1742 - accuracy: 0.8260 - jacard_coef: 0.023117/17 [==============================] - 3s 179ms/step - loss: 0.1741 - accuracy: 0.8269 - jacard_coef: 0.0263 - val_loss: 0.1808 - val_accuracy: 0.0802 - val_jacard_coef: 0.0683 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1569 - accuracy: 0.9362 - jacard_coef: 0.0553 2/17 [==>...........................] - ETA: 2s - loss: 0.1606 - accuracy: 0.9162 - jacard_coef: 0.0309 3/17 [====>.........................] - ETA: 2s - loss: 0.1688 - accuracy: 0.9087 - jacard_coef: 0.0392 4/17 [======>.......................] - ETA: 2s - loss: 0.1671 - accuracy: 0.9049 - jacard_coef: 0.0305 5/17 [=======>......................] - ETA: 2s - loss: 0.1665 - accuracy: 0.8999 - jacard_coef: 0.0271 6/17 [=========>....................] - ETA: 1s - loss: 0.1654 - accuracy: 0.8965 - jacard_coef: 0.0238 7/17 [===========>..................] - ETA: 1s - loss: 0.1647 - accuracy: 0.8958 - jacard_coef: 0.0288 8/17 [=============>................] - ETA: 1s - loss: 0.1639 - accuracy: 0.8963 - jacard_coef: 0.0252 9/17 [==============>...............] - ETA: 1s - loss: 0.1633 - accuracy: 0.8955 - jacard_coef: 0.024610/17 [================>.............] - ETA: 1s - loss: 0.1627 - accuracy: 0.8940 - jacard_coef: 0.025611/17 [==================>...........] - ETA: 1s - loss: 0.1696 - accuracy: 0.8420 - jacard_coef: 0.031512/17 [====================>.........] - ETA: 0s - loss: 0.1683 - accuracy: 0.8478 - jacard_coef: 0.028813/17 [=====================>........] - ETA: 0s - loss: 0.1670 - accuracy: 0.8547 - jacard_coef: 0.026614/17 [=======================>......] - ETA: 0s - loss: 0.1657 - accuracy: 0.8629 - jacard_coef: 0.024715/17 [=========================>....] - ETA: 0s - loss: 0.1661 - accuracy: 0.8302 - jacard_coef: 0.027416/17 [===========================>..] - ETA: 0s - loss: 0.1665 - accuracy: 0.8158 - jacard_coef: 0.030117/17 [==============================] - 3s 179ms/step - loss: 0.1664 - accuracy: 0.8167 - jacard_coef: 0.0283 - val_loss: 0.1677 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1515 - accuracy: 0.9031 - jacard_coef: 1.9688e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1595 - accuracy: 0.8069 - jacard_coef: 0.0342     3/17 [====>.........................] - ETA: 2s - loss: 0.1620 - accuracy: 0.7672 - jacard_coef: 0.0452 4/17 [======>.......................] - ETA: 2s - loss: 0.1637 - accuracy: 0.7409 - jacard_coef: 0.0578 5/17 [=======>......................] - ETA: 2s - loss: 0.1691 - accuracy: 0.7504 - jacard_coef: 0.0646 6/17 [=========>....................] - ETA: 1s - loss: 0.1736 - accuracy: 0.7645 - jacard_coef: 0.0610 7/17 [===========>..................] - ETA: 1s - loss: 0.1738 - accuracy: 0.7822 - jacard_coef: 0.0561 8/17 [=============>................] - ETA: 1s - loss: 0.1725 - accuracy: 0.7960 - jacard_coef: 0.0511 9/17 [==============>...............] - ETA: 1s - loss: 0.1713 - accuracy: 0.8068 - jacard_coef: 0.048210/17 [================>.............] - ETA: 1s - loss: 0.1707 - accuracy: 0.8129 - jacard_coef: 0.046811/17 [==================>...........] - ETA: 1s - loss: 0.1703 - accuracy: 0.8139 - jacard_coef: 0.047012/17 [====================>.........] - ETA: 0s - loss: 0.1718 - accuracy: 0.8091 - jacard_coef: 0.048813/17 [=====================>........] - ETA: 0s - loss: 0.1708 - accuracy: 0.8037 - jacard_coef: 0.049714/17 [=======================>......] - ETA: 0s - loss: 0.1706 - accuracy: 0.7947 - jacard_coef: 0.053915/17 [=========================>....] - ETA: 0s - loss: 0.1701 - accuracy: 0.7814 - jacard_coef: 0.054916/17 [===========================>..] - ETA: 0s - loss: 0.1707 - accuracy: 0.7879 - jacard_coef: 0.055717/17 [==============================] - 3s 180ms/step - loss: 0.1707 - accuracy: 0.7888 - jacard_coef: 0.0524 - val_loss: 0.1693 - val_accuracy: 0.8987 - val_jacard_coef: 0.0200 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1565 - accuracy: 0.8769 - jacard_coef: 0.0052 2/17 [==>...........................] - ETA: 2s - loss: 0.1552 - accuracy: 0.8833 - jacard_coef: 0.0183 3/17 [====>.........................] - ETA: 2s - loss: 0.1577 - accuracy: 0.8463 - jacard_coef: 0.0293 4/17 [======>.......................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8408 - jacard_coef: 0.0326 5/17 [=======>......................] - ETA: 2s - loss: 0.1595 - accuracy: 0.8479 - jacard_coef: 0.0334 6/17 [=========>....................] - ETA: 1s - loss: 0.1606 - accuracy: 0.8173 - jacard_coef: 0.0355 7/17 [===========>..................] - ETA: 1s - loss: 0.1591 - accuracy: 0.7927 - jacard_coef: 0.0366 8/17 [=============>................] - ETA: 1s - loss: 0.1584 - accuracy: 0.8107 - jacard_coef: 0.0320 9/17 [==============>...............] - ETA: 1s - loss: 0.1577 - accuracy: 0.8189 - jacard_coef: 0.028510/17 [================>.............] - ETA: 1s - loss: 0.1576 - accuracy: 0.8284 - jacard_coef: 0.025611/17 [==================>...........] - ETA: 1s - loss: 0.1569 - accuracy: 0.8390 - jacard_coef: 0.023312/17 [====================>.........] - ETA: 0s - loss: 0.1573 - accuracy: 0.8436 - jacard_coef: 0.021313/17 [=====================>........] - ETA: 0s - loss: 0.1566 - accuracy: 0.8525 - jacard_coef: 0.019714/17 [=======================>......] - ETA: 0s - loss: 0.1565 - accuracy: 0.8560 - jacard_coef: 0.018815/17 [=========================>....] - ETA: 0s - loss: 0.1562 - accuracy: 0.8592 - jacard_coef: 0.017916/17 [===========================>..] - ETA: 0s - loss: 0.1560 - accuracy: 0.8626 - jacard_coef: 0.017117/17 [==============================] - 3s 179ms/step - loss: 0.1560 - accuracy: 0.8616 - jacard_coef: 0.0216 - val_loss: 0.1469 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1524 - accuracy: 0.6924 - jacard_coef: 0.0662 2/17 [==>...........................] - ETA: 2s - loss: 0.1485 - accuracy: 0.8085 - jacard_coef: 0.0331 3/17 [====>.........................] - ETA: 2s - loss: 0.1487 - accuracy: 0.8466 - jacard_coef: 0.0221 4/17 [======>.......................] - ETA: 2s - loss: 0.1584 - accuracy: 0.8600 - jacard_coef: 0.0263 5/17 [=======>......................] - ETA: 2s - loss: 0.1600 - accuracy: 0.8683 - jacard_coef: 0.0291 6/17 [=========>....................] - ETA: 1s - loss: 0.1576 - accuracy: 0.8763 - jacard_coef: 0.0243 7/17 [===========>..................] - ETA: 1s - loss: 0.1567 - accuracy: 0.8774 - jacard_coef: 0.0228 8/17 [=============>................] - ETA: 1s - loss: 0.1558 - accuracy: 0.8774 - jacard_coef: 0.0218 9/17 [==============>...............] - ETA: 1s - loss: 0.1545 - accuracy: 0.8826 - jacard_coef: 0.019410/17 [================>.............] - ETA: 1s - loss: 0.1533 - accuracy: 0.8863 - jacard_coef: 0.018211/17 [==================>...........] - ETA: 1s - loss: 0.1537 - accuracy: 0.8736 - jacard_coef: 0.020912/17 [====================>.........] - ETA: 0s - loss: 0.1531 - accuracy: 0.8750 - jacard_coef: 0.021313/17 [=====================>........] - ETA: 0s - loss: 0.1528 - accuracy: 0.8775 - jacard_coef: 0.019814/17 [=======================>......] - ETA: 0s - loss: 0.1526 - accuracy: 0.8802 - jacard_coef: 0.018415/17 [=========================>....] - ETA: 0s - loss: 0.1524 - accuracy: 0.8821 - jacard_coef: 0.017216/17 [===========================>..] - ETA: 0s - loss: 0.1516 - accuracy: 0.8856 - jacard_coef: 0.016117/17 [==============================] - 3s 179ms/step - loss: 0.1516 - accuracy: 0.8841 - jacard_coef: 0.0217 - val_loss: 0.1314 - val_accuracy: 0.9261 - val_jacard_coef: 0.0056 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1518 - accuracy: 0.9206 - jacard_coef: 0.0011 2/17 [==>...........................] - ETA: 2s - loss: 0.1498 - accuracy: 0.9120 - jacard_coef: 5.6400e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1525 - accuracy: 0.9157 - jacard_coef: 0.0015     4/17 [======>.......................] - ETA: 2s - loss: 0.1500 - accuracy: 0.9238 - jacard_coef: 0.0025 5/17 [=======>......................] - ETA: 2s - loss: 0.1490 - accuracy: 0.9246 - jacard_coef: 0.0048 6/17 [=========>....................] - ETA: 1s - loss: 0.1480 - accuracy: 0.9206 - jacard_coef: 0.0040 7/17 [===========>..................] - ETA: 1s - loss: 0.1469 - accuracy: 0.9218 - jacard_coef: 0.0035 8/17 [=============>................] - ETA: 1s - loss: 0.1462 - accuracy: 0.9203 - jacard_coef: 0.0030 9/17 [==============>...............] - ETA: 1s - loss: 0.1456 - accuracy: 0.9192 - jacard_coef: 0.002710/17 [================>.............] - ETA: 1s - loss: 0.1456 - accuracy: 0.9146 - jacard_coef: 0.009111/17 [==================>...........] - ETA: 1s - loss: 0.1468 - accuracy: 0.8964 - jacard_coef: 0.015512/17 [====================>.........] - ETA: 0s - loss: 0.1463 - accuracy: 0.8974 - jacard_coef: 0.014613/17 [=====================>........] - ETA: 0s - loss: 0.1461 - accuracy: 0.8972 - jacard_coef: 0.013514/17 [=======================>......] - ETA: 0s - loss: 0.1456 - accuracy: 0.8992 - jacard_coef: 0.013015/17 [=========================>....] - ETA: 0s - loss: 0.1453 - accuracy: 0.9013 - jacard_coef: 0.012216/17 [===========================>..] - ETA: 0s - loss: 0.1455 - accuracy: 0.9012 - jacard_coef: 0.011917/17 [==============================] - 3s 180ms/step - loss: 0.1455 - accuracy: 0.9004 - jacard_coef: 0.0278 - val_loss: 0.1325 - val_accuracy: 0.9184 - val_jacard_coef: 0.0120 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1495 - accuracy: 0.8947 - jacard_coef: 0.0114 2/17 [==>...........................] - ETA: 2s - loss: 0.1478 - accuracy: 0.8829 - jacard_coef: 0.0336 3/17 [====>.........................] - ETA: 2s - loss: 0.1487 - accuracy: 0.8293 - jacard_coef: 0.0505 4/17 [======>.......................] - ETA: 2s - loss: 0.1479 - accuracy: 0.8162 - jacard_coef: 0.0476 5/17 [=======>......................] - ETA: 2s - loss: 0.1477 - accuracy: 0.8339 - jacard_coef: 0.0381 6/17 [=========>....................] - ETA: 1s - loss: 0.1469 - accuracy: 0.8466 - jacard_coef: 0.0318 7/17 [===========>..................] - ETA: 1s - loss: 0.1469 - accuracy: 0.8531 - jacard_coef: 0.0273 8/17 [=============>................] - ETA: 1s - loss: 0.1460 - accuracy: 0.8639 - jacard_coef: 0.0239 9/17 [==============>...............] - ETA: 1s - loss: 0.1448 - accuracy: 0.8752 - jacard_coef: 0.021210/17 [================>.............] - ETA: 1s - loss: 0.1443 - accuracy: 0.8806 - jacard_coef: 0.019111/17 [==================>...........] - ETA: 1s - loss: 0.1444 - accuracy: 0.8835 - jacard_coef: 0.017712/17 [====================>.........] - ETA: 0s - loss: 0.1440 - accuracy: 0.8860 - jacard_coef: 0.016213/17 [=====================>........] - ETA: 0s - loss: 0.1435 - accuracy: 0.8901 - jacard_coef: 0.015014/17 [=======================>......] - ETA: 0s - loss: 0.1446 - accuracy: 0.8671 - jacard_coef: 0.017715/17 [=========================>....] - ETA: 0s - loss: 0.1448 - accuracy: 0.8704 - jacard_coef: 0.016716/17 [===========================>..] - ETA: 0s - loss: 0.1448 - accuracy: 0.8720 - jacard_coef: 0.015717/17 [==============================] - 3s 180ms/step - loss: 0.1450 - accuracy: 0.8715 - jacard_coef: 0.0157 - val_loss: 0.1287 - val_accuracy: 0.9129 - val_jacard_coef: 0.0150 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1394 - accuracy: 0.9313 - jacard_coef: 2.7759e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1405 - accuracy: 0.9289 - jacard_coef: 6.4772e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1408 - accuracy: 0.9204 - jacard_coef: 0.0013     4/17 [======>.......................] - ETA: 2s - loss: 0.1399 - accuracy: 0.9238 - jacard_coef: 0.0024 5/17 [=======>......................] - ETA: 2s - loss: 0.1410 - accuracy: 0.9170 - jacard_coef: 0.0025 6/17 [=========>....................] - ETA: 1s - loss: 0.1415 - accuracy: 0.9123 - jacard_coef: 0.0040 7/17 [===========>..................] - ETA: 1s - loss: 0.1431 - accuracy: 0.9136 - jacard_coef: 0.0122 8/17 [=============>................] - ETA: 1s - loss: 0.1427 - accuracy: 0.9126 - jacard_coef: 0.0125 9/17 [==============>...............] - ETA: 1s - loss: 0.1431 - accuracy: 0.9117 - jacard_coef: 0.018410/17 [================>.............] - ETA: 1s - loss: 0.1428 - accuracy: 0.9103 - jacard_coef: 0.016611/17 [==================>...........] - ETA: 1s - loss: 0.1424 - accuracy: 0.9109 - jacard_coef: 0.015112/17 [====================>.........] - ETA: 0s - loss: 0.1422 - accuracy: 0.9124 - jacard_coef: 0.014013/17 [=====================>........] - ETA: 0s - loss: 0.1420 - accuracy: 0.9112 - jacard_coef: 0.013614/17 [=======================>......] - ETA: 0s - loss: 0.1419 - accuracy: 0.9101 - jacard_coef: 0.013615/17 [=========================>....] - ETA: 0s - loss: 0.1419 - accuracy: 0.9087 - jacard_coef: 0.013416/17 [===========================>..] - ETA: 0s - loss: 0.1415 - accuracy: 0.9091 - jacard_coef: 0.012917/17 [==============================] - 3s 180ms/step - loss: 0.1416 - accuracy: 0.9076 - jacard_coef: 0.0122 - val_loss: 0.1524 - val_accuracy: 0.8708 - val_jacard_coef: 0.0307 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1378 - accuracy: 0.9190 - jacard_coef: 2.3552e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1392 - accuracy: 0.9134 - jacard_coef: 5.1741e-05 3/17 [====>.........................] - ETA: 2s - loss: 0.1421 - accuracy: 0.9033 - jacard_coef: 3.4494e-05 4/17 [======>.......................] - ETA: 2s - loss: 0.1415 - accuracy: 0.9013 - jacard_coef: 2.5870e-05 5/17 [=======>......................] - ETA: 2s - loss: 0.1412 - accuracy: 0.9037 - jacard_coef: 2.0696e-05 6/17 [=========>....................] - ETA: 1s - loss: 0.1406 - accuracy: 0.9082 - jacard_coef: 1.7247e-05 7/17 [===========>..................] - ETA: 1s - loss: 0.1404 - accuracy: 0.9107 - jacard_coef: 1.4783e-05 8/17 [=============>................] - ETA: 1s - loss: 0.1402 - accuracy: 0.9138 - jacard_coef: 1.2935e-05 9/17 [==============>...............] - ETA: 1s - loss: 0.1401 - accuracy: 0.9150 - jacard_coef: 1.1498e-0510/17 [================>.............] - ETA: 1s - loss: 0.1400 - accuracy: 0.9152 - jacard_coef: 1.0348e-0511/17 [==================>...........] - ETA: 1s - loss: 0.1402 - accuracy: 0.9129 - jacard_coef: 9.4074e-0612/17 [====================>.........] - ETA: 0s - loss: 0.1405 - accuracy: 0.9087 - jacard_coef: 0.0018    13/17 [=====================>........] - ETA: 0s - loss: 0.1402 - accuracy: 0.9093 - jacard_coef: 0.001614/17 [=======================>......] - ETA: 0s - loss: 0.1397 - accuracy: 0.9117 - jacard_coef: 0.001515/17 [=========================>....] - ETA: 0s - loss: 0.1394 - accuracy: 0.9136 - jacard_coef: 0.001416/17 [===========================>..] - ETA: 0s - loss: 0.1391 - accuracy: 0.9154 - jacard_coef: 0.001317/17 [==============================] - 3s 180ms/step - loss: 0.1391 - accuracy: 0.9158 - jacard_coef: 0.0036 - val_loss: 0.1964 - val_accuracy: 0.0840 - val_jacard_coef: 0.0686 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1333 - accuracy: 0.9412 - jacard_coef: 3.2464e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1389 - accuracy: 0.9235 - jacard_coef: 0.0174     3/17 [====>.........................] - ETA: 2s - loss: 0.1394 - accuracy: 0.9191 - jacard_coef: 0.0131 4/17 [======>.......................] - ETA: 2s - loss: 0.1386 - accuracy: 0.9193 - jacard_coef: 0.0102 5/17 [=======>......................] - ETA: 2s - loss: 0.1378 - accuracy: 0.9270 - jacard_coef: 0.0179 6/17 [=========>....................] - ETA: 1s - loss: 0.1383 - accuracy: 0.9204 - jacard_coef: 0.0162 7/17 [===========>..................] - ETA: 1s - loss: 0.1390 - accuracy: 0.9158 - jacard_coef: 0.0149 8/17 [=============>................] - ETA: 1s - loss: 0.1391 - accuracy: 0.9109 - jacard_coef: 0.0137 9/17 [==============>...............] - ETA: 1s - loss: 0.1388 - accuracy: 0.9108 - jacard_coef: 0.012710/17 [================>.............] - ETA: 1s - loss: 0.1388 - accuracy: 0.9102 - jacard_coef: 0.012411/17 [==================>...........] - ETA: 1s - loss: 0.1380 - accuracy: 0.9149 - jacard_coef: 0.011312/17 [====================>.........] - ETA: 0s - loss: 0.1385 - accuracy: 0.9097 - jacard_coef: 0.012113/17 [=====================>........] - ETA: 0s - loss: 0.1379 - accuracy: 0.9130 - jacard_coef: 0.011214/17 [=======================>......] - ETA: 0s - loss: 0.1379 - accuracy: 0.9119 - jacard_coef: 0.010415/17 [=========================>....] - ETA: 0s - loss: 0.1379 - accuracy: 0.9129 - jacard_coef: 0.009716/17 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9112 - jacard_coef: 0.009117/17 [==============================] - 3s 180ms/step - loss: 0.1381 - accuracy: 0.9107 - jacard_coef: 0.0113 - val_loss: 0.1814 - val_accuracy: 0.0840 - val_jacard_coef: 0.0686 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0731 (epoch 5)
  Final Val Loss: 0.1814
  Training Time: 0:01:42.536695
  Stability (std): 0.0227

Results saved to: hyperparameter_optimization_20250926_165036/exp_19_Attention_UNet_lr1e-3_bs8/Attention_UNet_lr0.001_bs8_results.json

Experiment 19 completed in 119s
Progress: 19/36 completed
Estimated remaining time: 33 minutes

ðŸ”¬ EXPERIMENT 20/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878940.779686 1116760 service.cc:145] XLA service 0x153998bde140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878940.779709 1116760 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878940.915554 1116760 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 6:43 - loss: 0.3393 - accuracy: 0.5022 - jacard_coef: 0.07462/9 [=====>........................] - ETA: 58s - loss: 0.2913 - accuracy: 0.4719 - jacard_coef: 0.0890 3/9 [=========>....................] - ETA: 26s - loss: 0.2623 - accuracy: 0.4197 - jacard_coef: 0.08304/9 [============>.................] - ETA: 15s - loss: 0.2462 - accuracy: 0.3801 - jacard_coef: 0.07815/9 [===============>..............] - ETA: 9s - loss: 0.2360 - accuracy: 0.3794 - jacard_coef: 0.0820 6/9 [===================>..........] - ETA: 5s - loss: 0.2286 - accuracy: 0.3751 - jacard_coef: 0.07747/9 [======================>.......] - ETA: 3s - loss: 0.2234 - accuracy: 0.3578 - jacard_coef: 0.07948/9 [=========================>....] - ETA: 1s - loss: 0.2192 - accuracy: 0.3374 - jacard_coef: 0.08349/9 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.3361 - jacard_coef: 0.08069/9 [==============================] - 69s 2s/step - loss: 0.2189 - accuracy: 0.3361 - jacard_coef: 0.0806 - val_loss: 0.0811 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1863 - accuracy: 0.2213 - jacard_coef: 0.07492/9 [=====>........................] - ETA: 2s - loss: 0.1880 - accuracy: 0.2381 - jacard_coef: 0.08583/9 [=========>....................] - ETA: 2s - loss: 0.1883 - accuracy: 0.2236 - jacard_coef: 0.08744/9 [============>.................] - ETA: 1s - loss: 0.1876 - accuracy: 0.2096 - jacard_coef: 0.09065/9 [===============>..............] - ETA: 1s - loss: 0.1866 - accuracy: 0.2194 - jacard_coef: 0.08826/9 [===================>..........] - ETA: 1s - loss: 0.1861 - accuracy: 0.2310 - jacard_coef: 0.08927/9 [======================>.......] - ETA: 0s - loss: 0.1853 - accuracy: 0.2444 - jacard_coef: 0.08618/9 [=========================>....] - ETA: 0s - loss: 0.1845 - accuracy: 0.2666 - jacard_coef: 0.08139/9 [==============================] - 3s 332ms/step - loss: 0.1845 - accuracy: 0.2670 - jacard_coef: 0.0949 - val_loss: 0.6459 - val_accuracy: 0.9280 - val_jacard_coef: 0.0025 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1802 - accuracy: 0.3870 - jacard_coef: 0.07612/9 [=====>........................] - ETA: 2s - loss: 0.1796 - accuracy: 0.4394 - jacard_coef: 0.07373/9 [=========>....................] - ETA: 2s - loss: 0.1778 - accuracy: 0.4837 - jacard_coef: 0.08634/9 [============>.................] - ETA: 1s - loss: 0.1923 - accuracy: 0.4486 - jacard_coef: 0.08385/9 [===============>..............] - ETA: 1s - loss: 0.1895 - accuracy: 0.4379 - jacard_coef: 0.08556/9 [===================>..........] - ETA: 1s - loss: 0.1879 - accuracy: 0.4184 - jacard_coef: 0.08307/9 [======================>.......] - ETA: 0s - loss: 0.1872 - accuracy: 0.3992 - jacard_coef: 0.08458/9 [=========================>....] - ETA: 0s - loss: 0.1864 - accuracy: 0.3814 - jacard_coef: 0.08319/9 [==============================] - 3s 325ms/step - loss: 0.1863 - accuracy: 0.3807 - jacard_coef: 0.0872 - val_loss: 1.1129 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1807 - accuracy: 0.2768 - jacard_coef: 0.10732/9 [=====>........................] - ETA: 2s - loss: 0.1804 - accuracy: 0.2851 - jacard_coef: 0.08873/9 [=========>....................] - ETA: 2s - loss: 0.1800 - accuracy: 0.3035 - jacard_coef: 0.07934/9 [============>.................] - ETA: 1s - loss: 0.1797 - accuracy: 0.3166 - jacard_coef: 0.07765/9 [===============>..............] - ETA: 1s - loss: 0.1790 - accuracy: 0.3395 - jacard_coef: 0.07906/9 [===================>..........] - ETA: 1s - loss: 0.1784 - accuracy: 0.3594 - jacard_coef: 0.07927/9 [======================>.......] - ETA: 0s - loss: 0.1781 - accuracy: 0.3786 - jacard_coef: 0.07958/9 [=========================>....] - ETA: 0s - loss: 0.1779 - accuracy: 0.3947 - jacard_coef: 0.07959/9 [==============================] - 3s 326ms/step - loss: 0.1779 - accuracy: 0.3962 - jacard_coef: 0.0897 - val_loss: 1.1180 - val_accuracy: 0.9304 - val_jacard_coef: 1.4614e-12 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1715 - accuracy: 0.6607 - jacard_coef: 0.08832/9 [=====>........................] - ETA: 2s - loss: 0.1706 - accuracy: 0.6776 - jacard_coef: 0.07823/9 [=========>....................] - ETA: 2s - loss: 0.1717 - accuracy: 0.6131 - jacard_coef: 0.08054/9 [============>.................] - ETA: 1s - loss: 0.1719 - accuracy: 0.6385 - jacard_coef: 0.07235/9 [===============>..............] - ETA: 1s - loss: 0.1727 - accuracy: 0.6449 - jacard_coef: 0.07306/9 [===================>..........] - ETA: 1s - loss: 0.1729 - accuracy: 0.6465 - jacard_coef: 0.07267/9 [======================>.......] - ETA: 0s - loss: 0.1732 - accuracy: 0.6434 - jacard_coef: 0.07048/9 [=========================>....] - ETA: 0s - loss: 0.1732 - accuracy: 0.6431 - jacard_coef: 0.07209/9 [==============================] - 3s 326ms/step - loss: 0.1732 - accuracy: 0.6420 - jacard_coef: 0.0820 - val_loss: 1.1190 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1718 - accuracy: 0.6384 - jacard_coef: 0.09822/9 [=====>........................] - ETA: 2s - loss: 0.1727 - accuracy: 0.6160 - jacard_coef: 0.08033/9 [=========>....................] - ETA: 2s - loss: 0.1724 - accuracy: 0.6219 - jacard_coef: 0.08484/9 [============>.................] - ETA: 1s - loss: 0.1721 - accuracy: 0.6302 - jacard_coef: 0.08115/9 [===============>..............] - ETA: 1s - loss: 0.1724 - accuracy: 0.6296 - jacard_coef: 0.07896/9 [===================>..........] - ETA: 1s - loss: 0.1724 - accuracy: 0.6359 - jacard_coef: 0.08057/9 [======================>.......] - ETA: 0s - loss: 0.1726 - accuracy: 0.6429 - jacard_coef: 0.07958/9 [=========================>....] - ETA: 0s - loss: 0.1727 - accuracy: 0.6521 - jacard_coef: 0.07959/9 [==============================] - 3s 326ms/step - loss: 0.1727 - accuracy: 0.6532 - jacard_coef: 0.0763 - val_loss: 1.1038 - val_accuracy: 0.9304 - val_jacard_coef: 1.4607e-12 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1706 - accuracy: 0.7962 - jacard_coef: 0.07422/9 [=====>........................] - ETA: 2s - loss: 0.1706 - accuracy: 0.7910 - jacard_coef: 0.07173/9 [=========>....................] - ETA: 2s - loss: 0.1702 - accuracy: 0.7934 - jacard_coef: 0.06544/9 [============>.................] - ETA: 1s - loss: 0.1698 - accuracy: 0.7904 - jacard_coef: 0.06715/9 [===============>..............] - ETA: 1s - loss: 0.1693 - accuracy: 0.7983 - jacard_coef: 0.06236/9 [===================>..........] - ETA: 1s - loss: 0.1689 - accuracy: 0.8067 - jacard_coef: 0.05907/9 [======================>.......] - ETA: 0s - loss: 0.1686 - accuracy: 0.8133 - jacard_coef: 0.06108/9 [=========================>....] - ETA: 0s - loss: 0.1682 - accuracy: 0.8191 - jacard_coef: 0.06299/9 [==============================] - 3s 333ms/step - loss: 0.1681 - accuracy: 0.8196 - jacard_coef: 0.0560 - val_loss: 0.4530 - val_accuracy: 0.9183 - val_jacard_coef: 0.0042 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1614 - accuracy: 0.8034 - jacard_coef: 0.03902/9 [=====>........................] - ETA: 2s - loss: 0.1623 - accuracy: 0.7580 - jacard_coef: 0.06733/9 [=========>....................] - ETA: 2s - loss: 0.1619 - accuracy: 0.7992 - jacard_coef: 0.05504/9 [============>.................] - ETA: 1s - loss: 0.1615 - accuracy: 0.8241 - jacard_coef: 0.04765/9 [===============>..............] - ETA: 1s - loss: 0.1614 - accuracy: 0.8304 - jacard_coef: 0.04656/9 [===================>..........] - ETA: 1s - loss: 0.1610 - accuracy: 0.8415 - jacard_coef: 0.04487/9 [======================>.......] - ETA: 0s - loss: 0.1609 - accuracy: 0.8431 - jacard_coef: 0.04478/9 [=========================>....] - ETA: 0s - loss: 0.1607 - accuracy: 0.8366 - jacard_coef: 0.04759/9 [==============================] - 3s 331ms/step - loss: 0.1617 - accuracy: 0.8339 - jacard_coef: 0.0426 - val_loss: 2.7808 - val_accuracy: 0.2186 - val_jacard_coef: 0.0690 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.2068 - accuracy: 0.3581 - jacard_coef: 0.06522/9 [=====>........................] - ETA: 2s - loss: 0.1850 - accuracy: 0.6214 - jacard_coef: 0.04093/9 [=========>....................] - ETA: 2s - loss: 0.1782 - accuracy: 0.7111 - jacard_coef: 0.02984/9 [============>.................] - ETA: 1s - loss: 0.1754 - accuracy: 0.7352 - jacard_coef: 0.03485/9 [===============>..............] - ETA: 1s - loss: 0.1737 - accuracy: 0.7333 - jacard_coef: 0.04066/9 [===================>..........] - ETA: 1s - loss: 0.1727 - accuracy: 0.7326 - jacard_coef: 0.04707/9 [======================>.......] - ETA: 0s - loss: 0.1721 - accuracy: 0.7268 - jacard_coef: 0.05178/9 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.7252 - jacard_coef: 0.05359/9 [==============================] - 3s 327ms/step - loss: 0.1713 - accuracy: 0.7242 - jacard_coef: 0.0491 - val_loss: 0.8847 - val_accuracy: 0.9297 - val_jacard_coef: 2.4579e-04 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.7104 - jacard_coef: 0.06682/9 [=====>........................] - ETA: 2s - loss: 0.1671 - accuracy: 0.7249 - jacard_coef: 0.06173/9 [=========>....................] - ETA: 2s - loss: 0.1669 - accuracy: 0.7424 - jacard_coef: 0.06234/9 [============>.................] - ETA: 1s - loss: 0.1670 - accuracy: 0.7683 - jacard_coef: 0.05865/9 [===============>..............] - ETA: 1s - loss: 0.1671 - accuracy: 0.7808 - jacard_coef: 0.05436/9 [===================>..........] - ETA: 1s - loss: 0.1668 - accuracy: 0.7918 - jacard_coef: 0.05207/9 [======================>.......] - ETA: 0s - loss: 0.1665 - accuracy: 0.7998 - jacard_coef: 0.05248/9 [=========================>....] - ETA: 0s - loss: 0.1666 - accuracy: 0.8003 - jacard_coef: 0.05489/9 [==============================] - 3s 327ms/step - loss: 0.1665 - accuracy: 0.8004 - jacard_coef: 0.0552 - val_loss: 0.8887 - val_accuracy: 0.9256 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1654 - accuracy: 0.7939 - jacard_coef: 0.07072/9 [=====>........................] - ETA: 2s - loss: 0.1643 - accuracy: 0.8004 - jacard_coef: 0.09713/9 [=========>....................] - ETA: 2s - loss: 0.1633 - accuracy: 0.8106 - jacard_coef: 0.08434/9 [============>.................] - ETA: 1s - loss: 0.1633 - accuracy: 0.8134 - jacard_coef: 0.07475/9 [===============>..............] - ETA: 1s - loss: 0.1634 - accuracy: 0.8084 - jacard_coef: 0.07406/9 [===================>..........] - ETA: 1s - loss: 0.1630 - accuracy: 0.8124 - jacard_coef: 0.06937/9 [======================>.......] - ETA: 0s - loss: 0.1630 - accuracy: 0.8117 - jacard_coef: 0.06698/9 [=========================>....] - ETA: 0s - loss: 0.1628 - accuracy: 0.8176 - jacard_coef: 0.06519/9 [==============================] - 3s 327ms/step - loss: 0.1627 - accuracy: 0.8181 - jacard_coef: 0.0643 - val_loss: 0.8571 - val_accuracy: 0.9245 - val_jacard_coef: 0.0028 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1630 - accuracy: 0.8802 - jacard_coef: 0.04572/9 [=====>........................] - ETA: 2s - loss: 0.1621 - accuracy: 0.8845 - jacard_coef: 0.03773/9 [=========>....................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8943 - jacard_coef: 0.02624/9 [============>.................] - ETA: 1s - loss: 0.1608 - accuracy: 0.8941 - jacard_coef: 0.02035/9 [===============>..............] - ETA: 1s - loss: 0.1601 - accuracy: 0.9002 - jacard_coef: 0.01666/9 [===================>..........] - ETA: 1s - loss: 0.1598 - accuracy: 0.9042 - jacard_coef: 0.01437/9 [======================>.......] - ETA: 0s - loss: 0.1596 - accuracy: 0.9056 - jacard_coef: 0.01248/9 [=========================>....] - ETA: 0s - loss: 0.1594 - accuracy: 0.9074 - jacard_coef: 0.01109/9 [==============================] - 3s 327ms/step - loss: 0.1593 - accuracy: 0.9077 - jacard_coef: 0.0123 - val_loss: 0.6658 - val_accuracy: 0.9252 - val_jacard_coef: 0.0033 - lr: 0.0010
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1601 - accuracy: 0.8827 - jacard_coef: 0.00282/9 [=====>........................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8963 - jacard_coef: 0.00203/9 [=========>....................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8922 - jacard_coef: 0.00194/9 [============>.................] - ETA: 1s - loss: 0.1577 - accuracy: 0.9037 - jacard_coef: 0.00165/9 [===============>..............] - ETA: 1s - loss: 0.1572 - accuracy: 0.9106 - jacard_coef: 0.00166/9 [===================>..........] - ETA: 1s - loss: 0.1568 - accuracy: 0.9135 - jacard_coef: 0.00157/9 [======================>.......] - ETA: 0s - loss: 0.1563 - accuracy: 0.9148 - jacard_coef: 0.00168/9 [=========================>....] - ETA: 0s - loss: 0.1560 - accuracy: 0.9166 - jacard_coef: 0.00169/9 [==============================] - 3s 333ms/step - loss: 0.1559 - accuracy: 0.9163 - jacard_coef: 0.0024 - val_loss: 1.0142 - val_accuracy: 0.2446 - val_jacard_coef: 0.0760 - lr: 0.0010
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1533 - accuracy: 0.8997 - jacard_coef: 0.00232/9 [=====>........................] - ETA: 2s - loss: 0.1530 - accuracy: 0.9020 - jacard_coef: 0.00483/9 [=========>....................] - ETA: 2s - loss: 0.1523 - accuracy: 0.9065 - jacard_coef: 0.00634/9 [============>.................] - ETA: 1s - loss: 0.1517 - accuracy: 0.9039 - jacard_coef: 0.01365/9 [===============>..............] - ETA: 1s - loss: 0.1518 - accuracy: 0.8962 - jacard_coef: 0.01946/9 [===================>..........] - ETA: 1s - loss: 0.1514 - accuracy: 0.8936 - jacard_coef: 0.02377/9 [======================>.......] - ETA: 0s - loss: 0.1510 - accuracy: 0.8908 - jacard_coef: 0.02788/9 [=========================>....] - ETA: 0s - loss: 0.1510 - accuracy: 0.8891 - jacard_coef: 0.02649/9 [==============================] - 3s 327ms/step - loss: 0.1510 - accuracy: 0.8879 - jacard_coef: 0.0303 - val_loss: 0.4141 - val_accuracy: 0.1786 - val_jacard_coef: 0.0714 - lr: 0.0010
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1517 - accuracy: 0.8830 - jacard_coef: 0.00642/9 [=====>........................] - ETA: 2s - loss: 0.1528 - accuracy: 0.8757 - jacard_coef: 0.02093/9 [=========>....................] - ETA: 2s - loss: 0.1521 - accuracy: 0.8827 - jacard_coef: 0.02174/9 [============>.................] - ETA: 1s - loss: 0.1519 - accuracy: 0.8847 - jacard_coef: 0.02455/9 [===============>..............] - ETA: 1s - loss: 0.1521 - accuracy: 0.8844 - jacard_coef: 0.02626/9 [===================>..........] - ETA: 1s - loss: 0.1525 - accuracy: 0.8862 - jacard_coef: 0.02267/9 [======================>.......] - ETA: 0s - loss: 0.1523 - accuracy: 0.8917 - jacard_coef: 0.01948/9 [=========================>....] - ETA: 0s - loss: 0.1523 - accuracy: 0.8942 - jacard_coef: 0.01709/9 [==============================] - 3s 327ms/step - loss: 0.1523 - accuracy: 0.8942 - jacard_coef: 0.0151 - val_loss: 0.0865 - val_accuracy: 0.9304 - val_jacard_coef: 1.4610e-12 - lr: 0.0010
Epoch 16/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1522 - accuracy: 0.9198 - jacard_coef: 1.1889e-042/9 [=====>........................] - ETA: 2s - loss: 0.1531 - accuracy: 0.9134 - jacard_coef: 1.3128e-043/9 [=========>....................] - ETA: 2s - loss: 0.1530 - accuracy: 0.9129 - jacard_coef: 1.3071e-044/9 [============>.................] - ETA: 1s - loss: 0.1518 - accuracy: 0.9233 - jacard_coef: 1.1901e-045/9 [===============>..............] - ETA: 1s - loss: 0.1513 - accuracy: 0.9230 - jacard_coef: 1.2687e-046/9 [===================>..........] - ETA: 1s - loss: 0.1511 - accuracy: 0.9215 - jacard_coef: 1.9466e-047/9 [======================>.......] - ETA: 0s - loss: 0.1510 - accuracy: 0.9181 - jacard_coef: 4.0263e-048/9 [=========================>....] - ETA: 0s - loss: 0.1508 - accuracy: 0.9169 - jacard_coef: 5.2707e-049/9 [==============================] - 3s 327ms/step - loss: 0.1508 - accuracy: 0.9170 - jacard_coef: 4.6850e-04 - val_loss: 0.1355 - val_accuracy: 0.9303 - val_jacard_coef: 8.7623e-05 - lr: 0.0010
Epoch 17/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1483 - accuracy: 0.9192 - jacard_coef: 0.00322/9 [=====>........................] - ETA: 2s - loss: 0.1489 - accuracy: 0.9137 - jacard_coef: 0.00253/9 [=========>....................] - ETA: 2s - loss: 0.1491 - accuracy: 0.9040 - jacard_coef: 0.00204/9 [============>.................] - ETA: 1s - loss: 0.1477 - accuracy: 0.9150 - jacard_coef: 0.00175/9 [===============>..............] - ETA: 1s - loss: 0.1471 - accuracy: 0.9185 - jacard_coef: 0.00146/9 [===================>..........] - ETA: 1s - loss: 0.1470 - accuracy: 0.9162 - jacard_coef: 0.00127/9 [======================>.......] - ETA: 0s - loss: 0.1469 - accuracy: 0.9155 - jacard_coef: 0.00118/9 [=========================>....] - ETA: 0s - loss: 0.1465 - accuracy: 0.9176 - jacard_coef: 0.00109/9 [==============================] - 3s 327ms/step - loss: 0.1466 - accuracy: 0.9161 - jacard_coef: 0.0073 - val_loss: 0.1416 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 18/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9192 - jacard_coef: 5.3104e-042/9 [=====>........................] - ETA: 2s - loss: 0.1441 - accuracy: 0.9155 - jacard_coef: 2.7092e-043/9 [=========>....................] - ETA: 2s - loss: 0.1451 - accuracy: 0.9038 - jacard_coef: 3.4292e-044/9 [============>.................] - ETA: 1s - loss: 0.1446 - accuracy: 0.9109 - jacard_coef: 5.5443e-045/9 [===============>..............] - ETA: 1s - loss: 0.1444 - accuracy: 0.9103 - jacard_coef: 4.7666e-046/9 [===================>..........] - ETA: 1s - loss: 0.1441 - accuracy: 0.9125 - jacard_coef: 4.4718e-047/9 [======================>.......] - ETA: 0s - loss: 0.1438 - accuracy: 0.9156 - jacard_coef: 4.1237e-048/9 [=========================>....] - ETA: 0s - loss: 0.1435 - accuracy: 0.9167 - jacard_coef: 5.8323e-049/9 [==============================] - 3s 327ms/step - loss: 0.1435 - accuracy: 0.9170 - jacard_coef: 5.1842e-04 - val_loss: 0.1728 - val_accuracy: 0.3158 - val_jacard_coef: 0.0747 - lr: 0.0010
Epoch 19/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1439 - accuracy: 0.8932 - jacard_coef: 0.00702/9 [=====>........................] - ETA: 2s - loss: 0.1427 - accuracy: 0.9063 - jacard_coef: 0.00603/9 [=========>....................] - ETA: 2s - loss: 0.1418 - accuracy: 0.9147 - jacard_coef: 0.00404/9 [============>.................] - ETA: 1s - loss: 0.1424 - accuracy: 0.9083 - jacard_coef: 0.00315/9 [===============>..............] - ETA: 1s - loss: 0.1419 - accuracy: 0.9144 - jacard_coef: 0.00256/9 [===================>..........] - ETA: 1s - loss: 0.1422 - accuracy: 0.9097 - jacard_coef: 0.00347/9 [======================>.......] - ETA: 0s - loss: 0.1420 - accuracy: 0.9114 - jacard_coef: 0.00298/9 [=========================>....] - ETA: 0s - loss: 0.1417 - accuracy: 0.9135 - jacard_coef: 0.00259/9 [==============================] - 3s 325ms/step - loss: 0.1422 - accuracy: 0.9097 - jacard_coef: 0.0081 - val_loss: 0.1620 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1435 - accuracy: 0.8845 - jacard_coef: 0.02202/9 [=====>........................] - ETA: 2s - loss: 0.1438 - accuracy: 0.8887 - jacard_coef: 0.01493/9 [=========>....................] - ETA: 2s - loss: 0.1451 - accuracy: 0.8786 - jacard_coef: 0.01364/9 [============>.................] - ETA: 1s - loss: 0.1445 - accuracy: 0.8891 - jacard_coef: 0.01165/9 [===============>..............] - ETA: 1s - loss: 0.1446 - accuracy: 0.8923 - jacard_coef: 0.01326/9 [===================>..........] - ETA: 1s - loss: 0.1444 - accuracy: 0.8968 - jacard_coef: 0.01427/9 [======================>.......] - ETA: 0s - loss: 0.1443 - accuracy: 0.9005 - jacard_coef: 0.01268/9 [=========================>....] - ETA: 0s - loss: 0.1442 - accuracy: 0.9033 - jacard_coef: 0.01149/9 [==============================] - 3s 326ms/step - loss: 0.1442 - accuracy: 0.9036 - jacard_coef: 0.0101 - val_loss: 0.1429 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1428 - accuracy: 0.9365 - jacard_coef: 4.2031e-042/9 [=====>........................] - ETA: 2s - loss: 0.1426 - accuracy: 0.9361 - jacard_coef: 3.0651e-043/9 [=========>....................] - ETA: 2s - loss: 0.1446 - accuracy: 0.9203 - jacard_coef: 5.3559e-044/9 [============>.................] - ETA: 1s - loss: 0.1449 - accuracy: 0.9186 - jacard_coef: 9.6583e-045/9 [===============>..............] - ETA: 1s - loss: 0.1454 - accuracy: 0.9148 - jacard_coef: 9.8328e-046/9 [===================>..........] - ETA: 1s - loss: 0.1452 - accuracy: 0.9161 - jacard_coef: 0.0016    7/9 [======================>.......] - ETA: 0s - loss: 0.1454 - accuracy: 0.9139 - jacard_coef: 0.00158/9 [=========================>....] - ETA: 0s - loss: 0.1451 - accuracy: 0.9158 - jacard_coef: 0.00149/9 [==============================] - 3s 327ms/step - loss: 0.1451 - accuracy: 0.9164 - jacard_coef: 0.0012 - val_loss: 0.1399 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1443 - accuracy: 0.9207 - jacard_coef: 0.00232/9 [=====>........................] - ETA: 2s - loss: 0.1435 - accuracy: 0.9210 - jacard_coef: 0.00123/9 [=========>....................] - ETA: 2s - loss: 0.1451 - accuracy: 0.9117 - jacard_coef: 8.6431e-044/9 [============>.................] - ETA: 1s - loss: 0.1449 - accuracy: 0.9129 - jacard_coef: 6.8545e-045/9 [===============>..............] - ETA: 1s - loss: 0.1449 - accuracy: 0.9146 - jacard_coef: 6.3049e-046/9 [===================>..........] - ETA: 1s - loss: 0.1448 - accuracy: 0.9147 - jacard_coef: 6.0805e-047/9 [======================>.......] - ETA: 0s - loss: 0.1445 - accuracy: 0.9169 - jacard_coef: 6.3006e-048/9 [=========================>....] - ETA: 0s - loss: 0.1443 - accuracy: 0.9174 - jacard_coef: 5.8291e-049/9 [==============================] - 3s 326ms/step - loss: 0.1443 - accuracy: 0.9168 - jacard_coef: 5.5166e-04 - val_loss: 0.1393 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 23/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1467 - accuracy: 0.9007 - jacard_coef: 0.00262/9 [=====>........................] - ETA: 2s - loss: 0.1457 - accuracy: 0.9013 - jacard_coef: 0.00263/9 [=========>....................] - ETA: 2s - loss: 0.1455 - accuracy: 0.8982 - jacard_coef: 0.00284/9 [============>.................] - ETA: 1s - loss: 0.1443 - accuracy: 0.9087 - jacard_coef: 0.00255/9 [===============>..............] - ETA: 1s - loss: 0.1439 - accuracy: 0.9142 - jacard_coef: 0.00316/9 [===================>..........] - ETA: 1s - loss: 0.1440 - accuracy: 0.9147 - jacard_coef: 0.00327/9 [======================>.......] - ETA: 0s - loss: 0.1438 - accuracy: 0.9145 - jacard_coef: 0.00288/9 [=========================>....] - ETA: 0s - loss: 0.1434 - accuracy: 0.9164 - jacard_coef: 0.00269/9 [==============================] - 3s 327ms/step - loss: 0.1434 - accuracy: 0.9158 - jacard_coef: 0.0026 - val_loss: 0.1445 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0760 (epoch 13)
  Final Val Loss: 0.1445
  Training Time: 0:02:14.852494
  Stability (std): 0.0847

Results saved to: hyperparameter_optimization_20250926_165036/exp_20_Attention_UNet_lr1e-3_bs16/Attention_UNet_lr0.001_bs16_results.json

Experiment 20 completed in 150s
Progress: 20/36 completed
Estimated remaining time: 40 minutes

ðŸ”¬ EXPERIMENT 21/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879096.360586 1123403 service.cc:145] XLA service 0x14c959becc20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879096.360609 1123403 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879096.499540 1123403 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:01 - loss: 0.3392 - accuracy: 0.5464 - jacard_coef: 0.06952/5 [===========>..................] - ETA: 46s - loss: 0.2834 - accuracy: 0.5771 - jacard_coef: 0.0732 3/5 [=================>............] - ETA: 16s - loss: 0.2606 - accuracy: 0.5898 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 5s - loss: 0.2485 - accuracy: 0.6316 - jacard_coef: 0.0727 5/5 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.6328 - jacard_coef: 0.06265/5 [==============================] - 86s 6s/step - loss: 0.2482 - accuracy: 0.6328 - jacard_coef: 0.0626 - val_loss: 0.1687 - val_accuracy: 0.9159 - val_jacard_coef: 0.0163 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1877 - accuracy: 0.7205 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 2s - loss: 0.1876 - accuracy: 0.7323 - jacard_coef: 0.07013/5 [=================>............] - ETA: 1s - loss: 0.1841 - accuracy: 0.7193 - jacard_coef: 0.07304/5 [=======================>......] - ETA: 0s - loss: 0.1830 - accuracy: 0.7059 - jacard_coef: 0.07515/5 [==============================] - 3s 571ms/step - loss: 0.1837 - accuracy: 0.7052 - jacard_coef: 0.0702 - val_loss: 0.2438 - val_accuracy: 0.0845 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1795 - accuracy: 0.5219 - jacard_coef: 0.07322/5 [===========>..................] - ETA: 2s - loss: 0.1797 - accuracy: 0.5666 - jacard_coef: 0.06943/5 [=================>............] - ETA: 1s - loss: 0.1805 - accuracy: 0.6213 - jacard_coef: 0.06984/5 [=======================>......] - ETA: 0s - loss: 0.1798 - accuracy: 0.6316 - jacard_coef: 0.07275/5 [==============================] - 3s 558ms/step - loss: 0.1798 - accuracy: 0.6312 - jacard_coef: 0.0643 - val_loss: 0.0943 - val_accuracy: 0.8913 - val_jacard_coef: 0.0135 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1713 - accuracy: 0.6562 - jacard_coef: 0.07952/5 [===========>..................] - ETA: 2s - loss: 0.1708 - accuracy: 0.6928 - jacard_coef: 0.06163/5 [=================>............] - ETA: 1s - loss: 0.1705 - accuracy: 0.7012 - jacard_coef: 0.06754/5 [=======================>......] - ETA: 0s - loss: 0.1704 - accuracy: 0.7052 - jacard_coef: 0.06715/5 [==============================] - 3s 558ms/step - loss: 0.1704 - accuracy: 0.7052 - jacard_coef: 0.0841 - val_loss: 4.7568 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1687 - accuracy: 0.7246 - jacard_coef: 0.06892/5 [===========>..................] - ETA: 2s - loss: 0.1684 - accuracy: 0.7353 - jacard_coef: 0.07713/5 [=================>............] - ETA: 1s - loss: 0.1682 - accuracy: 0.7407 - jacard_coef: 0.07094/5 [=======================>......] - ETA: 0s - loss: 0.1681 - accuracy: 0.7457 - jacard_coef: 0.07475/5 [==============================] - 3s 558ms/step - loss: 0.1681 - accuracy: 0.7466 - jacard_coef: 0.0598 - val_loss: 0.4211 - val_accuracy: 0.9061 - val_jacard_coef: 0.0144 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1670 - accuracy: 0.8538 - jacard_coef: 0.04852/5 [===========>..................] - ETA: 2s - loss: 0.1670 - accuracy: 0.8569 - jacard_coef: 0.04723/5 [=================>............] - ETA: 1s - loss: 0.1669 - accuracy: 0.8603 - jacard_coef: 0.04334/5 [=======================>......] - ETA: 0s - loss: 0.1667 - accuracy: 0.8685 - jacard_coef: 0.04265/5 [==============================] - 3s 570ms/step - loss: 0.1667 - accuracy: 0.8685 - jacard_coef: 0.0497 - val_loss: 2.4437 - val_accuracy: 0.0740 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1655 - accuracy: 0.8603 - jacard_coef: 0.06902/5 [===========>..................] - ETA: 2s - loss: 0.1659 - accuracy: 0.8244 - jacard_coef: 0.05293/5 [=================>............] - ETA: 1s - loss: 0.1657 - accuracy: 0.8143 - jacard_coef: 0.05514/5 [=======================>......] - ETA: 0s - loss: 0.1655 - accuracy: 0.8116 - jacard_coef: 0.05935/5 [==============================] - 3s 558ms/step - loss: 0.1655 - accuracy: 0.8119 - jacard_coef: 0.1084 - val_loss: 13.4941 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1645 - accuracy: 0.7821 - jacard_coef: 0.07122/5 [===========>..................] - ETA: 2s - loss: 0.1648 - accuracy: 0.7724 - jacard_coef: 0.06393/5 [=================>............] - ETA: 1s - loss: 0.1647 - accuracy: 0.7722 - jacard_coef: 0.06864/5 [=======================>......] - ETA: 0s - loss: 0.1647 - accuracy: 0.7712 - jacard_coef: 0.06805/5 [==============================] - 3s 558ms/step - loss: 0.1647 - accuracy: 0.7711 - jacard_coef: 0.0544 - val_loss: 3.6969 - val_accuracy: 0.0697 - val_jacard_coef: 0.0696 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1640 - accuracy: 0.7825 - jacard_coef: 0.06462/5 [===========>..................] - ETA: 2s - loss: 0.1640 - accuracy: 0.7850 - jacard_coef: 0.06823/5 [=================>............] - ETA: 1s - loss: 0.1640 - accuracy: 0.7858 - jacard_coef: 0.07414/5 [=======================>......] - ETA: 0s - loss: 0.1638 - accuracy: 0.7873 - jacard_coef: 0.07485/5 [==============================] - 3s 558ms/step - loss: 0.1638 - accuracy: 0.7871 - jacard_coef: 0.0779 - val_loss: 0.4655 - val_accuracy: 0.9155 - val_jacard_coef: 0.0069 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1632 - accuracy: 0.7824 - jacard_coef: 0.08862/5 [===========>..................] - ETA: 2s - loss: 0.1633 - accuracy: 0.7769 - jacard_coef: 0.06843/5 [=================>............] - ETA: 1s - loss: 0.1632 - accuracy: 0.7830 - jacard_coef: 0.08214/5 [=======================>......] - ETA: 0s - loss: 0.1631 - accuracy: 0.7880 - jacard_coef: 0.08345/5 [==============================] - 3s 558ms/step - loss: 0.1631 - accuracy: 0.7877 - jacard_coef: 0.1092 - val_loss: 0.4930 - val_accuracy: 0.9196 - val_jacard_coef: 0.0060 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1623 - accuracy: 0.7956 - jacard_coef: 0.08912/5 [===========>..................] - ETA: 2s - loss: 0.1625 - accuracy: 0.7811 - jacard_coef: 0.09363/5 [=================>............] - ETA: 1s - loss: 0.1627 - accuracy: 0.7674 - jacard_coef: 0.08204/5 [=======================>......] - ETA: 0s - loss: 0.1626 - accuracy: 0.7660 - jacard_coef: 0.08145/5 [==============================] - 3s 558ms/step - loss: 0.1626 - accuracy: 0.7659 - jacard_coef: 0.0667 - val_loss: 0.2933 - val_accuracy: 0.9191 - val_jacard_coef: 0.0064 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8019 - jacard_coef: 0.07382/5 [===========>..................] - ETA: 2s - loss: 0.1619 - accuracy: 0.8075 - jacard_coef: 0.06593/5 [=================>............] - ETA: 1s - loss: 0.1620 - accuracy: 0.8207 - jacard_coef: 0.07564/5 [=======================>......] - ETA: 0s - loss: 0.1619 - accuracy: 0.8364 - jacard_coef: 0.06995/5 [==============================] - 3s 559ms/step - loss: 0.1618 - accuracy: 0.8369 - jacard_coef: 0.0560 - val_loss: 0.1584 - val_accuracy: 0.7727 - val_jacard_coef: 0.0673 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1616 - accuracy: 0.8879 - jacard_coef: 0.03572/5 [===========>..................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8873 - jacard_coef: 0.03373/5 [=================>............] - ETA: 1s - loss: 0.1614 - accuracy: 0.8918 - jacard_coef: 0.03504/5 [=======================>......] - ETA: 0s - loss: 0.1613 - accuracy: 0.8930 - jacard_coef: 0.03355/5 [==============================] - 3s 570ms/step - loss: 0.1613 - accuracy: 0.8935 - jacard_coef: 0.0268 - val_loss: 0.2766 - val_accuracy: 0.1164 - val_jacard_coef: 0.0708 - lr: 2.5000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.9002 - jacard_coef: 0.03162/5 [===========>..................] - ETA: 2s - loss: 0.1609 - accuracy: 0.8997 - jacard_coef: 0.04023/5 [=================>............] - ETA: 1s - loss: 0.1611 - accuracy: 0.8917 - jacard_coef: 0.03644/5 [=======================>......] - ETA: 0s - loss: 0.1610 - accuracy: 0.8933 - jacard_coef: 0.03785/5 [==============================] - 3s 559ms/step - loss: 0.1610 - accuracy: 0.8931 - jacard_coef: 0.0380 - val_loss: 0.1569 - val_accuracy: 0.7770 - val_jacard_coef: 0.0656 - lr: 2.5000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1604 - accuracy: 0.9083 - jacard_coef: 0.02512/5 [===========>..................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8920 - jacard_coef: 0.04353/5 [=================>............] - ETA: 1s - loss: 0.1607 - accuracy: 0.8933 - jacard_coef: 0.04014/5 [=======================>......] - ETA: 0s - loss: 0.1607 - accuracy: 0.8932 - jacard_coef: 0.04845/5 [==============================] - 3s 557ms/step - loss: 0.1607 - accuracy: 0.8931 - jacard_coef: 0.0429 - val_loss: 0.1119 - val_accuracy: 0.8751 - val_jacard_coef: 0.0409 - lr: 2.5000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8917 - jacard_coef: 0.06502/5 [===========>..................] - ETA: 2s - loss: 0.1605 - accuracy: 0.8845 - jacard_coef: 0.05873/5 [=================>............] - ETA: 1s - loss: 0.1604 - accuracy: 0.8854 - jacard_coef: 0.05784/5 [=======================>......] - ETA: 0s - loss: 0.1604 - accuracy: 0.8843 - jacard_coef: 0.06345/5 [==============================] - 3s 557ms/step - loss: 0.1604 - accuracy: 0.8847 - jacard_coef: 0.0508 - val_loss: 0.1025 - val_accuracy: 0.8973 - val_jacard_coef: 0.0273 - lr: 2.5000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1604 - accuracy: 0.8805 - jacard_coef: 0.03822/5 [===========>..................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8789 - jacard_coef: 0.04283/5 [=================>............] - ETA: 1s - loss: 0.1601 - accuracy: 0.8818 - jacard_coef: 0.05384/5 [=======================>......] - ETA: 0s - loss: 0.1601 - accuracy: 0.8814 - jacard_coef: 0.06515/5 [==============================] - 3s 558ms/step - loss: 0.1601 - accuracy: 0.8819 - jacard_coef: 0.0702 - val_loss: 0.1196 - val_accuracy: 0.8899 - val_jacard_coef: 0.0270 - lr: 2.5000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1600 - accuracy: 0.8787 - jacard_coef: 0.07822/5 [===========>..................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8696 - jacard_coef: 0.05263/5 [=================>............] - ETA: 1s - loss: 0.1601 - accuracy: 0.8754 - jacard_coef: 0.06334/5 [=======================>......] - ETA: 0s - loss: 0.1598 - accuracy: 0.8833 - jacard_coef: 0.06605/5 [==============================] - 3s 559ms/step - loss: 0.1598 - accuracy: 0.8837 - jacard_coef: 0.0567 - val_loss: 0.1246 - val_accuracy: 0.8949 - val_jacard_coef: 0.0227 - lr: 2.5000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1593 - accuracy: 0.8866 - jacard_coef: 0.03772/5 [===========>..................] - ETA: 2s - loss: 0.1595 - accuracy: 0.8806 - jacard_coef: 0.05563/5 [=================>............] - ETA: 1s - loss: 0.1595 - accuracy: 0.8788 - jacard_coef: 0.06004/5 [=======================>......] - ETA: 0s - loss: 0.1595 - accuracy: 0.8809 - jacard_coef: 0.06505/5 [==============================] - 3s 557ms/step - loss: 0.1595 - accuracy: 0.8806 - jacard_coef: 0.0539 - val_loss: 0.1288 - val_accuracy: 0.8979 - val_jacard_coef: 0.0230 - lr: 1.2500e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8700 - jacard_coef: 0.06272/5 [===========>..................] - ETA: 2s - loss: 0.1596 - accuracy: 0.8721 - jacard_coef: 0.06353/5 [=================>............] - ETA: 1s - loss: 0.1595 - accuracy: 0.8705 - jacard_coef: 0.06914/5 [=======================>......] - ETA: 0s - loss: 0.1593 - accuracy: 0.8739 - jacard_coef: 0.06895/5 [==============================] - 3s 558ms/step - loss: 0.1593 - accuracy: 0.8730 - jacard_coef: 0.0718 - val_loss: 0.1312 - val_accuracy: 0.8925 - val_jacard_coef: 0.0274 - lr: 1.2500e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8806 - jacard_coef: 0.08832/5 [===========>..................] - ETA: 2s - loss: 0.1590 - accuracy: 0.8657 - jacard_coef: 0.08183/5 [=================>............] - ETA: 1s - loss: 0.1590 - accuracy: 0.8654 - jacard_coef: 0.07694/5 [=======================>......] - ETA: 0s - loss: 0.1592 - accuracy: 0.8573 - jacard_coef: 0.07735/5 [==============================] - 3s 556ms/step - loss: 0.1592 - accuracy: 0.8569 - jacard_coef: 0.0785 - val_loss: 0.1341 - val_accuracy: 0.8759 - val_jacard_coef: 0.0562 - lr: 1.2500e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1585 - accuracy: 0.8649 - jacard_coef: 0.07062/5 [===========>..................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8414 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1592 - accuracy: 0.8388 - jacard_coef: 0.08334/5 [=======================>......] - ETA: 0s - loss: 0.1590 - accuracy: 0.8395 - jacard_coef: 0.08545/5 [==============================] - 3s 556ms/step - loss: 0.1590 - accuracy: 0.8390 - jacard_coef: 0.0964 - val_loss: 0.1420 - val_accuracy: 0.8457 - val_jacard_coef: 0.0648 - lr: 1.2500e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.8227 - jacard_coef: 0.09262/5 [===========>..................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8150 - jacard_coef: 0.10303/5 [=================>............] - ETA: 1s - loss: 0.1590 - accuracy: 0.8140 - jacard_coef: 0.09714/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.8108 - jacard_coef: 0.09775/5 [==============================] - 3s 569ms/step - loss: 0.1589 - accuracy: 0.8113 - jacard_coef: 0.0974 - val_loss: 0.1442 - val_accuracy: 0.7954 - val_jacard_coef: 0.0855 - lr: 1.2500e-04
Epoch 24/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8063 - jacard_coef: 0.09562/5 [===========>..................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8048 - jacard_coef: 0.09623/5 [=================>............] - ETA: 1s - loss: 0.1587 - accuracy: 0.7998 - jacard_coef: 0.09624/5 [=======================>......] - ETA: 0s - loss: 0.1587 - accuracy: 0.7991 - jacard_coef: 0.10415/5 [==============================] - 3s 567ms/step - loss: 0.1587 - accuracy: 0.7985 - jacard_coef: 0.1083 - val_loss: 0.1458 - val_accuracy: 0.7605 - val_jacard_coef: 0.0869 - lr: 1.2500e-04
Epoch 25/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1587 - accuracy: 0.8089 - jacard_coef: 0.08172/5 [===========>..................] - ETA: 2s - loss: 0.1588 - accuracy: 0.8067 - jacard_coef: 0.09703/5 [=================>............] - ETA: 1s - loss: 0.1587 - accuracy: 0.8086 - jacard_coef: 0.10184/5 [=======================>......] - ETA: 0s - loss: 0.1586 - accuracy: 0.8111 - jacard_coef: 0.10055/5 [==============================] - 3s 566ms/step - loss: 0.1586 - accuracy: 0.8117 - jacard_coef: 0.0804 - val_loss: 0.1441 - val_accuracy: 0.7631 - val_jacard_coef: 0.0872 - lr: 1.2500e-04
Epoch 26/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1587 - accuracy: 0.8115 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8195 - jacard_coef: 0.09503/5 [=================>............] - ETA: 1s - loss: 0.1585 - accuracy: 0.8277 - jacard_coef: 0.09204/5 [=======================>......] - ETA: 0s - loss: 0.1584 - accuracy: 0.8316 - jacard_coef: 0.09455/5 [==============================] - 3s 566ms/step - loss: 0.1584 - accuracy: 0.8313 - jacard_coef: 0.1059 - val_loss: 0.1469 - val_accuracy: 0.7604 - val_jacard_coef: 0.0873 - lr: 1.2500e-04
Epoch 27/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8374 - jacard_coef: 0.10982/5 [===========>..................] - ETA: 2s - loss: 0.1581 - accuracy: 0.8397 - jacard_coef: 0.09973/5 [=================>............] - ETA: 1s - loss: 0.1582 - accuracy: 0.8323 - jacard_coef: 0.09594/5 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8305 - jacard_coef: 0.10055/5 [==============================] - 3s 568ms/step - loss: 0.1583 - accuracy: 0.8304 - jacard_coef: 0.0833 - val_loss: 0.1517 - val_accuracy: 0.7532 - val_jacard_coef: 0.0875 - lr: 1.2500e-04
Epoch 28/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1579 - accuracy: 0.8313 - jacard_coef: 0.09632/5 [===========>..................] - ETA: 2s - loss: 0.1578 - accuracy: 0.8310 - jacard_coef: 0.10183/5 [=================>............] - ETA: 1s - loss: 0.1582 - accuracy: 0.8220 - jacard_coef: 0.09444/5 [=======================>......] - ETA: 0s - loss: 0.1582 - accuracy: 0.8209 - jacard_coef: 0.09485/5 [==============================] - 3s 567ms/step - loss: 0.1582 - accuracy: 0.8211 - jacard_coef: 0.1454 - val_loss: 0.1536 - val_accuracy: 0.7629 - val_jacard_coef: 0.0882 - lr: 1.2500e-04
Epoch 29/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1575 - accuracy: 0.8309 - jacard_coef: 0.13082/5 [===========>..................] - ETA: 2s - loss: 0.1579 - accuracy: 0.8186 - jacard_coef: 0.10503/5 [=================>............] - ETA: 1s - loss: 0.1580 - accuracy: 0.8154 - jacard_coef: 0.09964/5 [=======================>......] - ETA: 0s - loss: 0.1581 - accuracy: 0.8112 - jacard_coef: 0.09595/5 [==============================] - 3s 556ms/step - loss: 0.1581 - accuracy: 0.8109 - jacard_coef: 0.0768 - val_loss: 0.1591 - val_accuracy: 0.6959 - val_jacard_coef: 0.0703 - lr: 1.2500e-04
Epoch 30/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1576 - accuracy: 0.8332 - jacard_coef: 0.09542/5 [===========>..................] - ETA: 2s - loss: 0.1579 - accuracy: 0.8252 - jacard_coef: 0.08423/5 [=================>............] - ETA: 1s - loss: 0.1577 - accuracy: 0.8330 - jacard_coef: 0.09784/5 [=======================>......] - ETA: 0s - loss: 0.1579 - accuracy: 0.8315 - jacard_coef: 0.09745/5 [==============================] - 3s 558ms/step - loss: 0.1578 - accuracy: 0.8319 - jacard_coef: 0.0779 - val_loss: 0.1652 - val_accuracy: 0.6162 - val_jacard_coef: 0.0785 - lr: 1.2500e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0882 (epoch 28)
  Final Val Loss: 0.1652
  Training Time: 0:02:51.139547
  Stability (std): 0.0085

Results saved to: hyperparameter_optimization_20250926_165036/exp_21_Attention_UNet_lr1e-3_bs32/Attention_UNet_lr0.001_bs32_results.json

Experiment 21 completed in 188s
Progress: 21/36 completed
Estimated remaining time: 47 minutes

ðŸ”¬ EXPERIMENT 22/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879275.745407 1130299 service.cc:145] XLA service 0x154030456780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879275.745430 1130299 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879275.881810 1130299 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 12:03 - loss: 0.3464 - accuracy: 0.5205 - jacard_coef: 0.0763 2/17 [==>...........................] - ETA: 1:10 - loss: 0.3252 - accuracy: 0.4944 - jacard_coef: 0.0753  3/17 [====>.........................] - ETA: 34s - loss: 0.2952 - accuracy: 0.4741 - jacard_coef: 0.0691  4/17 [======>.......................] - ETA: 21s - loss: 0.2761 - accuracy: 0.4958 - jacard_coef: 0.0763 5/17 [=======>......................] - ETA: 15s - loss: 0.2626 - accuracy: 0.5301 - jacard_coef: 0.0754 6/17 [=========>....................] - ETA: 11s - loss: 0.2594 - accuracy: 0.5452 - jacard_coef: 0.0784 7/17 [===========>..................] - ETA: 9s - loss: 0.2491 - accuracy: 0.5716 - jacard_coef: 0.0765  8/17 [=============>................] - ETA: 7s - loss: 0.2410 - accuracy: 0.5992 - jacard_coef: 0.0778 9/17 [==============>...............] - ETA: 5s - loss: 0.2343 - accuracy: 0.6197 - jacard_coef: 0.075010/17 [================>.............] - ETA: 4s - loss: 0.2288 - accuracy: 0.6305 - jacard_coef: 0.074611/17 [==================>...........] - ETA: 3s - loss: 0.2272 - accuracy: 0.6362 - jacard_coef: 0.071812/17 [====================>.........] - ETA: 2s - loss: 0.2235 - accuracy: 0.6447 - jacard_coef: 0.072413/17 [=====================>........] - ETA: 2s - loss: 0.2202 - accuracy: 0.6497 - jacard_coef: 0.072114/17 [=======================>......] - ETA: 1s - loss: 0.2172 - accuracy: 0.6514 - jacard_coef: 0.071215/17 [=========================>....] - ETA: 1s - loss: 0.2148 - accuracy: 0.6556 - jacard_coef: 0.070316/17 [===========================>..] - ETA: 0s - loss: 0.2127 - accuracy: 0.6582 - jacard_coef: 0.068817/17 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.6588 - jacard_coef: 0.071717/17 [==============================] - 59s 863ms/step - loss: 0.2124 - accuracy: 0.6588 - jacard_coef: 0.0717 - val_loss: 1.0458 - val_accuracy: 0.8971 - val_jacard_coef: 0.0100 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1829 - accuracy: 0.8313 - jacard_coef: 0.0250 2/17 [==>...........................] - ETA: 2s - loss: 0.1825 - accuracy: 0.8583 - jacard_coef: 0.0343 3/17 [====>.........................] - ETA: 2s - loss: 0.1811 - accuracy: 0.8647 - jacard_coef: 0.0288 4/17 [======>.......................] - ETA: 2s - loss: 0.1791 - accuracy: 0.8731 - jacard_coef: 0.0414 5/17 [=======>......................] - ETA: 2s - loss: 0.1780 - accuracy: 0.8750 - jacard_coef: 0.0349 6/17 [=========>....................] - ETA: 1s - loss: 0.1771 - accuracy: 0.8729 - jacard_coef: 0.0330 7/17 [===========>..................] - ETA: 1s - loss: 0.1760 - accuracy: 0.8733 - jacard_coef: 0.0316 8/17 [=============>................] - ETA: 1s - loss: 0.1749 - accuracy: 0.8694 - jacard_coef: 0.0429 9/17 [==============>...............] - ETA: 1s - loss: 0.1776 - accuracy: 0.8386 - jacard_coef: 0.042210/17 [================>.............] - ETA: 1s - loss: 0.1768 - accuracy: 0.8460 - jacard_coef: 0.041111/17 [==================>...........] - ETA: 1s - loss: 0.1761 - accuracy: 0.8409 - jacard_coef: 0.040412/17 [====================>.........] - ETA: 0s - loss: 0.1758 - accuracy: 0.8401 - jacard_coef: 0.040813/17 [=====================>........] - ETA: 0s - loss: 0.1757 - accuracy: 0.8354 - jacard_coef: 0.043114/17 [=======================>......] - ETA: 0s - loss: 0.1753 - accuracy: 0.8323 - jacard_coef: 0.043415/17 [=========================>....] - ETA: 0s - loss: 0.1752 - accuracy: 0.8281 - jacard_coef: 0.045616/17 [===========================>..] - ETA: 0s - loss: 0.1750 - accuracy: 0.8259 - jacard_coef: 0.048617/17 [==============================] - 3s 179ms/step - loss: 0.1749 - accuracy: 0.8254 - jacard_coef: 0.0505 - val_loss: 1.0840 - val_accuracy: 0.9201 - val_jacard_coef: 0.0064 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1690 - accuracy: 0.8075 - jacard_coef: 0.1197 2/17 [==>...........................] - ETA: 2s - loss: 0.1683 - accuracy: 0.8190 - jacard_coef: 0.0754 3/17 [====>.........................] - ETA: 2s - loss: 0.1716 - accuracy: 0.8109 - jacard_coef: 0.0736 4/17 [======>.......................] - ETA: 2s - loss: 0.1712 - accuracy: 0.8272 - jacard_coef: 0.0600 5/17 [=======>......................] - ETA: 2s - loss: 0.1709 - accuracy: 0.8524 - jacard_coef: 0.0490 6/17 [=========>....................] - ETA: 1s - loss: 0.1709 - accuracy: 0.8587 - jacard_coef: 0.0431 7/17 [===========>..................] - ETA: 1s - loss: 0.1709 - accuracy: 0.8690 - jacard_coef: 0.0417 8/17 [=============>................] - ETA: 1s - loss: 0.1711 - accuracy: 0.8705 - jacard_coef: 0.0372 9/17 [==============>...............] - ETA: 1s - loss: 0.1714 - accuracy: 0.8748 - jacard_coef: 0.034010/17 [================>.............] - ETA: 1s - loss: 0.1712 - accuracy: 0.8793 - jacard_coef: 0.031711/17 [==================>...........] - ETA: 1s - loss: 0.1706 - accuracy: 0.8825 - jacard_coef: 0.030012/17 [====================>.........] - ETA: 0s - loss: 0.1700 - accuracy: 0.8849 - jacard_coef: 0.029713/17 [=====================>........] - ETA: 0s - loss: 0.1696 - accuracy: 0.8871 - jacard_coef: 0.028214/17 [=======================>......] - ETA: 0s - loss: 0.1692 - accuracy: 0.8867 - jacard_coef: 0.026715/17 [=========================>....] - ETA: 0s - loss: 0.1688 - accuracy: 0.8868 - jacard_coef: 0.026016/17 [===========================>..] - ETA: 0s - loss: 0.1684 - accuracy: 0.8871 - jacard_coef: 0.025817/17 [==============================] - 3s 183ms/step - loss: 0.1687 - accuracy: 0.8866 - jacard_coef: 0.0273 - val_loss: 5.7724 - val_accuracy: 0.0727 - val_jacard_coef: 0.0684 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1638 - accuracy: 0.8694 - jacard_coef: 0.0177 2/17 [==>...........................] - ETA: 2s - loss: 0.1629 - accuracy: 0.8846 - jacard_coef: 0.0090 3/17 [====>.........................] - ETA: 2s - loss: 0.2078 - accuracy: 0.7849 - jacard_coef: 0.0410 4/17 [======>.......................] - ETA: 2s - loss: 0.1961 - accuracy: 0.8146 - jacard_coef: 0.0329 5/17 [=======>......................] - ETA: 2s - loss: 0.2109 - accuracy: 0.7905 - jacard_coef: 0.0338 6/17 [=========>....................] - ETA: 1s - loss: 0.2076 - accuracy: 0.8003 - jacard_coef: 0.0358 7/17 [===========>..................] - ETA: 1s - loss: 0.2019 - accuracy: 0.8134 - jacard_coef: 0.0322 8/17 [=============>................] - ETA: 1s - loss: 0.1971 - accuracy: 0.8247 - jacard_coef: 0.0287 9/17 [==============>...............] - ETA: 1s - loss: 0.1938 - accuracy: 0.8344 - jacard_coef: 0.026310/17 [================>.............] - ETA: 1s - loss: 0.1913 - accuracy: 0.8372 - jacard_coef: 0.024711/17 [==================>...........] - ETA: 1s - loss: 0.1893 - accuracy: 0.8458 - jacard_coef: 0.022512/17 [====================>.........] - ETA: 0s - loss: 0.1876 - accuracy: 0.8525 - jacard_coef: 0.020713/17 [=====================>........] - ETA: 0s - loss: 0.1857 - accuracy: 0.8592 - jacard_coef: 0.019114/17 [=======================>......] - ETA: 0s - loss: 0.1843 - accuracy: 0.8625 - jacard_coef: 0.017815/17 [=========================>....] - ETA: 0s - loss: 0.1831 - accuracy: 0.8655 - jacard_coef: 0.016616/17 [===========================>..] - ETA: 0s - loss: 0.1818 - accuracy: 0.8712 - jacard_coef: 0.015617/17 [==============================] - 3s 182ms/step - loss: 0.1817 - accuracy: 0.8705 - jacard_coef: 0.0164 - val_loss: 13.2333 - val_accuracy: 0.0741 - val_jacard_coef: 0.0684 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1639 - accuracy: 0.8991 - jacard_coef: 0.0012 2/17 [==>...........................] - ETA: 2s - loss: 0.1637 - accuracy: 0.9088 - jacard_coef: 0.0022 3/17 [====>.........................] - ETA: 2s - loss: 0.1625 - accuracy: 0.9095 - jacard_coef: 0.0015 4/17 [======>.......................] - ETA: 2s - loss: 0.1619 - accuracy: 0.9064 - jacard_coef: 0.0016 5/17 [=======>......................] - ETA: 2s - loss: 0.1612 - accuracy: 0.9074 - jacard_coef: 0.0016 6/17 [=========>....................] - ETA: 1s - loss: 0.1604 - accuracy: 0.9086 - jacard_coef: 0.0020 7/17 [===========>..................] - ETA: 1s - loss: 0.1603 - accuracy: 0.9087 - jacard_coef: 0.0023 8/17 [=============>................] - ETA: 1s - loss: 0.1597 - accuracy: 0.9109 - jacard_coef: 0.0024 9/17 [==============>...............] - ETA: 1s - loss: 0.1595 - accuracy: 0.9118 - jacard_coef: 0.003010/17 [================>.............] - ETA: 1s - loss: 0.1592 - accuracy: 0.9100 - jacard_coef: 0.002811/17 [==================>...........] - ETA: 1s - loss: 0.1637 - accuracy: 0.8776 - jacard_coef: 0.008712/17 [====================>.........] - ETA: 0s - loss: 0.1641 - accuracy: 0.8723 - jacard_coef: 0.014913/17 [=====================>........] - ETA: 0s - loss: 0.1637 - accuracy: 0.8760 - jacard_coef: 0.013814/17 [=======================>......] - ETA: 0s - loss: 0.1635 - accuracy: 0.8771 - jacard_coef: 0.013015/17 [=========================>....] - ETA: 0s - loss: 0.1636 - accuracy: 0.8808 - jacard_coef: 0.012516/17 [===========================>..] - ETA: 0s - loss: 0.1639 - accuracy: 0.8867 - jacard_coef: 0.011717/17 [==============================] - 3s 179ms/step - loss: 0.1640 - accuracy: 0.8864 - jacard_coef: 0.0110 - val_loss: 0.0975 - val_accuracy: 0.9091 - val_jacard_coef: 0.0062 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1708 - accuracy: 0.9183 - jacard_coef: 0.0028 2/17 [==>...........................] - ETA: 2s - loss: 0.1722 - accuracy: 0.9182 - jacard_coef: 0.0015 3/17 [====>.........................] - ETA: 2s - loss: 0.1713 - accuracy: 0.9165 - jacard_coef: 0.0013 4/17 [======>.......................] - ETA: 2s - loss: 0.1708 - accuracy: 0.9149 - jacard_coef: 0.0015 5/17 [=======>......................] - ETA: 2s - loss: 0.1703 - accuracy: 0.9228 - jacard_coef: 0.0021 6/17 [=========>....................] - ETA: 1s - loss: 0.1699 - accuracy: 0.9137 - jacard_coef: 0.0027 7/17 [===========>..................] - ETA: 1s - loss: 0.1695 - accuracy: 0.9137 - jacard_coef: 0.0028 8/17 [=============>................] - ETA: 1s - loss: 0.1687 - accuracy: 0.9135 - jacard_coef: 0.0034 9/17 [==============>...............] - ETA: 1s - loss: 0.1683 - accuracy: 0.9146 - jacard_coef: 0.003710/17 [================>.............] - ETA: 1s - loss: 0.1674 - accuracy: 0.9171 - jacard_coef: 0.003311/17 [==================>...........] - ETA: 1s - loss: 0.1667 - accuracy: 0.9152 - jacard_coef: 0.005212/17 [====================>.........] - ETA: 0s - loss: 0.1661 - accuracy: 0.9105 - jacard_coef: 0.007013/17 [=====================>........] - ETA: 0s - loss: 0.1657 - accuracy: 0.9085 - jacard_coef: 0.007014/17 [=======================>......] - ETA: 0s - loss: 0.1652 - accuracy: 0.9089 - jacard_coef: 0.007115/17 [=========================>....] - ETA: 0s - loss: 0.1647 - accuracy: 0.9101 - jacard_coef: 0.007316/17 [===========================>..] - ETA: 0s - loss: 0.1644 - accuracy: 0.9093 - jacard_coef: 0.007017/17 [==============================] - 3s 182ms/step - loss: 0.1647 - accuracy: 0.9094 - jacard_coef: 0.0066 - val_loss: 0.1767 - val_accuracy: 0.2338 - val_jacard_coef: 0.0748 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1564 - accuracy: 0.8825 - jacard_coef: 0.0060 2/17 [==>...........................] - ETA: 2s - loss: 0.1570 - accuracy: 0.8892 - jacard_coef: 0.0062 3/17 [====>.........................] - ETA: 2s - loss: 0.1568 - accuracy: 0.9016 - jacard_coef: 0.0042 4/17 [======>.......................] - ETA: 2s - loss: 0.1571 - accuracy: 0.9058 - jacard_coef: 0.0035 5/17 [=======>......................] - ETA: 2s - loss: 0.1571 - accuracy: 0.9104 - jacard_coef: 0.0031 6/17 [=========>....................] - ETA: 1s - loss: 0.1584 - accuracy: 0.9112 - jacard_coef: 0.0032 7/17 [===========>..................] - ETA: 1s - loss: 0.1579 - accuracy: 0.9116 - jacard_coef: 0.0038 8/17 [=============>................] - ETA: 1s - loss: 0.1572 - accuracy: 0.9157 - jacard_coef: 0.0033 9/17 [==============>...............] - ETA: 1s - loss: 0.1568 - accuracy: 0.9160 - jacard_coef: 0.003010/17 [================>.............] - ETA: 1s - loss: 0.1568 - accuracy: 0.9150 - jacard_coef: 0.002811/17 [==================>...........] - ETA: 1s - loss: 0.1566 - accuracy: 0.9157 - jacard_coef: 0.002612/17 [====================>.........] - ETA: 0s - loss: 0.1562 - accuracy: 0.9177 - jacard_coef: 0.002413/17 [=====================>........] - ETA: 0s - loss: 0.1560 - accuracy: 0.9153 - jacard_coef: 0.002314/17 [=======================>......] - ETA: 0s - loss: 0.1558 - accuracy: 0.9125 - jacard_coef: 0.002215/17 [=========================>....] - ETA: 0s - loss: 0.1553 - accuracy: 0.9144 - jacard_coef: 0.002116/17 [===========================>..] - ETA: 0s - loss: 0.1550 - accuracy: 0.9150 - jacard_coef: 0.002017/17 [==============================] - 3s 180ms/step - loss: 0.1551 - accuracy: 0.9137 - jacard_coef: 0.0044 - val_loss: 0.1512 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1488 - accuracy: 0.9251 - jacard_coef: 0.0036 2/17 [==>...........................] - ETA: 2s - loss: 0.1488 - accuracy: 0.9348 - jacard_coef: 0.0021 3/17 [====>.........................] - ETA: 2s - loss: 0.1490 - accuracy: 0.9349 - jacard_coef: 0.0075 4/17 [======>.......................] - ETA: 2s - loss: 0.1506 - accuracy: 0.9225 - jacard_coef: 0.0093 5/17 [=======>......................] - ETA: 2s - loss: 0.1517 - accuracy: 0.9111 - jacard_coef: 0.0097 6/17 [=========>....................] - ETA: 1s - loss: 0.1515 - accuracy: 0.9113 - jacard_coef: 0.0099 7/17 [===========>..................] - ETA: 1s - loss: 0.1514 - accuracy: 0.9098 - jacard_coef: 0.0110 8/17 [=============>................] - ETA: 1s - loss: 0.1525 - accuracy: 0.9012 - jacard_coef: 0.0113 9/17 [==============>...............] - ETA: 1s - loss: 0.1523 - accuracy: 0.9048 - jacard_coef: 0.011310/17 [================>.............] - ETA: 1s - loss: 0.1520 - accuracy: 0.9052 - jacard_coef: 0.010911/17 [==================>...........] - ETA: 1s - loss: 0.1516 - accuracy: 0.9073 - jacard_coef: 0.010112/17 [====================>.........] - ETA: 0s - loss: 0.1516 - accuracy: 0.9051 - jacard_coef: 0.009613/17 [=====================>........] - ETA: 0s - loss: 0.1515 - accuracy: 0.9061 - jacard_coef: 0.009114/17 [=======================>......] - ETA: 0s - loss: 0.1516 - accuracy: 0.9026 - jacard_coef: 0.008715/17 [=========================>....] - ETA: 0s - loss: 0.1514 - accuracy: 0.9040 - jacard_coef: 0.008216/17 [===========================>..] - ETA: 0s - loss: 0.1513 - accuracy: 0.9047 - jacard_coef: 0.007717/17 [==============================] - 3s 179ms/step - loss: 0.1513 - accuracy: 0.9054 - jacard_coef: 0.0072 - val_loss: 0.1453 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1478 - accuracy: 0.9152 - jacard_coef: 0.0011 2/17 [==>...........................] - ETA: 2s - loss: 0.1461 - accuracy: 0.9312 - jacard_coef: 5.2815e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9215 - jacard_coef: 6.6980e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1462 - accuracy: 0.9238 - jacard_coef: 7.9151e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1465 - accuracy: 0.9175 - jacard_coef: 7.7456e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1459 - accuracy: 0.9206 - jacard_coef: 9.2370e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1458 - accuracy: 0.9208 - jacard_coef: 8.2318e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1454 - accuracy: 0.9217 - jacard_coef: 8.5244e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1455 - accuracy: 0.9211 - jacard_coef: 8.1832e-0410/17 [================>.............] - ETA: 1s - loss: 0.1453 - accuracy: 0.9201 - jacard_coef: 9.1248e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1453 - accuracy: 0.9180 - jacard_coef: 8.9012e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1450 - accuracy: 0.9202 - jacard_coef: 8.5883e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1449 - accuracy: 0.9187 - jacard_coef: 8.7078e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1450 - accuracy: 0.9160 - jacard_coef: 9.6180e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1448 - accuracy: 0.9163 - jacard_coef: 9.5418e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1447 - accuracy: 0.9163 - jacard_coef: 9.3551e-0417/17 [==============================] - 3s 180ms/step - loss: 0.1446 - accuracy: 0.9166 - jacard_coef: 8.8048e-04 - val_loss: 0.1398 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1429 - accuracy: 0.9004 - jacard_coef: 0.0290 2/17 [==>...........................] - ETA: 2s - loss: 0.1440 - accuracy: 0.8942 - jacard_coef: 0.0146 3/17 [====>.........................] - ETA: 2s - loss: 0.1424 - accuracy: 0.9177 - jacard_coef: 0.0098 4/17 [======>.......................] - ETA: 2s - loss: 0.1425 - accuracy: 0.9196 - jacard_coef: 0.0074 5/17 [=======>......................] - ETA: 2s - loss: 0.1425 - accuracy: 0.9191 - jacard_coef: 0.0059 6/17 [=========>....................] - ETA: 1s - loss: 0.1423 - accuracy: 0.9259 - jacard_coef: 0.0050 7/17 [===========>..................] - ETA: 1s - loss: 0.1430 - accuracy: 0.9218 - jacard_coef: 0.0042 8/17 [=============>................] - ETA: 1s - loss: 0.1431 - accuracy: 0.9198 - jacard_coef: 0.0039 9/17 [==============>...............] - ETA: 1s - loss: 0.1427 - accuracy: 0.9215 - jacard_coef: 0.003410/17 [================>.............] - ETA: 1s - loss: 0.1429 - accuracy: 0.9194 - jacard_coef: 0.003111/17 [==================>...........] - ETA: 1s - loss: 0.1431 - accuracy: 0.9171 - jacard_coef: 0.002912/17 [====================>.........] - ETA: 0s - loss: 0.1431 - accuracy: 0.9152 - jacard_coef: 0.002713/17 [=====================>........] - ETA: 0s - loss: 0.1427 - accuracy: 0.9176 - jacard_coef: 0.002514/17 [=======================>......] - ETA: 0s - loss: 0.1423 - accuracy: 0.9191 - jacard_coef: 0.002515/17 [=========================>....] - ETA: 0s - loss: 0.1424 - accuracy: 0.9172 - jacard_coef: 0.002416/17 [===========================>..] - ETA: 0s - loss: 0.1425 - accuracy: 0.9147 - jacard_coef: 0.002417/17 [==============================] - 3s 179ms/step - loss: 0.1426 - accuracy: 0.9142 - jacard_coef: 0.0024 - val_loss: 0.1391 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1400 - accuracy: 0.9018 - jacard_coef: 0.0023 2/17 [==>...........................] - ETA: 2s - loss: 0.1426 - accuracy: 0.8861 - jacard_coef: 0.0022 3/17 [====>.........................] - ETA: 2s - loss: 0.1420 - accuracy: 0.8934 - jacard_coef: 0.0027 4/17 [======>.......................] - ETA: 2s - loss: 0.1416 - accuracy: 0.8973 - jacard_coef: 0.0032 5/17 [=======>......................] - ETA: 2s - loss: 0.1406 - accuracy: 0.9053 - jacard_coef: 0.0026 6/17 [=========>....................] - ETA: 1s - loss: 0.1402 - accuracy: 0.9060 - jacard_coef: 0.0027 7/17 [===========>..................] - ETA: 1s - loss: 0.1396 - accuracy: 0.9099 - jacard_coef: 0.0030 8/17 [=============>................] - ETA: 1s - loss: 0.1398 - accuracy: 0.9077 - jacard_coef: 0.0029 9/17 [==============>...............] - ETA: 1s - loss: 0.1392 - accuracy: 0.9113 - jacard_coef: 0.002610/17 [================>.............] - ETA: 1s - loss: 0.1396 - accuracy: 0.8991 - jacard_coef: 0.008411/17 [==================>...........] - ETA: 1s - loss: 0.1397 - accuracy: 0.8950 - jacard_coef: 0.017912/17 [====================>.........] - ETA: 0s - loss: 0.1396 - accuracy: 0.8952 - jacard_coef: 0.018513/17 [=====================>........] - ETA: 0s - loss: 0.1395 - accuracy: 0.8966 - jacard_coef: 0.021614/17 [=======================>......] - ETA: 0s - loss: 0.1395 - accuracy: 0.8958 - jacard_coef: 0.022615/17 [=========================>....] - ETA: 0s - loss: 0.1397 - accuracy: 0.8956 - jacard_coef: 0.021216/17 [===========================>..] - ETA: 0s - loss: 0.1394 - accuracy: 0.9000 - jacard_coef: 0.019917/17 [==============================] - 3s 179ms/step - loss: 0.1394 - accuracy: 0.8997 - jacard_coef: 0.0191 - val_loss: 0.1356 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1369 - accuracy: 0.9258 - jacard_coef: 2.8250e-04 2/17 [==>...........................] - ETA: 2s - loss: 0.1385 - accuracy: 0.9156 - jacard_coef: 1.4125e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1397 - accuracy: 0.9063 - jacard_coef: 3.7128e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1392 - accuracy: 0.9084 - jacard_coef: 6.4127e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1393 - accuracy: 0.9059 - jacard_coef: 9.3743e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1386 - accuracy: 0.9099 - jacard_coef: 8.4016e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1383 - accuracy: 0.9125 - jacard_coef: 0.0010     8/17 [=============>................] - ETA: 1s - loss: 0.1379 - accuracy: 0.9161 - jacard_coef: 8.9347e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1376 - accuracy: 0.9192 - jacard_coef: 7.9419e-0410/17 [================>.............] - ETA: 1s - loss: 0.1379 - accuracy: 0.9174 - jacard_coef: 8.5401e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1377 - accuracy: 0.9178 - jacard_coef: 7.7637e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1374 - accuracy: 0.9200 - jacard_coef: 8.3591e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1374 - accuracy: 0.9192 - jacard_coef: 8.0090e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1373 - accuracy: 0.9189 - jacard_coef: 7.4370e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1374 - accuracy: 0.9176 - jacard_coef: 7.1162e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1373 - accuracy: 0.9172 - jacard_coef: 8.5924e-0417/17 [==============================] - 3s 180ms/step - loss: 0.1373 - accuracy: 0.9162 - jacard_coef: 0.0065 - val_loss: 0.1349 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1361 - accuracy: 0.9099 - jacard_coef: 0.0011 2/17 [==>...........................] - ETA: 2s - loss: 0.1371 - accuracy: 0.9081 - jacard_coef: 8.5415e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1372 - accuracy: 0.9040 - jacard_coef: 7.4036e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1360 - accuracy: 0.9120 - jacard_coef: 5.5527e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1354 - accuracy: 0.9153 - jacard_coef: 4.4422e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1355 - accuracy: 0.9133 - jacard_coef: 3.7018e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1352 - accuracy: 0.9175 - jacard_coef: 4.1685e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1356 - accuracy: 0.9154 - jacard_coef: 4.0794e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1355 - accuracy: 0.9160 - jacard_coef: 3.8409e-0410/17 [================>.............] - ETA: 1s - loss: 0.1354 - accuracy: 0.9175 - jacard_coef: 3.7041e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1354 - accuracy: 0.9165 - jacard_coef: 3.8338e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1358 - accuracy: 0.9133 - jacard_coef: 4.4096e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1357 - accuracy: 0.9143 - jacard_coef: 4.0704e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1357 - accuracy: 0.9133 - jacard_coef: 5.4982e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1356 - accuracy: 0.9138 - jacard_coef: 5.4660e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1352 - accuracy: 0.9165 - jacard_coef: 5.1244e-0417/17 [==============================] - 3s 180ms/step - loss: 0.1352 - accuracy: 0.9171 - jacard_coef: 4.8229e-04 - val_loss: 0.1322 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1329 - accuracy: 0.9283 - jacard_coef: 2.6584e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1309 - accuracy: 0.9477 - jacard_coef: 4.2294e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1316 - accuracy: 0.9372 - jacard_coef: 7.1948e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1327 - accuracy: 0.9261 - jacard_coef: 6.6429e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1331 - accuracy: 0.9246 - jacard_coef: 7.4644e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1332 - accuracy: 0.9230 - jacard_coef: 7.0058e-04 7/17 [===========>..................] - ETA: 1s - loss: 0.1331 - accuracy: 0.9242 - jacard_coef: 6.0050e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1332 - accuracy: 0.9225 - jacard_coef: 5.9739e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1331 - accuracy: 0.9218 - jacard_coef: 5.3101e-0410/17 [================>.............] - ETA: 1s - loss: 0.1336 - accuracy: 0.9178 - jacard_coef: 4.9400e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1338 - accuracy: 0.9163 - jacard_coef: 4.5085e-0412/17 [====================>.........] - ETA: 0s - loss: 0.1335 - accuracy: 0.9178 - jacard_coef: 4.2291e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1335 - accuracy: 0.9175 - jacard_coef: 4.1431e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1338 - accuracy: 0.9150 - jacard_coef: 8.1262e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1337 - accuracy: 0.9148 - jacard_coef: 7.5844e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.9162 - jacard_coef: 7.1104e-0417/17 [==============================] - 3s 180ms/step - loss: 0.1334 - accuracy: 0.9168 - jacard_coef: 6.6921e-04 - val_loss: 0.1277 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1337 - accuracy: 0.9016 - jacard_coef: 0.0020 2/17 [==>...........................] - ETA: 2s - loss: 0.1339 - accuracy: 0.9083 - jacard_coef: 0.0013 3/17 [====>.........................] - ETA: 2s - loss: 0.1331 - accuracy: 0.9126 - jacard_coef: 0.0015 4/17 [======>.......................] - ETA: 2s - loss: 0.1313 - accuracy: 0.9276 - jacard_coef: 0.0011 5/17 [=======>......................] - ETA: 2s - loss: 0.1309 - accuracy: 0.9293 - jacard_coef: 9.9994e-04 6/17 [=========>....................] - ETA: 1s - loss: 0.1315 - accuracy: 0.9218 - jacard_coef: 0.0021     7/17 [===========>..................] - ETA: 1s - loss: 0.1318 - accuracy: 0.9191 - jacard_coef: 0.0019 8/17 [=============>................] - ETA: 1s - loss: 0.1319 - accuracy: 0.9182 - jacard_coef: 0.0017 9/17 [==============>...............] - ETA: 1s - loss: 0.1321 - accuracy: 0.9153 - jacard_coef: 0.001510/17 [================>.............] - ETA: 1s - loss: 0.1318 - accuracy: 0.9180 - jacard_coef: 0.001411/17 [==================>...........] - ETA: 1s - loss: 0.1318 - accuracy: 0.9153 - jacard_coef: 0.010712/17 [====================>.........] - ETA: 0s - loss: 0.1319 - accuracy: 0.9144 - jacard_coef: 0.010013/17 [=====================>........] - ETA: 0s - loss: 0.1318 - accuracy: 0.9145 - jacard_coef: 0.009314/17 [=======================>......] - ETA: 0s - loss: 0.1316 - accuracy: 0.9160 - jacard_coef: 0.008615/17 [=========================>....] - ETA: 0s - loss: 0.1316 - accuracy: 0.9157 - jacard_coef: 0.008116/17 [===========================>..] - ETA: 0s - loss: 0.1316 - accuracy: 0.9153 - jacard_coef: 0.007617/17 [==============================] - 3s 180ms/step - loss: 0.1322 - accuracy: 0.9130 - jacard_coef: 0.0211 - val_loss: 0.1235 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1331 - accuracy: 0.9195 - jacard_coef: 2.3683e-12 2/17 [==>...........................] - ETA: 2s - loss: 0.1343 - accuracy: 0.8312 - jacard_coef: 0.0340     3/17 [====>.........................] - ETA: 2s - loss: 0.1349 - accuracy: 0.8405 - jacard_coef: 0.0327 4/17 [======>.......................] - ETA: 2s - loss: 0.1347 - accuracy: 0.8407 - jacard_coef: 0.0313 5/17 [=======>......................] - ETA: 2s - loss: 0.1353 - accuracy: 0.8513 - jacard_coef: 0.0276 6/17 [=========>....................] - ETA: 1s - loss: 0.1344 - accuracy: 0.8624 - jacard_coef: 0.0243 7/17 [===========>..................] - ETA: 1s - loss: 0.1348 - accuracy: 0.8650 - jacard_coef: 0.0223 8/17 [=============>................] - ETA: 1s - loss: 0.1352 - accuracy: 0.8637 - jacard_coef: 0.0243 9/17 [==============>...............] - ETA: 1s - loss: 0.1372 - accuracy: 0.8491 - jacard_coef: 0.029610/17 [================>.............] - ETA: 1s - loss: 0.1362 - accuracy: 0.8594 - jacard_coef: 0.026911/17 [==================>...........] - ETA: 1s - loss: 0.1362 - accuracy: 0.8646 - jacard_coef: 0.024512/17 [====================>.........] - ETA: 0s - loss: 0.1357 - accuracy: 0.8708 - jacard_coef: 0.022513/17 [=====================>........] - ETA: 0s - loss: 0.1358 - accuracy: 0.8735 - jacard_coef: 0.020814/17 [=======================>......] - ETA: 0s - loss: 0.1355 - accuracy: 0.8780 - jacard_coef: 0.019315/17 [=========================>....] - ETA: 0s - loss: 0.1355 - accuracy: 0.8802 - jacard_coef: 0.018016/17 [===========================>..] - ETA: 0s - loss: 0.1357 - accuracy: 0.8808 - jacard_coef: 0.016917/17 [==============================] - 3s 180ms/step - loss: 0.1356 - accuracy: 0.8816 - jacard_coef: 0.0159 - val_loss: 0.1226 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0748 (epoch 6)
  Final Val Loss: 0.1226
  Training Time: 0:01:45.463425
  Stability (std): 0.0087

Results saved to: hyperparameter_optimization_20250926_165036/exp_22_Attention_UNet_lr5e-3_bs8/Attention_UNet_lr0.005_bs8_results.json

Experiment 22 completed in 121s
Progress: 22/36 completed
Estimated remaining time: 28 minutes

ðŸ”¬ EXPERIMENT 23/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879400.171527 1136594 service.cc:145] XLA service 0x14c40e85f080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879400.171551 1136594 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879400.308352 1136594 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 6:42 - loss: 0.3420 - accuracy: 0.5080 - jacard_coef: 0.05332/9 [=====>........................] - ETA: 58s - loss: 0.3135 - accuracy: 0.4366 - jacard_coef: 0.0786 3/9 [=========>....................] - ETA: 26s - loss: 0.2896 - accuracy: 0.3834 - jacard_coef: 0.08064/9 [============>.................] - ETA: 15s - loss: 0.2690 - accuracy: 0.3356 - jacard_coef: 0.08175/9 [===============>..............] - ETA: 9s - loss: 0.2543 - accuracy: 0.3066 - jacard_coef: 0.0871 6/9 [===================>..........] - ETA: 5s - loss: 0.2446 - accuracy: 0.2822 - jacard_coef: 0.08377/9 [======================>.......] - ETA: 3s - loss: 0.2368 - accuracy: 0.2673 - jacard_coef: 0.08418/9 [=========================>....] - ETA: 1s - loss: 0.2312 - accuracy: 0.2549 - jacard_coef: 0.08149/9 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.2547 - jacard_coef: 0.09109/9 [==============================] - 69s 2s/step - loss: 0.2308 - accuracy: 0.2547 - jacard_coef: 0.0910 - val_loss: 1.1179 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1879 - accuracy: 0.1277 - jacard_coef: 0.07762/9 [=====>........................] - ETA: 2s - loss: 0.1881 - accuracy: 0.1244 - jacard_coef: 0.07633/9 [=========>....................] - ETA: 2s - loss: 0.1905 - accuracy: 0.1482 - jacard_coef: 0.08234/9 [============>.................] - ETA: 1s - loss: 0.1891 - accuracy: 0.1418 - jacard_coef: 0.08345/9 [===============>..............] - ETA: 1s - loss: 0.1884 - accuracy: 0.1409 - jacard_coef: 0.08726/9 [===================>..........] - ETA: 1s - loss: 0.1877 - accuracy: 0.1352 - jacard_coef: 0.08417/9 [======================>.......] - ETA: 0s - loss: 0.1873 - accuracy: 0.1357 - jacard_coef: 0.08488/9 [=========================>....] - ETA: 0s - loss: 0.1868 - accuracy: 0.1355 - jacard_coef: 0.08359/9 [==============================] - 3s 326ms/step - loss: 0.1867 - accuracy: 0.1355 - jacard_coef: 0.0776 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1799 - accuracy: 0.1311 - jacard_coef: 0.08632/9 [=====>........................] - ETA: 2s - loss: 0.1795 - accuracy: 0.1364 - jacard_coef: 0.09093/9 [=========>....................] - ETA: 2s - loss: 0.1794 - accuracy: 0.1427 - jacard_coef: 0.09354/9 [============>.................] - ETA: 1s - loss: 0.1792 - accuracy: 0.1424 - jacard_coef: 0.09075/9 [===============>..............] - ETA: 1s - loss: 0.1787 - accuracy: 0.1646 - jacard_coef: 0.08646/9 [===================>..........] - ETA: 1s - loss: 0.1782 - accuracy: 0.2029 - jacard_coef: 0.08527/9 [======================>.......] - ETA: 0s - loss: 0.1778 - accuracy: 0.2450 - jacard_coef: 0.08438/9 [=========================>....] - ETA: 0s - loss: 0.1777 - accuracy: 0.2792 - jacard_coef: 0.08119/9 [==============================] - 3s 331ms/step - loss: 0.1777 - accuracy: 0.2824 - jacard_coef: 0.0751 - val_loss: 1.0796 - val_accuracy: 0.9272 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1729 - accuracy: 0.6864 - jacard_coef: 0.08642/9 [=====>........................] - ETA: 2s - loss: 0.1914 - accuracy: 0.5119 - jacard_coef: 0.08603/9 [=========>....................] - ETA: 2s - loss: 0.1863 - accuracy: 0.4744 - jacard_coef: 0.09004/9 [============>.................] - ETA: 1s - loss: 0.1843 - accuracy: 0.3943 - jacard_coef: 0.08305/9 [===============>..............] - ETA: 1s - loss: 0.1832 - accuracy: 0.3437 - jacard_coef: 0.08066/9 [===================>..........] - ETA: 1s - loss: 0.1825 - accuracy: 0.3108 - jacard_coef: 0.08117/9 [======================>.......] - ETA: 0s - loss: 0.1820 - accuracy: 0.2857 - jacard_coef: 0.08018/9 [=========================>....] - ETA: 0s - loss: 0.1820 - accuracy: 0.2679 - jacard_coef: 0.08119/9 [==============================] - 3s 326ms/step - loss: 0.1819 - accuracy: 0.2665 - jacard_coef: 0.0755 - val_loss: 0.2478 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1771 - accuracy: 0.1298 - jacard_coef: 0.05572/9 [=====>........................] - ETA: 2s - loss: 0.1777 - accuracy: 0.1498 - jacard_coef: 0.07063/9 [=========>....................] - ETA: 2s - loss: 0.1779 - accuracy: 0.1801 - jacard_coef: 0.07484/9 [============>.................] - ETA: 1s - loss: 0.1772 - accuracy: 0.1943 - jacard_coef: 0.07585/9 [===============>..............] - ETA: 1s - loss: 0.1769 - accuracy: 0.1947 - jacard_coef: 0.08156/9 [===================>..........] - ETA: 1s - loss: 0.1770 - accuracy: 0.1870 - jacard_coef: 0.08067/9 [======================>.......] - ETA: 0s - loss: 0.1774 - accuracy: 0.1842 - jacard_coef: 0.08408/9 [=========================>....] - ETA: 0s - loss: 0.1775 - accuracy: 0.1795 - jacard_coef: 0.08329/9 [==============================] - 3s 326ms/step - loss: 0.1776 - accuracy: 0.1793 - jacard_coef: 0.0858 - val_loss: 0.1180 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1765 - accuracy: 0.2231 - jacard_coef: 0.10382/9 [=====>........................] - ETA: 2s - loss: 0.1786 - accuracy: 0.2851 - jacard_coef: 0.09103/9 [=========>....................] - ETA: 2s - loss: 0.1768 - accuracy: 0.3461 - jacard_coef: 0.09354/9 [============>.................] - ETA: 1s - loss: 0.1757 - accuracy: 0.4030 - jacard_coef: 0.08275/9 [===============>..............] - ETA: 1s - loss: 0.1748 - accuracy: 0.4549 - jacard_coef: 0.07886/9 [===================>..........] - ETA: 1s - loss: 0.1740 - accuracy: 0.5184 - jacard_coef: 0.07337/9 [======================>.......] - ETA: 0s - loss: 0.1731 - accuracy: 0.5709 - jacard_coef: 0.06618/9 [=========================>....] - ETA: 0s - loss: 0.1733 - accuracy: 0.5360 - jacard_coef: 0.06839/9 [==============================] - 3s 334ms/step - loss: 0.1733 - accuracy: 0.5381 - jacard_coef: 0.0612 - val_loss: 0.1352 - val_accuracy: 0.9245 - val_jacard_coef: 0.0061 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1710 - accuracy: 0.8840 - jacard_coef: 0.01602/9 [=====>........................] - ETA: 2s - loss: 0.1710 - accuracy: 0.8971 - jacard_coef: 0.01633/9 [=========>....................] - ETA: 2s - loss: 0.1712 - accuracy: 0.8877 - jacard_coef: 0.02144/9 [============>.................] - ETA: 1s - loss: 0.1714 - accuracy: 0.8555 - jacard_coef: 0.03655/9 [===============>..............] - ETA: 1s - loss: 0.1713 - accuracy: 0.8171 - jacard_coef: 0.04646/9 [===================>..........] - ETA: 1s - loss: 0.1714 - accuracy: 0.8011 - jacard_coef: 0.05387/9 [======================>.......] - ETA: 0s - loss: 0.1712 - accuracy: 0.7929 - jacard_coef: 0.05818/9 [=========================>....] - ETA: 0s - loss: 0.1709 - accuracy: 0.7958 - jacard_coef: 0.06139/9 [==============================] - 3s 327ms/step - loss: 0.1709 - accuracy: 0.7961 - jacard_coef: 0.0579 - val_loss: 0.1166 - val_accuracy: 0.9265 - val_jacard_coef: 0.0036 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1689 - accuracy: 0.9065 - jacard_coef: 0.01832/9 [=====>........................] - ETA: 2s - loss: 0.1696 - accuracy: 0.9136 - jacard_coef: 0.01223/9 [=========>....................] - ETA: 2s - loss: 0.1693 - accuracy: 0.9136 - jacard_coef: 0.01054/9 [============>.................] - ETA: 1s - loss: 0.1685 - accuracy: 0.9216 - jacard_coef: 0.00965/9 [===============>..............] - ETA: 1s - loss: 0.1681 - accuracy: 0.9173 - jacard_coef: 0.00876/9 [===================>..........] - ETA: 1s - loss: 0.1678 - accuracy: 0.9145 - jacard_coef: 0.00777/9 [======================>.......] - ETA: 0s - loss: 0.1674 - accuracy: 0.9134 - jacard_coef: 0.00718/9 [=========================>....] - ETA: 0s - loss: 0.1672 - accuracy: 0.9108 - jacard_coef: 0.00659/9 [==============================] - 3s 325ms/step - loss: 0.1671 - accuracy: 0.9108 - jacard_coef: 0.0148 - val_loss: 0.1689 - val_accuracy: 0.9301 - val_jacard_coef: 3.9257e-04 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1634 - accuracy: 0.9069 - jacard_coef: 0.00102/9 [=====>........................] - ETA: 2s - loss: 0.1625 - accuracy: 0.9128 - jacard_coef: 0.00133/9 [=========>....................] - ETA: 2s - loss: 0.1620 - accuracy: 0.9219 - jacard_coef: 0.00144/9 [============>.................] - ETA: 1s - loss: 0.1621 - accuracy: 0.9194 - jacard_coef: 0.00265/9 [===============>..............] - ETA: 1s - loss: 0.1619 - accuracy: 0.9190 - jacard_coef: 0.00496/9 [===================>..........] - ETA: 1s - loss: 0.1614 - accuracy: 0.9149 - jacard_coef: 0.00447/9 [======================>.......] - ETA: 0s - loss: 0.1610 - accuracy: 0.9124 - jacard_coef: 0.00658/9 [=========================>....] - ETA: 0s - loss: 0.1642 - accuracy: 0.8612 - jacard_coef: 0.01459/9 [==============================] - 3s 327ms/step - loss: 0.1643 - accuracy: 0.8573 - jacard_coef: 0.0132 - val_loss: 0.6478 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1654 - accuracy: 0.7839 - jacard_coef: 0.04622/9 [=====>........................] - ETA: 2s - loss: 0.1653 - accuracy: 0.8425 - jacard_coef: 0.02773/9 [=========>....................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8614 - jacard_coef: 0.01904/9 [============>.................] - ETA: 1s - loss: 0.1643 - accuracy: 0.8778 - jacard_coef: 0.01465/9 [===============>..............] - ETA: 1s - loss: 0.1647 - accuracy: 0.8812 - jacard_coef: 0.01276/9 [===================>..........] - ETA: 1s - loss: 0.1649 - accuracy: 0.8799 - jacard_coef: 0.01377/9 [======================>.......] - ETA: 0s - loss: 0.1648 - accuracy: 0.8829 - jacard_coef: 0.01488/9 [=========================>....] - ETA: 0s - loss: 0.1651 - accuracy: 0.8848 - jacard_coef: 0.01629/9 [==============================] - 3s 327ms/step - loss: 0.1651 - accuracy: 0.8836 - jacard_coef: 0.0164 - val_loss: 0.1806 - val_accuracy: 0.9304 - val_jacard_coef: 1.4608e-12 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1643 - accuracy: 0.9073 - jacard_coef: 0.00632/9 [=====>........................] - ETA: 2s - loss: 0.1636 - accuracy: 0.9082 - jacard_coef: 0.00503/9 [=========>....................] - ETA: 2s - loss: 0.1637 - accuracy: 0.9053 - jacard_coef: 0.00634/9 [============>.................] - ETA: 1s - loss: 0.1628 - accuracy: 0.9044 - jacard_coef: 0.00535/9 [===============>..............] - ETA: 1s - loss: 0.1623 - accuracy: 0.9018 - jacard_coef: 0.00466/9 [===================>..........] - ETA: 1s - loss: 0.1620 - accuracy: 0.9044 - jacard_coef: 0.00477/9 [======================>.......] - ETA: 0s - loss: 0.1615 - accuracy: 0.9065 - jacard_coef: 0.00468/9 [=========================>....] - ETA: 0s - loss: 0.1610 - accuracy: 0.9095 - jacard_coef: 0.00429/9 [==============================] - 3s 327ms/step - loss: 0.1610 - accuracy: 0.9098 - jacard_coef: 0.0037 - val_loss: 0.2483 - val_accuracy: 0.9303 - val_jacard_coef: 1.4600e-05 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1584 - accuracy: 0.8966 - jacard_coef: 0.00312/9 [=====>........................] - ETA: 2s - loss: 0.1581 - accuracy: 0.9027 - jacard_coef: 0.00243/9 [=========>....................] - ETA: 2s - loss: 0.1574 - accuracy: 0.9062 - jacard_coef: 0.00284/9 [============>.................] - ETA: 1s - loss: 0.1571 - accuracy: 0.9074 - jacard_coef: 0.00285/9 [===============>..............] - ETA: 1s - loss: 0.1570 - accuracy: 0.9104 - jacard_coef: 0.00266/9 [===================>..........] - ETA: 1s - loss: 0.1568 - accuracy: 0.9085 - jacard_coef: 0.00277/9 [======================>.......] - ETA: 0s - loss: 0.1565 - accuracy: 0.9119 - jacard_coef: 0.00318/9 [=========================>....] - ETA: 0s - loss: 0.1564 - accuracy: 0.9115 - jacard_coef: 0.00319/9 [==============================] - 3s 333ms/step - loss: 0.1564 - accuracy: 0.9112 - jacard_coef: 0.0159 - val_loss: 0.1664 - val_accuracy: 0.6298 - val_jacard_coef: 0.0775 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1548 - accuracy: 0.8916 - jacard_coef: 0.03332/9 [=====>........................] - ETA: 2s - loss: 0.1557 - accuracy: 0.8751 - jacard_coef: 0.02383/9 [=========>....................] - ETA: 2s - loss: 0.1552 - accuracy: 0.8695 - jacard_coef: 0.03914/9 [============>.................] - ETA: 1s - loss: 0.1548 - accuracy: 0.8761 - jacard_coef: 0.04515/9 [===============>..............] - ETA: 1s - loss: 0.1551 - accuracy: 0.8728 - jacard_coef: 0.04516/9 [===================>..........] - ETA: 1s - loss: 0.1548 - accuracy: 0.8734 - jacard_coef: 0.04757/9 [======================>.......] - ETA: 0s - loss: 0.1547 - accuracy: 0.8735 - jacard_coef: 0.04668/9 [=========================>....] - ETA: 0s - loss: 0.1545 - accuracy: 0.8743 - jacard_coef: 0.04709/9 [==============================] - 3s 327ms/step - loss: 0.1545 - accuracy: 0.8748 - jacard_coef: 0.0418 - val_loss: 0.1971 - val_accuracy: 0.3141 - val_jacard_coef: 0.0700 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1528 - accuracy: 0.8726 - jacard_coef: 0.05172/9 [=====>........................] - ETA: 2s - loss: 0.1523 - accuracy: 0.8868 - jacard_coef: 0.08293/9 [=========>....................] - ETA: 2s - loss: 0.1524 - accuracy: 0.8845 - jacard_coef: 0.06954/9 [============>.................] - ETA: 1s - loss: 0.1526 - accuracy: 0.8800 - jacard_coef: 0.06455/9 [===============>..............] - ETA: 1s - loss: 0.1525 - accuracy: 0.8843 - jacard_coef: 0.06136/9 [===================>..........] - ETA: 1s - loss: 0.1525 - accuracy: 0.8838 - jacard_coef: 0.05817/9 [======================>.......] - ETA: 0s - loss: 0.1524 - accuracy: 0.8862 - jacard_coef: 0.05388/9 [=========================>....] - ETA: 0s - loss: 0.1522 - accuracy: 0.8860 - jacard_coef: 0.04769/9 [==============================] - 3s 327ms/step - loss: 0.1531 - accuracy: 0.8831 - jacard_coef: 0.0563 - val_loss: 0.1061 - val_accuracy: 0.9302 - val_jacard_coef: 1.3108e-04 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1534 - accuracy: 0.8939 - jacard_coef: 3.0539e-042/9 [=====>........................] - ETA: 2s - loss: 0.1549 - accuracy: 0.8957 - jacard_coef: 6.4094e-043/9 [=========>....................] - ETA: 2s - loss: 0.1554 - accuracy: 0.9042 - jacard_coef: 8.0552e-044/9 [============>.................] - ETA: 1s - loss: 0.1553 - accuracy: 0.9094 - jacard_coef: 6.3908e-045/9 [===============>..............] - ETA: 1s - loss: 0.1551 - accuracy: 0.9160 - jacard_coef: 5.1126e-046/9 [===================>..........] - ETA: 1s - loss: 0.1554 - accuracy: 0.9182 - jacard_coef: 4.7105e-047/9 [======================>.......] - ETA: 0s - loss: 0.1554 - accuracy: 0.9171 - jacard_coef: 4.3270e-048/9 [=========================>....] - ETA: 0s - loss: 0.1555 - accuracy: 0.9163 - jacard_coef: 4.4921e-049/9 [==============================] - 3s 327ms/step - loss: 0.1555 - accuracy: 0.9169 - jacard_coef: 3.9930e-04 - val_loss: 0.9168 - val_accuracy: 0.9304 - val_jacard_coef: 1.4607e-12 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1555 - accuracy: 0.8984 - jacard_coef: 9.3891e-132/9 [=====>........................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9173 - jacard_coef: 2.2377e-053/9 [=========>....................] - ETA: 2s - loss: 0.1554 - accuracy: 0.9190 - jacard_coef: 6.1665e-044/9 [============>.................] - ETA: 1s - loss: 0.1558 - accuracy: 0.9149 - jacard_coef: 0.0010    5/9 [===============>..............] - ETA: 1s - loss: 0.1555 - accuracy: 0.9141 - jacard_coef: 8.2539e-046/9 [===================>..........] - ETA: 1s - loss: 0.1554 - accuracy: 0.9130 - jacard_coef: 6.9126e-047/9 [======================>.......] - ETA: 0s - loss: 0.1550 - accuracy: 0.9170 - jacard_coef: 7.5857e-048/9 [=========================>....] - ETA: 0s - loss: 0.1551 - accuracy: 0.9149 - jacard_coef: 8.1626e-049/9 [==============================] - 3s 327ms/step - loss: 0.1551 - accuracy: 0.9154 - jacard_coef: 7.2556e-04 - val_loss: 0.4737 - val_accuracy: 0.9274 - val_jacard_coef: 7.0098e-05 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1549 - accuracy: 0.9122 - jacard_coef: 0.00322/9 [=====>........................] - ETA: 2s - loss: 0.1547 - accuracy: 0.9092 - jacard_coef: 0.00173/9 [=========>....................] - ETA: 2s - loss: 0.1536 - accuracy: 0.9174 - jacard_coef: 0.00114/9 [============>.................] - ETA: 1s - loss: 0.1535 - accuracy: 0.9168 - jacard_coef: 8.6119e-045/9 [===============>..............] - ETA: 1s - loss: 0.1536 - accuracy: 0.9127 - jacard_coef: 9.5554e-046/9 [===================>..........] - ETA: 1s - loss: 0.1536 - accuracy: 0.9100 - jacard_coef: 7.9629e-047/9 [======================>.......] - ETA: 0s - loss: 0.1537 - accuracy: 0.9097 - jacard_coef: 0.0012    8/9 [=========================>....] - ETA: 0s - loss: 0.1537 - accuracy: 0.9120 - jacard_coef: 0.00169/9 [==============================] - 3s 327ms/step - loss: 0.1537 - accuracy: 0.9122 - jacard_coef: 0.0014 - val_loss: 0.2796 - val_accuracy: 0.9304 - val_jacard_coef: 1.4607e-12 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1545 - accuracy: 0.8893 - jacard_coef: 0.00182/9 [=====>........................] - ETA: 2s - loss: 0.1532 - accuracy: 0.9032 - jacard_coef: 0.00133/9 [=========>....................] - ETA: 2s - loss: 0.1530 - accuracy: 0.9078 - jacard_coef: 9.6672e-044/9 [============>.................] - ETA: 1s - loss: 0.1524 - accuracy: 0.9186 - jacard_coef: 7.2504e-045/9 [===============>..............] - ETA: 1s - loss: 0.1530 - accuracy: 0.9183 - jacard_coef: 0.0015    6/9 [===================>..........] - ETA: 1s - loss: 0.1525 - accuracy: 0.9211 - jacard_coef: 0.00157/9 [======================>.......] - ETA: 0s - loss: 0.1526 - accuracy: 0.9162 - jacard_coef: 0.00138/9 [=========================>....] - ETA: 0s - loss: 0.1528 - accuracy: 0.9133 - jacard_coef: 0.00139/9 [==============================] - 3s 327ms/step - loss: 0.1528 - accuracy: 0.9137 - jacard_coef: 0.0023 - val_loss: 0.2145 - val_accuracy: 0.9304 - val_jacard_coef: 1.4608e-12 - lr: 2.5000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1545 - accuracy: 0.8977 - jacard_coef: 0.00112/9 [=====>........................] - ETA: 2s - loss: 0.1541 - accuracy: 0.8989 - jacard_coef: 0.00123/9 [=========>....................] - ETA: 2s - loss: 0.1533 - accuracy: 0.9015 - jacard_coef: 7.9554e-044/9 [============>.................] - ETA: 1s - loss: 0.1527 - accuracy: 0.9060 - jacard_coef: 7.9507e-045/9 [===============>..............] - ETA: 1s - loss: 0.1525 - accuracy: 0.9093 - jacard_coef: 8.5924e-046/9 [===================>..........] - ETA: 1s - loss: 0.1522 - accuracy: 0.9131 - jacard_coef: 7.2072e-047/9 [======================>.......] - ETA: 0s - loss: 0.1520 - accuracy: 0.9156 - jacard_coef: 6.1776e-048/9 [=========================>....] - ETA: 0s - loss: 0.1518 - accuracy: 0.9161 - jacard_coef: 5.9110e-049/9 [==============================] - 3s 326ms/step - loss: 0.1519 - accuracy: 0.9156 - jacard_coef: 7.4979e-04 - val_loss: 0.1663 - val_accuracy: 0.9304 - val_jacard_coef: 1.4609e-12 - lr: 2.5000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1515 - accuracy: 0.8956 - jacard_coef: 1.0958e-042/9 [=====>........................] - ETA: 2s - loss: 0.1517 - accuracy: 0.9006 - jacard_coef: 1.1034e-043/9 [=========>....................] - ETA: 2s - loss: 0.1515 - accuracy: 0.9100 - jacard_coef: 2.1187e-044/9 [============>.................] - ETA: 1s - loss: 0.1511 - accuracy: 0.9119 - jacard_coef: 1.6469e-045/9 [===============>..............] - ETA: 1s - loss: 0.1511 - accuracy: 0.9125 - jacard_coef: 1.3175e-046/9 [===================>..........] - ETA: 1s - loss: 0.1506 - accuracy: 0.9189 - jacard_coef: 1.0979e-047/9 [======================>.......] - ETA: 0s - loss: 0.1511 - accuracy: 0.9160 - jacard_coef: 2.2879e-048/9 [=========================>....] - ETA: 0s - loss: 0.1511 - accuracy: 0.9164 - jacard_coef: 2.0019e-049/9 [==============================] - 3s 327ms/step - loss: 0.1512 - accuracy: 0.9130 - jacard_coef: 0.0036 - val_loss: 0.1165 - val_accuracy: 0.9304 - val_jacard_coef: 1.4609e-12 - lr: 2.5000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1505 - accuracy: 0.9096 - jacard_coef: 1.0552e-122/9 [=====>........................] - ETA: 2s - loss: 0.1504 - accuracy: 0.9101 - jacard_coef: 1.0606e-123/9 [=========>....................] - ETA: 2s - loss: 0.1505 - accuracy: 0.9100 - jacard_coef: 5.6258e-044/9 [============>.................] - ETA: 1s - loss: 0.1503 - accuracy: 0.9110 - jacard_coef: 4.2194e-045/9 [===============>..............] - ETA: 1s - loss: 0.1504 - accuracy: 0.9098 - jacard_coef: 3.8357e-046/9 [===================>..........] - ETA: 1s - loss: 0.1508 - accuracy: 0.9094 - jacard_coef: 0.0016    7/9 [======================>.......] - ETA: 0s - loss: 0.1505 - accuracy: 0.9126 - jacard_coef: 0.00168/9 [=========================>....] - ETA: 0s - loss: 0.1504 - accuracy: 0.9135 - jacard_coef: 0.00159/9 [==============================] - 3s 327ms/step - loss: 0.1504 - accuracy: 0.9138 - jacard_coef: 0.0014 - val_loss: 0.1060 - val_accuracy: 0.9236 - val_jacard_coef: 0.0045 - lr: 2.5000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1503 - accuracy: 0.9001 - jacard_coef: 2.3862e-042/9 [=====>........................] - ETA: 2s - loss: 0.1510 - accuracy: 0.9056 - jacard_coef: 0.0023    3/9 [=========>....................] - ETA: 2s - loss: 0.1505 - accuracy: 0.9102 - jacard_coef: 0.00214/9 [============>.................] - ETA: 1s - loss: 0.1504 - accuracy: 0.9094 - jacard_coef: 0.00175/9 [===============>..............] - ETA: 1s - loss: 0.1500 - accuracy: 0.9129 - jacard_coef: 0.00156/9 [===================>..........] - ETA: 1s - loss: 0.1499 - accuracy: 0.9143 - jacard_coef: 0.00137/9 [======================>.......] - ETA: 0s - loss: 0.1497 - accuracy: 0.9160 - jacard_coef: 0.00128/9 [=========================>....] - ETA: 0s - loss: 0.1497 - accuracy: 0.9155 - jacard_coef: 0.00109/9 [==============================] - 3s 328ms/step - loss: 0.1499 - accuracy: 0.9148 - jacard_coef: 0.0074 - val_loss: 0.1095 - val_accuracy: 0.9300 - val_jacard_coef: 3.4848e-04 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0775 (epoch 12)
  Final Val Loss: 0.1095
  Training Time: 0:02:11.775614
  Stability (std): 0.2413

Results saved to: hyperparameter_optimization_20250926_165036/exp_23_Attention_UNet_lr5e-3_bs16/Attention_UNet_lr0.005_bs16_results.json

Experiment 23 completed in 148s
Progress: 23/36 completed
Estimated remaining time: 32 minutes

ðŸ”¬ EXPERIMENT 24/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879553.809687 1143929 service.cc:145] XLA service 0x14e25e3152b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879553.809710 1143929 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879553.948135 1143929 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:02 - loss: 0.3393 - accuracy: 0.4962 - jacard_coef: 0.07132/5 [===========>..................] - ETA: 45s - loss: 0.3009 - accuracy: 0.4405 - jacard_coef: 0.0723 3/5 [=================>............] - ETA: 15s - loss: 0.2728 - accuracy: 0.4257 - jacard_coef: 0.07374/5 [=======================>......] - ETA: 5s - loss: 0.2552 - accuracy: 0.3901 - jacard_coef: 0.0752 5/5 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.3881 - jacard_coef: 0.06685/5 [==============================] - 85s 6s/step - loss: 0.2547 - accuracy: 0.3881 - jacard_coef: 0.0668 - val_loss: 0.0929 - val_accuracy: 0.9290 - val_jacard_coef: 0.0025 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1909 - accuracy: 0.1498 - jacard_coef: 0.08622/5 [===========>..................] - ETA: 2s - loss: 0.1899 - accuracy: 0.1452 - jacard_coef: 0.08653/5 [=================>............] - ETA: 1s - loss: 0.1904 - accuracy: 0.1426 - jacard_coef: 0.08664/5 [=======================>......] - ETA: 0s - loss: 0.1892 - accuracy: 0.1481 - jacard_coef: 0.08345/5 [==============================] - 3s 572ms/step - loss: 0.1894 - accuracy: 0.1492 - jacard_coef: 0.0775 - val_loss: 1.0548 - val_accuracy: 0.9201 - val_jacard_coef: 0.0075 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1988 - accuracy: 0.2835 - jacard_coef: 0.06962/5 [===========>..................] - ETA: 2s - loss: 0.1973 - accuracy: 0.2325 - jacard_coef: 0.07313/5 [=================>............] - ETA: 1s - loss: 0.1974 - accuracy: 0.2065 - jacard_coef: 0.07514/5 [=======================>......] - ETA: 0s - loss: 0.1968 - accuracy: 0.1907 - jacard_coef: 0.08165/5 [==============================] - 3s 557ms/step - loss: 0.1967 - accuracy: 0.1904 - jacard_coef: 0.0877 - val_loss: 1.1446 - val_accuracy: 0.9269 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1907 - accuracy: 0.1643 - jacard_coef: 0.06712/5 [===========>..................] - ETA: 2s - loss: 0.1900 - accuracy: 0.2104 - jacard_coef: 0.08163/5 [=================>............] - ETA: 1s - loss: 0.1901 - accuracy: 0.2552 - jacard_coef: 0.07994/5 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.2875 - jacard_coef: 0.08315/5 [==============================] - 3s 558ms/step - loss: 0.1888 - accuracy: 0.2877 - jacard_coef: 0.0667 - val_loss: 1.1368 - val_accuracy: 0.9269 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1853 - accuracy: 0.2870 - jacard_coef: 0.06532/5 [===========>..................] - ETA: 2s - loss: 0.1853 - accuracy: 0.2738 - jacard_coef: 0.08243/5 [=================>............] - ETA: 1s - loss: 0.1849 - accuracy: 0.2547 - jacard_coef: 0.08214/5 [=======================>......] - ETA: 0s - loss: 0.1849 - accuracy: 0.2403 - jacard_coef: 0.08475/5 [==============================] - 3s 558ms/step - loss: 0.1849 - accuracy: 0.2402 - jacard_coef: 0.0972 - val_loss: 1.1233 - val_accuracy: 0.9265 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1821 - accuracy: 0.1733 - jacard_coef: 0.09332/5 [===========>..................] - ETA: 2s - loss: 0.1829 - accuracy: 0.1715 - jacard_coef: 0.09233/5 [=================>............] - ETA: 1s - loss: 0.1825 - accuracy: 0.1720 - jacard_coef: 0.08424/5 [=======================>......] - ETA: 0s - loss: 0.1820 - accuracy: 0.1822 - jacard_coef: 0.08395/5 [==============================] - 3s 557ms/step - loss: 0.1820 - accuracy: 0.1823 - jacard_coef: 0.0740 - val_loss: 0.6648 - val_accuracy: 0.9163 - val_jacard_coef: 0.0073 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1801 - accuracy: 0.2392 - jacard_coef: 0.09842/5 [===========>..................] - ETA: 2s - loss: 0.1789 - accuracy: 0.2607 - jacard_coef: 0.09283/5 [=================>............] - ETA: 1s - loss: 0.1777 - accuracy: 0.3127 - jacard_coef: 0.08194/5 [=======================>......] - ETA: 0s - loss: 0.1766 - accuracy: 0.3492 - jacard_coef: 0.08115/5 [==============================] - 3s 572ms/step - loss: 0.1766 - accuracy: 0.3492 - jacard_coef: 0.0656 - val_loss: 0.2835 - val_accuracy: 0.9002 - val_jacard_coef: 0.0149 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1724 - accuracy: 0.4866 - jacard_coef: 0.07322/5 [===========>..................] - ETA: 2s - loss: 0.1732 - accuracy: 0.4495 - jacard_coef: 0.07163/5 [=================>............] - ETA: 1s - loss: 0.1733 - accuracy: 0.4469 - jacard_coef: 0.08194/5 [=======================>......] - ETA: 0s - loss: 0.1733 - accuracy: 0.4444 - jacard_coef: 0.08455/5 [==============================] - 3s 569ms/step - loss: 0.1733 - accuracy: 0.4441 - jacard_coef: 0.0688 - val_loss: 0.1406 - val_accuracy: 0.9073 - val_jacard_coef: 0.0161 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1706 - accuracy: 0.6372 - jacard_coef: 0.07232/5 [===========>..................] - ETA: 2s - loss: 0.1703 - accuracy: 0.6803 - jacard_coef: 0.07843/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.6998 - jacard_coef: 0.07344/5 [=======================>......] - ETA: 0s - loss: 0.1702 - accuracy: 0.7081 - jacard_coef: 0.07025/5 [==============================] - 3s 559ms/step - loss: 0.1702 - accuracy: 0.7073 - jacard_coef: 0.0740 - val_loss: 0.4146 - val_accuracy: 0.9192 - val_jacard_coef: 0.0084 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.7156 - jacard_coef: 0.05472/5 [===========>..................] - ETA: 2s - loss: 0.1688 - accuracy: 0.6793 - jacard_coef: 0.05953/5 [=================>............] - ETA: 1s - loss: 0.1680 - accuracy: 0.7021 - jacard_coef: 0.06184/5 [=======================>......] - ETA: 0s - loss: 0.1675 - accuracy: 0.7162 - jacard_coef: 0.06225/5 [==============================] - 3s 559ms/step - loss: 0.1675 - accuracy: 0.7166 - jacard_coef: 0.0497 - val_loss: 0.2315 - val_accuracy: 0.9191 - val_jacard_coef: 0.0082 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1647 - accuracy: 0.8494 - jacard_coef: 0.03562/5 [===========>..................] - ETA: 2s - loss: 0.1646 - accuracy: 0.8692 - jacard_coef: 0.02963/5 [=================>............] - ETA: 1s - loss: 0.1643 - accuracy: 0.8851 - jacard_coef: 0.02484/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.8882 - jacard_coef: 0.02355/5 [==============================] - 3s 558ms/step - loss: 0.1644 - accuracy: 0.8841 - jacard_coef: 0.0289 - val_loss: 0.1388 - val_accuracy: 0.9222 - val_jacard_coef: 0.0047 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1677 - accuracy: 0.8854 - jacard_coef: 0.01512/5 [===========>..................] - ETA: 2s - loss: 0.1684 - accuracy: 0.8909 - jacard_coef: 0.01733/5 [=================>............] - ETA: 1s - loss: 0.1696 - accuracy: 0.8753 - jacard_coef: 0.02634/5 [=======================>......] - ETA: 0s - loss: 0.1702 - accuracy: 0.8458 - jacard_coef: 0.03605/5 [==============================] - 3s 558ms/step - loss: 0.1703 - accuracy: 0.8442 - jacard_coef: 0.0573 - val_loss: 0.1056 - val_accuracy: 0.9208 - val_jacard_coef: 0.0097 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1737 - accuracy: 0.4949 - jacard_coef: 0.07972/5 [===========>..................] - ETA: 2s - loss: 0.1735 - accuracy: 0.4478 - jacard_coef: 0.08023/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.4305 - jacard_coef: 0.07964/5 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.4326 - jacard_coef: 0.07655/5 [==============================] - 3s 558ms/step - loss: 0.1732 - accuracy: 0.4351 - jacard_coef: 0.0614 - val_loss: 0.1647 - val_accuracy: 0.9280 - val_jacard_coef: 0.0018 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1700 - accuracy: 0.7618 - jacard_coef: 0.05322/5 [===========>..................] - ETA: 2s - loss: 0.1695 - accuracy: 0.7848 - jacard_coef: 0.05973/5 [=================>............] - ETA: 1s - loss: 0.1687 - accuracy: 0.8110 - jacard_coef: 0.05894/5 [=======================>......] - ETA: 0s - loss: 0.1679 - accuracy: 0.8290 - jacard_coef: 0.04895/5 [==============================] - 3s 557ms/step - loss: 0.1679 - accuracy: 0.8296 - jacard_coef: 0.0392 - val_loss: 0.1351 - val_accuracy: 0.9279 - val_jacard_coef: 0.0021 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1633 - accuracy: 0.9144 - jacard_coef: 0.00872/5 [===========>..................] - ETA: 2s - loss: 0.1629 - accuracy: 0.9063 - jacard_coef: 0.00683/5 [=================>............] - ETA: 1s - loss: 0.1624 - accuracy: 0.9115 - jacard_coef: 0.00604/5 [=======================>......] - ETA: 0s - loss: 0.1621 - accuracy: 0.9099 - jacard_coef: 0.00545/5 [==============================] - 3s 557ms/step - loss: 0.1621 - accuracy: 0.9103 - jacard_coef: 0.0043 - val_loss: 0.0970 - val_accuracy: 0.9285 - val_jacard_coef: 0.0011 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1596 - accuracy: 0.9055 - jacard_coef: 5.3986e-042/5 [===========>..................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8688 - jacard_coef: 0.0205    3/5 [=================>............] - ETA: 1s - loss: 0.1594 - accuracy: 0.8626 - jacard_coef: 0.04174/5 [=======================>......] - ETA: 0s - loss: 0.1594 - accuracy: 0.8764 - jacard_coef: 0.03155/5 [==============================] - 3s 557ms/step - loss: 0.1601 - accuracy: 0.8718 - jacard_coef: 0.0321 - val_loss: 0.0902 - val_accuracy: 0.9275 - val_jacard_coef: 0.0015 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1602 - accuracy: 0.9111 - jacard_coef: 0.00512/5 [===========>..................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9075 - jacard_coef: 0.00653/5 [=================>............] - ETA: 1s - loss: 0.1605 - accuracy: 0.9069 - jacard_coef: 0.00684/5 [=======================>......] - ETA: 0s - loss: 0.1604 - accuracy: 0.9082 - jacard_coef: 0.00705/5 [==============================] - 3s 558ms/step - loss: 0.1604 - accuracy: 0.9083 - jacard_coef: 0.0115 - val_loss: 0.0840 - val_accuracy: 0.9145 - val_jacard_coef: 0.0136 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1602 - accuracy: 0.9004 - jacard_coef: 0.01272/5 [===========>..................] - ETA: 2s - loss: 0.1605 - accuracy: 0.8956 - jacard_coef: 0.01413/5 [=================>............] - ETA: 1s - loss: 0.1602 - accuracy: 0.9003 - jacard_coef: 0.01374/5 [=======================>......] - ETA: 0s - loss: 0.1601 - accuracy: 0.9008 - jacard_coef: 0.01455/5 [==============================] - 3s 558ms/step - loss: 0.1601 - accuracy: 0.9001 - jacard_coef: 0.0198 - val_loss: 0.1445 - val_accuracy: 0.9218 - val_jacard_coef: 0.0078 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0161 (epoch 8)
  Final Val Loss: 0.1445
  Training Time: 0:02:15.649909
  Stability (std): 0.0943

Results saved to: hyperparameter_optimization_20250926_165036/exp_24_Attention_UNet_lr5e-3_bs32/Attention_UNet_lr0.005_bs32_results.json

Experiment 24 completed in 152s
Progress: 24/36 completed
Estimated remaining time: 30 minutes

ðŸ”¬ EXPERIMENT 25/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879702.986073 1150466 service.cc:145] XLA service 0x1529189ebfe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879702.986098 1150466 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879703.123615 1150466 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 13:53 - loss: 0.3449 - accuracy: 0.5191 - jacard_coef: 0.0852 2/17 [==>...........................] - ETA: 1:08 - loss: 0.3197 - accuracy: 0.4794 - jacard_coef: 0.0923  3/17 [====>.........................] - ETA: 33s - loss: 0.2989 - accuracy: 0.4200 - jacard_coef: 0.0872  4/17 [======>.......................] - ETA: 21s - loss: 0.2799 - accuracy: 0.3500 - jacard_coef: 0.0855 5/17 [=======>......................] - ETA: 15s - loss: 0.2660 - accuracy: 0.3050 - jacard_coef: 0.0837 6/17 [=========>....................] - ETA: 11s - loss: 0.2557 - accuracy: 0.2790 - jacard_coef: 0.0854 7/17 [===========>..................] - ETA: 9s - loss: 0.2473 - accuracy: 0.2622 - jacard_coef: 0.0828  8/17 [=============>................] - ETA: 7s - loss: 0.2409 - accuracy: 0.2509 - jacard_coef: 0.0803 9/17 [==============>...............] - ETA: 6s - loss: 0.2352 - accuracy: 0.2452 - jacard_coef: 0.083510/17 [================>.............] - ETA: 4s - loss: 0.2300 - accuracy: 0.2463 - jacard_coef: 0.085511/17 [==================>...........] - ETA: 3s - loss: 0.2256 - accuracy: 0.2508 - jacard_coef: 0.082012/17 [====================>.........] - ETA: 3s - loss: 0.2218 - accuracy: 0.2575 - jacard_coef: 0.082313/17 [=====================>........] - ETA: 2s - loss: 0.2183 - accuracy: 0.2665 - jacard_coef: 0.082014/17 [=======================>......] - ETA: 1s - loss: 0.2158 - accuracy: 0.2748 - jacard_coef: 0.083415/17 [=========================>....] - ETA: 1s - loss: 0.2130 - accuracy: 0.2874 - jacard_coef: 0.082916/17 [===========================>..] - ETA: 0s - loss: 0.2108 - accuracy: 0.2982 - jacard_coef: 0.083617/17 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.2983 - jacard_coef: 0.081217/17 [==============================] - 67s 934ms/step - loss: 0.2106 - accuracy: 0.2983 - jacard_coef: 0.0812 - val_loss: 1.0480 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1787 - accuracy: 0.4339 - jacard_coef: 0.0615 2/17 [==>...........................] - ETA: 3s - loss: 0.1781 - accuracy: 0.4010 - jacard_coef: 0.0860 3/17 [====>.........................] - ETA: 2s - loss: 0.1779 - accuracy: 0.4171 - jacard_coef: 0.0805 4/17 [======>.......................] - ETA: 2s - loss: 0.1770 - accuracy: 0.4452 - jacard_coef: 0.0820 5/17 [=======>......................] - ETA: 2s - loss: 0.1772 - accuracy: 0.4250 - jacard_coef: 0.0894 6/17 [=========>....................] - ETA: 2s - loss: 0.1774 - accuracy: 0.4495 - jacard_coef: 0.0817 7/17 [===========>..................] - ETA: 2s - loss: 0.1773 - accuracy: 0.4701 - jacard_coef: 0.0800 8/17 [=============>................] - ETA: 1s - loss: 0.1771 - accuracy: 0.4820 - jacard_coef: 0.0807 9/17 [==============>...............] - ETA: 1s - loss: 0.1769 - accuracy: 0.4804 - jacard_coef: 0.081510/17 [================>.............] - ETA: 1s - loss: 0.1766 - accuracy: 0.4724 - jacard_coef: 0.085011/17 [==================>...........] - ETA: 1s - loss: 0.1763 - accuracy: 0.4725 - jacard_coef: 0.082312/17 [====================>.........] - ETA: 1s - loss: 0.1764 - accuracy: 0.4736 - jacard_coef: 0.080713/17 [=====================>........] - ETA: 0s - loss: 0.1761 - accuracy: 0.4807 - jacard_coef: 0.079414/17 [=======================>......] - ETA: 0s - loss: 0.1758 - accuracy: 0.4858 - jacard_coef: 0.079715/17 [=========================>....] - ETA: 0s - loss: 0.1755 - accuracy: 0.4962 - jacard_coef: 0.080316/17 [===========================>..] - ETA: 0s - loss: 0.1752 - accuracy: 0.5042 - jacard_coef: 0.080217/17 [==============================] - 3s 205ms/step - loss: 0.1752 - accuracy: 0.5035 - jacard_coef: 0.0789 - val_loss: 0.8912 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1713 - accuracy: 0.5805 - jacard_coef: 0.1166 2/17 [==>...........................] - ETA: 3s - loss: 0.1697 - accuracy: 0.6281 - jacard_coef: 0.1201 3/17 [====>.........................] - ETA: 2s - loss: 0.1703 - accuracy: 0.6197 - jacard_coef: 0.1019 4/17 [======>.......................] - ETA: 2s - loss: 0.1701 - accuracy: 0.6401 - jacard_coef: 0.1063 5/17 [=======>......................] - ETA: 2s - loss: 0.1700 - accuracy: 0.6528 - jacard_coef: 0.0983 6/17 [=========>....................] - ETA: 2s - loss: 0.1700 - accuracy: 0.6546 - jacard_coef: 0.0966 7/17 [===========>..................] - ETA: 2s - loss: 0.1697 - accuracy: 0.6620 - jacard_coef: 0.0887 8/17 [=============>................] - ETA: 1s - loss: 0.1699 - accuracy: 0.6654 - jacard_coef: 0.0829 9/17 [==============>...............] - ETA: 1s - loss: 0.1729 - accuracy: 0.6178 - jacard_coef: 0.083310/17 [================>.............] - ETA: 1s - loss: 0.1726 - accuracy: 0.6331 - jacard_coef: 0.082611/17 [==================>...........] - ETA: 1s - loss: 0.1724 - accuracy: 0.6464 - jacard_coef: 0.082112/17 [====================>.........] - ETA: 1s - loss: 0.1725 - accuracy: 0.6450 - jacard_coef: 0.077113/17 [=====================>........] - ETA: 0s - loss: 0.1725 - accuracy: 0.6484 - jacard_coef: 0.074814/17 [=======================>......] - ETA: 0s - loss: 0.1726 - accuracy: 0.6492 - jacard_coef: 0.076015/17 [=========================>....] - ETA: 0s - loss: 0.1726 - accuracy: 0.6403 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1725 - accuracy: 0.6342 - jacard_coef: 0.078217/17 [==============================] - 4s 209ms/step - loss: 0.1725 - accuracy: 0.6323 - jacard_coef: 0.0759 - val_loss: 0.9188 - val_accuracy: 0.8200 - val_jacard_coef: 0.0739 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1743 - accuracy: 0.6502 - jacard_coef: 0.0747 2/17 [==>...........................] - ETA: 3s - loss: 0.1715 - accuracy: 0.7104 - jacard_coef: 0.0555 3/17 [====>.........................] - ETA: 2s - loss: 0.1700 - accuracy: 0.7084 - jacard_coef: 0.0712 4/17 [======>.......................] - ETA: 2s - loss: 0.1692 - accuracy: 0.6924 - jacard_coef: 0.0649 5/17 [=======>......................] - ETA: 2s - loss: 0.1688 - accuracy: 0.6768 - jacard_coef: 0.0706 6/17 [=========>....................] - ETA: 2s - loss: 0.1686 - accuracy: 0.6676 - jacard_coef: 0.0772 7/17 [===========>..................] - ETA: 2s - loss: 0.1681 - accuracy: 0.6825 - jacard_coef: 0.0825 8/17 [=============>................] - ETA: 1s - loss: 0.1676 - accuracy: 0.6953 - jacard_coef: 0.0801 9/17 [==============>...............] - ETA: 1s - loss: 0.1689 - accuracy: 0.6702 - jacard_coef: 0.079910/17 [================>.............] - ETA: 1s - loss: 0.1688 - accuracy: 0.6867 - jacard_coef: 0.075811/17 [==================>...........] - ETA: 1s - loss: 0.1689 - accuracy: 0.6902 - jacard_coef: 0.075312/17 [====================>.........] - ETA: 1s - loss: 0.1694 - accuracy: 0.6798 - jacard_coef: 0.072613/17 [=====================>........] - ETA: 0s - loss: 0.1697 - accuracy: 0.6637 - jacard_coef: 0.073414/17 [=======================>......] - ETA: 0s - loss: 0.1700 - accuracy: 0.6361 - jacard_coef: 0.071015/17 [=========================>....] - ETA: 0s - loss: 0.1702 - accuracy: 0.6199 - jacard_coef: 0.072716/17 [===========================>..] - ETA: 0s - loss: 0.1702 - accuracy: 0.6091 - jacard_coef: 0.075317/17 [==============================] - 3s 205ms/step - loss: 0.1703 - accuracy: 0.6056 - jacard_coef: 0.0731 - val_loss: 0.7284 - val_accuracy: 0.9271 - val_jacard_coef: 0.0038 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1710 - accuracy: 0.6027 - jacard_coef: 0.0705 2/17 [==>...........................] - ETA: 3s - loss: 0.1702 - accuracy: 0.6443 - jacard_coef: 0.0736 3/17 [====>.........................] - ETA: 2s - loss: 0.1701 - accuracy: 0.6747 - jacard_coef: 0.0754 4/17 [======>.......................] - ETA: 2s - loss: 0.1694 - accuracy: 0.7165 - jacard_coef: 0.0619 5/17 [=======>......................] - ETA: 2s - loss: 0.1692 - accuracy: 0.7384 - jacard_coef: 0.0566 6/17 [=========>....................] - ETA: 2s - loss: 0.1694 - accuracy: 0.7449 - jacard_coef: 0.0595 7/17 [===========>..................] - ETA: 2s - loss: 0.1692 - accuracy: 0.7515 - jacard_coef: 0.0586 8/17 [=============>................] - ETA: 1s - loss: 0.1688 - accuracy: 0.7600 - jacard_coef: 0.0597 9/17 [==============>...............] - ETA: 1s - loss: 0.1689 - accuracy: 0.7583 - jacard_coef: 0.061410/17 [================>.............] - ETA: 1s - loss: 0.1688 - accuracy: 0.7638 - jacard_coef: 0.063611/17 [==================>...........] - ETA: 1s - loss: 0.1690 - accuracy: 0.7670 - jacard_coef: 0.060512/17 [====================>.........] - ETA: 1s - loss: 0.1685 - accuracy: 0.7747 - jacard_coef: 0.066613/17 [=====================>........] - ETA: 0s - loss: 0.1681 - accuracy: 0.7796 - jacard_coef: 0.064714/17 [=======================>......] - ETA: 0s - loss: 0.1675 - accuracy: 0.7835 - jacard_coef: 0.065015/17 [=========================>....] - ETA: 0s - loss: 0.1673 - accuracy: 0.7798 - jacard_coef: 0.063916/17 [===========================>..] - ETA: 0s - loss: 0.1669 - accuracy: 0.7798 - jacard_coef: 0.062817/17 [==============================] - 3s 205ms/step - loss: 0.1669 - accuracy: 0.7796 - jacard_coef: 0.0617 - val_loss: 0.0962 - val_accuracy: 0.8967 - val_jacard_coef: 0.0191 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1592 - accuracy: 0.8418 - jacard_coef: 0.0443 2/17 [==>...........................] - ETA: 3s - loss: 0.1587 - accuracy: 0.8428 - jacard_coef: 0.0433 3/17 [====>.........................] - ETA: 2s - loss: 0.1590 - accuracy: 0.8418 - jacard_coef: 0.0478 4/17 [======>.......................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8541 - jacard_coef: 0.0390 5/17 [=======>......................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8626 - jacard_coef: 0.0449 6/17 [=========>....................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8589 - jacard_coef: 0.0414 7/17 [===========>..................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8585 - jacard_coef: 0.0429 8/17 [=============>................] - ETA: 1s - loss: 0.1584 - accuracy: 0.8640 - jacard_coef: 0.0485 9/17 [==============>...............] - ETA: 1s - loss: 0.1583 - accuracy: 0.8639 - jacard_coef: 0.051910/17 [================>.............] - ETA: 1s - loss: 0.1583 - accuracy: 0.8605 - jacard_coef: 0.050211/17 [==================>...........] - ETA: 1s - loss: 0.1584 - accuracy: 0.8566 - jacard_coef: 0.051112/17 [====================>.........] - ETA: 1s - loss: 0.1580 - accuracy: 0.8615 - jacard_coef: 0.048413/17 [=====================>........] - ETA: 0s - loss: 0.1581 - accuracy: 0.8664 - jacard_coef: 0.045814/17 [=======================>......] - ETA: 0s - loss: 0.1580 - accuracy: 0.8670 - jacard_coef: 0.044515/17 [=========================>....] - ETA: 0s - loss: 0.1578 - accuracy: 0.8705 - jacard_coef: 0.042016/17 [===========================>..] - ETA: 0s - loss: 0.1576 - accuracy: 0.8737 - jacard_coef: 0.039417/17 [==============================] - 3s 205ms/step - loss: 0.1577 - accuracy: 0.8711 - jacard_coef: 0.0475 - val_loss: 0.0930 - val_accuracy: 0.8617 - val_jacard_coef: 0.0115 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1584 - accuracy: 0.7056 - jacard_coef: 0.0731 2/17 [==>...........................] - ETA: 3s - loss: 0.1562 - accuracy: 0.8015 - jacard_coef: 0.0462 3/17 [====>.........................] - ETA: 2s - loss: 0.1572 - accuracy: 0.8149 - jacard_coef: 0.0396 4/17 [======>.......................] - ETA: 2s - loss: 0.1559 - accuracy: 0.8040 - jacard_coef: 0.0350 5/17 [=======>......................] - ETA: 2s - loss: 0.1570 - accuracy: 0.7688 - jacard_coef: 0.0352 6/17 [=========>....................] - ETA: 2s - loss: 0.1571 - accuracy: 0.7661 - jacard_coef: 0.0351 7/17 [===========>..................] - ETA: 2s - loss: 0.1569 - accuracy: 0.7724 - jacard_coef: 0.0374 8/17 [=============>................] - ETA: 1s - loss: 0.1568 - accuracy: 0.7867 - jacard_coef: 0.0366 9/17 [==============>...............] - ETA: 1s - loss: 0.1568 - accuracy: 0.8025 - jacard_coef: 0.033610/17 [================>.............] - ETA: 1s - loss: 0.1566 - accuracy: 0.8142 - jacard_coef: 0.030611/17 [==================>...........] - ETA: 1s - loss: 0.1565 - accuracy: 0.8248 - jacard_coef: 0.029712/17 [====================>.........] - ETA: 1s - loss: 0.1567 - accuracy: 0.8309 - jacard_coef: 0.027713/17 [=====================>........] - ETA: 0s - loss: 0.1569 - accuracy: 0.8315 - jacard_coef: 0.028714/17 [=======================>......] - ETA: 0s - loss: 0.1567 - accuracy: 0.8377 - jacard_coef: 0.026915/17 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.8264 - jacard_coef: 0.029416/17 [===========================>..] - ETA: 0s - loss: 0.1576 - accuracy: 0.8316 - jacard_coef: 0.027717/17 [==============================] - 3s 205ms/step - loss: 0.1576 - accuracy: 0.8327 - jacard_coef: 0.0261 - val_loss: 0.1782 - val_accuracy: 0.4392 - val_jacard_coef: 0.0507 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1552 - accuracy: 0.8874 - jacard_coef: 0.0072 2/17 [==>...........................] - ETA: 3s - loss: 0.1553 - accuracy: 0.8932 - jacard_coef: 0.0038 3/17 [====>.........................] - ETA: 2s - loss: 0.1551 - accuracy: 0.9068 - jacard_coef: 0.0036 4/17 [======>.......................] - ETA: 2s - loss: 0.1549 - accuracy: 0.9074 - jacard_coef: 0.0034 5/17 [=======>......................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9099 - jacard_coef: 0.0045 6/17 [=========>....................] - ETA: 2s - loss: 0.1549 - accuracy: 0.9114 - jacard_coef: 0.0055 7/17 [===========>..................] - ETA: 2s - loss: 0.1545 - accuracy: 0.9132 - jacard_coef: 0.0051 8/17 [=============>................] - ETA: 1s - loss: 0.1541 - accuracy: 0.9146 - jacard_coef: 0.0069 9/17 [==============>...............] - ETA: 1s - loss: 0.1542 - accuracy: 0.9080 - jacard_coef: 0.006910/17 [================>.............] - ETA: 1s - loss: 0.1540 - accuracy: 0.9064 - jacard_coef: 0.007611/17 [==================>...........] - ETA: 1s - loss: 0.1537 - accuracy: 0.9061 - jacard_coef: 0.007112/17 [====================>.........] - ETA: 1s - loss: 0.1536 - accuracy: 0.9052 - jacard_coef: 0.006713/17 [=====================>........] - ETA: 0s - loss: 0.1539 - accuracy: 0.9053 - jacard_coef: 0.006214/17 [=======================>......] - ETA: 0s - loss: 0.1534 - accuracy: 0.9068 - jacard_coef: 0.005815/17 [=========================>....] - ETA: 0s - loss: 0.1532 - accuracy: 0.9069 - jacard_coef: 0.005516/17 [===========================>..] - ETA: 0s - loss: 0.1530 - accuracy: 0.9073 - jacard_coef: 0.005117/17 [==============================] - 3s 205ms/step - loss: 0.1531 - accuracy: 0.9070 - jacard_coef: 0.0076 - val_loss: 0.1449 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1512 - accuracy: 0.8888 - jacard_coef: 1.7149e-12 2/17 [==>...........................] - ETA: 3s - loss: 0.1510 - accuracy: 0.8921 - jacard_coef: 1.7698e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1511 - accuracy: 0.8881 - jacard_coef: 1.9094e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1510 - accuracy: 0.8867 - jacard_coef: 3.6156e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1507 - accuracy: 0.8877 - jacard_coef: 3.9165e-04 6/17 [=========>....................] - ETA: 2s - loss: 0.1505 - accuracy: 0.8892 - jacard_coef: 4.1231e-04 7/17 [===========>..................] - ETA: 2s - loss: 0.1501 - accuracy: 0.8944 - jacard_coef: 3.5341e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1499 - accuracy: 0.8966 - jacard_coef: 3.2009e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1496 - accuracy: 0.9001 - jacard_coef: 2.8453e-0410/17 [================>.............] - ETA: 1s - loss: 0.1497 - accuracy: 0.9026 - jacard_coef: 2.7903e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1495 - accuracy: 0.9035 - jacard_coef: 2.7555e-0412/17 [====================>.........] - ETA: 1s - loss: 0.1491 - accuracy: 0.9084 - jacard_coef: 2.6921e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9102 - jacard_coef: 2.5279e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1487 - accuracy: 0.9108 - jacard_coef: 2.6489e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1484 - accuracy: 0.9139 - jacard_coef: 5.4439e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1483 - accuracy: 0.9147 - jacard_coef: 0.0018    17/17 [==============================] - 3s 205ms/step - loss: 0.1483 - accuracy: 0.9141 - jacard_coef: 0.0023 - val_loss: 0.1432 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1452 - accuracy: 0.9275 - jacard_coef: 0.0016 2/17 [==>...........................] - ETA: 3s - loss: 0.1458 - accuracy: 0.9147 - jacard_coef: 9.0842e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1456 - accuracy: 0.9176 - jacard_coef: 0.0018     4/17 [======>.......................] - ETA: 2s - loss: 0.1454 - accuracy: 0.9181 - jacard_coef: 0.0013 5/17 [=======>......................] - ETA: 2s - loss: 0.1452 - accuracy: 0.9208 - jacard_coef: 0.0011 6/17 [=========>....................] - ETA: 2s - loss: 0.1457 - accuracy: 0.9147 - jacard_coef: 9.1769e-04 7/17 [===========>..................] - ETA: 2s - loss: 0.1458 - accuracy: 0.9125 - jacard_coef: 9.2480e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1459 - accuracy: 0.9125 - jacard_coef: 8.1464e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1460 - accuracy: 0.9110 - jacard_coef: 7.3674e-0410/17 [================>.............] - ETA: 1s - loss: 0.1462 - accuracy: 0.9130 - jacard_coef: 8.2862e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1459 - accuracy: 0.9144 - jacard_coef: 0.0029    12/17 [====================>.........] - ETA: 1s - loss: 0.1460 - accuracy: 0.9119 - jacard_coef: 0.002713/17 [=====================>........] - ETA: 0s - loss: 0.1456 - accuracy: 0.9159 - jacard_coef: 0.002514/17 [=======================>......] - ETA: 0s - loss: 0.1456 - accuracy: 0.9141 - jacard_coef: 0.002315/17 [=========================>....] - ETA: 0s - loss: 0.1454 - accuracy: 0.9164 - jacard_coef: 0.002316/17 [===========================>..] - ETA: 0s - loss: 0.1454 - accuracy: 0.9151 - jacard_coef: 0.003017/17 [==============================] - 3s 205ms/step - loss: 0.1454 - accuracy: 0.9145 - jacard_coef: 0.0040 - val_loss: 0.1428 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1476 - accuracy: 0.8674 - jacard_coef: 0.0051 2/17 [==>...........................] - ETA: 3s - loss: 0.1456 - accuracy: 0.8915 - jacard_coef: 0.0026 3/17 [====>.........................] - ETA: 2s - loss: 0.1455 - accuracy: 0.8939 - jacard_coef: 0.0021 4/17 [======>.......................] - ETA: 2s - loss: 0.1444 - accuracy: 0.9103 - jacard_coef: 0.0016 5/17 [=======>......................] - ETA: 2s - loss: 0.1441 - accuracy: 0.9132 - jacard_coef: 0.0024 6/17 [=========>....................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9156 - jacard_coef: 0.0020 7/17 [===========>..................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9161 - jacard_coef: 0.0027 8/17 [=============>................] - ETA: 1s - loss: 0.1440 - accuracy: 0.9160 - jacard_coef: 0.0024 9/17 [==============>...............] - ETA: 1s - loss: 0.1441 - accuracy: 0.9145 - jacard_coef: 0.002510/17 [================>.............] - ETA: 1s - loss: 0.1441 - accuracy: 0.9132 - jacard_coef: 0.002611/17 [==================>...........] - ETA: 1s - loss: 0.1442 - accuracy: 0.9110 - jacard_coef: 0.002412/17 [====================>.........] - ETA: 1s - loss: 0.1439 - accuracy: 0.9135 - jacard_coef: 0.002513/17 [=====================>........] - ETA: 0s - loss: 0.1439 - accuracy: 0.9134 - jacard_coef: 0.002514/17 [=======================>......] - ETA: 0s - loss: 0.1440 - accuracy: 0.9116 - jacard_coef: 0.004215/17 [=========================>....] - ETA: 0s - loss: 0.1437 - accuracy: 0.9127 - jacard_coef: 0.004716/17 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9150 - jacard_coef: 0.004517/17 [==============================] - 3s 205ms/step - loss: 0.1440 - accuracy: 0.9121 - jacard_coef: 0.0102 - val_loss: 0.1546 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1434 - accuracy: 0.9091 - jacard_coef: 1.4688e-04 2/17 [==>...........................] - ETA: 3s - loss: 0.1415 - accuracy: 0.9418 - jacard_coef: 0.0013     3/17 [====>.........................] - ETA: 2s - loss: 0.1431 - accuracy: 0.9256 - jacard_coef: 9.2095e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9228 - jacard_coef: 0.0013     5/17 [=======>......................] - ETA: 2s - loss: 0.1441 - accuracy: 0.9230 - jacard_coef: 0.0012 6/17 [=========>....................] - ETA: 2s - loss: 0.1445 - accuracy: 0.9200 - jacard_coef: 0.0016 7/17 [===========>..................] - ETA: 2s - loss: 0.1456 - accuracy: 0.9168 - jacard_coef: 0.0026 8/17 [=============>................] - ETA: 1s - loss: 0.1465 - accuracy: 0.9142 - jacard_coef: 0.0025 9/17 [==============>...............] - ETA: 1s - loss: 0.1464 - accuracy: 0.9173 - jacard_coef: 0.002710/17 [================>.............] - ETA: 1s - loss: 0.1484 - accuracy: 0.9089 - jacard_coef: 0.007511/17 [==================>...........] - ETA: 1s - loss: 0.1481 - accuracy: 0.9104 - jacard_coef: 0.007312/17 [====================>.........] - ETA: 1s - loss: 0.1479 - accuracy: 0.9098 - jacard_coef: 0.006713/17 [=====================>........] - ETA: 0s - loss: 0.1479 - accuracy: 0.9076 - jacard_coef: 0.006614/17 [=======================>......] - ETA: 0s - loss: 0.1474 - accuracy: 0.9095 - jacard_coef: 0.006815/17 [=========================>....] - ETA: 0s - loss: 0.1472 - accuracy: 0.9099 - jacard_coef: 0.006816/17 [===========================>..] - ETA: 0s - loss: 0.1471 - accuracy: 0.9081 - jacard_coef: 0.007617/17 [==============================] - 3s 205ms/step - loss: 0.1473 - accuracy: 0.9074 - jacard_coef: 0.0092 - val_loss: 0.0974 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1461 - accuracy: 0.9220 - jacard_coef: 0.0113 2/17 [==>...........................] - ETA: 3s - loss: 0.1454 - accuracy: 0.9143 - jacard_coef: 0.0109 3/17 [====>.........................] - ETA: 2s - loss: 0.1450 - accuracy: 0.9109 - jacard_coef: 0.0088 4/17 [======>.......................] - ETA: 2s - loss: 0.1449 - accuracy: 0.9094 - jacard_coef: 0.0066 5/17 [=======>......................] - ETA: 2s - loss: 0.1444 - accuracy: 0.9136 - jacard_coef: 0.0058 6/17 [=========>....................] - ETA: 2s - loss: 0.1450 - accuracy: 0.9088 - jacard_coef: 0.0054 7/17 [===========>..................] - ETA: 2s - loss: 0.1451 - accuracy: 0.9050 - jacard_coef: 0.0050 8/17 [=============>................] - ETA: 1s - loss: 0.1447 - accuracy: 0.9078 - jacard_coef: 0.0044 9/17 [==============>...............] - ETA: 1s - loss: 0.1440 - accuracy: 0.9135 - jacard_coef: 0.004510/17 [================>.............] - ETA: 1s - loss: 0.1443 - accuracy: 0.9135 - jacard_coef: 0.004211/17 [==================>...........] - ETA: 1s - loss: 0.1444 - accuracy: 0.9106 - jacard_coef: 0.004312/17 [====================>.........] - ETA: 1s - loss: 0.1445 - accuracy: 0.9085 - jacard_coef: 0.004013/17 [=====================>........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9088 - jacard_coef: 0.003714/17 [=======================>......] - ETA: 0s - loss: 0.1441 - accuracy: 0.9101 - jacard_coef: 0.003515/17 [=========================>....] - ETA: 0s - loss: 0.1439 - accuracy: 0.9106 - jacard_coef: 0.003216/17 [===========================>..] - ETA: 0s - loss: 0.1437 - accuracy: 0.9119 - jacard_coef: 0.003117/17 [==============================] - 3s 205ms/step - loss: 0.1438 - accuracy: 0.9100 - jacard_coef: 0.0031 - val_loss: 0.1556 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0739 (epoch 3)
  Final Val Loss: 0.1556
  Training Time: 0:01:49.451725
  Stability (std): 0.1804

Results saved to: hyperparameter_optimization_20250926_165036/exp_25_Attention_ResUNet_lr1e-4_bs8/Attention_ResUNet_lr0.0001_bs8_results.json

Experiment 25 completed in 126s
Progress: 25/36 completed
Estimated remaining time: 23 minutes

ðŸ”¬ EXPERIMENT 26/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879831.767326 1158050 service.cc:145] XLA service 0x14ef9e9f8ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879831.767351 1158050 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879831.903805 1158050 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 7:43 - loss: 0.3457 - accuracy: 0.5271 - jacard_coef: 0.08122/9 [=====>........................] - ETA: 58s - loss: 0.3279 - accuracy: 0.4747 - jacard_coef: 0.0836 3/9 [=========>....................] - ETA: 26s - loss: 0.3052 - accuracy: 0.4014 - jacard_coef: 0.08544/9 [============>.................] - ETA: 15s - loss: 0.2851 - accuracy: 0.3458 - jacard_coef: 0.08605/9 [===============>..............] - ETA: 9s - loss: 0.2707 - accuracy: 0.3041 - jacard_coef: 0.0844 6/9 [===================>..........] - ETA: 5s - loss: 0.2602 - accuracy: 0.2775 - jacard_coef: 0.08437/9 [======================>.......] - ETA: 3s - loss: 0.2519 - accuracy: 0.2626 - jacard_coef: 0.08488/9 [=========================>....] - ETA: 1s - loss: 0.2462 - accuracy: 0.2659 - jacard_coef: 0.08359/9 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.2658 - jacard_coef: 0.08939/9 [==============================] - 78s 2s/step - loss: 0.2459 - accuracy: 0.2658 - jacard_coef: 0.0893 - val_loss: 1.1157 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1916 - accuracy: 0.4153 - jacard_coef: 0.06842/9 [=====>........................] - ETA: 2s - loss: 0.1909 - accuracy: 0.3840 - jacard_coef: 0.06313/9 [=========>....................] - ETA: 2s - loss: 0.1893 - accuracy: 0.3408 - jacard_coef: 0.06354/9 [============>.................] - ETA: 2s - loss: 0.1875 - accuracy: 0.3468 - jacard_coef: 0.06855/9 [===============>..............] - ETA: 1s - loss: 0.1856 - accuracy: 0.3650 - jacard_coef: 0.07666/9 [===================>..........] - ETA: 1s - loss: 0.1844 - accuracy: 0.3694 - jacard_coef: 0.07627/9 [======================>.......] - ETA: 0s - loss: 0.1831 - accuracy: 0.3767 - jacard_coef: 0.07818/9 [=========================>....] - ETA: 0s - loss: 0.1821 - accuracy: 0.3813 - jacard_coef: 0.07999/9 [==============================] - 3s 374ms/step - loss: 0.1821 - accuracy: 0.3811 - jacard_coef: 0.0829 - val_loss: 1.1107 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1743 - accuracy: 0.5123 - jacard_coef: 0.06162/9 [=====>........................] - ETA: 2s - loss: 0.1746 - accuracy: 0.5267 - jacard_coef: 0.07853/9 [=========>....................] - ETA: 2s - loss: 0.1750 - accuracy: 0.5261 - jacard_coef: 0.07684/9 [============>.................] - ETA: 2s - loss: 0.1750 - accuracy: 0.5293 - jacard_coef: 0.07655/9 [===============>..............] - ETA: 1s - loss: 0.1746 - accuracy: 0.5295 - jacard_coef: 0.08076/9 [===================>..........] - ETA: 1s - loss: 0.1740 - accuracy: 0.5349 - jacard_coef: 0.07897/9 [======================>.......] - ETA: 0s - loss: 0.1737 - accuracy: 0.5406 - jacard_coef: 0.08398/9 [=========================>....] - ETA: 0s - loss: 0.1734 - accuracy: 0.5431 - jacard_coef: 0.08229/9 [==============================] - 3s 381ms/step - loss: 0.1737 - accuracy: 0.5422 - jacard_coef: 0.0955 - val_loss: 1.0739 - val_accuracy: 0.9256 - val_jacard_coef: 0.0053 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1763 - accuracy: 0.3847 - jacard_coef: 0.07262/9 [=====>........................] - ETA: 2s - loss: 0.1814 - accuracy: 0.2682 - jacard_coef: 0.07303/9 [=========>....................] - ETA: 2s - loss: 0.1817 - accuracy: 0.2808 - jacard_coef: 0.08814/9 [============>.................] - ETA: 2s - loss: 0.1810 - accuracy: 0.2789 - jacard_coef: 0.09095/9 [===============>..............] - ETA: 1s - loss: 0.1807 - accuracy: 0.2785 - jacard_coef: 0.08606/9 [===================>..........] - ETA: 1s - loss: 0.1814 - accuracy: 0.2739 - jacard_coef: 0.08187/9 [======================>.......] - ETA: 0s - loss: 0.1809 - accuracy: 0.2774 - jacard_coef: 0.08448/9 [=========================>....] - ETA: 0s - loss: 0.1805 - accuracy: 0.2857 - jacard_coef: 0.08189/9 [==============================] - 3s 380ms/step - loss: 0.1805 - accuracy: 0.2863 - jacard_coef: 0.0883 - val_loss: 1.1323 - val_accuracy: 0.9205 - val_jacard_coef: 0.0107 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1755 - accuracy: 0.3899 - jacard_coef: 0.11102/9 [=====>........................] - ETA: 2s - loss: 0.1753 - accuracy: 0.3975 - jacard_coef: 0.10343/9 [=========>....................] - ETA: 2s - loss: 0.1748 - accuracy: 0.4082 - jacard_coef: 0.09374/9 [============>.................] - ETA: 2s - loss: 0.1744 - accuracy: 0.4223 - jacard_coef: 0.09005/9 [===============>..............] - ETA: 1s - loss: 0.1739 - accuracy: 0.4400 - jacard_coef: 0.08536/9 [===================>..........] - ETA: 1s - loss: 0.1734 - accuracy: 0.4591 - jacard_coef: 0.08657/9 [======================>.......] - ETA: 0s - loss: 0.1728 - accuracy: 0.4906 - jacard_coef: 0.08388/9 [=========================>....] - ETA: 0s - loss: 0.1726 - accuracy: 0.5059 - jacard_coef: 0.08079/9 [==============================] - 3s 380ms/step - loss: 0.1727 - accuracy: 0.5049 - jacard_coef: 0.0792 - val_loss: 0.7504 - val_accuracy: 0.8829 - val_jacard_coef: 0.0415 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1708 - accuracy: 0.6029 - jacard_coef: 0.08212/9 [=====>........................] - ETA: 2s - loss: 0.1716 - accuracy: 0.5715 - jacard_coef: 0.07763/9 [=========>....................] - ETA: 2s - loss: 0.1718 - accuracy: 0.5560 - jacard_coef: 0.08094/9 [============>.................] - ETA: 2s - loss: 0.1722 - accuracy: 0.5484 - jacard_coef: 0.07905/9 [===============>..............] - ETA: 1s - loss: 0.1719 - accuracy: 0.5531 - jacard_coef: 0.08066/9 [===================>..........] - ETA: 1s - loss: 0.1720 - accuracy: 0.5620 - jacard_coef: 0.07667/9 [======================>.......] - ETA: 0s - loss: 0.1717 - accuracy: 0.5595 - jacard_coef: 0.08138/9 [=========================>....] - ETA: 0s - loss: 0.1718 - accuracy: 0.5510 - jacard_coef: 0.08159/9 [==============================] - 3s 373ms/step - loss: 0.1718 - accuracy: 0.5500 - jacard_coef: 0.0855 - val_loss: 0.9609 - val_accuracy: 0.9269 - val_jacard_coef: 0.0023 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1720 - accuracy: 0.5744 - jacard_coef: 0.05302/9 [=====>........................] - ETA: 2s - loss: 0.1698 - accuracy: 0.6049 - jacard_coef: 0.06763/9 [=========>....................] - ETA: 2s - loss: 0.1693 - accuracy: 0.6203 - jacard_coef: 0.07114/9 [============>.................] - ETA: 2s - loss: 0.1686 - accuracy: 0.6452 - jacard_coef: 0.06775/9 [===============>..............] - ETA: 1s - loss: 0.1682 - accuracy: 0.6681 - jacard_coef: 0.06606/9 [===================>..........] - ETA: 1s - loss: 0.1677 - accuracy: 0.6871 - jacard_coef: 0.06677/9 [======================>.......] - ETA: 0s - loss: 0.1678 - accuracy: 0.6992 - jacard_coef: 0.06538/9 [=========================>....] - ETA: 0s - loss: 0.1673 - accuracy: 0.7149 - jacard_coef: 0.06409/9 [==============================] - 3s 374ms/step - loss: 0.1677 - accuracy: 0.7114 - jacard_coef: 0.0617 - val_loss: 0.2426 - val_accuracy: 0.8362 - val_jacard_coef: 0.0405 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1641 - accuracy: 0.8544 - jacard_coef: 0.02702/9 [=====>........................] - ETA: 2s - loss: 0.1653 - accuracy: 0.8428 - jacard_coef: 0.02933/9 [=========>....................] - ETA: 2s - loss: 0.1657 - accuracy: 0.8182 - jacard_coef: 0.03854/9 [============>.................] - ETA: 2s - loss: 0.1655 - accuracy: 0.8286 - jacard_coef: 0.03165/9 [===============>..............] - ETA: 1s - loss: 0.1658 - accuracy: 0.8401 - jacard_coef: 0.02846/9 [===================>..........] - ETA: 1s - loss: 0.1657 - accuracy: 0.8384 - jacard_coef: 0.02737/9 [======================>.......] - ETA: 0s - loss: 0.1654 - accuracy: 0.8431 - jacard_coef: 0.02698/9 [=========================>....] - ETA: 0s - loss: 0.1652 - accuracy: 0.8505 - jacard_coef: 0.02809/9 [==============================] - 3s 374ms/step - loss: 0.1652 - accuracy: 0.8508 - jacard_coef: 0.0253 - val_loss: 0.1243 - val_accuracy: 0.9195 - val_jacard_coef: 0.0073 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1619 - accuracy: 0.8916 - jacard_coef: 0.03392/9 [=====>........................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8852 - jacard_coef: 0.03143/9 [=========>....................] - ETA: 2s - loss: 0.1620 - accuracy: 0.8740 - jacard_coef: 0.03404/9 [============>.................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8817 - jacard_coef: 0.03065/9 [===============>..............] - ETA: 1s - loss: 0.1615 - accuracy: 0.8831 - jacard_coef: 0.03106/9 [===================>..........] - ETA: 1s - loss: 0.1614 - accuracy: 0.8830 - jacard_coef: 0.02807/9 [======================>.......] - ETA: 0s - loss: 0.1615 - accuracy: 0.8864 - jacard_coef: 0.02618/9 [=========================>....] - ETA: 0s - loss: 0.1615 - accuracy: 0.8837 - jacard_coef: 0.02369/9 [==============================] - 3s 374ms/step - loss: 0.1615 - accuracy: 0.8833 - jacard_coef: 0.0278 - val_loss: 0.1356 - val_accuracy: 0.9292 - val_jacard_coef: 8.0445e-04 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1585 - accuracy: 0.8918 - jacard_coef: 0.00672/9 [=====>........................] - ETA: 2s - loss: 0.1598 - accuracy: 0.8828 - jacard_coef: 0.01563/9 [=========>....................] - ETA: 2s - loss: 0.1588 - accuracy: 0.8953 - jacard_coef: 0.01634/9 [============>.................] - ETA: 2s - loss: 0.1587 - accuracy: 0.8993 - jacard_coef: 0.01795/9 [===============>..............] - ETA: 1s - loss: 0.1586 - accuracy: 0.8985 - jacard_coef: 0.01826/9 [===================>..........] - ETA: 1s - loss: 0.1584 - accuracy: 0.8960 - jacard_coef: 0.02027/9 [======================>.......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8954 - jacard_coef: 0.02108/9 [=========================>....] - ETA: 0s - loss: 0.1581 - accuracy: 0.8962 - jacard_coef: 0.01899/9 [==============================] - 3s 374ms/step - loss: 0.1583 - accuracy: 0.8923 - jacard_coef: 0.0209 - val_loss: 0.1523 - val_accuracy: 0.9137 - val_jacard_coef: 0.0245 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1581 - accuracy: 0.9146 - jacard_coef: 0.01942/9 [=====>........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.9090 - jacard_coef: 0.01913/9 [=========>....................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8962 - jacard_coef: 0.02274/9 [============>.................] - ETA: 2s - loss: 0.1601 - accuracy: 0.8869 - jacard_coef: 0.02525/9 [===============>..............] - ETA: 1s - loss: 0.1599 - accuracy: 0.8855 - jacard_coef: 0.02396/9 [===================>..........] - ETA: 1s - loss: 0.1600 - accuracy: 0.8830 - jacard_coef: 0.02527/9 [======================>.......] - ETA: 0s - loss: 0.1610 - accuracy: 0.8652 - jacard_coef: 0.03068/9 [=========================>....] - ETA: 0s - loss: 0.1609 - accuracy: 0.8630 - jacard_coef: 0.03209/9 [==============================] - 3s 374ms/step - loss: 0.1609 - accuracy: 0.8638 - jacard_coef: 0.0294 - val_loss: 0.1551 - val_accuracy: 0.9134 - val_jacard_coef: 0.0161 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1637 - accuracy: 0.8287 - jacard_coef: 0.04522/9 [=====>........................] - ETA: 2s - loss: 0.1640 - accuracy: 0.8252 - jacard_coef: 0.05163/9 [=========>....................] - ETA: 2s - loss: 0.1623 - accuracy: 0.8429 - jacard_coef: 0.04294/9 [============>.................] - ETA: 2s - loss: 0.1610 - accuracy: 0.8553 - jacard_coef: 0.04105/9 [===============>..............] - ETA: 1s - loss: 0.1609 - accuracy: 0.8531 - jacard_coef: 0.04336/9 [===================>..........] - ETA: 1s - loss: 0.1605 - accuracy: 0.8503 - jacard_coef: 0.04547/9 [======================>.......] - ETA: 0s - loss: 0.1602 - accuracy: 0.8507 - jacard_coef: 0.04738/9 [=========================>....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8533 - jacard_coef: 0.04549/9 [==============================] - 3s 374ms/step - loss: 0.1599 - accuracy: 0.8517 - jacard_coef: 0.0406 - val_loss: 0.1450 - val_accuracy: 0.9242 - val_jacard_coef: 0.0031 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1570 - accuracy: 0.8798 - jacard_coef: 0.02742/9 [=====>........................] - ETA: 2s - loss: 0.1581 - accuracy: 0.8773 - jacard_coef: 0.03003/9 [=========>....................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8825 - jacard_coef: 0.02454/9 [============>.................] - ETA: 2s - loss: 0.1581 - accuracy: 0.8908 - jacard_coef: 0.02095/9 [===============>..............] - ETA: 1s - loss: 0.1580 - accuracy: 0.8890 - jacard_coef: 0.01946/9 [===================>..........] - ETA: 1s - loss: 0.1581 - accuracy: 0.8921 - jacard_coef: 0.01817/9 [======================>.......] - ETA: 0s - loss: 0.1576 - accuracy: 0.8974 - jacard_coef: 0.01618/9 [=========================>....] - ETA: 0s - loss: 0.1573 - accuracy: 0.9018 - jacard_coef: 0.01469/9 [==============================] - 3s 374ms/step - loss: 0.1573 - accuracy: 0.9011 - jacard_coef: 0.0193 - val_loss: 0.1463 - val_accuracy: 0.9078 - val_jacard_coef: 0.0157 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1560 - accuracy: 0.9303 - jacard_coef: 0.00422/9 [=====>........................] - ETA: 2s - loss: 0.1560 - accuracy: 0.9109 - jacard_coef: 0.00423/9 [=========>....................] - ETA: 2s - loss: 0.1561 - accuracy: 0.9044 - jacard_coef: 0.00494/9 [============>.................] - ETA: 2s - loss: 0.1557 - accuracy: 0.9097 - jacard_coef: 0.00465/9 [===============>..............] - ETA: 1s - loss: 0.1556 - accuracy: 0.9108 - jacard_coef: 0.00506/9 [===================>..........] - ETA: 1s - loss: 0.1555 - accuracy: 0.9121 - jacard_coef: 0.00587/9 [======================>.......] - ETA: 0s - loss: 0.1554 - accuracy: 0.9136 - jacard_coef: 0.00628/9 [=========================>....] - ETA: 0s - loss: 0.1551 - accuracy: 0.9137 - jacard_coef: 0.00709/9 [==============================] - 3s 374ms/step - loss: 0.1551 - accuracy: 0.9123 - jacard_coef: 0.0256 - val_loss: 0.1459 - val_accuracy: 0.9181 - val_jacard_coef: 0.0098 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1534 - accuracy: 0.9086 - jacard_coef: 0.02062/9 [=====>........................] - ETA: 2s - loss: 0.1543 - accuracy: 0.8854 - jacard_coef: 0.03043/9 [=========>....................] - ETA: 2s - loss: 0.1542 - accuracy: 0.8849 - jacard_coef: 0.03364/9 [============>.................] - ETA: 2s - loss: 0.1543 - accuracy: 0.8798 - jacard_coef: 0.03685/9 [===============>..............] - ETA: 1s - loss: 0.1540 - accuracy: 0.8837 - jacard_coef: 0.03896/9 [===================>..........] - ETA: 1s - loss: 0.1537 - accuracy: 0.8845 - jacard_coef: 0.03817/9 [======================>.......] - ETA: 0s - loss: 0.1534 - accuracy: 0.8902 - jacard_coef: 0.03528/9 [=========================>....] - ETA: 0s - loss: 0.1535 - accuracy: 0.8876 - jacard_coef: 0.03269/9 [==============================] - 3s 374ms/step - loss: 0.1535 - accuracy: 0.8858 - jacard_coef: 0.0421 - val_loss: 0.1512 - val_accuracy: 0.9290 - val_jacard_coef: 0.0017 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0415 (epoch 5)
  Final Val Loss: 0.1512
  Training Time: 0:02:06.044353
  Stability (std): 0.2436

Results saved to: hyperparameter_optimization_20250926_165036/exp_26_Attention_ResUNet_lr1e-4_bs16/Attention_ResUNet_lr0.0001_bs16_results.json

Experiment 26 completed in 143s
Progress: 26/36 completed
Estimated remaining time: 23 minutes

ðŸ”¬ EXPERIMENT 27/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879981.184808 1166133 service.cc:145] XLA service 0x14d955589e10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879981.184835 1166133 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879981.326318 1166133 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:40 - loss: 0.3361 - accuracy: 0.4870 - jacard_coef: 0.07792/5 [===========>..................] - ETA: 46s - loss: 0.2956 - accuracy: 0.4141 - jacard_coef: 0.0801 3/5 [=================>............] - ETA: 16s - loss: 0.2695 - accuracy: 0.3393 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 5s - loss: 0.2528 - accuracy: 0.2984 - jacard_coef: 0.0798 5/5 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.2976 - jacard_coef: 0.09075/5 [==============================] - 96s 7s/step - loss: 0.2523 - accuracy: 0.2976 - jacard_coef: 0.0907 - val_loss: 0.0944 - val_accuracy: 0.9303 - val_jacard_coef: 1.4597e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1942 - accuracy: 0.1784 - jacard_coef: 0.08242/5 [===========>..................] - ETA: 2s - loss: 0.1933 - accuracy: 0.1897 - jacard_coef: 0.08483/5 [=================>............] - ETA: 1s - loss: 0.1919 - accuracy: 0.1927 - jacard_coef: 0.08924/5 [=======================>......] - ETA: 0s - loss: 0.1909 - accuracy: 0.1840 - jacard_coef: 0.08525/5 [==============================] - 3s 654ms/step - loss: 0.1908 - accuracy: 0.1847 - jacard_coef: 0.1049 - val_loss: 0.4662 - val_accuracy: 0.9304 - val_jacard_coef: 1.4613e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1840 - accuracy: 0.1964 - jacard_coef: 0.09642/5 [===========>..................] - ETA: 2s - loss: 0.1826 - accuracy: 0.2113 - jacard_coef: 0.08643/5 [=================>............] - ETA: 1s - loss: 0.1848 - accuracy: 0.2274 - jacard_coef: 0.08214/5 [=======================>......] - ETA: 0s - loss: 0.1848 - accuracy: 0.2501 - jacard_coef: 0.08485/5 [==============================] - 3s 652ms/step - loss: 0.1847 - accuracy: 0.2515 - jacard_coef: 0.0957 - val_loss: 0.1499 - val_accuracy: 0.8470 - val_jacard_coef: 0.0288 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1802 - accuracy: 0.3809 - jacard_coef: 0.08612/5 [===========>..................] - ETA: 2s - loss: 0.1786 - accuracy: 0.4097 - jacard_coef: 0.08963/5 [=================>............] - ETA: 1s - loss: 0.1818 - accuracy: 0.4162 - jacard_coef: 0.08954/5 [=======================>......] - ETA: 0s - loss: 0.1820 - accuracy: 0.3934 - jacard_coef: 0.08415/5 [==============================] - 3s 639ms/step - loss: 0.1820 - accuracy: 0.3938 - jacard_coef: 0.1091 - val_loss: 0.4713 - val_accuracy: 0.9201 - val_jacard_coef: 0.0062 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1796 - accuracy: 0.4038 - jacard_coef: 0.08612/5 [===========>..................] - ETA: 2s - loss: 0.1800 - accuracy: 0.4092 - jacard_coef: 0.07873/5 [=================>............] - ETA: 1s - loss: 0.1784 - accuracy: 0.4342 - jacard_coef: 0.08054/5 [=======================>......] - ETA: 0s - loss: 0.1772 - accuracy: 0.4396 - jacard_coef: 0.08165/5 [==============================] - 3s 638ms/step - loss: 0.1772 - accuracy: 0.4393 - jacard_coef: 0.0653 - val_loss: 1.0926 - val_accuracy: 0.9237 - val_jacard_coef: 0.0038 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1719 - accuracy: 0.5072 - jacard_coef: 0.11102/5 [===========>..................] - ETA: 2s - loss: 0.1718 - accuracy: 0.5389 - jacard_coef: 0.09303/5 [=================>............] - ETA: 1s - loss: 0.1715 - accuracy: 0.5649 - jacard_coef: 0.09054/5 [=======================>......] - ETA: 0s - loss: 0.1711 - accuracy: 0.5788 - jacard_coef: 0.08485/5 [==============================] - 3s 639ms/step - loss: 0.1717 - accuracy: 0.5771 - jacard_coef: 0.0795 - val_loss: 0.9179 - val_accuracy: 0.9302 - val_jacard_coef: 1.4584e-05 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1729 - accuracy: 0.5200 - jacard_coef: 0.09602/5 [===========>..................] - ETA: 2s - loss: 0.1722 - accuracy: 0.5241 - jacard_coef: 0.09063/5 [=================>............] - ETA: 1s - loss: 0.1719 - accuracy: 0.5275 - jacard_coef: 0.08304/5 [=======================>......] - ETA: 0s - loss: 0.1756 - accuracy: 0.4814 - jacard_coef: 0.08575/5 [==============================] - 3s 638ms/step - loss: 0.1756 - accuracy: 0.4802 - jacard_coef: 0.0969 - val_loss: 0.7079 - val_accuracy: 0.9294 - val_jacard_coef: 1.4407e-12 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1775 - accuracy: 0.3207 - jacard_coef: 0.07422/5 [===========>..................] - ETA: 2s - loss: 0.1765 - accuracy: 0.3813 - jacard_coef: 0.07673/5 [=================>............] - ETA: 1s - loss: 0.1759 - accuracy: 0.4419 - jacard_coef: 0.07694/5 [=======================>......] - ETA: 0s - loss: 0.1753 - accuracy: 0.4898 - jacard_coef: 0.07455/5 [==============================] - 3s 638ms/step - loss: 0.1752 - accuracy: 0.4908 - jacard_coef: 0.0598 - val_loss: 1.1488 - val_accuracy: 0.9229 - val_jacard_coef: 0.0028 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1703 - accuracy: 0.6628 - jacard_coef: 0.09852/5 [===========>..................] - ETA: 2s - loss: 0.1707 - accuracy: 0.6517 - jacard_coef: 0.08163/5 [=================>............] - ETA: 1s - loss: 0.1705 - accuracy: 0.6531 - jacard_coef: 0.08314/5 [=======================>......] - ETA: 0s - loss: 0.1703 - accuracy: 0.6623 - jacard_coef: 0.08535/5 [==============================] - 3s 637ms/step - loss: 0.1703 - accuracy: 0.6610 - jacard_coef: 0.0719 - val_loss: 1.0778 - val_accuracy: 0.9194 - val_jacard_coef: 0.0041 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1678 - accuracy: 0.7527 - jacard_coef: 0.07042/5 [===========>..................] - ETA: 2s - loss: 0.1680 - accuracy: 0.7539 - jacard_coef: 0.09003/5 [=================>............] - ETA: 1s - loss: 0.1678 - accuracy: 0.7583 - jacard_coef: 0.08884/5 [=======================>......] - ETA: 0s - loss: 0.1675 - accuracy: 0.7654 - jacard_coef: 0.08595/5 [==============================] - 3s 639ms/step - loss: 0.1675 - accuracy: 0.7646 - jacard_coef: 0.0687 - val_loss: 0.4799 - val_accuracy: 0.9146 - val_jacard_coef: 0.0052 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1665 - accuracy: 0.7704 - jacard_coef: 0.08162/5 [===========>..................] - ETA: 2s - loss: 0.1660 - accuracy: 0.7768 - jacard_coef: 0.08913/5 [=================>............] - ETA: 1s - loss: 0.1657 - accuracy: 0.7777 - jacard_coef: 0.07964/5 [=======================>......] - ETA: 0s - loss: 0.1654 - accuracy: 0.7759 - jacard_coef: 0.08565/5 [==============================] - 3s 639ms/step - loss: 0.1655 - accuracy: 0.7742 - jacard_coef: 0.0964 - val_loss: 0.2858 - val_accuracy: 0.9150 - val_jacard_coef: 0.0055 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1647 - accuracy: 0.7717 - jacard_coef: 0.09852/5 [===========>..................] - ETA: 2s - loss: 0.1648 - accuracy: 0.7514 - jacard_coef: 0.08793/5 [=================>............] - ETA: 1s - loss: 0.1655 - accuracy: 0.7268 - jacard_coef: 0.07664/5 [=======================>......] - ETA: 0s - loss: 0.1656 - accuracy: 0.7238 - jacard_coef: 0.07305/5 [==============================] - 3s 638ms/step - loss: 0.1657 - accuracy: 0.7224 - jacard_coef: 0.0594 - val_loss: 0.1183 - val_accuracy: 0.9191 - val_jacard_coef: 0.0036 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1661 - accuracy: 0.7280 - jacard_coef: 0.05692/5 [===========>..................] - ETA: 2s - loss: 0.1664 - accuracy: 0.7391 - jacard_coef: 0.07253/5 [=================>............] - ETA: 1s - loss: 0.1665 - accuracy: 0.7511 - jacard_coef: 0.06484/5 [=======================>......] - ETA: 0s - loss: 0.1666 - accuracy: 0.7583 - jacard_coef: 0.06235/5 [==============================] - 3s 639ms/step - loss: 0.1667 - accuracy: 0.7581 - jacard_coef: 0.0536 - val_loss: 0.1099 - val_accuracy: 0.8811 - val_jacard_coef: 0.0175 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0288 (epoch 3)
  Final Val Loss: 0.1099
  Training Time: 0:02:17.194284
  Stability (std): 0.3834

Results saved to: hyperparameter_optimization_20250926_165036/exp_27_Attention_ResUNet_lr1e-4_bs32/Attention_ResUNet_lr0.0001_bs32_results.json

Experiment 27 completed in 154s
Progress: 27/36 completed
Estimated remaining time: 23 minutes

ðŸ”¬ EXPERIMENT 28/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880126.160595 1173743 service.cc:145] XLA service 0x14be4156de70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880126.160619 1173743 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880126.299393 1173743 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 13:55 - loss: 0.3221 - accuracy: 0.4867 - jacard_coef: 0.0804 2/17 [==>...........................] - ETA: 1:07 - loss: 0.2836 - accuracy: 0.4254 - jacard_coef: 0.0756  3/17 [====>.........................] - ETA: 33s - loss: 0.2632 - accuracy: 0.3846 - jacard_coef: 0.0844  4/17 [======>.......................] - ETA: 21s - loss: 0.2484 - accuracy: 0.3398 - jacard_coef: 0.0854 5/17 [=======>......................] - ETA: 15s - loss: 0.2378 - accuracy: 0.3077 - jacard_coef: 0.0826 6/17 [=========>....................] - ETA: 11s - loss: 0.2308 - accuracy: 0.2872 - jacard_coef: 0.0868 7/17 [===========>..................] - ETA: 9s - loss: 0.2272 - accuracy: 0.2670 - jacard_coef: 0.0869  8/17 [=============>................] - ETA: 7s - loss: 0.2231 - accuracy: 0.2534 - jacard_coef: 0.0867 9/17 [==============>...............] - ETA: 5s - loss: 0.2196 - accuracy: 0.2443 - jacard_coef: 0.087610/17 [================>.............] - ETA: 4s - loss: 0.2166 - accuracy: 0.2395 - jacard_coef: 0.085711/17 [==================>...........] - ETA: 3s - loss: 0.2145 - accuracy: 0.2284 - jacard_coef: 0.083012/17 [====================>.........] - ETA: 2s - loss: 0.2124 - accuracy: 0.2206 - jacard_coef: 0.081413/17 [=====================>........] - ETA: 2s - loss: 0.2101 - accuracy: 0.2225 - jacard_coef: 0.080014/17 [=======================>......] - ETA: 1s - loss: 0.2078 - accuracy: 0.2282 - jacard_coef: 0.080415/17 [=========================>....] - ETA: 1s - loss: 0.2057 - accuracy: 0.2367 - jacard_coef: 0.082116/17 [===========================>..] - ETA: 0s - loss: 0.2037 - accuracy: 0.2494 - jacard_coef: 0.082817/17 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.2506 - jacard_coef: 0.086817/17 [==============================] - 67s 933ms/step - loss: 0.2035 - accuracy: 0.2506 - jacard_coef: 0.0868 - val_loss: 0.0879 - val_accuracy: 0.9304 - val_jacard_coef: 3.4105e-12 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1738 - accuracy: 0.3725 - jacard_coef: 0.1111 2/17 [==>...........................] - ETA: 3s - loss: 0.1743 - accuracy: 0.3782 - jacard_coef: 0.1026 3/17 [====>.........................] - ETA: 2s - loss: 0.1743 - accuracy: 0.3930 - jacard_coef: 0.1035 4/17 [======>.......................] - ETA: 2s - loss: 0.1749 - accuracy: 0.3838 - jacard_coef: 0.0995 5/17 [=======>......................] - ETA: 2s - loss: 0.1747 - accuracy: 0.3955 - jacard_coef: 0.0969 6/17 [=========>....................] - ETA: 2s - loss: 0.1743 - accuracy: 0.4153 - jacard_coef: 0.1023 7/17 [===========>..................] - ETA: 2s - loss: 0.1741 - accuracy: 0.4345 - jacard_coef: 0.0939 8/17 [=============>................] - ETA: 1s - loss: 0.1739 - accuracy: 0.4597 - jacard_coef: 0.0958 9/17 [==============>...............] - ETA: 1s - loss: 0.1736 - accuracy: 0.4785 - jacard_coef: 0.094310/17 [================>.............] - ETA: 1s - loss: 0.1734 - accuracy: 0.4933 - jacard_coef: 0.093311/17 [==================>...........] - ETA: 1s - loss: 0.1732 - accuracy: 0.5087 - jacard_coef: 0.094812/17 [====================>.........] - ETA: 1s - loss: 0.1730 - accuracy: 0.5132 - jacard_coef: 0.090113/17 [=====================>........] - ETA: 0s - loss: 0.1727 - accuracy: 0.5230 - jacard_coef: 0.087814/17 [=======================>......] - ETA: 0s - loss: 0.1724 - accuracy: 0.5366 - jacard_coef: 0.085415/17 [=========================>....] - ETA: 0s - loss: 0.1722 - accuracy: 0.5472 - jacard_coef: 0.084116/17 [===========================>..] - ETA: 0s - loss: 0.1719 - accuracy: 0.5555 - jacard_coef: 0.083317/17 [==============================] - 4s 209ms/step - loss: 0.1719 - accuracy: 0.5569 - jacard_coef: 0.0827 - val_loss: 0.1401 - val_accuracy: 0.9298 - val_jacard_coef: 0.0016 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1666 - accuracy: 0.6901 - jacard_coef: 0.0905 2/17 [==>...........................] - ETA: 3s - loss: 0.2287 - accuracy: 0.5335 - jacard_coef: 0.0671 3/17 [====>.........................] - ETA: 2s - loss: 0.2092 - accuracy: 0.5753 - jacard_coef: 0.0617 4/17 [======>.......................] - ETA: 2s - loss: 0.2008 - accuracy: 0.5293 - jacard_coef: 0.0726 5/17 [=======>......................] - ETA: 2s - loss: 0.1960 - accuracy: 0.4983 - jacard_coef: 0.0774 6/17 [=========>....................] - ETA: 2s - loss: 0.1916 - accuracy: 0.5225 - jacard_coef: 0.0773 7/17 [===========>..................] - ETA: 2s - loss: 0.1883 - accuracy: 0.5556 - jacard_coef: 0.0757 8/17 [=============>................] - ETA: 1s - loss: 0.1857 - accuracy: 0.5871 - jacard_coef: 0.0725 9/17 [==============>...............] - ETA: 1s - loss: 0.1837 - accuracy: 0.6114 - jacard_coef: 0.072110/17 [================>.............] - ETA: 1s - loss: 0.1821 - accuracy: 0.6350 - jacard_coef: 0.069211/17 [==================>...........] - ETA: 1s - loss: 0.1807 - accuracy: 0.6551 - jacard_coef: 0.069512/17 [====================>.........] - ETA: 1s - loss: 0.1796 - accuracy: 0.6714 - jacard_coef: 0.067313/17 [=====================>........] - ETA: 0s - loss: 0.1786 - accuracy: 0.6865 - jacard_coef: 0.063514/17 [=======================>......] - ETA: 0s - loss: 0.1777 - accuracy: 0.6987 - jacard_coef: 0.061115/17 [=========================>....] - ETA: 0s - loss: 0.1770 - accuracy: 0.7084 - jacard_coef: 0.059916/17 [===========================>..] - ETA: 0s - loss: 0.1764 - accuracy: 0.7177 - jacard_coef: 0.059317/17 [==============================] - 4s 208ms/step - loss: 0.1763 - accuracy: 0.7194 - jacard_coef: 0.0558 - val_loss: 0.1120 - val_accuracy: 0.9234 - val_jacard_coef: 0.0050 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1656 - accuracy: 0.8820 - jacard_coef: 0.0364 2/17 [==>...........................] - ETA: 3s - loss: 0.1652 - accuracy: 0.9085 - jacard_coef: 0.0284 3/17 [====>.........................] - ETA: 2s - loss: 0.1652 - accuracy: 0.9004 - jacard_coef: 0.0283 4/17 [======>.......................] - ETA: 2s - loss: 0.1650 - accuracy: 0.9022 - jacard_coef: 0.0299 5/17 [=======>......................] - ETA: 2s - loss: 0.1650 - accuracy: 0.8971 - jacard_coef: 0.0290 6/17 [=========>....................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8925 - jacard_coef: 0.0303 7/17 [===========>..................] - ETA: 2s - loss: 0.1647 - accuracy: 0.8920 - jacard_coef: 0.0312 8/17 [=============>................] - ETA: 1s - loss: 0.1646 - accuracy: 0.8863 - jacard_coef: 0.0335 9/17 [==============>...............] - ETA: 1s - loss: 0.1644 - accuracy: 0.8842 - jacard_coef: 0.036610/17 [================>.............] - ETA: 1s - loss: 0.1642 - accuracy: 0.8836 - jacard_coef: 0.037711/17 [==================>...........] - ETA: 1s - loss: 0.1640 - accuracy: 0.8808 - jacard_coef: 0.038912/17 [====================>.........] - ETA: 1s - loss: 0.1639 - accuracy: 0.8797 - jacard_coef: 0.041013/17 [=====================>........] - ETA: 0s - loss: 0.1638 - accuracy: 0.8781 - jacard_coef: 0.038014/17 [=======================>......] - ETA: 0s - loss: 0.1636 - accuracy: 0.8776 - jacard_coef: 0.036915/17 [=========================>....] - ETA: 0s - loss: 0.1635 - accuracy: 0.8791 - jacard_coef: 0.034916/17 [===========================>..] - ETA: 0s - loss: 0.1632 - accuracy: 0.8836 - jacard_coef: 0.032717/17 [==============================] - 3s 205ms/step - loss: 0.1632 - accuracy: 0.8838 - jacard_coef: 0.0308 - val_loss: 0.1405 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1601 - accuracy: 0.9084 - jacard_coef: 0.0016 2/17 [==>...........................] - ETA: 3s - loss: 0.1601 - accuracy: 0.9080 - jacard_coef: 0.0087 3/17 [====>.........................] - ETA: 2s - loss: 0.1594 - accuracy: 0.9242 - jacard_coef: 0.0061 4/17 [======>.......................] - ETA: 2s - loss: 0.1596 - accuracy: 0.9133 - jacard_coef: 0.0049 5/17 [=======>......................] - ETA: 2s - loss: 0.1592 - accuracy: 0.9187 - jacard_coef: 0.0046 6/17 [=========>....................] - ETA: 2s - loss: 0.1592 - accuracy: 0.9152 - jacard_coef: 0.0076 7/17 [===========>..................] - ETA: 2s - loss: 0.1590 - accuracy: 0.9120 - jacard_coef: 0.0098 8/17 [=============>................] - ETA: 1s - loss: 0.1590 - accuracy: 0.9057 - jacard_coef: 0.0140 9/17 [==============>...............] - ETA: 1s - loss: 0.1589 - accuracy: 0.8989 - jacard_coef: 0.020310/17 [================>.............] - ETA: 1s - loss: 0.1587 - accuracy: 0.8914 - jacard_coef: 0.025111/17 [==================>...........] - ETA: 1s - loss: 0.1586 - accuracy: 0.8850 - jacard_coef: 0.031012/17 [====================>.........] - ETA: 1s - loss: 0.1584 - accuracy: 0.8825 - jacard_coef: 0.033813/17 [=====================>........] - ETA: 0s - loss: 0.1584 - accuracy: 0.8778 - jacard_coef: 0.037014/17 [=======================>......] - ETA: 0s - loss: 0.1582 - accuracy: 0.8762 - jacard_coef: 0.035815/17 [=========================>....] - ETA: 0s - loss: 0.1581 - accuracy: 0.8759 - jacard_coef: 0.033416/17 [===========================>..] - ETA: 0s - loss: 0.1593 - accuracy: 0.8461 - jacard_coef: 0.036217/17 [==============================] - 3s 205ms/step - loss: 0.1593 - accuracy: 0.8459 - jacard_coef: 0.0365 - val_loss: 0.1569 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1608 - accuracy: 0.8958 - jacard_coef: 0.0319 2/17 [==>...........................] - ETA: 3s - loss: 0.1631 - accuracy: 0.8852 - jacard_coef: 0.0395 3/17 [====>.........................] - ETA: 2s - loss: 0.1653 - accuracy: 0.8778 - jacard_coef: 0.0553 4/17 [======>.......................] - ETA: 2s - loss: 0.1657 - accuracy: 0.8736 - jacard_coef: 0.0515 5/17 [=======>......................] - ETA: 2s - loss: 0.1662 - accuracy: 0.8680 - jacard_coef: 0.0477 6/17 [=========>....................] - ETA: 2s - loss: 0.1678 - accuracy: 0.8665 - jacard_coef: 0.0508 7/17 [===========>..................] - ETA: 2s - loss: 0.1676 - accuracy: 0.8660 - jacard_coef: 0.0496 8/17 [=============>................] - ETA: 1s - loss: 0.1677 - accuracy: 0.8690 - jacard_coef: 0.0501 9/17 [==============>...............] - ETA: 1s - loss: 0.1680 - accuracy: 0.8678 - jacard_coef: 0.046910/17 [================>.............] - ETA: 1s - loss: 0.1685 - accuracy: 0.8686 - jacard_coef: 0.047511/17 [==================>...........] - ETA: 1s - loss: 0.1684 - accuracy: 0.8710 - jacard_coef: 0.044712/17 [====================>.........] - ETA: 1s - loss: 0.1686 - accuracy: 0.8698 - jacard_coef: 0.043013/17 [=====================>........] - ETA: 0s - loss: 0.1685 - accuracy: 0.8720 - jacard_coef: 0.040814/17 [=======================>......] - ETA: 0s - loss: 0.1682 - accuracy: 0.8735 - jacard_coef: 0.039015/17 [=========================>....] - ETA: 0s - loss: 0.1684 - accuracy: 0.8757 - jacard_coef: 0.039216/17 [===========================>..] - ETA: 0s - loss: 0.1682 - accuracy: 0.8742 - jacard_coef: 0.037717/17 [==============================] - 3s 205ms/step - loss: 0.1681 - accuracy: 0.8744 - jacard_coef: 0.0355 - val_loss: 0.1367 - val_accuracy: 0.9304 - val_jacard_coef: 3.4096e-12 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1703 - accuracy: 0.8558 - jacard_coef: 0.0394 2/17 [==>...........................] - ETA: 3s - loss: 0.1704 - accuracy: 0.8686 - jacard_coef: 0.0607 3/17 [====>.........................] - ETA: 2s - loss: 0.1678 - accuracy: 0.8730 - jacard_coef: 0.0518 4/17 [======>.......................] - ETA: 2s - loss: 0.1664 - accuracy: 0.8744 - jacard_coef: 0.0448 5/17 [=======>......................] - ETA: 2s - loss: 0.1667 - accuracy: 0.8717 - jacard_coef: 0.0493 6/17 [=========>....................] - ETA: 2s - loss: 0.1662 - accuracy: 0.8738 - jacard_coef: 0.0483 7/17 [===========>..................] - ETA: 2s - loss: 0.1663 - accuracy: 0.8681 - jacard_coef: 0.0479 8/17 [=============>................] - ETA: 1s - loss: 0.1658 - accuracy: 0.8662 - jacard_coef: 0.0455 9/17 [==============>...............] - ETA: 1s - loss: 0.1659 - accuracy: 0.8667 - jacard_coef: 0.049010/17 [================>.............] - ETA: 1s - loss: 0.1662 - accuracy: 0.8645 - jacard_coef: 0.047111/17 [==================>...........] - ETA: 1s - loss: 0.1658 - accuracy: 0.8685 - jacard_coef: 0.043912/17 [====================>.........] - ETA: 1s - loss: 0.1654 - accuracy: 0.8726 - jacard_coef: 0.040513/17 [=====================>........] - ETA: 0s - loss: 0.1651 - accuracy: 0.8770 - jacard_coef: 0.037714/17 [=======================>......] - ETA: 0s - loss: 0.1652 - accuracy: 0.8787 - jacard_coef: 0.035415/17 [=========================>....] - ETA: 0s - loss: 0.1649 - accuracy: 0.8823 - jacard_coef: 0.033316/17 [===========================>..] - ETA: 0s - loss: 0.1647 - accuracy: 0.8850 - jacard_coef: 0.031517/17 [==============================] - 3s 205ms/step - loss: 0.1646 - accuracy: 0.8849 - jacard_coef: 0.0296 - val_loss: 0.1519 - val_accuracy: 0.9304 - val_jacard_coef: 3.4085e-12 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1632 - accuracy: 0.9114 - jacard_coef: 0.0049 2/17 [==>...........................] - ETA: 3s - loss: 0.1640 - accuracy: 0.9066 - jacard_coef: 0.0058 3/17 [====>.........................] - ETA: 2s - loss: 0.1635 - accuracy: 0.9050 - jacard_coef: 0.0074 4/17 [======>.......................] - ETA: 2s - loss: 0.1619 - accuracy: 0.9105 - jacard_coef: 0.0124 5/17 [=======>......................] - ETA: 2s - loss: 0.1621 - accuracy: 0.9052 - jacard_coef: 0.0173 6/17 [=========>....................] - ETA: 2s - loss: 0.1614 - accuracy: 0.9014 - jacard_coef: 0.0163 7/17 [===========>..................] - ETA: 2s - loss: 0.1616 - accuracy: 0.8942 - jacard_coef: 0.0167 8/17 [=============>................] - ETA: 1s - loss: 0.1614 - accuracy: 0.8932 - jacard_coef: 0.0175 9/17 [==============>...............] - ETA: 1s - loss: 0.1615 - accuracy: 0.8946 - jacard_coef: 0.024310/17 [================>.............] - ETA: 1s - loss: 0.1609 - accuracy: 0.8983 - jacard_coef: 0.022811/17 [==================>...........] - ETA: 1s - loss: 0.1606 - accuracy: 0.8990 - jacard_coef: 0.022512/17 [====================>.........] - ETA: 1s - loss: 0.1600 - accuracy: 0.9021 - jacard_coef: 0.020813/17 [=====================>........] - ETA: 0s - loss: 0.1599 - accuracy: 0.9019 - jacard_coef: 0.019514/17 [=======================>......] - ETA: 0s - loss: 0.1599 - accuracy: 0.9013 - jacard_coef: 0.018415/17 [=========================>....] - ETA: 0s - loss: 0.1597 - accuracy: 0.9006 - jacard_coef: 0.017616/17 [===========================>..] - ETA: 0s - loss: 0.1593 - accuracy: 0.9024 - jacard_coef: 0.017717/17 [==============================] - 3s 205ms/step - loss: 0.1592 - accuracy: 0.9027 - jacard_coef: 0.0166 - val_loss: 0.1519 - val_accuracy: 0.9304 - val_jacard_coef: 3.4078e-12 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1524 - accuracy: 0.9223 - jacard_coef: 0.0041 2/17 [==>...........................] - ETA: 3s - loss: 0.1534 - accuracy: 0.9137 - jacard_coef: 0.0065 3/17 [====>.........................] - ETA: 2s - loss: 0.1534 - accuracy: 0.9126 - jacard_coef: 0.0175 4/17 [======>.......................] - ETA: 2s - loss: 0.1543 - accuracy: 0.9079 - jacard_coef: 0.0138 5/17 [=======>......................] - ETA: 2s - loss: 0.1542 - accuracy: 0.9121 - jacard_coef: 0.0239 6/17 [=========>....................] - ETA: 2s - loss: 0.1553 - accuracy: 0.9037 - jacard_coef: 0.0275 7/17 [===========>..................] - ETA: 2s - loss: 0.1551 - accuracy: 0.9025 - jacard_coef: 0.0238 8/17 [=============>................] - ETA: 1s - loss: 0.1549 - accuracy: 0.9019 - jacard_coef: 0.0214 9/17 [==============>...............] - ETA: 1s - loss: 0.1549 - accuracy: 0.9022 - jacard_coef: 0.027110/17 [================>.............] - ETA: 1s - loss: 0.1545 - accuracy: 0.9027 - jacard_coef: 0.024511/17 [==================>...........] - ETA: 1s - loss: 0.1542 - accuracy: 0.9031 - jacard_coef: 0.024012/17 [====================>.........] - ETA: 1s - loss: 0.1539 - accuracy: 0.9034 - jacard_coef: 0.024513/17 [=====================>........] - ETA: 0s - loss: 0.1539 - accuracy: 0.9039 - jacard_coef: 0.026114/17 [=======================>......] - ETA: 0s - loss: 0.1536 - accuracy: 0.9063 - jacard_coef: 0.024215/17 [=========================>....] - ETA: 0s - loss: 0.1535 - accuracy: 0.9056 - jacard_coef: 0.023016/17 [===========================>..] - ETA: 0s - loss: 0.1536 - accuracy: 0.9044 - jacard_coef: 0.022017/17 [==============================] - 3s 205ms/step - loss: 0.1536 - accuracy: 0.9038 - jacard_coef: 0.0227 - val_loss: 0.1515 - val_accuracy: 0.9304 - val_jacard_coef: 3.4095e-12 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1490 - accuracy: 0.9380 - jacard_coef: 0.0010 2/17 [==>...........................] - ETA: 3s - loss: 0.1505 - accuracy: 0.9283 - jacard_coef: 0.0012 3/17 [====>.........................] - ETA: 2s - loss: 0.1507 - accuracy: 0.9251 - jacard_coef: 0.0023 4/17 [======>.......................] - ETA: 2s - loss: 0.1499 - accuracy: 0.9323 - jacard_coef: 0.0060 5/17 [=======>......................] - ETA: 2s - loss: 0.1511 - accuracy: 0.9199 - jacard_coef: 0.0062 6/17 [=========>....................] - ETA: 2s - loss: 0.1513 - accuracy: 0.9172 - jacard_coef: 0.0074 7/17 [===========>..................] - ETA: 2s - loss: 0.1516 - accuracy: 0.9163 - jacard_coef: 0.0096 8/17 [=============>................] - ETA: 1s - loss: 0.1518 - accuracy: 0.9103 - jacard_coef: 0.0120 9/17 [==============>...............] - ETA: 1s - loss: 0.1514 - accuracy: 0.9101 - jacard_coef: 0.011410/17 [================>.............] - ETA: 1s - loss: 0.1513 - accuracy: 0.9047 - jacard_coef: 0.011711/17 [==================>...........] - ETA: 1s - loss: 0.1513 - accuracy: 0.9029 - jacard_coef: 0.017012/17 [====================>.........] - ETA: 1s - loss: 0.1514 - accuracy: 0.8981 - jacard_coef: 0.018613/17 [=====================>........] - ETA: 0s - loss: 0.1510 - accuracy: 0.8995 - jacard_coef: 0.018514/17 [=======================>......] - ETA: 0s - loss: 0.1509 - accuracy: 0.8992 - jacard_coef: 0.022115/17 [=========================>....] - ETA: 0s - loss: 0.1505 - accuracy: 0.9018 - jacard_coef: 0.021116/17 [===========================>..] - ETA: 0s - loss: 0.1504 - accuracy: 0.9025 - jacard_coef: 0.020217/17 [==============================] - 3s 205ms/step - loss: 0.1503 - accuracy: 0.9031 - jacard_coef: 0.0190 - val_loss: 0.1516 - val_accuracy: 0.9304 - val_jacard_coef: 3.4109e-12 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1498 - accuracy: 0.9124 - jacard_coef: 0.0058 2/17 [==>...........................] - ETA: 3s - loss: 0.1488 - accuracy: 0.9036 - jacard_coef: 0.0060 3/17 [====>.........................] - ETA: 2s - loss: 0.1475 - accuracy: 0.9099 - jacard_coef: 0.0043 4/17 [======>.......................] - ETA: 2s - loss: 0.1495 - accuracy: 0.8973 - jacard_coef: 0.0053 5/17 [=======>......................] - ETA: 2s - loss: 0.1483 - accuracy: 0.9042 - jacard_coef: 0.0049 6/17 [=========>....................] - ETA: 2s - loss: 0.1484 - accuracy: 0.9007 - jacard_coef: 0.0109 7/17 [===========>..................] - ETA: 2s - loss: 0.1483 - accuracy: 0.8958 - jacard_coef: 0.0123 8/17 [=============>................] - ETA: 1s - loss: 0.1479 - accuracy: 0.8975 - jacard_coef: 0.0121 9/17 [==============>...............] - ETA: 1s - loss: 0.1473 - accuracy: 0.9012 - jacard_coef: 0.012110/17 [================>.............] - ETA: 1s - loss: 0.1473 - accuracy: 0.8985 - jacard_coef: 0.011611/17 [==================>...........] - ETA: 1s - loss: 0.1473 - accuracy: 0.8984 - jacard_coef: 0.010912/17 [====================>.........] - ETA: 1s - loss: 0.1471 - accuracy: 0.8987 - jacard_coef: 0.011013/17 [=====================>........] - ETA: 0s - loss: 0.1468 - accuracy: 0.9014 - jacard_coef: 0.010814/17 [=======================>......] - ETA: 0s - loss: 0.1466 - accuracy: 0.9028 - jacard_coef: 0.010215/17 [=========================>....] - ETA: 0s - loss: 0.1463 - accuracy: 0.9045 - jacard_coef: 0.009716/17 [===========================>..] - ETA: 0s - loss: 0.1461 - accuracy: 0.9056 - jacard_coef: 0.009417/17 [==============================] - 3s 205ms/step - loss: 0.1462 - accuracy: 0.9052 - jacard_coef: 0.0088 - val_loss: 0.1511 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1422 - accuracy: 0.9203 - jacard_coef: 5.9789e-04 2/17 [==>...........................] - ETA: 3s - loss: 0.1443 - accuracy: 0.8972 - jacard_coef: 0.0287     3/17 [====>.........................] - ETA: 2s - loss: 0.1448 - accuracy: 0.9005 - jacard_coef: 0.0194 4/17 [======>.......................] - ETA: 2s - loss: 0.1448 - accuracy: 0.9065 - jacard_coef: 0.0170 5/17 [=======>......................] - ETA: 2s - loss: 0.1448 - accuracy: 0.9088 - jacard_coef: 0.0149 6/17 [=========>....................] - ETA: 2s - loss: 0.1447 - accuracy: 0.9098 - jacard_coef: 0.0126 7/17 [===========>..................] - ETA: 2s - loss: 0.1453 - accuracy: 0.9059 - jacard_coef: 0.0112 8/17 [=============>................] - ETA: 1s - loss: 0.1454 - accuracy: 0.9068 - jacard_coef: 0.0102 9/17 [==============>...............] - ETA: 1s - loss: 0.1453 - accuracy: 0.9075 - jacard_coef: 0.010110/17 [================>.............] - ETA: 1s - loss: 0.1451 - accuracy: 0.9111 - jacard_coef: 0.013411/17 [==================>...........] - ETA: 1s - loss: 0.1451 - accuracy: 0.9097 - jacard_coef: 0.015812/17 [====================>.........] - ETA: 1s - loss: 0.1454 - accuracy: 0.9049 - jacard_coef: 0.014713/17 [=====================>........] - ETA: 0s - loss: 0.1452 - accuracy: 0.9065 - jacard_coef: 0.019914/17 [=======================>......] - ETA: 0s - loss: 0.1449 - accuracy: 0.9078 - jacard_coef: 0.020115/17 [=========================>....] - ETA: 0s - loss: 0.1448 - accuracy: 0.9060 - jacard_coef: 0.019116/17 [===========================>..] - ETA: 0s - loss: 0.1446 - accuracy: 0.9070 - jacard_coef: 0.017917/17 [==============================] - 3s 205ms/step - loss: 0.1447 - accuracy: 0.9065 - jacard_coef: 0.0169 - val_loss: 0.1580 - val_accuracy: 0.9304 - val_jacard_coef: 3.4094e-12 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1460 - accuracy: 0.8820 - jacard_coef: 0.0051 2/17 [==>...........................] - ETA: 3s - loss: 0.1465 - accuracy: 0.8783 - jacard_coef: 0.0118 3/17 [====>.........................] - ETA: 2s - loss: 0.1452 - accuracy: 0.8847 - jacard_coef: 0.0126 4/17 [======>.......................] - ETA: 2s - loss: 0.1445 - accuracy: 0.8874 - jacard_coef: 0.0099 5/17 [=======>......................] - ETA: 2s - loss: 0.1442 - accuracy: 0.8922 - jacard_coef: 0.0087 6/17 [=========>....................] - ETA: 2s - loss: 0.1442 - accuracy: 0.8907 - jacard_coef: 0.0084 7/17 [===========>..................] - ETA: 2s - loss: 0.1436 - accuracy: 0.8937 - jacard_coef: 0.0195 8/17 [=============>................] - ETA: 1s - loss: 0.1434 - accuracy: 0.8943 - jacard_coef: 0.0174 9/17 [==============>...............] - ETA: 1s - loss: 0.1435 - accuracy: 0.8932 - jacard_coef: 0.017610/17 [================>.............] - ETA: 1s - loss: 0.1430 - accuracy: 0.8968 - jacard_coef: 0.016611/17 [==================>...........] - ETA: 1s - loss: 0.1426 - accuracy: 0.8941 - jacard_coef: 0.019712/17 [====================>.........] - ETA: 1s - loss: 0.1422 - accuracy: 0.8976 - jacard_coef: 0.018313/17 [=====================>........] - ETA: 0s - loss: 0.1420 - accuracy: 0.8981 - jacard_coef: 0.022614/17 [=======================>......] - ETA: 0s - loss: 0.1416 - accuracy: 0.9022 - jacard_coef: 0.022915/17 [=========================>....] - ETA: 0s - loss: 0.1415 - accuracy: 0.9033 - jacard_coef: 0.021416/17 [===========================>..] - ETA: 0s - loss: 0.1415 - accuracy: 0.9033 - jacard_coef: 0.020117/17 [==============================] - 3s 205ms/step - loss: 0.1414 - accuracy: 0.9037 - jacard_coef: 0.0190 - val_loss: 0.1551 - val_accuracy: 0.9304 - val_jacard_coef: 3.4108e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0050 (epoch 3)
  Final Val Loss: 0.1551
  Training Time: 0:01:49.688664
  Stability (std): 0.0064

Results saved to: hyperparameter_optimization_20250926_165036/exp_28_Attention_ResUNet_lr5e-4_bs8/Attention_ResUNet_lr0.0005_bs8_results.json

Experiment 28 completed in 127s
Progress: 28/36 completed
Estimated remaining time: 16 minutes

ðŸ”¬ EXPERIMENT 29/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880255.951361 1181249 service.cc:145] XLA service 0x14f1d4463ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880255.951382 1181249 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880256.088128 1181249 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 7:43 - loss: 0.3431 - accuracy: 0.5182 - jacard_coef: 0.08192/9 [=====>........................] - ETA: 58s - loss: 0.3118 - accuracy: 0.4488 - jacard_coef: 0.0819 3/9 [=========>....................] - ETA: 26s - loss: 0.2855 - accuracy: 0.3860 - jacard_coef: 0.08324/9 [============>.................] - ETA: 15s - loss: 0.2683 - accuracy: 0.3329 - jacard_coef: 0.07345/9 [===============>..............] - ETA: 9s - loss: 0.2534 - accuracy: 0.3401 - jacard_coef: 0.0785 6/9 [===================>..........] - ETA: 6s - loss: 0.2420 - accuracy: 0.3373 - jacard_coef: 0.08107/9 [======================>.......] - ETA: 3s - loss: 0.2329 - accuracy: 0.3426 - jacard_coef: 0.07878/9 [=========================>....] - ETA: 1s - loss: 0.2257 - accuracy: 0.3512 - jacard_coef: 0.08089/9 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.3511 - jacard_coef: 0.07899/9 [==============================] - 78s 3s/step - loss: 0.2259 - accuracy: 0.3511 - jacard_coef: 0.0789 - val_loss: 0.3527 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1787 - accuracy: 0.3539 - jacard_coef: 0.06922/9 [=====>........................] - ETA: 2s - loss: 0.1810 - accuracy: 0.2990 - jacard_coef: 0.07633/9 [=========>....................] - ETA: 2s - loss: 0.1846 - accuracy: 0.2548 - jacard_coef: 0.08254/9 [============>.................] - ETA: 2s - loss: 0.1858 - accuracy: 0.2261 - jacard_coef: 0.07955/9 [===============>..............] - ETA: 1s - loss: 0.1854 - accuracy: 0.2204 - jacard_coef: 0.07906/9 [===================>..........] - ETA: 1s - loss: 0.1844 - accuracy: 0.2379 - jacard_coef: 0.08017/9 [======================>.......] - ETA: 0s - loss: 0.1833 - accuracy: 0.2628 - jacard_coef: 0.08458/9 [=========================>....] - ETA: 0s - loss: 0.1824 - accuracy: 0.2816 - jacard_coef: 0.08229/9 [==============================] - 3s 381ms/step - loss: 0.1823 - accuracy: 0.2829 - jacard_coef: 0.0925 - val_loss: 0.3872 - val_accuracy: 0.9261 - val_jacard_coef: 0.0052 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1743 - accuracy: 0.4756 - jacard_coef: 0.08682/9 [=====>........................] - ETA: 2s - loss: 0.1738 - accuracy: 0.4824 - jacard_coef: 0.08453/9 [=========>....................] - ETA: 2s - loss: 0.1734 - accuracy: 0.5020 - jacard_coef: 0.08224/9 [============>.................] - ETA: 2s - loss: 0.1861 - accuracy: 0.4266 - jacard_coef: 0.08125/9 [===============>..............] - ETA: 1s - loss: 0.1899 - accuracy: 0.3774 - jacard_coef: 0.08246/9 [===================>..........] - ETA: 1s - loss: 0.1915 - accuracy: 0.3418 - jacard_coef: 0.08437/9 [======================>.......] - ETA: 0s - loss: 0.1929 - accuracy: 0.3104 - jacard_coef: 0.08208/9 [=========================>....] - ETA: 0s - loss: 0.1939 - accuracy: 0.2901 - jacard_coef: 0.08139/9 [==============================] - 3s 380ms/step - loss: 0.1939 - accuracy: 0.2892 - jacard_coef: 0.0867 - val_loss: 0.0964 - val_accuracy: 0.8983 - val_jacard_coef: 0.0267 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1984 - accuracy: 0.1228 - jacard_coef: 0.09032/9 [=====>........................] - ETA: 2s - loss: 0.1902 - accuracy: 0.1706 - jacard_coef: 0.09143/9 [=========>....................] - ETA: 2s - loss: 0.1856 - accuracy: 0.2264 - jacard_coef: 0.09064/9 [============>.................] - ETA: 2s - loss: 0.1828 - accuracy: 0.2697 - jacard_coef: 0.08465/9 [===============>..............] - ETA: 1s - loss: 0.1808 - accuracy: 0.3117 - jacard_coef: 0.08516/9 [===================>..........] - ETA: 1s - loss: 0.1796 - accuracy: 0.3405 - jacard_coef: 0.08697/9 [======================>.......] - ETA: 0s - loss: 0.1789 - accuracy: 0.3562 - jacard_coef: 0.08568/9 [=========================>....] - ETA: 0s - loss: 0.1782 - accuracy: 0.3759 - jacard_coef: 0.08389/9 [==============================] - 3s 374ms/step - loss: 0.1782 - accuracy: 0.3779 - jacard_coef: 0.0931 - val_loss: 0.5073 - val_accuracy: 0.9200 - val_jacard_coef: 0.0064 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1699 - accuracy: 0.5941 - jacard_coef: 0.09032/9 [=====>........................] - ETA: 2s - loss: 0.1696 - accuracy: 0.5902 - jacard_coef: 0.08543/9 [=========>....................] - ETA: 2s - loss: 0.1695 - accuracy: 0.5826 - jacard_coef: 0.09094/9 [============>.................] - ETA: 2s - loss: 0.1693 - accuracy: 0.5837 - jacard_coef: 0.08885/9 [===============>..............] - ETA: 1s - loss: 0.1693 - accuracy: 0.5845 - jacard_coef: 0.08766/9 [===================>..........] - ETA: 1s - loss: 0.1692 - accuracy: 0.5873 - jacard_coef: 0.08617/9 [======================>.......] - ETA: 0s - loss: 0.1691 - accuracy: 0.5947 - jacard_coef: 0.08378/9 [=========================>....] - ETA: 0s - loss: 0.1690 - accuracy: 0.6052 - jacard_coef: 0.08219/9 [==============================] - 3s 374ms/step - loss: 0.1690 - accuracy: 0.6057 - jacard_coef: 0.0858 - val_loss: 0.1035 - val_accuracy: 0.9258 - val_jacard_coef: 0.0031 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1669 - accuracy: 0.6903 - jacard_coef: 0.10962/9 [=====>........................] - ETA: 2s - loss: 0.1668 - accuracy: 0.6668 - jacard_coef: 0.09493/9 [=========>....................] - ETA: 2s - loss: 0.1669 - accuracy: 0.6389 - jacard_coef: 0.08504/9 [============>.................] - ETA: 2s - loss: 0.1667 - accuracy: 0.6290 - jacard_coef: 0.08655/9 [===============>..............] - ETA: 1s - loss: 0.1666 - accuracy: 0.6285 - jacard_coef: 0.08816/9 [===================>..........] - ETA: 1s - loss: 0.1664 - accuracy: 0.6345 - jacard_coef: 0.09227/9 [======================>.......] - ETA: 0s - loss: 0.1663 - accuracy: 0.6416 - jacard_coef: 0.09048/9 [=========================>....] - ETA: 0s - loss: 0.1661 - accuracy: 0.6549 - jacard_coef: 0.08439/9 [==============================] - 3s 373ms/step - loss: 0.1661 - accuracy: 0.6557 - jacard_coef: 0.0907 - val_loss: 0.1486 - val_accuracy: 0.9202 - val_jacard_coef: 0.0076 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1643 - accuracy: 0.7721 - jacard_coef: 0.07002/9 [=====>........................] - ETA: 2s - loss: 0.1642 - accuracy: 0.7460 - jacard_coef: 0.07083/9 [=========>....................] - ETA: 2s - loss: 0.1641 - accuracy: 0.7354 - jacard_coef: 0.07594/9 [============>.................] - ETA: 2s - loss: 0.1639 - accuracy: 0.7256 - jacard_coef: 0.08225/9 [===============>..............] - ETA: 1s - loss: 0.1640 - accuracy: 0.7161 - jacard_coef: 0.08526/9 [===================>..........] - ETA: 1s - loss: 0.1639 - accuracy: 0.7120 - jacard_coef: 0.08567/9 [======================>.......] - ETA: 0s - loss: 0.1637 - accuracy: 0.7132 - jacard_coef: 0.08108/9 [=========================>....] - ETA: 0s - loss: 0.1634 - accuracy: 0.7236 - jacard_coef: 0.07839/9 [==============================] - 3s 374ms/step - loss: 0.1635 - accuracy: 0.7243 - jacard_coef: 0.0731 - val_loss: 0.1637 - val_accuracy: 0.9176 - val_jacard_coef: 0.0092 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1621 - accuracy: 0.8218 - jacard_coef: 0.06582/9 [=====>........................] - ETA: 2s - loss: 0.1620 - accuracy: 0.7815 - jacard_coef: 0.07013/9 [=========>....................] - ETA: 2s - loss: 0.1621 - accuracy: 0.7553 - jacard_coef: 0.07584/9 [============>.................] - ETA: 2s - loss: 0.1621 - accuracy: 0.7407 - jacard_coef: 0.07535/9 [===============>..............] - ETA: 1s - loss: 0.1618 - accuracy: 0.7401 - jacard_coef: 0.07926/9 [===================>..........] - ETA: 1s - loss: 0.1615 - accuracy: 0.7423 - jacard_coef: 0.07237/9 [======================>.......] - ETA: 0s - loss: 0.1614 - accuracy: 0.7489 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1612 - accuracy: 0.7571 - jacard_coef: 0.07859/9 [==============================] - 3s 380ms/step - loss: 0.1612 - accuracy: 0.7578 - jacard_coef: 0.0698 - val_loss: 0.1660 - val_accuracy: 0.8415 - val_jacard_coef: 0.0356 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1591 - accuracy: 0.8529 - jacard_coef: 0.07152/9 [=====>........................] - ETA: 2s - loss: 0.1596 - accuracy: 0.8508 - jacard_coef: 0.06683/9 [=========>....................] - ETA: 2s - loss: 0.1594 - accuracy: 0.8641 - jacard_coef: 0.05804/9 [============>.................] - ETA: 2s - loss: 0.1591 - accuracy: 0.8738 - jacard_coef: 0.05325/9 [===============>..............] - ETA: 1s - loss: 0.1591 - accuracy: 0.8756 - jacard_coef: 0.05326/9 [===================>..........] - ETA: 1s - loss: 0.1590 - accuracy: 0.8744 - jacard_coef: 0.05157/9 [======================>.......] - ETA: 0s - loss: 0.1589 - accuracy: 0.8779 - jacard_coef: 0.05928/9 [=========================>....] - ETA: 0s - loss: 0.1587 - accuracy: 0.8791 - jacard_coef: 0.05569/9 [==============================] - 3s 379ms/step - loss: 0.1587 - accuracy: 0.8796 - jacard_coef: 0.0576 - val_loss: 0.1696 - val_accuracy: 0.7380 - val_jacard_coef: 0.0761 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1572 - accuracy: 0.8875 - jacard_coef: 0.04582/9 [=====>........................] - ETA: 2s - loss: 0.1570 - accuracy: 0.8898 - jacard_coef: 0.06173/9 [=========>....................] - ETA: 2s - loss: 0.1569 - accuracy: 0.8917 - jacard_coef: 0.04694/9 [============>.................] - ETA: 2s - loss: 0.1567 - accuracy: 0.8913 - jacard_coef: 0.05505/9 [===============>..............] - ETA: 1s - loss: 0.1568 - accuracy: 0.8887 - jacard_coef: 0.05876/9 [===================>..........] - ETA: 1s - loss: 0.1565 - accuracy: 0.8887 - jacard_coef: 0.06127/9 [======================>.......] - ETA: 0s - loss: 0.1564 - accuracy: 0.8863 - jacard_coef: 0.05868/9 [=========================>....] - ETA: 0s - loss: 0.1564 - accuracy: 0.8801 - jacard_coef: 0.05799/9 [==============================] - 3s 379ms/step - loss: 0.1564 - accuracy: 0.8799 - jacard_coef: 0.0515 - val_loss: 0.1718 - val_accuracy: 0.5215 - val_jacard_coef: 0.0831 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1550 - accuracy: 0.8980 - jacard_coef: 0.02972/9 [=====>........................] - ETA: 2s - loss: 0.1548 - accuracy: 0.9056 - jacard_coef: 0.02263/9 [=========>....................] - ETA: 2s - loss: 0.1548 - accuracy: 0.9075 - jacard_coef: 0.01524/9 [============>.................] - ETA: 2s - loss: 0.1546 - accuracy: 0.9136 - jacard_coef: 0.01165/9 [===============>..............] - ETA: 1s - loss: 0.1544 - accuracy: 0.9161 - jacard_coef: 0.00946/9 [===================>..........] - ETA: 1s - loss: 0.1544 - accuracy: 0.9157 - jacard_coef: 0.00797/9 [======================>.......] - ETA: 0s - loss: 0.1544 - accuracy: 0.9138 - jacard_coef: 0.00688/9 [=========================>....] - ETA: 0s - loss: 0.1543 - accuracy: 0.9142 - jacard_coef: 0.00599/9 [==============================] - 3s 375ms/step - loss: 0.1543 - accuracy: 0.9136 - jacard_coef: 0.0097 - val_loss: 0.1688 - val_accuracy: 0.8864 - val_jacard_coef: 0.0612 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1530 - accuracy: 0.9197 - jacard_coef: 6.4071e-042/9 [=====>........................] - ETA: 2s - loss: 0.1524 - accuracy: 0.9328 - jacard_coef: 7.9562e-043/9 [=========>....................] - ETA: 2s - loss: 0.1524 - accuracy: 0.9279 - jacard_coef: 8.4444e-044/9 [============>.................] - ETA: 2s - loss: 0.1527 - accuracy: 0.9167 - jacard_coef: 7.7632e-045/9 [===============>..............] - ETA: 1s - loss: 0.1525 - accuracy: 0.9166 - jacard_coef: 7.0957e-046/9 [===================>..........] - ETA: 1s - loss: 0.1523 - accuracy: 0.9163 - jacard_coef: 0.0038    7/9 [======================>.......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9107 - jacard_coef: 0.01498/9 [=========================>....] - ETA: 0s - loss: 0.1522 - accuracy: 0.9053 - jacard_coef: 0.01859/9 [==============================] - 3s 374ms/step - loss: 0.1522 - accuracy: 0.9049 - jacard_coef: 0.0164 - val_loss: 0.1670 - val_accuracy: 0.9221 - val_jacard_coef: 0.0028 - lr: 0.0010
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1513 - accuracy: 0.8965 - jacard_coef: 0.04422/9 [=====>........................] - ETA: 2s - loss: 0.1518 - accuracy: 0.8908 - jacard_coef: 0.02333/9 [=========>....................] - ETA: 2s - loss: 0.1520 - accuracy: 0.8845 - jacard_coef: 0.01934/9 [============>.................] - ETA: 2s - loss: 0.1516 - accuracy: 0.8915 - jacard_coef: 0.01615/9 [===============>..............] - ETA: 1s - loss: 0.1515 - accuracy: 0.8938 - jacard_coef: 0.01326/9 [===================>..........] - ETA: 1s - loss: 0.1513 - accuracy: 0.8986 - jacard_coef: 0.01117/9 [======================>.......] - ETA: 0s - loss: 0.1511 - accuracy: 0.9019 - jacard_coef: 0.00958/9 [=========================>....] - ETA: 0s - loss: 0.1508 - accuracy: 0.9072 - jacard_coef: 0.00839/9 [==============================] - 3s 374ms/step - loss: 0.1508 - accuracy: 0.9077 - jacard_coef: 0.0074 - val_loss: 0.1649 - val_accuracy: 0.9231 - val_jacard_coef: 0.0024 - lr: 0.0010
Epoch 14/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1491 - accuracy: 0.9239 - jacard_coef: 1.2526e-052/9 [=====>........................] - ETA: 2s - loss: 0.1492 - accuracy: 0.9201 - jacard_coef: 2.9083e-053/9 [=========>....................] - ETA: 2s - loss: 0.1488 - accuracy: 0.9269 - jacard_coef: 3.4967e-044/9 [============>.................] - ETA: 2s - loss: 0.1489 - accuracy: 0.9227 - jacard_coef: 4.7712e-045/9 [===============>..............] - ETA: 1s - loss: 0.1488 - accuracy: 0.9221 - jacard_coef: 4.8369e-046/9 [===================>..........] - ETA: 1s - loss: 0.1491 - accuracy: 0.9161 - jacard_coef: 5.3865e-047/9 [======================>.......] - ETA: 0s - loss: 0.1490 - accuracy: 0.9160 - jacard_coef: 6.4663e-048/9 [=========================>....] - ETA: 0s - loss: 0.1489 - accuracy: 0.9167 - jacard_coef: 6.1750e-049/9 [==============================] - 3s 374ms/step - loss: 0.1489 - accuracy: 0.9160 - jacard_coef: 0.0029 - val_loss: 0.1607 - val_accuracy: 0.9233 - val_jacard_coef: 0.0024 - lr: 0.0010
Epoch 15/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1475 - accuracy: 0.9162 - jacard_coef: 5.6894e-052/9 [=====>........................] - ETA: 2s - loss: 0.1471 - accuracy: 0.9202 - jacard_coef: 3.4740e-053/9 [=========>....................] - ETA: 2s - loss: 0.1474 - accuracy: 0.9174 - jacard_coef: 4.1200e-054/9 [============>.................] - ETA: 2s - loss: 0.1470 - accuracy: 0.9206 - jacard_coef: 3.0900e-055/9 [===============>..............] - ETA: 1s - loss: 0.1468 - accuracy: 0.9226 - jacard_coef: 3.7016e-046/9 [===================>..........] - ETA: 1s - loss: 0.1468 - accuracy: 0.9222 - jacard_coef: 3.3443e-047/9 [======================>.......] - ETA: 0s - loss: 0.1472 - accuracy: 0.9153 - jacard_coef: 4.4695e-048/9 [=========================>....] - ETA: 0s - loss: 0.1470 - accuracy: 0.9169 - jacard_coef: 5.3626e-049/9 [==============================] - 3s 374ms/step - loss: 0.1470 - accuracy: 0.9167 - jacard_coef: 4.9218e-04 - val_loss: 0.1539 - val_accuracy: 0.9237 - val_jacard_coef: 0.0025 - lr: 0.0010
Epoch 16/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1466 - accuracy: 0.9061 - jacard_coef: 6.8000e-042/9 [=====>........................] - ETA: 2s - loss: 0.1460 - accuracy: 0.9142 - jacard_coef: 5.9748e-043/9 [=========>....................] - ETA: 2s - loss: 0.1460 - accuracy: 0.9134 - jacard_coef: 0.0014    4/9 [============>.................] - ETA: 2s - loss: 0.1458 - accuracy: 0.9153 - jacard_coef: 0.00115/9 [===============>..............] - ETA: 1s - loss: 0.1458 - accuracy: 0.9153 - jacard_coef: 9.1573e-046/9 [===================>..........] - ETA: 1s - loss: 0.1455 - accuracy: 0.9179 - jacard_coef: 8.9158e-047/9 [======================>.......] - ETA: 0s - loss: 0.1455 - accuracy: 0.9176 - jacard_coef: 7.6421e-048/9 [=========================>....] - ETA: 0s - loss: 0.1456 - accuracy: 0.9160 - jacard_coef: 6.8863e-049/9 [==============================] - 3s 374ms/step - loss: 0.1457 - accuracy: 0.9155 - jacard_coef: 6.1211e-04 - val_loss: 0.1549 - val_accuracy: 0.9240 - val_jacard_coef: 0.0025 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1431 - accuracy: 0.9475 - jacard_coef: 1.8155e-122/9 [=====>........................] - ETA: 2s - loss: 0.1436 - accuracy: 0.9366 - jacard_coef: 1.5494e-123/9 [=========>....................] - ETA: 2s - loss: 0.1440 - accuracy: 0.9286 - jacard_coef: 1.3971e-124/9 [============>.................] - ETA: 2s - loss: 0.1446 - accuracy: 0.9207 - jacard_coef: 1.2792e-125/9 [===============>..............] - ETA: 1s - loss: 0.1445 - accuracy: 0.9206 - jacard_coef: 0.0074    6/9 [===================>..........] - ETA: 1s - loss: 0.1446 - accuracy: 0.9159 - jacard_coef: 0.01717/9 [======================>.......] - ETA: 0s - loss: 0.1448 - accuracy: 0.9102 - jacard_coef: 0.01698/9 [=========================>....] - ETA: 0s - loss: 0.1447 - accuracy: 0.9071 - jacard_coef: 0.02279/9 [==============================] - 3s 374ms/step - loss: 0.1447 - accuracy: 0.9072 - jacard_coef: 0.0315 - val_loss: 0.1530 - val_accuracy: 0.9303 - val_jacard_coef: 1.4597e-12 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1419 - accuracy: 0.9195 - jacard_coef: 0.09742/9 [=====>........................] - ETA: 2s - loss: 0.1431 - accuracy: 0.9044 - jacard_coef: 0.07013/9 [=========>....................] - ETA: 2s - loss: 0.1433 - accuracy: 0.9009 - jacard_coef: 0.06124/9 [============>.................] - ETA: 2s - loss: 0.1434 - accuracy: 0.9004 - jacard_coef: 0.05735/9 [===============>..............] - ETA: 1s - loss: 0.1435 - accuracy: 0.8990 - jacard_coef: 0.05046/9 [===================>..........] - ETA: 1s - loss: 0.1439 - accuracy: 0.8958 - jacard_coef: 0.04967/9 [======================>.......] - ETA: 0s - loss: 0.1439 - accuracy: 0.8965 - jacard_coef: 0.04698/9 [=========================>....] - ETA: 0s - loss: 0.1438 - accuracy: 0.8988 - jacard_coef: 0.04309/9 [==============================] - 3s 374ms/step - loss: 0.1439 - accuracy: 0.8983 - jacard_coef: 0.0429 - val_loss: 0.1528 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1410 - accuracy: 0.9359 - jacard_coef: 0.00142/9 [=====>........................] - ETA: 2s - loss: 0.1422 - accuracy: 0.9213 - jacard_coef: 0.01203/9 [=========>....................] - ETA: 2s - loss: 0.1425 - accuracy: 0.9163 - jacard_coef: 0.01984/9 [============>.................] - ETA: 2s - loss: 0.1431 - accuracy: 0.9059 - jacard_coef: 0.02685/9 [===============>..............] - ETA: 1s - loss: 0.1432 - accuracy: 0.9064 - jacard_coef: 0.02536/9 [===================>..........] - ETA: 1s - loss: 0.1433 - accuracy: 0.9068 - jacard_coef: 0.02317/9 [======================>.......] - ETA: 0s - loss: 0.1431 - accuracy: 0.9077 - jacard_coef: 0.02128/9 [=========================>....] - ETA: 0s - loss: 0.1430 - accuracy: 0.9084 - jacard_coef: 0.02039/9 [==============================] - 3s 375ms/step - loss: 0.1430 - accuracy: 0.9082 - jacard_coef: 0.0212 - val_loss: 0.1522 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1419 - accuracy: 0.9257 - jacard_coef: 5.2607e-042/9 [=====>........................] - ETA: 2s - loss: 0.1418 - accuracy: 0.9228 - jacard_coef: 8.7005e-043/9 [=========>....................] - ETA: 2s - loss: 0.1424 - accuracy: 0.9175 - jacard_coef: 5.8004e-044/9 [============>.................] - ETA: 2s - loss: 0.1422 - accuracy: 0.9202 - jacard_coef: 9.0296e-045/9 [===============>..............] - ETA: 1s - loss: 0.1420 - accuracy: 0.9215 - jacard_coef: 0.0013    6/9 [===================>..........] - ETA: 1s - loss: 0.1423 - accuracy: 0.9168 - jacard_coef: 0.00117/9 [======================>.......] - ETA: 0s - loss: 0.1424 - accuracy: 0.9165 - jacard_coef: 9.7893e-048/9 [=========================>....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9161 - jacard_coef: 8.5931e-049/9 [==============================] - 3s 375ms/step - loss: 0.1424 - accuracy: 0.9157 - jacard_coef: 7.6383e-04 - val_loss: 0.1497 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0831 (epoch 10)
  Final Val Loss: 0.1497
  Training Time: 0:02:23.287295
  Stability (std): 0.0066

Results saved to: hyperparameter_optimization_20250926_165036/exp_29_Attention_ResUNet_lr5e-4_bs16/Attention_ResUNet_lr0.0005_bs16_results.json

Experiment 29 completed in 160s
Progress: 29/36 completed
Estimated remaining time: 18 minutes

ðŸ”¬ EXPERIMENT 30/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880423.017874 1189018 service.cc:145] XLA service 0x1458e1e443a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880423.017894 1189018 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880423.156565 1189018 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:39 - loss: 0.3391 - accuracy: 0.4947 - jacard_coef: 0.09022/5 [===========>..................] - ETA: 46s - loss: 0.3190 - accuracy: 0.4212 - jacard_coef: 0.0869 3/5 [=================>............] - ETA: 16s - loss: 0.2936 - accuracy: 0.3588 - jacard_coef: 0.08204/5 [=======================>......] - ETA: 5s - loss: 0.2780 - accuracy: 0.3287 - jacard_coef: 0.0818 5/5 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.3273 - jacard_coef: 0.07325/5 [==============================] - 96s 7s/step - loss: 0.2775 - accuracy: 0.3273 - jacard_coef: 0.0732 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2159 - accuracy: 0.2215 - jacard_coef: 0.08992/5 [===========>..................] - ETA: 2s - loss: 0.2069 - accuracy: 0.2110 - jacard_coef: 0.07793/5 [=================>............] - ETA: 1s - loss: 0.2043 - accuracy: 0.2434 - jacard_coef: 0.07584/5 [=======================>......] - ETA: 0s - loss: 0.2000 - accuracy: 0.2939 - jacard_coef: 0.07915/5 [==============================] - 3s 640ms/step - loss: 0.2004 - accuracy: 0.2939 - jacard_coef: 0.0679 - val_loss: 1.1240 - val_accuracy: 0.9302 - val_jacard_coef: 1.4581e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2290 - accuracy: 0.1366 - jacard_coef: 0.06702/5 [===========>..................] - ETA: 2s - loss: 0.2220 - accuracy: 0.1486 - jacard_coef: 0.08683/5 [=================>............] - ETA: 1s - loss: 0.2130 - accuracy: 0.1544 - jacard_coef: 0.08604/5 [=======================>......] - ETA: 0s - loss: 0.2099 - accuracy: 0.1498 - jacard_coef: 0.08265/5 [==============================] - 3s 639ms/step - loss: 0.2097 - accuracy: 0.1505 - jacard_coef: 0.1017 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1935 - accuracy: 0.2017 - jacard_coef: 0.09512/5 [===========>..................] - ETA: 2s - loss: 0.1930 - accuracy: 0.2004 - jacard_coef: 0.08553/5 [=================>............] - ETA: 1s - loss: 0.1907 - accuracy: 0.2159 - jacard_coef: 0.08584/5 [=======================>......] - ETA: 0s - loss: 0.1902 - accuracy: 0.2341 - jacard_coef: 0.08215/5 [==============================] - 3s 649ms/step - loss: 0.1903 - accuracy: 0.2344 - jacard_coef: 0.0794 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1878 - accuracy: 0.4199 - jacard_coef: 0.09062/5 [===========>..................] - ETA: 2s - loss: 0.1860 - accuracy: 0.3933 - jacard_coef: 0.08653/5 [=================>............] - ETA: 1s - loss: 0.1848 - accuracy: 0.3773 - jacard_coef: 0.08404/5 [=======================>......] - ETA: 0s - loss: 0.1843 - accuracy: 0.3622 - jacard_coef: 0.08105/5 [==============================] - 3s 639ms/step - loss: 0.1843 - accuracy: 0.3619 - jacard_coef: 0.0652 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1794 - accuracy: 0.3515 - jacard_coef: 0.08602/5 [===========>..................] - ETA: 2s - loss: 0.1800 - accuracy: 0.3802 - jacard_coef: 0.08303/5 [=================>............] - ETA: 1s - loss: 0.1851 - accuracy: 0.3504 - jacard_coef: 0.07924/5 [=======================>......] - ETA: 0s - loss: 0.1855 - accuracy: 0.3577 - jacard_coef: 0.07975/5 [==============================] - 3s 638ms/step - loss: 0.1856 - accuracy: 0.3562 - jacard_coef: 0.0675 - val_loss: 1.1202 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1827 - accuracy: 0.3255 - jacard_coef: 0.07142/5 [===========>..................] - ETA: 2s - loss: 0.1871 - accuracy: 0.2831 - jacard_coef: 0.07563/5 [=================>............] - ETA: 1s - loss: 0.1849 - accuracy: 0.2784 - jacard_coef: 0.08034/5 [=======================>......] - ETA: 0s - loss: 0.1835 - accuracy: 0.2812 - jacard_coef: 0.08345/5 [==============================] - 3s 639ms/step - loss: 0.1836 - accuracy: 0.2803 - jacard_coef: 0.0675 - val_loss: 1.1199 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1777 - accuracy: 0.3790 - jacard_coef: 0.08222/5 [===========>..................] - ETA: 2s - loss: 0.1814 - accuracy: 0.3957 - jacard_coef: 0.08733/5 [=================>............] - ETA: 1s - loss: 0.1805 - accuracy: 0.3826 - jacard_coef: 0.08694/5 [=======================>......] - ETA: 0s - loss: 0.1794 - accuracy: 0.4030 - jacard_coef: 0.08415/5 [==============================] - 3s 655ms/step - loss: 0.1794 - accuracy: 0.4013 - jacard_coef: 0.0685 - val_loss: 1.1016 - val_accuracy: 0.9303 - val_jacard_coef: 6.7043e-04 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1747 - accuracy: 0.5537 - jacard_coef: 0.07782/5 [===========>..................] - ETA: 2s - loss: 0.1743 - accuracy: 0.5528 - jacard_coef: 0.07743/5 [=================>............] - ETA: 1s - loss: 0.1752 - accuracy: 0.5333 - jacard_coef: 0.07754/5 [=======================>......] - ETA: 0s - loss: 0.1748 - accuracy: 0.5264 - jacard_coef: 0.08485/5 [==============================] - 3s 641ms/step - loss: 0.1749 - accuracy: 0.5260 - jacard_coef: 0.0776 - val_loss: 1.0931 - val_accuracy: 0.9293 - val_jacard_coef: 4.3156e-04 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1717 - accuracy: 0.5468 - jacard_coef: 0.08882/5 [===========>..................] - ETA: 2s - loss: 0.1717 - accuracy: 0.5190 - jacard_coef: 0.09123/5 [=================>............] - ETA: 1s - loss: 0.1718 - accuracy: 0.5113 - jacard_coef: 0.08664/5 [=======================>......] - ETA: 0s - loss: 0.1722 - accuracy: 0.5176 - jacard_coef: 0.08245/5 [==============================] - 3s 638ms/step - loss: 0.1724 - accuracy: 0.5173 - jacard_coef: 0.0825 - val_loss: 1.1155 - val_accuracy: 0.9301 - val_jacard_coef: 2.4728e-04 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1712 - accuracy: 0.4695 - jacard_coef: 0.08602/5 [===========>..................] - ETA: 2s - loss: 0.1723 - accuracy: 0.4175 - jacard_coef: 0.08943/5 [=================>............] - ETA: 1s - loss: 0.1751 - accuracy: 0.3777 - jacard_coef: 0.08244/5 [=======================>......] - ETA: 0s - loss: 0.1751 - accuracy: 0.3595 - jacard_coef: 0.08005/5 [==============================] - 3s 639ms/step - loss: 0.1757 - accuracy: 0.3583 - jacard_coef: 0.0849 - val_loss: 1.0813 - val_accuracy: 0.9271 - val_jacard_coef: 2.9305e-04 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1742 - accuracy: 0.2950 - jacard_coef: 0.10662/5 [===========>..................] - ETA: 2s - loss: 0.1775 - accuracy: 0.2814 - jacard_coef: 0.08873/5 [=================>............] - ETA: 1s - loss: 0.1761 - accuracy: 0.2866 - jacard_coef: 0.08524/5 [=======================>......] - ETA: 0s - loss: 0.1757 - accuracy: 0.2840 - jacard_coef: 0.08155/5 [==============================] - 3s 651ms/step - loss: 0.1757 - accuracy: 0.2836 - jacard_coef: 0.0941 - val_loss: 0.3510 - val_accuracy: 0.5818 - val_jacard_coef: 0.0773 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1776 - accuracy: 0.4937 - jacard_coef: 0.08402/5 [===========>..................] - ETA: 2s - loss: 0.1752 - accuracy: 0.5458 - jacard_coef: 0.06793/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.5766 - jacard_coef: 0.06954/5 [=======================>......] - ETA: 0s - loss: 0.1751 - accuracy: 0.5719 - jacard_coef: 0.07305/5 [==============================] - 3s 652ms/step - loss: 0.1754 - accuracy: 0.5705 - jacard_coef: 0.0765 - val_loss: 0.1902 - val_accuracy: 0.4898 - val_jacard_coef: 0.0776 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1718 - accuracy: 0.7516 - jacard_coef: 0.06042/5 [===========>..................] - ETA: 2s - loss: 0.1723 - accuracy: 0.7600 - jacard_coef: 0.05943/5 [=================>............] - ETA: 1s - loss: 0.1729 - accuracy: 0.7524 - jacard_coef: 0.05444/5 [=======================>......] - ETA: 0s - loss: 0.1726 - accuracy: 0.7504 - jacard_coef: 0.06105/5 [==============================] - 3s 638ms/step - loss: 0.1726 - accuracy: 0.7495 - jacard_coef: 0.0491 - val_loss: 0.1667 - val_accuracy: 0.7261 - val_jacard_coef: 0.0567 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1680 - accuracy: 0.7309 - jacard_coef: 0.05742/5 [===========>..................] - ETA: 2s - loss: 0.1679 - accuracy: 0.7232 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1681 - accuracy: 0.7047 - jacard_coef: 0.07004/5 [=======================>......] - ETA: 0s - loss: 0.1680 - accuracy: 0.6963 - jacard_coef: 0.07255/5 [==============================] - 3s 638ms/step - loss: 0.1681 - accuracy: 0.6948 - jacard_coef: 0.0592 - val_loss: 0.1514 - val_accuracy: 0.8635 - val_jacard_coef: 0.0428 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1681 - accuracy: 0.6677 - jacard_coef: 0.08282/5 [===========>..................] - ETA: 2s - loss: 0.1671 - accuracy: 0.7197 - jacard_coef: 0.07803/5 [=================>............] - ETA: 1s - loss: 0.1662 - accuracy: 0.7815 - jacard_coef: 0.05774/5 [=======================>......] - ETA: 0s - loss: 0.1661 - accuracy: 0.7647 - jacard_coef: 0.05875/5 [==============================] - 3s 640ms/step - loss: 0.1663 - accuracy: 0.7616 - jacard_coef: 0.0808 - val_loss: 0.1609 - val_accuracy: 0.8122 - val_jacard_coef: 0.0402 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1962 - accuracy: 0.3702 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 2s - loss: 0.1852 - accuracy: 0.4126 - jacard_coef: 0.06923/5 [=================>............] - ETA: 1s - loss: 0.1817 - accuracy: 0.4936 - jacard_coef: 0.07364/5 [=======================>......] - ETA: 0s - loss: 0.1794 - accuracy: 0.5356 - jacard_coef: 0.08035/5 [==============================] - 3s 641ms/step - loss: 0.1794 - accuracy: 0.5342 - jacard_coef: 0.0720 - val_loss: 0.1357 - val_accuracy: 0.9139 - val_jacard_coef: 0.0219 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1727 - accuracy: 0.6331 - jacard_coef: 0.08132/5 [===========>..................] - ETA: 2s - loss: 0.1726 - accuracy: 0.6975 - jacard_coef: 0.06833/5 [=================>............] - ETA: 1s - loss: 0.1735 - accuracy: 0.6213 - jacard_coef: 0.08024/5 [=======================>......] - ETA: 0s - loss: 0.1735 - accuracy: 0.6678 - jacard_coef: 0.07645/5 [==============================] - 3s 638ms/step - loss: 0.1735 - accuracy: 0.6644 - jacard_coef: 0.0616 - val_loss: 0.1063 - val_accuracy: 0.9210 - val_jacard_coef: 0.0106 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1689 - accuracy: 0.8360 - jacard_coef: 0.04972/5 [===========>..................] - ETA: 2s - loss: 0.1709 - accuracy: 0.7694 - jacard_coef: 0.06183/5 [=================>............] - ETA: 1s - loss: 0.1709 - accuracy: 0.7790 - jacard_coef: 0.06124/5 [=======================>......] - ETA: 0s - loss: 0.1706 - accuracy: 0.7787 - jacard_coef: 0.05765/5 [==============================] - 3s 639ms/step - loss: 0.1706 - accuracy: 0.7738 - jacard_coef: 0.0489 - val_loss: 0.1036 - val_accuracy: 0.9195 - val_jacard_coef: 0.0141 - lr: 2.5000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1700 - accuracy: 0.8094 - jacard_coef: 0.04972/5 [===========>..................] - ETA: 2s - loss: 0.1689 - accuracy: 0.8097 - jacard_coef: 0.04643/5 [=================>............] - ETA: 1s - loss: 0.1695 - accuracy: 0.7802 - jacard_coef: 0.05844/5 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.7928 - jacard_coef: 0.05555/5 [==============================] - 3s 638ms/step - loss: 0.1695 - accuracy: 0.7880 - jacard_coef: 0.0553 - val_loss: 0.1056 - val_accuracy: 0.9213 - val_jacard_coef: 0.0111 - lr: 2.5000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1678 - accuracy: 0.8048 - jacard_coef: 0.06092/5 [===========>..................] - ETA: 2s - loss: 0.1676 - accuracy: 0.8076 - jacard_coef: 0.05383/5 [=================>............] - ETA: 1s - loss: 0.1675 - accuracy: 0.8158 - jacard_coef: 0.05074/5 [=======================>......] - ETA: 0s - loss: 0.1675 - accuracy: 0.8099 - jacard_coef: 0.05335/5 [==============================] - 3s 638ms/step - loss: 0.1676 - accuracy: 0.8064 - jacard_coef: 0.0441 - val_loss: 0.1124 - val_accuracy: 0.9247 - val_jacard_coef: 0.0068 - lr: 2.5000e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1661 - accuracy: 0.8373 - jacard_coef: 0.06322/5 [===========>..................] - ETA: 2s - loss: 0.1657 - accuracy: 0.8391 - jacard_coef: 0.05613/5 [=================>............] - ETA: 1s - loss: 0.1664 - accuracy: 0.8165 - jacard_coef: 0.05594/5 [=======================>......] - ETA: 0s - loss: 0.1664 - accuracy: 0.8065 - jacard_coef: 0.05585/5 [==============================] - 3s 639ms/step - loss: 0.1665 - accuracy: 0.8037 - jacard_coef: 0.0459 - val_loss: 0.1216 - val_accuracy: 0.9271 - val_jacard_coef: 0.0041 - lr: 2.5000e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1642 - accuracy: 0.8541 - jacard_coef: 0.06422/5 [===========>..................] - ETA: 2s - loss: 0.1653 - accuracy: 0.8171 - jacard_coef: 0.06713/5 [=================>............] - ETA: 1s - loss: 0.1664 - accuracy: 0.7388 - jacard_coef: 0.06574/5 [=======================>......] - ETA: 0s - loss: 0.1663 - accuracy: 0.7605 - jacard_coef: 0.06485/5 [==============================] - 3s 639ms/step - loss: 0.1663 - accuracy: 0.7580 - jacard_coef: 0.0841 - val_loss: 0.1346 - val_accuracy: 0.9278 - val_jacard_coef: 0.0033 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0776 (epoch 13)
  Final Val Loss: 0.1346
  Training Time: 0:02:50.585235
  Stability (std): 0.0225

Results saved to: hyperparameter_optimization_20250926_165036/exp_30_Attention_ResUNet_lr5e-4_bs32/Attention_ResUNet_lr0.0005_bs32_results.json

Experiment 30 completed in 188s
Progress: 30/36 completed
Estimated remaining time: 18 minutes

ðŸ”¬ EXPERIMENT 31/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880600.992249 1197065 service.cc:145] XLA service 0x1498c857ab90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880600.992273 1197065 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880601.128887 1197065 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 13:57 - loss: 0.3418 - accuracy: 0.5276 - jacard_coef: 0.0940 2/17 [==>...........................] - ETA: 1:08 - loss: 0.3119 - accuracy: 0.4817 - jacard_coef: 0.0869  3/17 [====>.........................] - ETA: 33s - loss: 0.2924 - accuracy: 0.4231 - jacard_coef: 0.0856  4/17 [======>.......................] - ETA: 21s - loss: 0.2761 - accuracy: 0.4078 - jacard_coef: 0.0862 5/17 [=======>......................] - ETA: 15s - loss: 0.2653 - accuracy: 0.4290 - jacard_coef: 0.0800 6/17 [=========>....................] - ETA: 11s - loss: 0.2561 - accuracy: 0.3934 - jacard_coef: 0.0708 7/17 [===========>..................] - ETA: 9s - loss: 0.2466 - accuracy: 0.3829 - jacard_coef: 0.0752  8/17 [=============>................] - ETA: 7s - loss: 0.2415 - accuracy: 0.3739 - jacard_coef: 0.0738 9/17 [==============>...............] - ETA: 6s - loss: 0.2366 - accuracy: 0.3780 - jacard_coef: 0.070410/17 [================>.............] - ETA: 4s - loss: 0.2318 - accuracy: 0.3777 - jacard_coef: 0.072311/17 [==================>...........] - ETA: 3s - loss: 0.2284 - accuracy: 0.3799 - jacard_coef: 0.074212/17 [====================>.........] - ETA: 3s - loss: 0.2247 - accuracy: 0.3784 - jacard_coef: 0.075413/17 [=====================>........] - ETA: 2s - loss: 0.2213 - accuracy: 0.3773 - jacard_coef: 0.078014/17 [=======================>......] - ETA: 1s - loss: 0.2184 - accuracy: 0.3723 - jacard_coef: 0.077715/17 [=========================>....] - ETA: 1s - loss: 0.2159 - accuracy: 0.3761 - jacard_coef: 0.078416/17 [===========================>..] - ETA: 0s - loss: 0.2137 - accuracy: 0.3794 - jacard_coef: 0.078417/17 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.3806 - jacard_coef: 0.075617/17 [==============================] - 67s 937ms/step - loss: 0.2135 - accuracy: 0.3806 - jacard_coef: 0.0756 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1771 - accuracy: 0.4070 - jacard_coef: 0.0846 2/17 [==>...........................] - ETA: 3s - loss: 0.1764 - accuracy: 0.4230 - jacard_coef: 0.0635 3/17 [====>.........................] - ETA: 2s - loss: 0.1909 - accuracy: 0.3873 - jacard_coef: 0.0650 4/17 [======>.......................] - ETA: 2s - loss: 0.1874 - accuracy: 0.4568 - jacard_coef: 0.0707 5/17 [=======>......................] - ETA: 2s - loss: 0.1858 - accuracy: 0.4908 - jacard_coef: 0.0755 6/17 [=========>....................] - ETA: 2s - loss: 0.1856 - accuracy: 0.5055 - jacard_coef: 0.0785 7/17 [===========>..................] - ETA: 2s - loss: 0.1849 - accuracy: 0.5093 - jacard_coef: 0.0788 8/17 [=============>................] - ETA: 1s - loss: 0.1844 - accuracy: 0.5072 - jacard_coef: 0.0780 9/17 [==============>...............] - ETA: 1s - loss: 0.1840 - accuracy: 0.4985 - jacard_coef: 0.079010/17 [================>.............] - ETA: 1s - loss: 0.1834 - accuracy: 0.4887 - jacard_coef: 0.080111/17 [==================>...........] - ETA: 1s - loss: 0.1835 - accuracy: 0.4705 - jacard_coef: 0.075912/17 [====================>.........] - ETA: 1s - loss: 0.1832 - accuracy: 0.4583 - jacard_coef: 0.075913/17 [=====================>........] - ETA: 0s - loss: 0.1829 - accuracy: 0.4479 - jacard_coef: 0.078014/17 [=======================>......] - ETA: 0s - loss: 0.1825 - accuracy: 0.4406 - jacard_coef: 0.078115/17 [=========================>....] - ETA: 0s - loss: 0.1819 - accuracy: 0.4454 - jacard_coef: 0.077616/17 [===========================>..] - ETA: 0s - loss: 0.1814 - accuracy: 0.4474 - jacard_coef: 0.078617/17 [==============================] - 3s 204ms/step - loss: 0.1813 - accuracy: 0.4472 - jacard_coef: 0.0823 - val_loss: 1.1172 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1716 - accuracy: 0.6184 - jacard_coef: 0.0920 2/17 [==>...........................] - ETA: 3s - loss: 0.1707 - accuracy: 0.6375 - jacard_coef: 0.0930 3/17 [====>.........................] - ETA: 2s - loss: 0.1700 - accuracy: 0.6434 - jacard_coef: 0.0914 4/17 [======>.......................] - ETA: 2s - loss: 0.1698 - accuracy: 0.6402 - jacard_coef: 0.0968 5/17 [=======>......................] - ETA: 2s - loss: 0.1794 - accuracy: 0.6116 - jacard_coef: 0.0891 6/17 [=========>....................] - ETA: 2s - loss: 0.1791 - accuracy: 0.5877 - jacard_coef: 0.0876 7/17 [===========>..................] - ETA: 2s - loss: 0.1788 - accuracy: 0.5644 - jacard_coef: 0.0834 8/17 [=============>................] - ETA: 1s - loss: 0.1791 - accuracy: 0.5402 - jacard_coef: 0.0809 9/17 [==============>...............] - ETA: 1s - loss: 0.1791 - accuracy: 0.5264 - jacard_coef: 0.079510/17 [================>.............] - ETA: 1s - loss: 0.1786 - accuracy: 0.5285 - jacard_coef: 0.076511/17 [==================>...........] - ETA: 1s - loss: 0.1791 - accuracy: 0.5238 - jacard_coef: 0.077312/17 [====================>.........] - ETA: 1s - loss: 0.1795 - accuracy: 0.5279 - jacard_coef: 0.080113/17 [=====================>........] - ETA: 0s - loss: 0.1790 - accuracy: 0.5330 - jacard_coef: 0.082214/17 [=======================>......] - ETA: 0s - loss: 0.1788 - accuracy: 0.5393 - jacard_coef: 0.080615/17 [=========================>....] - ETA: 0s - loss: 0.1785 - accuracy: 0.5442 - jacard_coef: 0.079116/17 [===========================>..] - ETA: 0s - loss: 0.1781 - accuracy: 0.5502 - jacard_coef: 0.079117/17 [==============================] - 4s 209ms/step - loss: 0.1781 - accuracy: 0.5494 - jacard_coef: 0.0769 - val_loss: 0.6661 - val_accuracy: 0.9290 - val_jacard_coef: 0.0016 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1712 - accuracy: 0.7253 - jacard_coef: 0.0653 2/17 [==>...........................] - ETA: 3s - loss: 0.1727 - accuracy: 0.6783 - jacard_coef: 0.0743 3/17 [====>.........................] - ETA: 2s - loss: 0.1723 - accuracy: 0.6481 - jacard_coef: 0.0696 4/17 [======>.......................] - ETA: 2s - loss: 0.1717 - accuracy: 0.6408 - jacard_coef: 0.0733 5/17 [=======>......................] - ETA: 2s - loss: 0.1723 - accuracy: 0.6106 - jacard_coef: 0.0780 6/17 [=========>....................] - ETA: 2s - loss: 0.1718 - accuracy: 0.6051 - jacard_coef: 0.0818 7/17 [===========>..................] - ETA: 2s - loss: 0.1716 - accuracy: 0.6054 - jacard_coef: 0.0816 8/17 [=============>................] - ETA: 1s - loss: 0.1713 - accuracy: 0.6209 - jacard_coef: 0.0780 9/17 [==============>...............] - ETA: 1s - loss: 0.1709 - accuracy: 0.6364 - jacard_coef: 0.074110/17 [================>.............] - ETA: 1s - loss: 0.1706 - accuracy: 0.6500 - jacard_coef: 0.072311/17 [==================>...........] - ETA: 1s - loss: 0.1703 - accuracy: 0.6545 - jacard_coef: 0.073312/17 [====================>.........] - ETA: 1s - loss: 0.1700 - accuracy: 0.6614 - jacard_coef: 0.074613/17 [=====================>........] - ETA: 0s - loss: 0.1696 - accuracy: 0.6738 - jacard_coef: 0.072214/17 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.6781 - jacard_coef: 0.070815/17 [=========================>....] - ETA: 0s - loss: 0.1690 - accuracy: 0.6807 - jacard_coef: 0.071516/17 [===========================>..] - ETA: 0s - loss: 0.1687 - accuracy: 0.6818 - jacard_coef: 0.071817/17 [==============================] - 4s 208ms/step - loss: 0.1689 - accuracy: 0.6804 - jacard_coef: 0.0747 - val_loss: 0.1496 - val_accuracy: 0.8003 - val_jacard_coef: 0.0240 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1650 - accuracy: 0.8086 - jacard_coef: 0.0837 2/17 [==>...........................] - ETA: 3s - loss: 0.1663 - accuracy: 0.8147 - jacard_coef: 0.0682 3/17 [====>.........................] - ETA: 2s - loss: 0.1662 - accuracy: 0.8151 - jacard_coef: 0.0603 4/17 [======>.......................] - ETA: 2s - loss: 0.1665 - accuracy: 0.8231 - jacard_coef: 0.0670 5/17 [=======>......................] - ETA: 2s - loss: 0.1669 - accuracy: 0.8183 - jacard_coef: 0.0631 6/17 [=========>....................] - ETA: 2s - loss: 0.1671 - accuracy: 0.8133 - jacard_coef: 0.0643 7/17 [===========>..................] - ETA: 2s - loss: 0.1673 - accuracy: 0.8037 - jacard_coef: 0.0641 8/17 [=============>................] - ETA: 1s - loss: 0.1672 - accuracy: 0.8021 - jacard_coef: 0.0659 9/17 [==============>...............] - ETA: 1s - loss: 0.1674 - accuracy: 0.7978 - jacard_coef: 0.067910/17 [================>.............] - ETA: 1s - loss: 0.1675 - accuracy: 0.7959 - jacard_coef: 0.067811/17 [==================>...........] - ETA: 1s - loss: 0.1674 - accuracy: 0.7930 - jacard_coef: 0.074712/17 [====================>.........] - ETA: 1s - loss: 0.1672 - accuracy: 0.7903 - jacard_coef: 0.074713/17 [=====================>........] - ETA: 0s - loss: 0.1671 - accuracy: 0.7868 - jacard_coef: 0.074914/17 [=======================>......] - ETA: 0s - loss: 0.1670 - accuracy: 0.7818 - jacard_coef: 0.072015/17 [=========================>....] - ETA: 0s - loss: 0.1669 - accuracy: 0.7776 - jacard_coef: 0.071016/17 [===========================>..] - ETA: 0s - loss: 0.1668 - accuracy: 0.7772 - jacard_coef: 0.070217/17 [==============================] - 3s 205ms/step - loss: 0.1668 - accuracy: 0.7776 - jacard_coef: 0.0717 - val_loss: 0.1240 - val_accuracy: 0.9177 - val_jacard_coef: 0.0042 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1621 - accuracy: 0.8617 - jacard_coef: 0.0816 2/17 [==>...........................] - ETA: 3s - loss: 0.1618 - accuracy: 0.8529 - jacard_coef: 0.0788 3/17 [====>.........................] - ETA: 2s - loss: 0.1623 - accuracy: 0.8321 - jacard_coef: 0.0837 4/17 [======>.......................] - ETA: 2s - loss: 0.1626 - accuracy: 0.8125 - jacard_coef: 0.0841 5/17 [=======>......................] - ETA: 2s - loss: 0.1622 - accuracy: 0.7936 - jacard_coef: 0.0753 6/17 [=========>....................] - ETA: 2s - loss: 0.1618 - accuracy: 0.7905 - jacard_coef: 0.0745 7/17 [===========>..................] - ETA: 2s - loss: 0.1615 - accuracy: 0.7888 - jacard_coef: 0.0743 8/17 [=============>................] - ETA: 1s - loss: 0.1611 - accuracy: 0.7934 - jacard_coef: 0.0701 9/17 [==============>...............] - ETA: 1s - loss: 0.1608 - accuracy: 0.8013 - jacard_coef: 0.067910/17 [================>.............] - ETA: 1s - loss: 0.1607 - accuracy: 0.8085 - jacard_coef: 0.061511/17 [==================>...........] - ETA: 1s - loss: 0.1605 - accuracy: 0.8142 - jacard_coef: 0.058012/17 [====================>.........] - ETA: 1s - loss: 0.1601 - accuracy: 0.8207 - jacard_coef: 0.056913/17 [=====================>........] - ETA: 0s - loss: 0.1600 - accuracy: 0.8243 - jacard_coef: 0.055214/17 [=======================>......] - ETA: 0s - loss: 0.1599 - accuracy: 0.8258 - jacard_coef: 0.055015/17 [=========================>....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8300 - jacard_coef: 0.054216/17 [===========================>..] - ETA: 0s - loss: 0.1595 - accuracy: 0.8328 - jacard_coef: 0.056517/17 [==============================] - 4s 208ms/step - loss: 0.1598 - accuracy: 0.8303 - jacard_coef: 0.0541 - val_loss: 0.1636 - val_accuracy: 0.8896 - val_jacard_coef: 0.0545 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1554 - accuracy: 0.9246 - jacard_coef: 0.0060 2/17 [==>...........................] - ETA: 3s - loss: 0.1572 - accuracy: 0.9147 - jacard_coef: 0.0106 3/17 [====>.........................] - ETA: 2s - loss: 0.1578 - accuracy: 0.9095 - jacard_coef: 0.0103 4/17 [======>.......................] - ETA: 2s - loss: 0.1587 - accuracy: 0.9181 - jacard_coef: 0.0085 5/17 [=======>......................] - ETA: 2s - loss: 0.1585 - accuracy: 0.9167 - jacard_coef: 0.0085 6/17 [=========>....................] - ETA: 2s - loss: 0.1588 - accuracy: 0.9099 - jacard_coef: 0.0083 7/17 [===========>..................] - ETA: 2s - loss: 0.1586 - accuracy: 0.9133 - jacard_coef: 0.0078 8/17 [=============>................] - ETA: 1s - loss: 0.1587 - accuracy: 0.9131 - jacard_coef: 0.0078 9/17 [==============>...............] - ETA: 1s - loss: 0.1586 - accuracy: 0.9132 - jacard_coef: 0.007410/17 [================>.............] - ETA: 1s - loss: 0.1588 - accuracy: 0.9153 - jacard_coef: 0.007811/17 [==================>...........] - ETA: 1s - loss: 0.1588 - accuracy: 0.9160 - jacard_coef: 0.007512/17 [====================>.........] - ETA: 1s - loss: 0.1586 - accuracy: 0.9147 - jacard_coef: 0.006813/17 [=====================>........] - ETA: 0s - loss: 0.1584 - accuracy: 0.9134 - jacard_coef: 0.006414/17 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.9117 - jacard_coef: 0.006015/17 [=========================>....] - ETA: 0s - loss: 0.1581 - accuracy: 0.9109 - jacard_coef: 0.005716/17 [===========================>..] - ETA: 0s - loss: 0.1580 - accuracy: 0.9112 - jacard_coef: 0.005417/17 [==============================] - 3s 205ms/step - loss: 0.1580 - accuracy: 0.9100 - jacard_coef: 0.0052 - val_loss: 0.1697 - val_accuracy: 0.8333 - val_jacard_coef: 0.0495 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1558 - accuracy: 0.8797 - jacard_coef: 0.0015 2/17 [==>...........................] - ETA: 3s - loss: 0.1567 - accuracy: 0.8785 - jacard_coef: 0.0032 3/17 [====>.........................] - ETA: 2s - loss: 0.1557 - accuracy: 0.8875 - jacard_coef: 0.0021 4/17 [======>.......................] - ETA: 2s - loss: 0.1557 - accuracy: 0.9048 - jacard_coef: 0.0044 5/17 [=======>......................] - ETA: 2s - loss: 0.1553 - accuracy: 0.9004 - jacard_coef: 0.0037 6/17 [=========>....................] - ETA: 2s - loss: 0.1551 - accuracy: 0.8996 - jacard_coef: 0.0047 7/17 [===========>..................] - ETA: 2s - loss: 0.1546 - accuracy: 0.9030 - jacard_coef: 0.0044 8/17 [=============>................] - ETA: 1s - loss: 0.1543 - accuracy: 0.9059 - jacard_coef: 0.0050 9/17 [==============>...............] - ETA: 1s - loss: 0.1540 - accuracy: 0.9089 - jacard_coef: 0.005510/17 [================>.............] - ETA: 1s - loss: 0.1539 - accuracy: 0.9082 - jacard_coef: 0.007111/17 [==================>...........] - ETA: 1s - loss: 0.1537 - accuracy: 0.9070 - jacard_coef: 0.006612/17 [====================>.........] - ETA: 1s - loss: 0.1534 - accuracy: 0.9106 - jacard_coef: 0.006613/17 [=====================>........] - ETA: 0s - loss: 0.1532 - accuracy: 0.9109 - jacard_coef: 0.007014/17 [=======================>......] - ETA: 0s - loss: 0.1528 - accuracy: 0.9127 - jacard_coef: 0.007215/17 [=========================>....] - ETA: 0s - loss: 0.1526 - accuracy: 0.9131 - jacard_coef: 0.006716/17 [===========================>..] - ETA: 0s - loss: 0.1525 - accuracy: 0.9121 - jacard_coef: 0.006417/17 [==============================] - 3s 205ms/step - loss: 0.1525 - accuracy: 0.9124 - jacard_coef: 0.0102 - val_loss: 0.1589 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1487 - accuracy: 0.9099 - jacard_coef: 0.0100 2/17 [==>...........................] - ETA: 3s - loss: 0.1492 - accuracy: 0.8990 - jacard_coef: 0.0060 3/17 [====>.........................] - ETA: 2s - loss: 0.1490 - accuracy: 0.8994 - jacard_coef: 0.0174 4/17 [======>.......................] - ETA: 2s - loss: 0.1494 - accuracy: 0.8901 - jacard_coef: 0.0221 5/17 [=======>......................] - ETA: 2s - loss: 0.1487 - accuracy: 0.8970 - jacard_coef: 0.0198 6/17 [=========>....................] - ETA: 2s - loss: 0.1489 - accuracy: 0.8953 - jacard_coef: 0.0188 7/17 [===========>..................] - ETA: 2s - loss: 0.1484 - accuracy: 0.9015 - jacard_coef: 0.0196 8/17 [=============>................] - ETA: 1s - loss: 0.1485 - accuracy: 0.8978 - jacard_coef: 0.0175 9/17 [==============>...............] - ETA: 1s - loss: 0.1488 - accuracy: 0.8969 - jacard_coef: 0.015810/17 [================>.............] - ETA: 1s - loss: 0.1486 - accuracy: 0.9008 - jacard_coef: 0.015211/17 [==================>...........] - ETA: 1s - loss: 0.1483 - accuracy: 0.9049 - jacard_coef: 0.014212/17 [====================>.........] - ETA: 1s - loss: 0.1483 - accuracy: 0.9030 - jacard_coef: 0.013213/17 [=====================>........] - ETA: 0s - loss: 0.1482 - accuracy: 0.9029 - jacard_coef: 0.012314/17 [=======================>......] - ETA: 0s - loss: 0.1481 - accuracy: 0.9055 - jacard_coef: 0.011715/17 [=========================>....] - ETA: 0s - loss: 0.1479 - accuracy: 0.9064 - jacard_coef: 0.010916/17 [===========================>..] - ETA: 0s - loss: 0.1476 - accuracy: 0.9082 - jacard_coef: 0.010717/17 [==============================] - 3s 205ms/step - loss: 0.1478 - accuracy: 0.9053 - jacard_coef: 0.0174 - val_loss: 0.1507 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1458 - accuracy: 0.9067 - jacard_coef: 0.0042 2/17 [==>...........................] - ETA: 3s - loss: 0.1469 - accuracy: 0.8913 - jacard_coef: 0.0033 3/17 [====>.........................] - ETA: 2s - loss: 0.1484 - accuracy: 0.8807 - jacard_coef: 0.0129 4/17 [======>.......................] - ETA: 2s - loss: 0.1484 - accuracy: 0.8798 - jacard_coef: 0.0188 5/17 [=======>......................] - ETA: 2s - loss: 0.1484 - accuracy: 0.8821 - jacard_coef: 0.0159 6/17 [=========>....................] - ETA: 2s - loss: 0.1479 - accuracy: 0.8893 - jacard_coef: 0.0146 7/17 [===========>..................] - ETA: 2s - loss: 0.1478 - accuracy: 0.8909 - jacard_coef: 0.0130 8/17 [=============>................] - ETA: 1s - loss: 0.1475 - accuracy: 0.8931 - jacard_coef: 0.0122 9/17 [==============>...............] - ETA: 1s - loss: 0.1472 - accuracy: 0.8976 - jacard_coef: 0.011010/17 [================>.............] - ETA: 1s - loss: 0.1467 - accuracy: 0.9047 - jacard_coef: 0.009911/17 [==================>...........] - ETA: 1s - loss: 0.1468 - accuracy: 0.9060 - jacard_coef: 0.009212/17 [====================>.........] - ETA: 1s - loss: 0.1466 - accuracy: 0.9080 - jacard_coef: 0.008513/17 [=====================>........] - ETA: 0s - loss: 0.1467 - accuracy: 0.9058 - jacard_coef: 0.008014/17 [=======================>......] - ETA: 0s - loss: 0.1466 - accuracy: 0.9070 - jacard_coef: 0.007615/17 [=========================>....] - ETA: 0s - loss: 0.1466 - accuracy: 0.9070 - jacard_coef: 0.007216/17 [===========================>..] - ETA: 0s - loss: 0.1463 - accuracy: 0.9103 - jacard_coef: 0.007417/17 [==============================] - 3s 205ms/step - loss: 0.1463 - accuracy: 0.9105 - jacard_coef: 0.0070 - val_loss: 0.1459 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1415 - accuracy: 0.9480 - jacard_coef: 0.0012 2/17 [==>...........................] - ETA: 3s - loss: 0.1425 - accuracy: 0.9253 - jacard_coef: 0.0012 3/17 [====>.........................] - ETA: 2s - loss: 0.1433 - accuracy: 0.9151 - jacard_coef: 0.0014 4/17 [======>.......................] - ETA: 2s - loss: 0.1439 - accuracy: 0.9048 - jacard_coef: 0.0015 5/17 [=======>......................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9033 - jacard_coef: 0.0013 6/17 [=========>....................] - ETA: 2s - loss: 0.1437 - accuracy: 0.9042 - jacard_coef: 0.0014 7/17 [===========>..................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9052 - jacard_coef: 0.0021 8/17 [=============>................] - ETA: 1s - loss: 0.1436 - accuracy: 0.9073 - jacard_coef: 0.0021 9/17 [==============>...............] - ETA: 1s - loss: 0.1433 - accuracy: 0.9090 - jacard_coef: 0.002110/17 [================>.............] - ETA: 1s - loss: 0.1433 - accuracy: 0.9074 - jacard_coef: 0.002211/17 [==================>...........] - ETA: 1s - loss: 0.1427 - accuracy: 0.9128 - jacard_coef: 0.002312/17 [====================>.........] - ETA: 1s - loss: 0.1423 - accuracy: 0.9167 - jacard_coef: 0.002513/17 [=====================>........] - ETA: 0s - loss: 0.1423 - accuracy: 0.9153 - jacard_coef: 0.002514/17 [=======================>......] - ETA: 0s - loss: 0.1421 - accuracy: 0.9158 - jacard_coef: 0.002515/17 [=========================>....] - ETA: 0s - loss: 0.1421 - accuracy: 0.9146 - jacard_coef: 0.002516/17 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.9147 - jacard_coef: 0.002517/17 [==============================] - 3s 205ms/step - loss: 0.1419 - accuracy: 0.9149 - jacard_coef: 0.0023 - val_loss: 0.1429 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1406 - accuracy: 0.9100 - jacard_coef: 3.8148e-04 2/17 [==>...........................] - ETA: 3s - loss: 0.1406 - accuracy: 0.9130 - jacard_coef: 7.6898e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1407 - accuracy: 0.9081 - jacard_coef: 0.0015     4/17 [======>.......................] - ETA: 2s - loss: 0.1399 - accuracy: 0.9150 - jacard_coef: 0.0019 5/17 [=======>......................] - ETA: 2s - loss: 0.1400 - accuracy: 0.9114 - jacard_coef: 0.0017 6/17 [=========>....................] - ETA: 2s - loss: 0.1401 - accuracy: 0.9084 - jacard_coef: 0.0018 7/17 [===========>..................] - ETA: 2s - loss: 0.1395 - accuracy: 0.9137 - jacard_coef: 0.0025 8/17 [=============>................] - ETA: 1s - loss: 0.1392 - accuracy: 0.9164 - jacard_coef: 0.0022 9/17 [==============>...............] - ETA: 1s - loss: 0.1386 - accuracy: 0.9215 - jacard_coef: 0.002610/17 [================>.............] - ETA: 1s - loss: 0.1388 - accuracy: 0.9178 - jacard_coef: 0.002411/17 [==================>...........] - ETA: 1s - loss: 0.1387 - accuracy: 0.9186 - jacard_coef: 0.002212/17 [====================>.........] - ETA: 1s - loss: 0.1386 - accuracy: 0.9181 - jacard_coef: 0.002013/17 [=====================>........] - ETA: 0s - loss: 0.1384 - accuracy: 0.9188 - jacard_coef: 0.001914/17 [=======================>......] - ETA: 0s - loss: 0.1386 - accuracy: 0.9168 - jacard_coef: 0.001915/17 [=========================>....] - ETA: 0s - loss: 0.1385 - accuracy: 0.9169 - jacard_coef: 0.001916/17 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.9165 - jacard_coef: 0.001817/17 [==============================] - 3s 205ms/step - loss: 0.1386 - accuracy: 0.9157 - jacard_coef: 0.0056 - val_loss: 0.1409 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1356 - accuracy: 0.9333 - jacard_coef: 2.8576e-12 2/17 [==>...........................] - ETA: 3s - loss: 0.1375 - accuracy: 0.9182 - jacard_coef: 0.0011     3/17 [====>.........................] - ETA: 2s - loss: 0.1378 - accuracy: 0.9114 - jacard_coef: 0.0018 4/17 [======>.......................] - ETA: 2s - loss: 0.1372 - accuracy: 0.9186 - jacard_coef: 0.0019 5/17 [=======>......................] - ETA: 2s - loss: 0.1370 - accuracy: 0.9189 - jacard_coef: 0.0032 6/17 [=========>....................] - ETA: 2s - loss: 0.1384 - accuracy: 0.9062 - jacard_coef: 0.0119 7/17 [===========>..................] - ETA: 2s - loss: 0.1381 - accuracy: 0.9089 - jacard_coef: 0.0111 8/17 [=============>................] - ETA: 1s - loss: 0.1377 - accuracy: 0.9114 - jacard_coef: 0.0097 9/17 [==============>...............] - ETA: 1s - loss: 0.1373 - accuracy: 0.9146 - jacard_coef: 0.009210/17 [================>.............] - ETA: 1s - loss: 0.1375 - accuracy: 0.9127 - jacard_coef: 0.008311/17 [==================>...........] - ETA: 1s - loss: 0.1374 - accuracy: 0.9134 - jacard_coef: 0.008712/17 [====================>.........] - ETA: 1s - loss: 0.1375 - accuracy: 0.9137 - jacard_coef: 0.008013/17 [=====================>........] - ETA: 0s - loss: 0.1372 - accuracy: 0.9162 - jacard_coef: 0.007514/17 [=======================>......] - ETA: 0s - loss: 0.1372 - accuracy: 0.9148 - jacard_coef: 0.007115/17 [=========================>....] - ETA: 0s - loss: 0.1373 - accuracy: 0.9133 - jacard_coef: 0.006616/17 [===========================>..] - ETA: 0s - loss: 0.1372 - accuracy: 0.9135 - jacard_coef: 0.006217/17 [==============================] - 3s 205ms/step - loss: 0.1372 - accuracy: 0.9142 - jacard_coef: 0.0081 - val_loss: 0.1466 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1356 - accuracy: 0.9129 - jacard_coef: 2.4072e-04 2/17 [==>...........................] - ETA: 3s - loss: 0.1369 - accuracy: 0.9042 - jacard_coef: 1.9339e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1366 - accuracy: 0.9073 - jacard_coef: 3.5634e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1363 - accuracy: 0.9100 - jacard_coef: 2.6726e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1360 - accuracy: 0.9118 - jacard_coef: 4.8646e-04 6/17 [=========>....................] - ETA: 2s - loss: 0.1363 - accuracy: 0.9106 - jacard_coef: 5.7193e-04 7/17 [===========>..................] - ETA: 2s - loss: 0.1365 - accuracy: 0.9069 - jacard_coef: 4.9259e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1363 - accuracy: 0.9086 - jacard_coef: 5.3814e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1363 - accuracy: 0.9089 - jacard_coef: 6.6764e-0410/17 [================>.............] - ETA: 1s - loss: 0.1363 - accuracy: 0.9080 - jacard_coef: 6.7111e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1362 - accuracy: 0.9067 - jacard_coef: 6.1336e-0412/17 [====================>.........] - ETA: 1s - loss: 0.1358 - accuracy: 0.9102 - jacard_coef: 9.0368e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1355 - accuracy: 0.9131 - jacard_coef: 8.6467e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1354 - accuracy: 0.9129 - jacard_coef: 0.0012    15/17 [=========================>....] - ETA: 0s - loss: 0.1354 - accuracy: 0.9127 - jacard_coef: 0.002516/17 [===========================>..] - ETA: 0s - loss: 0.1352 - accuracy: 0.9132 - jacard_coef: 0.003517/17 [==============================] - 3s 205ms/step - loss: 0.1353 - accuracy: 0.9127 - jacard_coef: 0.0043 - val_loss: 0.1393 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1319 - accuracy: 0.9366 - jacard_coef: 2.1041e-04 2/17 [==>...........................] - ETA: 3s - loss: 0.1348 - accuracy: 0.9127 - jacard_coef: 0.0020     3/17 [====>.........................] - ETA: 2s - loss: 0.1348 - accuracy: 0.9102 - jacard_coef: 0.0013 4/17 [======>.......................] - ETA: 2s - loss: 0.1354 - accuracy: 0.9036 - jacard_coef: 0.0013 5/17 [=======>......................] - ETA: 2s - loss: 0.1341 - accuracy: 0.9167 - jacard_coef: 0.0012 6/17 [=========>....................] - ETA: 2s - loss: 0.1345 - accuracy: 0.9127 - jacard_coef: 0.0012 7/17 [===========>..................] - ETA: 2s - loss: 0.1345 - accuracy: 0.9124 - jacard_coef: 0.0010 8/17 [=============>................] - ETA: 1s - loss: 0.1346 - accuracy: 0.9127 - jacard_coef: 0.0012 9/17 [==============>...............] - ETA: 1s - loss: 0.1344 - accuracy: 0.9137 - jacard_coef: 0.001210/17 [================>.............] - ETA: 1s - loss: 0.1344 - accuracy: 0.9128 - jacard_coef: 0.001311/17 [==================>...........] - ETA: 1s - loss: 0.1341 - accuracy: 0.9153 - jacard_coef: 0.001312/17 [====================>.........] - ETA: 1s - loss: 0.1338 - accuracy: 0.9178 - jacard_coef: 0.001413/17 [=====================>........] - ETA: 0s - loss: 0.1337 - accuracy: 0.9186 - jacard_coef: 0.001514/17 [=======================>......] - ETA: 0s - loss: 0.1338 - accuracy: 0.9171 - jacard_coef: 0.001415/17 [=========================>....] - ETA: 0s - loss: 0.1336 - accuracy: 0.9191 - jacard_coef: 0.001316/17 [===========================>..] - ETA: 0s - loss: 0.1339 - accuracy: 0.9162 - jacard_coef: 0.001217/17 [==============================] - 3s 205ms/step - loss: 0.1338 - accuracy: 0.9168 - jacard_coef: 0.0012 - val_loss: 0.1430 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1350 - accuracy: 0.8971 - jacard_coef: 0.0027 2/17 [==>...........................] - ETA: 3s - loss: 0.1321 - accuracy: 0.9274 - jacard_coef: 0.0017 3/17 [====>.........................] - ETA: 2s - loss: 0.1324 - accuracy: 0.9225 - jacard_coef: 0.0021 4/17 [======>.......................] - ETA: 2s - loss: 0.1330 - accuracy: 0.9170 - jacard_coef: 0.0019 5/17 [=======>......................] - ETA: 2s - loss: 0.1325 - accuracy: 0.9214 - jacard_coef: 0.0015 6/17 [=========>....................] - ETA: 2s - loss: 0.1321 - accuracy: 0.9220 - jacard_coef: 0.0016 7/17 [===========>..................] - ETA: 2s - loss: 0.1319 - accuracy: 0.9226 - jacard_coef: 0.0016 8/17 [=============>................] - ETA: 1s - loss: 0.1323 - accuracy: 0.9179 - jacard_coef: 0.0014 9/17 [==============>...............] - ETA: 1s - loss: 0.1325 - accuracy: 0.9148 - jacard_coef: 0.001310/17 [================>.............] - ETA: 1s - loss: 0.1322 - accuracy: 0.9172 - jacard_coef: 0.001311/17 [==================>...........] - ETA: 1s - loss: 0.1322 - accuracy: 0.9175 - jacard_coef: 0.001212/17 [====================>.........] - ETA: 1s - loss: 0.1321 - accuracy: 0.9179 - jacard_coef: 0.001113/17 [=====================>........] - ETA: 0s - loss: 0.1320 - accuracy: 0.9194 - jacard_coef: 0.001114/17 [=======================>......] - ETA: 0s - loss: 0.1320 - accuracy: 0.9185 - jacard_coef: 0.001015/17 [=========================>....] - ETA: 0s - loss: 0.1321 - accuracy: 0.9172 - jacard_coef: 9.5511e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1322 - accuracy: 0.9164 - jacard_coef: 9.9040e-0417/17 [==============================] - 3s 205ms/step - loss: 0.1321 - accuracy: 0.9164 - jacard_coef: 0.0145 - val_loss: 0.1394 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0545 (epoch 6)
  Final Val Loss: 0.1394
  Training Time: 0:02:00.294048
  Stability (std): 0.0092

Results saved to: hyperparameter_optimization_20250926_165036/exp_31_Attention_ResUNet_lr1e-3_bs8/Attention_ResUNet_lr0.001_bs8_results.json

Experiment 31 completed in 137s
Progress: 31/36 completed
Estimated remaining time: 11 minutes

ðŸ”¬ EXPERIMENT 32/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880741.100670 1204736 service.cc:145] XLA service 0x1549d0358ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880741.100691 1204736 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880741.237394 1204736 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 7:42 - loss: 0.3403 - accuracy: 0.4853 - jacard_coef: 0.07192/9 [=====>........................] - ETA: 58s - loss: 0.3023 - accuracy: 0.3838 - jacard_coef: 0.0757 3/9 [=========>....................] - ETA: 26s - loss: 0.2757 - accuracy: 0.3153 - jacard_coef: 0.07484/9 [============>.................] - ETA: 15s - loss: 0.2585 - accuracy: 0.2973 - jacard_coef: 0.08125/9 [===============>..............] - ETA: 9s - loss: 0.2482 - accuracy: 0.2928 - jacard_coef: 0.0785 6/9 [===================>..........] - ETA: 6s - loss: 0.2388 - accuracy: 0.2919 - jacard_coef: 0.08147/9 [======================>.......] - ETA: 3s - loss: 0.2314 - accuracy: 0.2916 - jacard_coef: 0.07968/9 [=========================>....] - ETA: 1s - loss: 0.2250 - accuracy: 0.3026 - jacard_coef: 0.08169/9 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.3039 - jacard_coef: 0.07269/9 [==============================] - 78s 3s/step - loss: 0.2247 - accuracy: 0.3039 - jacard_coef: 0.0726 - val_loss: 1.1205 - val_accuracy: 0.9304 - val_jacard_coef: 1.4609e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.2343 - accuracy: 0.3548 - jacard_coef: 0.07072/9 [=====>........................] - ETA: 2s - loss: 0.2231 - accuracy: 0.2884 - jacard_coef: 0.08163/9 [=========>....................] - ETA: 2s - loss: 0.2096 - accuracy: 0.2691 - jacard_coef: 0.08744/9 [============>.................] - ETA: 2s - loss: 0.2031 - accuracy: 0.2546 - jacard_coef: 0.08335/9 [===============>..............] - ETA: 1s - loss: 0.1989 - accuracy: 0.2459 - jacard_coef: 0.08236/9 [===================>..........] - ETA: 1s - loss: 0.1959 - accuracy: 0.2394 - jacard_coef: 0.08247/9 [======================>.......] - ETA: 0s - loss: 0.1935 - accuracy: 0.2362 - jacard_coef: 0.08178/9 [=========================>....] - ETA: 0s - loss: 0.1914 - accuracy: 0.2478 - jacard_coef: 0.08329/9 [==============================] - 3s 380ms/step - loss: 0.1916 - accuracy: 0.2473 - jacard_coef: 0.0740 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1777 - accuracy: 0.5290 - jacard_coef: 0.07562/9 [=====>........................] - ETA: 2s - loss: 0.1787 - accuracy: 0.5630 - jacard_coef: 0.07123/9 [=========>....................] - ETA: 2s - loss: 0.1788 - accuracy: 0.5700 - jacard_coef: 0.06254/9 [============>.................] - ETA: 2s - loss: 0.1785 - accuracy: 0.5697 - jacard_coef: 0.06585/9 [===============>..............] - ETA: 1s - loss: 0.1780 - accuracy: 0.5728 - jacard_coef: 0.06976/9 [===================>..........] - ETA: 1s - loss: 0.1779 - accuracy: 0.5744 - jacard_coef: 0.07147/9 [======================>.......] - ETA: 0s - loss: 0.1774 - accuracy: 0.5743 - jacard_coef: 0.07188/9 [=========================>....] - ETA: 0s - loss: 0.1771 - accuracy: 0.5670 - jacard_coef: 0.07479/9 [==============================] - 3s 374ms/step - loss: 0.1771 - accuracy: 0.5667 - jacard_coef: 0.0915 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1728 - accuracy: 0.4093 - jacard_coef: 0.06262/9 [=====>........................] - ETA: 2s - loss: 0.1731 - accuracy: 0.4295 - jacard_coef: 0.07273/9 [=========>....................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4436 - jacard_coef: 0.07274/9 [============>.................] - ETA: 2s - loss: 0.1720 - accuracy: 0.4793 - jacard_coef: 0.07865/9 [===============>..............] - ETA: 1s - loss: 0.1716 - accuracy: 0.5010 - jacard_coef: 0.07866/9 [===================>..........] - ETA: 1s - loss: 0.1711 - accuracy: 0.5196 - jacard_coef: 0.08227/9 [======================>.......] - ETA: 0s - loss: 0.1718 - accuracy: 0.5097 - jacard_coef: 0.08458/9 [=========================>....] - ETA: 0s - loss: 0.1717 - accuracy: 0.5238 - jacard_coef: 0.08209/9 [==============================] - 3s 373ms/step - loss: 0.1718 - accuracy: 0.5235 - jacard_coef: 0.0880 - val_loss: 1.1139 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1718 - accuracy: 0.6445 - jacard_coef: 0.04302/9 [=====>........................] - ETA: 2s - loss: 0.1717 - accuracy: 0.6281 - jacard_coef: 0.05693/9 [=========>....................] - ETA: 2s - loss: 0.1718 - accuracy: 0.6122 - jacard_coef: 0.06354/9 [============>.................] - ETA: 2s - loss: 0.1718 - accuracy: 0.6081 - jacard_coef: 0.06785/9 [===============>..............] - ETA: 1s - loss: 0.1719 - accuracy: 0.6118 - jacard_coef: 0.06656/9 [===================>..........] - ETA: 1s - loss: 0.1718 - accuracy: 0.6157 - jacard_coef: 0.06757/9 [======================>.......] - ETA: 0s - loss: 0.1717 - accuracy: 0.6245 - jacard_coef: 0.07028/9 [=========================>....] - ETA: 0s - loss: 0.1715 - accuracy: 0.6276 - jacard_coef: 0.07109/9 [==============================] - 3s 374ms/step - loss: 0.1715 - accuracy: 0.6281 - jacard_coef: 0.0710 - val_loss: 0.8931 - val_accuracy: 0.9303 - val_jacard_coef: 1.4594e-12 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1695 - accuracy: 0.5756 - jacard_coef: 0.05162/9 [=====>........................] - ETA: 2s - loss: 0.1689 - accuracy: 0.6306 - jacard_coef: 0.05713/9 [=========>....................] - ETA: 2s - loss: 0.1687 - accuracy: 0.6742 - jacard_coef: 0.05644/9 [============>.................] - ETA: 2s - loss: 0.1683 - accuracy: 0.7107 - jacard_coef: 0.05955/9 [===============>..............] - ETA: 1s - loss: 0.1681 - accuracy: 0.7357 - jacard_coef: 0.05856/9 [===================>..........] - ETA: 1s - loss: 0.1678 - accuracy: 0.7562 - jacard_coef: 0.05557/9 [======================>.......] - ETA: 0s - loss: 0.1674 - accuracy: 0.7719 - jacard_coef: 0.05398/9 [=========================>....] - ETA: 0s - loss: 0.1670 - accuracy: 0.7844 - jacard_coef: 0.05019/9 [==============================] - 3s 381ms/step - loss: 0.1674 - accuracy: 0.7825 - jacard_coef: 0.0574 - val_loss: 0.3068 - val_accuracy: 0.9257 - val_jacard_coef: 5.4795e-05 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1647 - accuracy: 0.5391 - jacard_coef: 0.10172/9 [=====>........................] - ETA: 2s - loss: 0.1901 - accuracy: 0.4821 - jacard_coef: 0.10763/9 [=========>....................] - ETA: 2s - loss: 0.1980 - accuracy: 0.4496 - jacard_coef: 0.09694/9 [============>.................] - ETA: 2s - loss: 0.1977 - accuracy: 0.4274 - jacard_coef: 0.09295/9 [===============>..............] - ETA: 1s - loss: 0.1983 - accuracy: 0.3996 - jacard_coef: 0.09126/9 [===================>..........] - ETA: 1s - loss: 0.1956 - accuracy: 0.3855 - jacard_coef: 0.08877/9 [======================>.......] - ETA: 0s - loss: 0.1921 - accuracy: 0.4290 - jacard_coef: 0.08338/9 [=========================>....] - ETA: 0s - loss: 0.1895 - accuracy: 0.4740 - jacard_coef: 0.08169/9 [==============================] - 3s 379ms/step - loss: 0.1893 - accuracy: 0.4772 - jacard_coef: 0.0737 - val_loss: 0.1926 - val_accuracy: 0.9096 - val_jacard_coef: 0.0084 - lr: 5.0000e-04
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1669 - accuracy: 0.8415 - jacard_coef: 0.02632/9 [=====>........................] - ETA: 2s - loss: 0.1660 - accuracy: 0.8486 - jacard_coef: 0.04583/9 [=========>....................] - ETA: 2s - loss: 0.1656 - accuracy: 0.8431 - jacard_coef: 0.04724/9 [============>.................] - ETA: 2s - loss: 0.1657 - accuracy: 0.8375 - jacard_coef: 0.04595/9 [===============>..............] - ETA: 1s - loss: 0.1655 - accuracy: 0.8324 - jacard_coef: 0.05056/9 [===================>..........] - ETA: 1s - loss: 0.1654 - accuracy: 0.8295 - jacard_coef: 0.05107/9 [======================>.......] - ETA: 0s - loss: 0.1653 - accuracy: 0.8221 - jacard_coef: 0.04918/9 [=========================>....] - ETA: 0s - loss: 0.1653 - accuracy: 0.8187 - jacard_coef: 0.05019/9 [==============================] - 3s 374ms/step - loss: 0.1659 - accuracy: 0.8155 - jacard_coef: 0.0545 - val_loss: 0.1878 - val_accuracy: 0.9299 - val_jacard_coef: 7.2559e-05 - lr: 5.0000e-04
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1646 - accuracy: 0.8713 - jacard_coef: 0.02202/9 [=====>........................] - ETA: 2s - loss: 0.1648 - accuracy: 0.8701 - jacard_coef: 0.01513/9 [=========>....................] - ETA: 2s - loss: 0.1642 - accuracy: 0.8936 - jacard_coef: 0.01734/9 [============>.................] - ETA: 2s - loss: 0.1640 - accuracy: 0.8998 - jacard_coef: 0.01365/9 [===============>..............] - ETA: 1s - loss: 0.1638 - accuracy: 0.9000 - jacard_coef: 0.01286/9 [===================>..........] - ETA: 1s - loss: 0.1641 - accuracy: 0.9006 - jacard_coef: 0.01297/9 [======================>.......] - ETA: 0s - loss: 0.1645 - accuracy: 0.8829 - jacard_coef: 0.02018/9 [=========================>....] - ETA: 0s - loss: 0.1650 - accuracy: 0.8650 - jacard_coef: 0.02329/9 [==============================] - 3s 374ms/step - loss: 0.1651 - accuracy: 0.8630 - jacard_coef: 0.0288 - val_loss: 0.0832 - val_accuracy: 0.9291 - val_jacard_coef: 1.4342e-12 - lr: 5.0000e-04
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1696 - accuracy: 0.7409 - jacard_coef: 0.09512/9 [=====>........................] - ETA: 2s - loss: 0.1683 - accuracy: 0.7568 - jacard_coef: 0.06693/9 [=========>....................] - ETA: 2s - loss: 0.1682 - accuracy: 0.7635 - jacard_coef: 0.06554/9 [============>.................] - ETA: 2s - loss: 0.1681 - accuracy: 0.7904 - jacard_coef: 0.05815/9 [===============>..............] - ETA: 1s - loss: 0.1679 - accuracy: 0.8030 - jacard_coef: 0.05216/9 [===================>..........] - ETA: 1s - loss: 0.1680 - accuracy: 0.8199 - jacard_coef: 0.04647/9 [======================>.......] - ETA: 0s - loss: 0.1677 - accuracy: 0.8328 - jacard_coef: 0.04058/9 [=========================>....] - ETA: 0s - loss: 0.1676 - accuracy: 0.8385 - jacard_coef: 0.03599/9 [==============================] - 3s 379ms/step - loss: 0.1676 - accuracy: 0.8392 - jacard_coef: 0.0360 - val_loss: 0.1626 - val_accuracy: 0.8956 - val_jacard_coef: 0.0133 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1660 - accuracy: 0.9178 - jacard_coef: 0.01382/9 [=====>........................] - ETA: 2s - loss: 0.1661 - accuracy: 0.9103 - jacard_coef: 0.01233/9 [=========>....................] - ETA: 2s - loss: 0.1656 - accuracy: 0.9043 - jacard_coef: 0.01084/9 [============>.................] - ETA: 2s - loss: 0.1650 - accuracy: 0.9077 - jacard_coef: 0.01075/9 [===============>..............] - ETA: 1s - loss: 0.1647 - accuracy: 0.9085 - jacard_coef: 0.00996/9 [===================>..........] - ETA: 1s - loss: 0.1645 - accuracy: 0.9056 - jacard_coef: 0.00977/9 [======================>.......] - ETA: 0s - loss: 0.1642 - accuracy: 0.9052 - jacard_coef: 0.01038/9 [=========================>....] - ETA: 0s - loss: 0.1638 - accuracy: 0.9062 - jacard_coef: 0.01049/9 [==============================] - 3s 380ms/step - loss: 0.1638 - accuracy: 0.9066 - jacard_coef: 0.0092 - val_loss: 0.1735 - val_accuracy: 0.2668 - val_jacard_coef: 0.0721 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1604 - accuracy: 0.8979 - jacard_coef: 0.00972/9 [=====>........................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9030 - jacard_coef: 0.01183/9 [=========>....................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8964 - jacard_coef: 0.01084/9 [============>.................] - ETA: 2s - loss: 0.1607 - accuracy: 0.8975 - jacard_coef: 0.01085/9 [===============>..............] - ETA: 1s - loss: 0.1604 - accuracy: 0.9013 - jacard_coef: 0.01136/9 [===================>..........] - ETA: 1s - loss: 0.1604 - accuracy: 0.8996 - jacard_coef: 0.01157/9 [======================>.......] - ETA: 0s - loss: 0.1601 - accuracy: 0.9015 - jacard_coef: 0.01138/9 [=========================>....] - ETA: 0s - loss: 0.1597 - accuracy: 0.9046 - jacard_coef: 0.01129/9 [==============================] - 3s 374ms/step - loss: 0.1599 - accuracy: 0.8989 - jacard_coef: 0.0109 - val_loss: 0.1690 - val_accuracy: 0.9216 - val_jacard_coef: 0.0083 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1836 - accuracy: 0.2198 - jacard_coef: 0.07272/9 [=====>........................] - ETA: 2s - loss: 0.1821 - accuracy: 0.2478 - jacard_coef: 0.08663/9 [=========>....................] - ETA: 2s - loss: 0.1820 - accuracy: 0.3424 - jacard_coef: 0.08734/9 [============>.................] - ETA: 2s - loss: 0.1803 - accuracy: 0.4455 - jacard_coef: 0.09355/9 [===============>..............] - ETA: 1s - loss: 0.1795 - accuracy: 0.5112 - jacard_coef: 0.09256/9 [===================>..........] - ETA: 1s - loss: 0.1785 - accuracy: 0.5526 - jacard_coef: 0.08907/9 [======================>.......] - ETA: 0s - loss: 0.1782 - accuracy: 0.5821 - jacard_coef: 0.08418/9 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.6012 - jacard_coef: 0.08049/9 [==============================] - 3s 374ms/step - loss: 0.1773 - accuracy: 0.6016 - jacard_coef: 0.0792 - val_loss: 0.1676 - val_accuracy: 0.9258 - val_jacard_coef: 0.0053 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1688 - accuracy: 0.7731 - jacard_coef: 0.06702/9 [=====>........................] - ETA: 2s - loss: 0.1671 - accuracy: 0.7882 - jacard_coef: 0.07213/9 [=========>....................] - ETA: 2s - loss: 0.1660 - accuracy: 0.7890 - jacard_coef: 0.07104/9 [============>.................] - ETA: 2s - loss: 0.1655 - accuracy: 0.7878 - jacard_coef: 0.08035/9 [===============>..............] - ETA: 1s - loss: 0.1654 - accuracy: 0.7831 - jacard_coef: 0.08166/9 [===================>..........] - ETA: 1s - loss: 0.1654 - accuracy: 0.7796 - jacard_coef: 0.07877/9 [======================>.......] - ETA: 0s - loss: 0.1651 - accuracy: 0.7808 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1648 - accuracy: 0.7826 - jacard_coef: 0.07849/9 [==============================] - 3s 374ms/step - loss: 0.1648 - accuracy: 0.7828 - jacard_coef: 0.0700 - val_loss: 0.1685 - val_accuracy: 0.9071 - val_jacard_coef: 0.0167 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1614 - accuracy: 0.8317 - jacard_coef: 0.04882/9 [=====>........................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8334 - jacard_coef: 0.05803/9 [=========>....................] - ETA: 2s - loss: 0.1612 - accuracy: 0.8307 - jacard_coef: 0.05974/9 [============>.................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8220 - jacard_coef: 0.05715/9 [===============>..............] - ETA: 1s - loss: 0.1608 - accuracy: 0.8194 - jacard_coef: 0.05756/9 [===================>..........] - ETA: 1s - loss: 0.1607 - accuracy: 0.8192 - jacard_coef: 0.05517/9 [======================>.......] - ETA: 0s - loss: 0.1607 - accuracy: 0.8180 - jacard_coef: 0.05608/9 [=========================>....] - ETA: 0s - loss: 0.1604 - accuracy: 0.8245 - jacard_coef: 0.05499/9 [==============================] - 3s 374ms/step - loss: 0.1604 - accuracy: 0.8249 - jacard_coef: 0.0493 - val_loss: 0.1687 - val_accuracy: 0.9229 - val_jacard_coef: 0.0063 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1597 - accuracy: 0.7678 - jacard_coef: 0.07882/9 [=====>........................] - ETA: 2s - loss: 0.1587 - accuracy: 0.8007 - jacard_coef: 0.06583/9 [=========>....................] - ETA: 2s - loss: 0.1587 - accuracy: 0.7992 - jacard_coef: 0.06754/9 [============>.................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8007 - jacard_coef: 0.06505/9 [===============>..............] - ETA: 1s - loss: 0.1581 - accuracy: 0.8084 - jacard_coef: 0.06376/9 [===================>..........] - ETA: 1s - loss: 0.1579 - accuracy: 0.8175 - jacard_coef: 0.07397/9 [======================>.......] - ETA: 0s - loss: 0.1577 - accuracy: 0.8198 - jacard_coef: 0.07528/9 [=========================>....] - ETA: 0s - loss: 0.1577 - accuracy: 0.8194 - jacard_coef: 0.07839/9 [==============================] - 3s 374ms/step - loss: 0.1577 - accuracy: 0.8202 - jacard_coef: 0.0727 - val_loss: 0.1650 - val_accuracy: 0.9303 - val_jacard_coef: 3.7921e-04 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1560 - accuracy: 0.8373 - jacard_coef: 0.06572/9 [=====>........................] - ETA: 2s - loss: 0.1557 - accuracy: 0.8516 - jacard_coef: 0.06213/9 [=========>....................] - ETA: 2s - loss: 0.1562 - accuracy: 0.8354 - jacard_coef: 0.05964/9 [============>.................] - ETA: 2s - loss: 0.1560 - accuracy: 0.8432 - jacard_coef: 0.05665/9 [===============>..............] - ETA: 1s - loss: 0.1557 - accuracy: 0.8518 - jacard_coef: 0.05306/9 [===================>..........] - ETA: 1s - loss: 0.1557 - accuracy: 0.8547 - jacard_coef: 0.05267/9 [======================>.......] - ETA: 0s - loss: 0.1556 - accuracy: 0.8470 - jacard_coef: 0.05788/9 [=========================>....] - ETA: 0s - loss: 0.1555 - accuracy: 0.8503 - jacard_coef: 0.05839/9 [==============================] - 3s 373ms/step - loss: 0.1556 - accuracy: 0.8499 - jacard_coef: 0.0596 - val_loss: 0.1648 - val_accuracy: 0.9303 - val_jacard_coef: 5.8408e-05 - lr: 2.5000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1559 - accuracy: 0.8602 - jacard_coef: 0.03702/9 [=====>........................] - ETA: 2s - loss: 0.1548 - accuracy: 0.8607 - jacard_coef: 0.04343/9 [=========>....................] - ETA: 2s - loss: 0.1545 - accuracy: 0.8702 - jacard_coef: 0.03954/9 [============>.................] - ETA: 2s - loss: 0.1543 - accuracy: 0.8785 - jacard_coef: 0.03495/9 [===============>..............] - ETA: 1s - loss: 0.1543 - accuracy: 0.8757 - jacard_coef: 0.03836/9 [===================>..........] - ETA: 1s - loss: 0.1542 - accuracy: 0.8817 - jacard_coef: 0.03807/9 [======================>.......] - ETA: 0s - loss: 0.1542 - accuracy: 0.8847 - jacard_coef: 0.03798/9 [=========================>....] - ETA: 0s - loss: 0.1542 - accuracy: 0.8849 - jacard_coef: 0.03719/9 [==============================] - 3s 374ms/step - loss: 0.1542 - accuracy: 0.8852 - jacard_coef: 0.0373 - val_loss: 0.1644 - val_accuracy: 0.9304 - val_jacard_coef: 1.4614e-12 - lr: 2.5000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1532 - accuracy: 0.9088 - jacard_coef: 0.05192/9 [=====>........................] - ETA: 2s - loss: 0.1540 - accuracy: 0.8831 - jacard_coef: 0.04373/9 [=========>....................] - ETA: 2s - loss: 0.1540 - accuracy: 0.8799 - jacard_coef: 0.04384/9 [============>.................] - ETA: 2s - loss: 0.1537 - accuracy: 0.8739 - jacard_coef: 0.05865/9 [===============>..............] - ETA: 1s - loss: 0.1537 - accuracy: 0.8594 - jacard_coef: 0.06376/9 [===================>..........] - ETA: 1s - loss: 0.1533 - accuracy: 0.8603 - jacard_coef: 0.06447/9 [======================>.......] - ETA: 0s - loss: 0.1536 - accuracy: 0.8426 - jacard_coef: 0.06718/9 [=========================>....] - ETA: 0s - loss: 0.1536 - accuracy: 0.8393 - jacard_coef: 0.06569/9 [==============================] - 3s 374ms/step - loss: 0.1537 - accuracy: 0.8386 - jacard_coef: 0.0589 - val_loss: 0.1611 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1526 - accuracy: 0.8946 - jacard_coef: 0.03562/9 [=====>........................] - ETA: 2s - loss: 0.1527 - accuracy: 0.9057 - jacard_coef: 0.02163/9 [=========>....................] - ETA: 2s - loss: 0.1532 - accuracy: 0.8952 - jacard_coef: 0.02064/9 [============>.................] - ETA: 2s - loss: 0.1532 - accuracy: 0.8997 - jacard_coef: 0.01785/9 [===============>..............] - ETA: 1s - loss: 0.1531 - accuracy: 0.9032 - jacard_coef: 0.01456/9 [===================>..........] - ETA: 1s - loss: 0.1532 - accuracy: 0.9058 - jacard_coef: 0.01397/9 [======================>.......] - ETA: 0s - loss: 0.1532 - accuracy: 0.9087 - jacard_coef: 0.01268/9 [=========================>....] - ETA: 0s - loss: 0.1532 - accuracy: 0.9089 - jacard_coef: 0.01179/9 [==============================] - 3s 374ms/step - loss: 0.1536 - accuracy: 0.9052 - jacard_coef: 0.0125 - val_loss: 0.1614 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1545 - accuracy: 0.8874 - jacard_coef: 0.01272/9 [=====>........................] - ETA: 2s - loss: 0.1537 - accuracy: 0.8931 - jacard_coef: 0.01643/9 [=========>....................] - ETA: 2s - loss: 0.1538 - accuracy: 0.8913 - jacard_coef: 0.02354/9 [============>.................] - ETA: 2s - loss: 0.1537 - accuracy: 0.8906 - jacard_coef: 0.02315/9 [===============>..............] - ETA: 1s - loss: 0.1537 - accuracy: 0.8889 - jacard_coef: 0.02396/9 [===================>..........] - ETA: 1s - loss: 0.1542 - accuracy: 0.8834 - jacard_coef: 0.02817/9 [======================>.......] - ETA: 0s - loss: 0.1546 - accuracy: 0.8774 - jacard_coef: 0.03138/9 [=========================>....] - ETA: 0s - loss: 0.1546 - accuracy: 0.8775 - jacard_coef: 0.03289/9 [==============================] - 3s 374ms/step - loss: 0.1548 - accuracy: 0.8746 - jacard_coef: 0.0368 - val_loss: 0.1402 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0721 (epoch 11)
  Final Val Loss: 0.1402
  Training Time: 0:02:26.506792
  Stability (std): 0.0081

Results saved to: hyperparameter_optimization_20250926_165036/exp_32_Attention_ResUNet_lr1e-3_bs16/Attention_ResUNet_lr0.001_bs16_results.json

Experiment 32 completed in 164s
Progress: 32/36 completed
Estimated remaining time: 10 minutes

ðŸ”¬ EXPERIMENT 33/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880911.039746 1212661 service.cc:145] XLA service 0x14e5a15a38e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880911.039767 1212661 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880911.179421 1212661 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:38 - loss: 0.3403 - accuracy: 0.5118 - jacard_coef: 0.07862/5 [===========>..................] - ETA: 46s - loss: 0.2932 - accuracy: 0.4657 - jacard_coef: 0.0817 3/5 [=================>............] - ETA: 16s - loss: 0.2781 - accuracy: 0.4312 - jacard_coef: 0.07784/5 [=======================>......] - ETA: 5s - loss: 0.2612 - accuracy: 0.3907 - jacard_coef: 0.0762 5/5 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.3892 - jacard_coef: 0.07185/5 [==============================] - 96s 7s/step - loss: 0.2607 - accuracy: 0.3892 - jacard_coef: 0.0718 - val_loss: 0.1066 - val_accuracy: 0.9135 - val_jacard_coef: 0.0173 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1970 - accuracy: 0.3558 - jacard_coef: 0.08232/5 [===========>..................] - ETA: 2s - loss: 0.1941 - accuracy: 0.3637 - jacard_coef: 0.08943/5 [=================>............] - ETA: 1s - loss: 0.1921 - accuracy: 0.3409 - jacard_coef: 0.08914/5 [=======================>......] - ETA: 0s - loss: 0.1908 - accuracy: 0.3219 - jacard_coef: 0.08515/5 [==============================] - 3s 639ms/step - loss: 0.1908 - accuracy: 0.3219 - jacard_coef: 0.0741 - val_loss: 0.4288 - val_accuracy: 0.9175 - val_jacard_coef: 0.0093 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1812 - accuracy: 0.3636 - jacard_coef: 0.08442/5 [===========>..................] - ETA: 2s - loss: 0.1816 - accuracy: 0.3712 - jacard_coef: 0.07533/5 [=================>............] - ETA: 1s - loss: 0.1801 - accuracy: 0.4020 - jacard_coef: 0.08204/5 [=======================>......] - ETA: 0s - loss: 0.1787 - accuracy: 0.4242 - jacard_coef: 0.08265/5 [==============================] - 3s 640ms/step - loss: 0.1786 - accuracy: 0.4244 - jacard_coef: 0.0741 - val_loss: 1.0837 - val_accuracy: 0.8935 - val_jacard_coef: 0.0126 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1727 - accuracy: 0.4441 - jacard_coef: 0.08162/5 [===========>..................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4823 - jacard_coef: 0.08473/5 [=================>............] - ETA: 1s - loss: 0.1722 - accuracy: 0.5063 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1719 - accuracy: 0.5198 - jacard_coef: 0.07935/5 [==============================] - 3s 654ms/step - loss: 0.1719 - accuracy: 0.5202 - jacard_coef: 0.0877 - val_loss: 13.3118 - val_accuracy: 0.0737 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1704 - accuracy: 0.5248 - jacard_coef: 0.10912/5 [===========>..................] - ETA: 2s - loss: 0.1704 - accuracy: 0.5218 - jacard_coef: 0.09393/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.5329 - jacard_coef: 0.09194/5 [=======================>......] - ETA: 0s - loss: 0.1701 - accuracy: 0.5435 - jacard_coef: 0.08625/5 [==============================] - 3s 640ms/step - loss: 0.1701 - accuracy: 0.5441 - jacard_coef: 0.0690 - val_loss: 12.9488 - val_accuracy: 0.0731 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1688 - accuracy: 0.6660 - jacard_coef: 0.08452/5 [===========>..................] - ETA: 2s - loss: 0.1687 - accuracy: 0.6742 - jacard_coef: 0.09223/5 [=================>............] - ETA: 1s - loss: 0.1685 - accuracy: 0.6635 - jacard_coef: 0.09004/5 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.6495 - jacard_coef: 0.08495/5 [==============================] - 3s 641ms/step - loss: 0.1697 - accuracy: 0.6475 - jacard_coef: 0.0720 - val_loss: 1.0713 - val_accuracy: 0.9235 - val_jacard_coef: 0.0045 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1716 - accuracy: 0.4545 - jacard_coef: 0.08142/5 [===========>..................] - ETA: 2s - loss: 0.1725 - accuracy: 0.4534 - jacard_coef: 0.07953/5 [=================>............] - ETA: 1s - loss: 0.1725 - accuracy: 0.4539 - jacard_coef: 0.08644/5 [=======================>......] - ETA: 0s - loss: 0.1730 - accuracy: 0.4493 - jacard_coef: 0.08425/5 [==============================] - 3s 640ms/step - loss: 0.1731 - accuracy: 0.4491 - jacard_coef: 0.1023 - val_loss: 1.1193 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1727 - accuracy: 0.4153 - jacard_coef: 0.08672/5 [===========>..................] - ETA: 2s - loss: 0.1719 - accuracy: 0.4050 - jacard_coef: 0.08733/5 [=================>............] - ETA: 1s - loss: 0.1716 - accuracy: 0.4014 - jacard_coef: 0.08464/5 [=======================>......] - ETA: 0s - loss: 0.1715 - accuracy: 0.4190 - jacard_coef: 0.08575/5 [==============================] - 3s 639ms/step - loss: 0.1714 - accuracy: 0.4208 - jacard_coef: 0.0870 - val_loss: 1.0728 - val_accuracy: 0.9304 - val_jacard_coef: 1.4611e-12 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1707 - accuracy: 0.6752 - jacard_coef: 0.08052/5 [===========>..................] - ETA: 2s - loss: 0.1701 - accuracy: 0.6862 - jacard_coef: 0.07433/5 [=================>............] - ETA: 1s - loss: 0.1692 - accuracy: 0.6939 - jacard_coef: 0.06664/5 [=======================>......] - ETA: 0s - loss: 0.1779 - accuracy: 0.6134 - jacard_coef: 0.07015/5 [==============================] - 3s 654ms/step - loss: 0.1778 - accuracy: 0.6150 - jacard_coef: 0.0847 - val_loss: 9.9415 - val_accuracy: 0.0749 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1705 - accuracy: 0.8126 - jacard_coef: 0.04452/5 [===========>..................] - ETA: 2s - loss: 0.1723 - accuracy: 0.7927 - jacard_coef: 0.04643/5 [=================>............] - ETA: 1s - loss: 0.1734 - accuracy: 0.7873 - jacard_coef: 0.04664/5 [=======================>......] - ETA: 0s - loss: 0.1738 - accuracy: 0.7740 - jacard_coef: 0.04945/5 [==============================] - 3s 654ms/step - loss: 0.1738 - accuracy: 0.7732 - jacard_coef: 0.0563 - val_loss: 8.8215 - val_accuracy: 0.0768 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1705 - accuracy: 0.7706 - jacard_coef: 0.05332/5 [===========>..................] - ETA: 2s - loss: 0.1694 - accuracy: 0.7836 - jacard_coef: 0.04503/5 [=================>............] - ETA: 1s - loss: 0.1684 - accuracy: 0.7993 - jacard_coef: 0.04544/5 [=======================>......] - ETA: 0s - loss: 0.1677 - accuracy: 0.8059 - jacard_coef: 0.04775/5 [==============================] - 3s 639ms/step - loss: 0.1677 - accuracy: 0.8060 - jacard_coef: 0.0442 - val_loss: 0.6266 - val_accuracy: 0.9259 - val_jacard_coef: 2.3330e-04 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1645 - accuracy: 0.7920 - jacard_coef: 0.05402/5 [===========>..................] - ETA: 2s - loss: 0.1645 - accuracy: 0.7621 - jacard_coef: 0.05753/5 [=================>............] - ETA: 1s - loss: 0.1642 - accuracy: 0.7497 - jacard_coef: 0.06964/5 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.7383 - jacard_coef: 0.07365/5 [==============================] - 3s 639ms/step - loss: 0.1640 - accuracy: 0.7386 - jacard_coef: 0.1124 - val_loss: 0.5084 - val_accuracy: 0.9264 - val_jacard_coef: 1.3829e-12 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1631 - accuracy: 0.6663 - jacard_coef: 0.07852/5 [===========>..................] - ETA: 2s - loss: 0.1625 - accuracy: 0.6681 - jacard_coef: 0.08553/5 [=================>............] - ETA: 1s - loss: 0.1622 - accuracy: 0.6653 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 0s - loss: 0.1622 - accuracy: 0.6601 - jacard_coef: 0.08095/5 [==============================] - 3s 640ms/step - loss: 0.1622 - accuracy: 0.6593 - jacard_coef: 0.0772 - val_loss: 0.1664 - val_accuracy: 0.9247 - val_jacard_coef: 1.3515e-05 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1610 - accuracy: 0.6747 - jacard_coef: 0.08562/5 [===========>..................] - ETA: 2s - loss: 0.1606 - accuracy: 0.6921 - jacard_coef: 0.09373/5 [=================>............] - ETA: 1s - loss: 0.1604 - accuracy: 0.7007 - jacard_coef: 0.08404/5 [=======================>......] - ETA: 0s - loss: 0.1603 - accuracy: 0.7182 - jacard_coef: 0.07905/5 [==============================] - 3s 640ms/step - loss: 0.1603 - accuracy: 0.7183 - jacard_coef: 0.0703 - val_loss: 0.1195 - val_accuracy: 0.8874 - val_jacard_coef: 0.0157 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1595 - accuracy: 0.8783 - jacard_coef: 0.02672/5 [===========>..................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8869 - jacard_coef: 0.02283/5 [=================>............] - ETA: 1s - loss: 0.1592 - accuracy: 0.8860 - jacard_coef: 0.02334/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.8928 - jacard_coef: 0.02665/5 [==============================] - 3s 640ms/step - loss: 0.1588 - accuracy: 0.8934 - jacard_coef: 0.0212 - val_loss: 0.1528 - val_accuracy: 0.8576 - val_jacard_coef: 0.0191 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1589 - accuracy: 0.8769 - jacard_coef: 0.03082/5 [===========>..................] - ETA: 2s - loss: 0.1585 - accuracy: 0.8809 - jacard_coef: 0.03303/5 [=================>............] - ETA: 1s - loss: 0.1583 - accuracy: 0.8841 - jacard_coef: 0.02954/5 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8831 - jacard_coef: 0.03265/5 [==============================] - 3s 639ms/step - loss: 0.1583 - accuracy: 0.8826 - jacard_coef: 0.0273 - val_loss: 0.1473 - val_accuracy: 0.9102 - val_jacard_coef: 0.0063 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1584 - accuracy: 0.8856 - jacard_coef: 0.02972/5 [===========>..................] - ETA: 2s - loss: 0.1578 - accuracy: 0.9008 - jacard_coef: 0.02443/5 [=================>............] - ETA: 1s - loss: 0.1578 - accuracy: 0.9011 - jacard_coef: 0.01994/5 [=======================>......] - ETA: 0s - loss: 0.1576 - accuracy: 0.9045 - jacard_coef: 0.01655/5 [==============================] - 3s 640ms/step - loss: 0.1576 - accuracy: 0.9048 - jacard_coef: 0.0153 - val_loss: 0.1383 - val_accuracy: 0.9204 - val_jacard_coef: 0.0023 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1570 - accuracy: 0.9192 - jacard_coef: 0.00382/5 [===========>..................] - ETA: 2s - loss: 0.1570 - accuracy: 0.9183 - jacard_coef: 0.00653/5 [=================>............] - ETA: 1s - loss: 0.1570 - accuracy: 0.9141 - jacard_coef: 0.00674/5 [=======================>......] - ETA: 0s - loss: 0.1570 - accuracy: 0.9130 - jacard_coef: 0.00605/5 [==============================] - 3s 640ms/step - loss: 0.1570 - accuracy: 0.9134 - jacard_coef: 0.0048 - val_loss: 0.1362 - val_accuracy: 0.9272 - val_jacard_coef: 0.0038 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1568 - accuracy: 0.9075 - jacard_coef: 0.00702/5 [===========>..................] - ETA: 2s - loss: 0.1565 - accuracy: 0.9130 - jacard_coef: 0.00613/5 [=================>............] - ETA: 1s - loss: 0.1566 - accuracy: 0.9113 - jacard_coef: 0.00554/5 [=======================>......] - ETA: 0s - loss: 0.1564 - accuracy: 0.9146 - jacard_coef: 0.00465/5 [==============================] - 3s 640ms/step - loss: 0.1564 - accuracy: 0.9148 - jacard_coef: 0.0039 - val_loss: 0.1399 - val_accuracy: 0.9282 - val_jacard_coef: 0.0034 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1566 - accuracy: 0.8955 - jacard_coef: 0.00652/5 [===========>..................] - ETA: 2s - loss: 0.1559 - accuracy: 0.9097 - jacard_coef: 0.00733/5 [=================>............] - ETA: 1s - loss: 0.1558 - accuracy: 0.9135 - jacard_coef: 0.00874/5 [=======================>......] - ETA: 0s - loss: 0.1558 - accuracy: 0.9112 - jacard_coef: 0.00935/5 [==============================] - 3s 640ms/step - loss: 0.1558 - accuracy: 0.9105 - jacard_coef: 0.0183 - val_loss: 0.1440 - val_accuracy: 0.9263 - val_jacard_coef: 0.0015 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0698 (epoch 10)
  Final Val Loss: 0.1440
  Training Time: 0:02:40.490381
  Stability (std): 0.1722

Results saved to: hyperparameter_optimization_20250926_165036/exp_33_Attention_ResUNet_lr1e-3_bs32/Attention_ResUNet_lr0.001_bs32_results.json

Experiment 33 completed in 176s
Progress: 33/36 completed
Estimated remaining time: 8 minutes

ðŸ”¬ EXPERIMENT 34/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758881078.386426 1220477 service.cc:145] XLA service 0x14d54e013d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758881078.386450 1220477 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758881078.523937 1220477 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 13:56 - loss: 0.3473 - accuracy: 0.5108 - jacard_coef: 0.1082 2/17 [==>...........................] - ETA: 1:08 - loss: 0.3164 - accuracy: 0.4644 - jacard_coef: 0.0914  3/17 [====>.........................] - ETA: 33s - loss: 0.2903 - accuracy: 0.3591 - jacard_coef: 0.0776  4/17 [======>.......................] - ETA: 21s - loss: 0.2734 - accuracy: 0.3129 - jacard_coef: 0.0789 5/17 [=======>......................] - ETA: 15s - loss: 0.2588 - accuracy: 0.2898 - jacard_coef: 0.0855 6/17 [=========>....................] - ETA: 11s - loss: 0.2483 - accuracy: 0.2756 - jacard_coef: 0.0864 7/17 [===========>..................] - ETA: 9s - loss: 0.2394 - accuracy: 0.2657 - jacard_coef: 0.0847  8/17 [=============>................] - ETA: 7s - loss: 0.2371 - accuracy: 0.2695 - jacard_coef: 0.0821 9/17 [==============>...............] - ETA: 6s - loss: 0.2336 - accuracy: 0.2663 - jacard_coef: 0.078910/17 [================>.............] - ETA: 4s - loss: 0.2297 - accuracy: 0.2568 - jacard_coef: 0.078911/17 [==================>...........] - ETA: 3s - loss: 0.2261 - accuracy: 0.2504 - jacard_coef: 0.082912/17 [====================>.........] - ETA: 3s - loss: 0.2231 - accuracy: 0.2436 - jacard_coef: 0.082413/17 [=====================>........] - ETA: 2s - loss: 0.2203 - accuracy: 0.2398 - jacard_coef: 0.081714/17 [=======================>......] - ETA: 1s - loss: 0.2183 - accuracy: 0.2382 - jacard_coef: 0.083215/17 [=========================>....] - ETA: 1s - loss: 0.2157 - accuracy: 0.2402 - jacard_coef: 0.082916/17 [===========================>..] - ETA: 0s - loss: 0.2138 - accuracy: 0.2409 - jacard_coef: 0.083617/17 [==============================] - ETA: 0s - loss: 0.2135 - accuracy: 0.2416 - jacard_coef: 0.087117/17 [==============================] - 67s 937ms/step - loss: 0.2135 - accuracy: 0.2416 - jacard_coef: 0.0871 - val_loss: 1.1216 - val_accuracy: 0.9304 - val_jacard_coef: 3.4079e-12 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1796 - accuracy: 0.3016 - jacard_coef: 0.1012 2/17 [==>...........................] - ETA: 3s - loss: 0.1812 - accuracy: 0.2995 - jacard_coef: 0.1091 3/17 [====>.........................] - ETA: 2s - loss: 0.1814 - accuracy: 0.2919 - jacard_coef: 0.0915 4/17 [======>.......................] - ETA: 2s - loss: 0.1824 - accuracy: 0.2942 - jacard_coef: 0.0895 5/17 [=======>......................] - ETA: 2s - loss: 0.1832 - accuracy: 0.3032 - jacard_coef: 0.0818 6/17 [=========>....................] - ETA: 2s - loss: 0.1827 - accuracy: 0.3236 - jacard_coef: 0.0831 7/17 [===========>..................] - ETA: 2s - loss: 0.1826 - accuracy: 0.3466 - jacard_coef: 0.0826 8/17 [=============>................] - ETA: 1s - loss: 0.1822 - accuracy: 0.3627 - jacard_coef: 0.0830 9/17 [==============>...............] - ETA: 1s - loss: 0.1820 - accuracy: 0.3759 - jacard_coef: 0.084410/17 [================>.............] - ETA: 1s - loss: 0.1816 - accuracy: 0.3850 - jacard_coef: 0.082211/17 [==================>...........] - ETA: 1s - loss: 0.1810 - accuracy: 0.3894 - jacard_coef: 0.083612/17 [====================>.........] - ETA: 1s - loss: 0.1809 - accuracy: 0.3985 - jacard_coef: 0.082413/17 [=====================>........] - ETA: 0s - loss: 0.1805 - accuracy: 0.4114 - jacard_coef: 0.081114/17 [=======================>......] - ETA: 0s - loss: 0.1800 - accuracy: 0.4222 - jacard_coef: 0.080115/17 [=========================>....] - ETA: 0s - loss: 0.1795 - accuracy: 0.4330 - jacard_coef: 0.079916/17 [===========================>..] - ETA: 0s - loss: 0.1791 - accuracy: 0.4368 - jacard_coef: 0.080117/17 [==============================] - 3s 205ms/step - loss: 0.1795 - accuracy: 0.4357 - jacard_coef: 0.0756 - val_loss: 1.1107 - val_accuracy: 0.9302 - val_jacard_coef: 3.3968e-12 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1865 - accuracy: 0.2611 - jacard_coef: 0.0689 2/17 [==>...........................] - ETA: 3s - loss: 0.1826 - accuracy: 0.3492 - jacard_coef: 0.0822 3/17 [====>.........................] - ETA: 2s - loss: 0.1814 - accuracy: 0.3989 - jacard_coef: 0.0846 4/17 [======>.......................] - ETA: 2s - loss: 0.1805 - accuracy: 0.4366 - jacard_coef: 0.0856 5/17 [=======>......................] - ETA: 2s - loss: 0.1800 - accuracy: 0.4590 - jacard_coef: 0.0783 6/17 [=========>....................] - ETA: 2s - loss: 0.1798 - accuracy: 0.4662 - jacard_coef: 0.0772 7/17 [===========>..................] - ETA: 2s - loss: 0.1794 - accuracy: 0.4667 - jacard_coef: 0.0791 8/17 [=============>................] - ETA: 1s - loss: 0.1791 - accuracy: 0.4580 - jacard_coef: 0.0806 9/17 [==============>...............] - ETA: 1s - loss: 0.1797 - accuracy: 0.4520 - jacard_coef: 0.086710/17 [================>.............] - ETA: 1s - loss: 0.1797 - accuracy: 0.4471 - jacard_coef: 0.086211/17 [==================>...........] - ETA: 1s - loss: 0.1794 - accuracy: 0.4440 - jacard_coef: 0.084612/17 [====================>.........] - ETA: 1s - loss: 0.1793 - accuracy: 0.4456 - jacard_coef: 0.084313/17 [=====================>........] - ETA: 0s - loss: 0.1787 - accuracy: 0.4550 - jacard_coef: 0.084414/17 [=======================>......] - ETA: 0s - loss: 0.1781 - accuracy: 0.4687 - jacard_coef: 0.084415/17 [=========================>....] - ETA: 0s - loss: 0.1778 - accuracy: 0.4761 - jacard_coef: 0.082716/17 [===========================>..] - ETA: 0s - loss: 0.1773 - accuracy: 0.4848 - jacard_coef: 0.080017/17 [==============================] - 4s 209ms/step - loss: 0.1772 - accuracy: 0.4857 - jacard_coef: 0.0783 - val_loss: 0.7900 - val_accuracy: 0.9260 - val_jacard_coef: 0.0018 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1788 - accuracy: 0.3775 - jacard_coef: 0.0824 2/17 [==>...........................] - ETA: 3s - loss: 0.1738 - accuracy: 0.5043 - jacard_coef: 0.0849 3/17 [====>.........................] - ETA: 2s - loss: 0.1732 - accuracy: 0.5640 - jacard_coef: 0.0835 4/17 [======>.......................] - ETA: 2s - loss: 0.1721 - accuracy: 0.5839 - jacard_coef: 0.0770 5/17 [=======>......................] - ETA: 2s - loss: 0.1716 - accuracy: 0.5885 - jacard_coef: 0.0672 6/17 [=========>....................] - ETA: 2s - loss: 0.1711 - accuracy: 0.5918 - jacard_coef: 0.0727 7/17 [===========>..................] - ETA: 2s - loss: 0.1707 - accuracy: 0.5928 - jacard_coef: 0.0715 8/17 [=============>................] - ETA: 1s - loss: 0.1703 - accuracy: 0.6058 - jacard_coef: 0.0730 9/17 [==============>...............] - ETA: 1s - loss: 0.1700 - accuracy: 0.6115 - jacard_coef: 0.073210/17 [================>.............] - ETA: 1s - loss: 0.1696 - accuracy: 0.6141 - jacard_coef: 0.075011/17 [==================>...........] - ETA: 1s - loss: 0.1693 - accuracy: 0.6179 - jacard_coef: 0.077212/17 [====================>.........] - ETA: 1s - loss: 0.1692 - accuracy: 0.6180 - jacard_coef: 0.074613/17 [=====================>........] - ETA: 0s - loss: 0.1690 - accuracy: 0.6224 - jacard_coef: 0.074614/17 [=======================>......] - ETA: 0s - loss: 0.1687 - accuracy: 0.6350 - jacard_coef: 0.074515/17 [=========================>....] - ETA: 0s - loss: 0.1684 - accuracy: 0.6474 - jacard_coef: 0.073616/17 [===========================>..] - ETA: 0s - loss: 0.1681 - accuracy: 0.6489 - jacard_coef: 0.072917/17 [==============================] - 4s 208ms/step - loss: 0.1681 - accuracy: 0.6475 - jacard_coef: 0.0732 - val_loss: 0.1308 - val_accuracy: 0.8974 - val_jacard_coef: 0.0121 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1619 - accuracy: 0.7270 - jacard_coef: 0.0650 2/17 [==>...........................] - ETA: 3s - loss: 0.1621 - accuracy: 0.7592 - jacard_coef: 0.0551 3/17 [====>.........................] - ETA: 2s - loss: 0.1641 - accuracy: 0.6863 - jacard_coef: 0.0621 4/17 [======>.......................] - ETA: 2s - loss: 0.1638 - accuracy: 0.7091 - jacard_coef: 0.0625 5/17 [=======>......................] - ETA: 2s - loss: 0.1631 - accuracy: 0.7299 - jacard_coef: 0.0529 6/17 [=========>....................] - ETA: 2s - loss: 0.1629 - accuracy: 0.7392 - jacard_coef: 0.0574 7/17 [===========>..................] - ETA: 2s - loss: 0.1627 - accuracy: 0.7545 - jacard_coef: 0.0553 8/17 [=============>................] - ETA: 1s - loss: 0.1626 - accuracy: 0.7620 - jacard_coef: 0.0555 9/17 [==============>...............] - ETA: 1s - loss: 0.1624 - accuracy: 0.7744 - jacard_coef: 0.052110/17 [================>.............] - ETA: 1s - loss: 0.1621 - accuracy: 0.7874 - jacard_coef: 0.049911/17 [==================>...........] - ETA: 1s - loss: 0.1618 - accuracy: 0.7998 - jacard_coef: 0.047012/17 [====================>.........] - ETA: 1s - loss: 0.1622 - accuracy: 0.7840 - jacard_coef: 0.053213/17 [=====================>........] - ETA: 0s - loss: 0.1620 - accuracy: 0.7929 - jacard_coef: 0.050014/17 [=======================>......] - ETA: 0s - loss: 0.1619 - accuracy: 0.7988 - jacard_coef: 0.047415/17 [=========================>....] - ETA: 0s - loss: 0.1618 - accuracy: 0.8066 - jacard_coef: 0.045816/17 [===========================>..] - ETA: 0s - loss: 0.1616 - accuracy: 0.8134 - jacard_coef: 0.043717/17 [==============================] - 4s 208ms/step - loss: 0.1616 - accuracy: 0.8139 - jacard_coef: 0.0421 - val_loss: 0.1670 - val_accuracy: 0.4744 - val_jacard_coef: 0.0800 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1604 - accuracy: 0.8906 - jacard_coef: 0.0237 2/17 [==>...........................] - ETA: 3s - loss: 0.1598 - accuracy: 0.8910 - jacard_coef: 0.0165 3/17 [====>.........................] - ETA: 2s - loss: 0.1596 - accuracy: 0.8931 - jacard_coef: 0.0248 4/17 [======>.......................] - ETA: 2s - loss: 0.1594 - accuracy: 0.8950 - jacard_coef: 0.0333 5/17 [=======>......................] - ETA: 2s - loss: 0.1591 - accuracy: 0.8946 - jacard_coef: 0.0426 6/17 [=========>....................] - ETA: 2s - loss: 0.1590 - accuracy: 0.8880 - jacard_coef: 0.0427 7/17 [===========>..................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8785 - jacard_coef: 0.0458 8/17 [=============>................] - ETA: 1s - loss: 0.1585 - accuracy: 0.8705 - jacard_coef: 0.0476 9/17 [==============>...............] - ETA: 1s - loss: 0.1584 - accuracy: 0.8638 - jacard_coef: 0.048610/17 [================>.............] - ETA: 1s - loss: 0.1583 - accuracy: 0.8592 - jacard_coef: 0.051211/17 [==================>...........] - ETA: 1s - loss: 0.1582 - accuracy: 0.8584 - jacard_coef: 0.050212/17 [====================>.........] - ETA: 1s - loss: 0.1581 - accuracy: 0.8600 - jacard_coef: 0.054113/17 [=====================>........] - ETA: 0s - loss: 0.1579 - accuracy: 0.8594 - jacard_coef: 0.052714/17 [=======================>......] - ETA: 0s - loss: 0.1585 - accuracy: 0.8449 - jacard_coef: 0.051915/17 [=========================>....] - ETA: 0s - loss: 0.1585 - accuracy: 0.8482 - jacard_coef: 0.048816/17 [===========================>..] - ETA: 0s - loss: 0.1586 - accuracy: 0.8420 - jacard_coef: 0.048217/17 [==============================] - 3s 205ms/step - loss: 0.1587 - accuracy: 0.8396 - jacard_coef: 0.0499 - val_loss: 0.2043 - val_accuracy: 0.1395 - val_jacard_coef: 0.0680 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1687 - accuracy: 0.5143 - jacard_coef: 0.0629 2/17 [==>...........................] - ETA: 3s - loss: 0.1667 - accuracy: 0.5048 - jacard_coef: 0.0626 3/17 [====>.........................] - ETA: 2s - loss: 0.1663 - accuracy: 0.5081 - jacard_coef: 0.0623 4/17 [======>.......................] - ETA: 2s - loss: 0.1667 - accuracy: 0.5125 - jacard_coef: 0.0717 5/17 [=======>......................] - ETA: 2s - loss: 0.1657 - accuracy: 0.5360 - jacard_coef: 0.0672 6/17 [=========>....................] - ETA: 2s - loss: 0.1650 - accuracy: 0.5757 - jacard_coef: 0.0627 7/17 [===========>..................] - ETA: 2s - loss: 0.1643 - accuracy: 0.6251 - jacard_coef: 0.0542 8/17 [=============>................] - ETA: 1s - loss: 0.1642 - accuracy: 0.6603 - jacard_coef: 0.0484 9/17 [==============>...............] - ETA: 1s - loss: 0.1634 - accuracy: 0.6891 - jacard_coef: 0.043410/17 [================>.............] - ETA: 1s - loss: 0.1629 - accuracy: 0.7079 - jacard_coef: 0.041011/17 [==================>...........] - ETA: 1s - loss: 0.1625 - accuracy: 0.7231 - jacard_coef: 0.039712/17 [====================>.........] - ETA: 1s - loss: 0.1620 - accuracy: 0.7369 - jacard_coef: 0.039313/17 [=====================>........] - ETA: 0s - loss: 0.1616 - accuracy: 0.7491 - jacard_coef: 0.037514/17 [=======================>......] - ETA: 0s - loss: 0.1612 - accuracy: 0.7591 - jacard_coef: 0.034915/17 [=========================>....] - ETA: 0s - loss: 0.1610 - accuracy: 0.7682 - jacard_coef: 0.032716/17 [===========================>..] - ETA: 0s - loss: 0.1606 - accuracy: 0.7781 - jacard_coef: 0.030617/17 [==============================] - 3s 205ms/step - loss: 0.1606 - accuracy: 0.7795 - jacard_coef: 0.0288 - val_loss: 0.0843 - val_accuracy: 0.9241 - val_jacard_coef: 0.0054 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1535 - accuracy: 0.9146 - jacard_coef: 2.2322e-12 2/17 [==>...........................] - ETA: 3s - loss: 0.1543 - accuracy: 0.8967 - jacard_coef: 1.9038e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1536 - accuracy: 0.9102 - jacard_coef: 2.2786e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1531 - accuracy: 0.9115 - jacard_coef: 2.2743e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1530 - accuracy: 0.9130 - jacard_coef: 4.7079e-05 6/17 [=========>....................] - ETA: 2s - loss: 0.1529 - accuracy: 0.9186 - jacard_coef: 0.0017     7/17 [===========>..................] - ETA: 2s - loss: 0.1525 - accuracy: 0.9224 - jacard_coef: 0.0015 8/17 [=============>................] - ETA: 1s - loss: 0.1519 - accuracy: 0.9263 - jacard_coef: 0.0013 9/17 [==============>...............] - ETA: 1s - loss: 0.1518 - accuracy: 0.9217 - jacard_coef: 0.001210/17 [================>.............] - ETA: 1s - loss: 0.1518 - accuracy: 0.9188 - jacard_coef: 0.002011/17 [==================>...........] - ETA: 1s - loss: 0.1517 - accuracy: 0.9164 - jacard_coef: 0.002012/17 [====================>.........] - ETA: 1s - loss: 0.1517 - accuracy: 0.9128 - jacard_coef: 0.001813/17 [=====================>........] - ETA: 0s - loss: 0.1515 - accuracy: 0.9144 - jacard_coef: 0.001714/17 [=======================>......] - ETA: 0s - loss: 0.1514 - accuracy: 0.9135 - jacard_coef: 0.001615/17 [=========================>....] - ETA: 0s - loss: 0.1511 - accuracy: 0.9142 - jacard_coef: 0.001516/17 [===========================>..] - ETA: 0s - loss: 0.1509 - accuracy: 0.9163 - jacard_coef: 0.001417/17 [==============================] - 3s 205ms/step - loss: 0.1510 - accuracy: 0.9147 - jacard_coef: 0.0117 - val_loss: 0.1328 - val_accuracy: 0.9235 - val_jacard_coef: 0.0064 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1470 - accuracy: 0.9521 - jacard_coef: 3.9815e-12 2/17 [==>...........................] - ETA: 3s - loss: 0.1499 - accuracy: 0.9245 - jacard_coef: 3.0515e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1499 - accuracy: 0.9273 - jacard_coef: 2.0344e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1505 - accuracy: 0.9158 - jacard_coef: 1.6060e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1516 - accuracy: 0.9138 - jacard_coef: 3.3916e-04 6/17 [=========>....................] - ETA: 2s - loss: 0.1513 - accuracy: 0.9209 - jacard_coef: 2.8263e-04 7/17 [===========>..................] - ETA: 2s - loss: 0.1514 - accuracy: 0.9197 - jacard_coef: 2.7033e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1517 - accuracy: 0.9162 - jacard_coef: 3.5979e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1520 - accuracy: 0.9178 - jacard_coef: 6.7380e-0410/17 [================>.............] - ETA: 1s - loss: 0.1517 - accuracy: 0.9209 - jacard_coef: 6.0642e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1515 - accuracy: 0.9205 - jacard_coef: 5.5129e-0412/17 [====================>.........] - ETA: 1s - loss: 0.1512 - accuracy: 0.9212 - jacard_coef: 5.0535e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1512 - accuracy: 0.9174 - jacard_coef: 4.6647e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1511 - accuracy: 0.9154 - jacard_coef: 4.3316e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1508 - accuracy: 0.9166 - jacard_coef: 4.0428e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1506 - accuracy: 0.9165 - jacard_coef: 3.7901e-0417/17 [==============================] - 3s 205ms/step - loss: 0.1506 - accuracy: 0.9160 - jacard_coef: 3.5672e-04 - val_loss: 0.1393 - val_accuracy: 0.9267 - val_jacard_coef: 0.0067 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1474 - accuracy: 0.9125 - jacard_coef: 0.0252 2/17 [==>...........................] - ETA: 3s - loss: 0.1466 - accuracy: 0.8926 - jacard_coef: 0.0448 3/17 [====>.........................] - ETA: 2s - loss: 0.1463 - accuracy: 0.8758 - jacard_coef: 0.0425 4/17 [======>.......................] - ETA: 2s - loss: 0.1468 - accuracy: 0.8724 - jacard_coef: 0.0439 5/17 [=======>......................] - ETA: 2s - loss: 0.1465 - accuracy: 0.8776 - jacard_coef: 0.0366 6/17 [=========>....................] - ETA: 2s - loss: 0.1457 - accuracy: 0.8816 - jacard_coef: 0.0640 7/17 [===========>..................] - ETA: 2s - loss: 0.1456 - accuracy: 0.8747 - jacard_coef: 0.0701 8/17 [=============>................] - ETA: 1s - loss: 0.1454 - accuracy: 0.8804 - jacard_coef: 0.0615 9/17 [==============>...............] - ETA: 1s - loss: 0.1458 - accuracy: 0.8818 - jacard_coef: 0.054810/17 [================>.............] - ETA: 1s - loss: 0.1457 - accuracy: 0.8852 - jacard_coef: 0.049611/17 [==================>...........] - ETA: 1s - loss: 0.1454 - accuracy: 0.8895 - jacard_coef: 0.045012/17 [====================>.........] - ETA: 1s - loss: 0.1453 - accuracy: 0.8902 - jacard_coef: 0.041613/17 [=====================>........] - ETA: 0s - loss: 0.1454 - accuracy: 0.8892 - jacard_coef: 0.039414/17 [=======================>......] - ETA: 0s - loss: 0.1451 - accuracy: 0.8933 - jacard_coef: 0.036615/17 [=========================>....] - ETA: 0s - loss: 0.1451 - accuracy: 0.8934 - jacard_coef: 0.034316/17 [===========================>..] - ETA: 0s - loss: 0.1450 - accuracy: 0.8949 - jacard_coef: 0.032117/17 [==============================] - 3s 205ms/step - loss: 0.1452 - accuracy: 0.8928 - jacard_coef: 0.0306 - val_loss: 0.1577 - val_accuracy: 0.9289 - val_jacard_coef: 0.0031 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1425 - accuracy: 0.9300 - jacard_coef: 0.0010 2/17 [==>...........................] - ETA: 3s - loss: 0.1425 - accuracy: 0.9326 - jacard_coef: 5.0339e-04 3/17 [====>.........................] - ETA: 2s - loss: 0.1433 - accuracy: 0.9208 - jacard_coef: 3.3559e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1445 - accuracy: 0.9103 - jacard_coef: 8.2040e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1453 - accuracy: 0.9021 - jacard_coef: 0.0011     6/17 [=========>....................] - ETA: 2s - loss: 0.1448 - accuracy: 0.9083 - jacard_coef: 0.0012 7/17 [===========>..................] - ETA: 2s - loss: 0.1450 - accuracy: 0.9052 - jacard_coef: 0.0010 8/17 [=============>................] - ETA: 1s - loss: 0.1451 - accuracy: 0.9038 - jacard_coef: 9.3373e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1449 - accuracy: 0.9070 - jacard_coef: 8.2999e-0410/17 [================>.............] - ETA: 1s - loss: 0.1446 - accuracy: 0.9101 - jacard_coef: 7.4699e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1448 - accuracy: 0.9102 - jacard_coef: 7.7101e-0412/17 [====================>.........] - ETA: 1s - loss: 0.1448 - accuracy: 0.9102 - jacard_coef: 7.3162e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1447 - accuracy: 0.9109 - jacard_coef: 6.7534e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1445 - accuracy: 0.9133 - jacard_coef: 6.2710e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1447 - accuracy: 0.9167 - jacard_coef: 7.7713e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1447 - accuracy: 0.9161 - jacard_coef: 7.2856e-0417/17 [==============================] - 3s 205ms/step - loss: 0.1447 - accuracy: 0.9154 - jacard_coef: 6.8570e-04 - val_loss: 0.1430 - val_accuracy: 0.9283 - val_jacard_coef: 0.0021 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1428 - accuracy: 0.9203 - jacard_coef: 2.3943e-12 2/17 [==>...........................] - ETA: 3s - loss: 0.1430 - accuracy: 0.9152 - jacard_coef: 5.2998e-05 3/17 [====>.........................] - ETA: 2s - loss: 0.1423 - accuracy: 0.9231 - jacard_coef: 3.5332e-05 4/17 [======>.......................] - ETA: 2s - loss: 0.1424 - accuracy: 0.9230 - jacard_coef: 2.6499e-05 5/17 [=======>......................] - ETA: 2s - loss: 0.1419 - accuracy: 0.9243 - jacard_coef: 2.1199e-05 6/17 [=========>....................] - ETA: 2s - loss: 0.1423 - accuracy: 0.9187 - jacard_coef: 1.7666e-05 7/17 [===========>..................] - ETA: 2s - loss: 0.1425 - accuracy: 0.9163 - jacard_coef: 1.5142e-05 8/17 [=============>................] - ETA: 1s - loss: 0.1421 - accuracy: 0.9206 - jacard_coef: 4.7524e-05 9/17 [==============>...............] - ETA: 1s - loss: 0.1420 - accuracy: 0.9208 - jacard_coef: 4.4954e-0510/17 [================>.............] - ETA: 1s - loss: 0.1418 - accuracy: 0.9214 - jacard_coef: 5.0899e-0511/17 [==================>...........] - ETA: 1s - loss: 0.1420 - accuracy: 0.9187 - jacard_coef: 4.6271e-0512/17 [====================>.........] - ETA: 1s - loss: 0.1422 - accuracy: 0.9151 - jacard_coef: 4.2415e-0513/17 [=====================>........] - ETA: 0s - loss: 0.1420 - accuracy: 0.9164 - jacard_coef: 3.9153e-0514/17 [=======================>......] - ETA: 0s - loss: 0.1420 - accuracy: 0.9176 - jacard_coef: 1.2082e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1418 - accuracy: 0.9180 - jacard_coef: 1.1276e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1423 - accuracy: 0.9160 - jacard_coef: 5.2316e-0417/17 [==============================] - 3s 205ms/step - loss: 0.1422 - accuracy: 0.9166 - jacard_coef: 4.9239e-04 - val_loss: 0.1291 - val_accuracy: 0.9303 - val_jacard_coef: 3.3999e-12 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1450 - accuracy: 0.8695 - jacard_coef: 2.9217e-04 2/17 [==>...........................] - ETA: 3s - loss: 0.1436 - accuracy: 0.8946 - jacard_coef: 0.0042     3/17 [====>.........................] - ETA: 2s - loss: 0.1417 - accuracy: 0.9083 - jacard_coef: 0.0043 4/17 [======>.......................] - ETA: 2s - loss: 0.1416 - accuracy: 0.9034 - jacard_coef: 0.0032 5/17 [=======>......................] - ETA: 2s - loss: 0.1410 - accuracy: 0.9082 - jacard_coef: 0.0027 6/17 [=========>....................] - ETA: 2s - loss: 0.1406 - accuracy: 0.9120 - jacard_coef: 0.0023 7/17 [===========>..................] - ETA: 2s - loss: 0.1399 - accuracy: 0.9150 - jacard_coef: 0.0020 8/17 [=============>................] - ETA: 1s - loss: 0.1398 - accuracy: 0.9172 - jacard_coef: 0.0017 9/17 [==============>...............] - ETA: 1s - loss: 0.1399 - accuracy: 0.9155 - jacard_coef: 0.001510/17 [================>.............] - ETA: 1s - loss: 0.1398 - accuracy: 0.9149 - jacard_coef: 0.001611/17 [==================>...........] - ETA: 1s - loss: 0.1396 - accuracy: 0.9163 - jacard_coef: 0.001512/17 [====================>.........] - ETA: 1s - loss: 0.1395 - accuracy: 0.9160 - jacard_coef: 0.003513/17 [=====================>........] - ETA: 0s - loss: 0.1392 - accuracy: 0.9170 - jacard_coef: 0.004414/17 [=======================>......] - ETA: 0s - loss: 0.1393 - accuracy: 0.9157 - jacard_coef: 0.004315/17 [=========================>....] - ETA: 0s - loss: 0.1392 - accuracy: 0.9160 - jacard_coef: 0.004016/17 [===========================>..] - ETA: 0s - loss: 0.1391 - accuracy: 0.9156 - jacard_coef: 0.003817/17 [==============================] - 3s 205ms/step - loss: 0.1391 - accuracy: 0.9150 - jacard_coef: 0.0035 - val_loss: 0.1338 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1374 - accuracy: 0.9212 - jacard_coef: 2.4208e-05 2/17 [==>...........................] - ETA: 3s - loss: 0.1362 - accuracy: 0.9295 - jacard_coef: 1.2104e-05 3/17 [====>.........................] - ETA: 2s - loss: 0.1368 - accuracy: 0.9189 - jacard_coef: 3.1272e-04 4/17 [======>.......................] - ETA: 2s - loss: 0.1372 - accuracy: 0.9178 - jacard_coef: 5.2941e-04 5/17 [=======>......................] - ETA: 2s - loss: 0.1373 - accuracy: 0.9170 - jacard_coef: 4.2352e-04 6/17 [=========>....................] - ETA: 2s - loss: 0.1373 - accuracy: 0.9169 - jacard_coef: 3.7194e-04 7/17 [===========>..................] - ETA: 2s - loss: 0.1372 - accuracy: 0.9186 - jacard_coef: 5.0109e-04 8/17 [=============>................] - ETA: 1s - loss: 0.1372 - accuracy: 0.9166 - jacard_coef: 4.3846e-04 9/17 [==============>...............] - ETA: 1s - loss: 0.1371 - accuracy: 0.9186 - jacard_coef: 3.8974e-0410/17 [================>.............] - ETA: 1s - loss: 0.1371 - accuracy: 0.9179 - jacard_coef: 4.4362e-0411/17 [==================>...........] - ETA: 1s - loss: 0.1370 - accuracy: 0.9188 - jacard_coef: 4.6827e-0412/17 [====================>.........] - ETA: 1s - loss: 0.1371 - accuracy: 0.9175 - jacard_coef: 4.2925e-0413/17 [=====================>........] - ETA: 0s - loss: 0.1369 - accuracy: 0.9185 - jacard_coef: 3.9623e-0414/17 [=======================>......] - ETA: 0s - loss: 0.1368 - accuracy: 0.9197 - jacard_coef: 4.8809e-0415/17 [=========================>....] - ETA: 0s - loss: 0.1370 - accuracy: 0.9175 - jacard_coef: 4.5555e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1371 - accuracy: 0.9163 - jacard_coef: 4.2708e-0417/17 [==============================] - 3s 205ms/step - loss: 0.1371 - accuracy: 0.9161 - jacard_coef: 0.0076 - val_loss: 0.1335 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1365 - accuracy: 0.9162 - jacard_coef: 2.2764e-12 2/17 [==>...........................] - ETA: 3s - loss: 0.1351 - accuracy: 0.9225 - jacard_coef: 2.4786e-12 3/17 [====>.........................] - ETA: 2s - loss: 0.1360 - accuracy: 0.9127 - jacard_coef: 2.2463e-12 4/17 [======>.......................] - ETA: 2s - loss: 0.1360 - accuracy: 0.9146 - jacard_coef: 2.2844e-12 5/17 [=======>......................] - ETA: 2s - loss: 0.1356 - accuracy: 0.9175 - jacard_coef: 5.3606e-06 6/17 [=========>....................] - ETA: 2s - loss: 0.1359 - accuracy: 0.9123 - jacard_coef: 0.0014     7/17 [===========>..................] - ETA: 2s - loss: 0.1365 - accuracy: 0.9052 - jacard_coef: 0.0012 8/17 [=============>................] - ETA: 1s - loss: 0.1363 - accuracy: 0.9085 - jacard_coef: 0.0017 9/17 [==============>...............] - ETA: 1s - loss: 0.1368 - accuracy: 0.9035 - jacard_coef: 0.001710/17 [================>.............] - ETA: 1s - loss: 0.1369 - accuracy: 0.9015 - jacard_coef: 0.001511/17 [==================>...........] - ETA: 1s - loss: 0.1368 - accuracy: 0.9022 - jacard_coef: 0.001412/17 [====================>.........] - ETA: 1s - loss: 0.1363 - accuracy: 0.9075 - jacard_coef: 0.001213/17 [=====================>........] - ETA: 0s - loss: 0.1361 - accuracy: 0.9089 - jacard_coef: 0.001214/17 [=======================>......] - ETA: 0s - loss: 0.1357 - accuracy: 0.9132 - jacard_coef: 0.001115/17 [=========================>....] - ETA: 0s - loss: 0.1355 - accuracy: 0.9139 - jacard_coef: 9.9850e-0416/17 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.9137 - jacard_coef: 9.3609e-0417/17 [==============================] - 3s 205ms/step - loss: 0.1355 - accuracy: 0.9135 - jacard_coef: 0.0018 - val_loss: 0.1265 - val_accuracy: 0.9304 - val_jacard_coef: 3.4110e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0800 (epoch 5)
  Final Val Loss: 0.1265
  Training Time: 0:01:56.832695
  Stability (std): 0.0282

Results saved to: hyperparameter_optimization_20250926_165036/exp_34_Attention_ResUNet_lr5e-3_bs8/Attention_ResUNet_lr0.005_bs8_results.json

Experiment 34 completed in 134s
Progress: 34/36 completed
Estimated remaining time: 4 minutes

ðŸ”¬ EXPERIMENT 35/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758881214.787462 1228086 service.cc:145] XLA service 0x149c3957f9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758881214.787482 1228086 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758881214.924131 1228086 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 7:44 - loss: 0.3403 - accuracy: 0.5023 - jacard_coef: 0.08942/9 [=====>........................] - ETA: 56s - loss: 0.3036 - accuracy: 0.4126 - jacard_coef: 0.0864 3/9 [=========>....................] - ETA: 25s - loss: 0.2773 - accuracy: 0.3271 - jacard_coef: 0.07994/9 [============>.................] - ETA: 14s - loss: 0.2614 - accuracy: 0.2994 - jacard_coef: 0.07675/9 [===============>..............] - ETA: 9s - loss: 0.2487 - accuracy: 0.3235 - jacard_coef: 0.0825 6/9 [===================>..........] - ETA: 5s - loss: 0.2392 - accuracy: 0.3003 - jacard_coef: 0.08267/9 [======================>.......] - ETA: 3s - loss: 0.2320 - accuracy: 0.2847 - jacard_coef: 0.08138/9 [=========================>....] - ETA: 1s - loss: 0.2264 - accuracy: 0.2776 - jacard_coef: 0.08309/9 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.2770 - jacard_coef: 0.07779/9 [==============================] - 78s 2s/step - loss: 0.2261 - accuracy: 0.2770 - jacard_coef: 0.0777 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1847 - accuracy: 0.2358 - jacard_coef: 0.10782/9 [=====>........................] - ETA: 2s - loss: 0.1848 - accuracy: 0.2310 - jacard_coef: 0.10433/9 [=========>....................] - ETA: 2s - loss: 0.1834 - accuracy: 0.2440 - jacard_coef: 0.09364/9 [============>.................] - ETA: 2s - loss: 0.1854 - accuracy: 0.2695 - jacard_coef: 0.09205/9 [===============>..............] - ETA: 1s - loss: 0.1855 - accuracy: 0.2759 - jacard_coef: 0.08916/9 [===================>..........] - ETA: 1s - loss: 0.1858 - accuracy: 0.2782 - jacard_coef: 0.08437/9 [======================>.......] - ETA: 0s - loss: 0.1860 - accuracy: 0.2875 - jacard_coef: 0.08108/9 [=========================>....] - ETA: 0s - loss: 0.1857 - accuracy: 0.3038 - jacard_coef: 0.08129/9 [==============================] - 3s 381ms/step - loss: 0.1856 - accuracy: 0.3048 - jacard_coef: 0.0773 - val_loss: 14.9378 - val_accuracy: 0.0730 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1807 - accuracy: 0.5746 - jacard_coef: 0.08852/9 [=====>........................] - ETA: 2s - loss: 0.1782 - accuracy: 0.5730 - jacard_coef: 0.08723/9 [=========>....................] - ETA: 2s - loss: 0.1777 - accuracy: 0.5067 - jacard_coef: 0.08304/9 [============>.................] - ETA: 2s - loss: 0.1774 - accuracy: 0.4635 - jacard_coef: 0.08455/9 [===============>..............] - ETA: 1s - loss: 0.1772 - accuracy: 0.4468 - jacard_coef: 0.08776/9 [===================>..........] - ETA: 1s - loss: 0.1768 - accuracy: 0.4512 - jacard_coef: 0.08607/9 [======================>.......] - ETA: 0s - loss: 0.1764 - accuracy: 0.4614 - jacard_coef: 0.08538/9 [=========================>....] - ETA: 0s - loss: 0.1759 - accuracy: 0.4808 - jacard_coef: 0.08869/9 [==============================] - 3s 380ms/step - loss: 0.1760 - accuracy: 0.4811 - jacard_coef: 0.0875 - val_loss: 14.5147 - val_accuracy: 0.0731 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1745 - accuracy: 0.6144 - jacard_coef: 0.06082/9 [=====>........................] - ETA: 2s - loss: 0.1754 - accuracy: 0.5454 - jacard_coef: 0.06503/9 [=========>....................] - ETA: 2s - loss: 0.1758 - accuracy: 0.4873 - jacard_coef: 0.06824/9 [============>.................] - ETA: 2s - loss: 0.1758 - accuracy: 0.4563 - jacard_coef: 0.07465/9 [===============>..............] - ETA: 1s - loss: 0.1761 - accuracy: 0.4374 - jacard_coef: 0.08216/9 [===================>..........] - ETA: 1s - loss: 0.1765 - accuracy: 0.4217 - jacard_coef: 0.08037/9 [======================>.......] - ETA: 0s - loss: 0.1765 - accuracy: 0.4136 - jacard_coef: 0.08148/9 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.4044 - jacard_coef: 0.08359/9 [==============================] - 3s 380ms/step - loss: 0.1765 - accuracy: 0.4036 - jacard_coef: 0.0802 - val_loss: 0.4173 - val_accuracy: 0.1350 - val_jacard_coef: 0.0721 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1768 - accuracy: 0.5954 - jacard_coef: 0.07652/9 [=====>........................] - ETA: 2s - loss: 0.1762 - accuracy: 0.6108 - jacard_coef: 0.06983/9 [=========>....................] - ETA: 2s - loss: 0.1764 - accuracy: 0.6107 - jacard_coef: 0.06754/9 [============>.................] - ETA: 2s - loss: 0.1763 - accuracy: 0.6165 - jacard_coef: 0.06425/9 [===============>..............] - ETA: 1s - loss: 0.1758 - accuracy: 0.6204 - jacard_coef: 0.06716/9 [===================>..........] - ETA: 1s - loss: 0.1753 - accuracy: 0.6251 - jacard_coef: 0.06977/9 [======================>.......] - ETA: 0s - loss: 0.1750 - accuracy: 0.6203 - jacard_coef: 0.07208/9 [=========================>....] - ETA: 0s - loss: 0.1746 - accuracy: 0.6204 - jacard_coef: 0.07649/9 [==============================] - 3s 373ms/step - loss: 0.1746 - accuracy: 0.6209 - jacard_coef: 0.0682 - val_loss: 0.0878 - val_accuracy: 0.9287 - val_jacard_coef: 9.8321e-04 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1697 - accuracy: 0.6080 - jacard_coef: 0.09872/9 [=====>........................] - ETA: 2s - loss: 0.1688 - accuracy: 0.6276 - jacard_coef: 0.08623/9 [=========>....................] - ETA: 2s - loss: 0.1992 - accuracy: 0.5461 - jacard_coef: 0.09384/9 [============>.................] - ETA: 2s - loss: 0.2029 - accuracy: 0.4791 - jacard_coef: 0.08835/9 [===============>..............] - ETA: 1s - loss: 0.2056 - accuracy: 0.4421 - jacard_coef: 0.08716/9 [===================>..........] - ETA: 1s - loss: 0.2036 - accuracy: 0.4158 - jacard_coef: 0.08517/9 [======================>.......] - ETA: 0s - loss: 0.2013 - accuracy: 0.3932 - jacard_coef: 0.08328/9 [=========================>....] - ETA: 0s - loss: 0.1984 - accuracy: 0.3901 - jacard_coef: 0.08469/9 [==============================] - 3s 373ms/step - loss: 0.1983 - accuracy: 0.3901 - jacard_coef: 0.0988 - val_loss: 1.1035 - val_accuracy: 0.9304 - val_jacard_coef: 1.4609e-12 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1741 - accuracy: 0.5213 - jacard_coef: 0.10502/9 [=====>........................] - ETA: 2s - loss: 0.1775 - accuracy: 0.4131 - jacard_coef: 0.08623/9 [=========>....................] - ETA: 2s - loss: 0.1782 - accuracy: 0.3997 - jacard_coef: 0.07664/9 [============>.................] - ETA: 2s - loss: 0.1828 - accuracy: 0.3425 - jacard_coef: 0.06995/9 [===============>..............] - ETA: 1s - loss: 0.1827 - accuracy: 0.3301 - jacard_coef: 0.07576/9 [===================>..........] - ETA: 1s - loss: 0.1848 - accuracy: 0.3130 - jacard_coef: 0.08067/9 [======================>.......] - ETA: 0s - loss: 0.1847 - accuracy: 0.3189 - jacard_coef: 0.07968/9 [=========================>....] - ETA: 0s - loss: 0.1840 - accuracy: 0.3378 - jacard_coef: 0.08009/9 [==============================] - 3s 373ms/step - loss: 0.1840 - accuracy: 0.3376 - jacard_coef: 0.0720 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1755 - accuracy: 0.5236 - jacard_coef: 0.10142/9 [=====>........................] - ETA: 2s - loss: 0.1751 - accuracy: 0.5073 - jacard_coef: 0.08963/9 [=========>....................] - ETA: 2s - loss: 0.1729 - accuracy: 0.5446 - jacard_coef: 0.08354/9 [============>.................] - ETA: 2s - loss: 0.1738 - accuracy: 0.5169 - jacard_coef: 0.07805/9 [===============>..............] - ETA: 1s - loss: 0.1725 - accuracy: 0.5726 - jacard_coef: 0.07226/9 [===================>..........] - ETA: 1s - loss: 0.1738 - accuracy: 0.5990 - jacard_coef: 0.07267/9 [======================>.......] - ETA: 0s - loss: 0.1728 - accuracy: 0.6114 - jacard_coef: 0.07498/9 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.5885 - jacard_coef: 0.07769/9 [==============================] - 3s 374ms/step - loss: 0.1733 - accuracy: 0.5869 - jacard_coef: 0.0727 - val_loss: 1.0798 - val_accuracy: 0.9294 - val_jacard_coef: 5.1853e-04 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1881 - accuracy: 0.6284 - jacard_coef: 0.05832/9 [=====>........................] - ETA: 2s - loss: 0.1847 - accuracy: 0.4977 - jacard_coef: 0.06253/9 [=========>....................] - ETA: 2s - loss: 0.1841 - accuracy: 0.4133 - jacard_coef: 0.07104/9 [============>.................] - ETA: 2s - loss: 0.1818 - accuracy: 0.3877 - jacard_coef: 0.07455/9 [===============>..............] - ETA: 1s - loss: 0.1808 - accuracy: 0.3812 - jacard_coef: 0.07796/9 [===================>..........] - ETA: 1s - loss: 0.1829 - accuracy: 0.3760 - jacard_coef: 0.08267/9 [======================>.......] - ETA: 0s - loss: 0.1847 - accuracy: 0.3828 - jacard_coef: 0.08048/9 [=========================>....] - ETA: 0s - loss: 0.1842 - accuracy: 0.4022 - jacard_coef: 0.08239/9 [==============================] - 3s 373ms/step - loss: 0.1841 - accuracy: 0.4035 - jacard_coef: 0.0869 - val_loss: 1.1126 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1749 - accuracy: 0.6407 - jacard_coef: 0.08532/9 [=====>........................] - ETA: 2s - loss: 0.1716 - accuracy: 0.6627 - jacard_coef: 0.06893/9 [=========>....................] - ETA: 2s - loss: 0.1706 - accuracy: 0.6678 - jacard_coef: 0.06814/9 [============>.................] - ETA: 2s - loss: 0.1717 - accuracy: 0.6679 - jacard_coef: 0.07355/9 [===============>..............] - ETA: 1s - loss: 0.1717 - accuracy: 0.6909 - jacard_coef: 0.07066/9 [===================>..........] - ETA: 1s - loss: 0.1705 - accuracy: 0.7136 - jacard_coef: 0.06817/9 [======================>.......] - ETA: 0s - loss: 0.1692 - accuracy: 0.7276 - jacard_coef: 0.06718/9 [=========================>....] - ETA: 0s - loss: 0.1688 - accuracy: 0.7233 - jacard_coef: 0.06709/9 [==============================] - 3s 373ms/step - loss: 0.1688 - accuracy: 0.7215 - jacard_coef: 0.0669 - val_loss: 0.6287 - val_accuracy: 0.9274 - val_jacard_coef: 0.0117 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1748 - accuracy: 0.8869 - jacard_coef: 0.08782/9 [=====>........................] - ETA: 2s - loss: 0.1697 - accuracy: 0.8702 - jacard_coef: 0.06313/9 [=========>....................] - ETA: 2s - loss: 0.1695 - accuracy: 0.8727 - jacard_coef: 0.05344/9 [============>.................] - ETA: 2s - loss: 0.1675 - accuracy: 0.8761 - jacard_coef: 0.04875/9 [===============>..............] - ETA: 1s - loss: 0.1674 - accuracy: 0.8622 - jacard_coef: 0.05086/9 [===================>..........] - ETA: 1s - loss: 0.1683 - accuracy: 0.8545 - jacard_coef: 0.04997/9 [======================>.......] - ETA: 0s - loss: 0.1676 - accuracy: 0.8525 - jacard_coef: 0.05208/9 [=========================>....] - ETA: 0s - loss: 0.1674 - accuracy: 0.8499 - jacard_coef: 0.05119/9 [==============================] - 3s 374ms/step - loss: 0.1674 - accuracy: 0.8494 - jacard_coef: 0.0463 - val_loss: 0.3601 - val_accuracy: 0.9286 - val_jacard_coef: 8.6881e-04 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1600 - accuracy: 0.8921 - jacard_coef: 0.04832/9 [=====>........................] - ETA: 2s - loss: 0.1656 - accuracy: 0.8733 - jacard_coef: 0.05563/9 [=========>....................] - ETA: 2s - loss: 0.1659 - accuracy: 0.8698 - jacard_coef: 0.05094/9 [============>.................] - ETA: 2s - loss: 0.1664 - accuracy: 0.8701 - jacard_coef: 0.05545/9 [===============>..............] - ETA: 1s - loss: 0.1655 - accuracy: 0.8692 - jacard_coef: 0.04806/9 [===================>..........] - ETA: 1s - loss: 0.1643 - accuracy: 0.8721 - jacard_coef: 0.04447/9 [======================>.......] - ETA: 0s - loss: 0.1638 - accuracy: 0.8642 - jacard_coef: 0.05188/9 [=========================>....] - ETA: 0s - loss: 0.1629 - accuracy: 0.8689 - jacard_coef: 0.04899/9 [==============================] - 3s 374ms/step - loss: 0.1629 - accuracy: 0.8661 - jacard_coef: 0.0466 - val_loss: 0.2213 - val_accuracy: 0.9263 - val_jacard_coef: 0.0041 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1566 - accuracy: 0.9012 - jacard_coef: 0.04222/9 [=====>........................] - ETA: 2s - loss: 0.1617 - accuracy: 0.8953 - jacard_coef: 0.03883/9 [=========>....................] - ETA: 2s - loss: 0.1604 - accuracy: 0.8868 - jacard_coef: 0.03354/9 [============>.................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8960 - jacard_coef: 0.02735/9 [===============>..............] - ETA: 1s - loss: 0.1587 - accuracy: 0.8973 - jacard_coef: 0.02816/9 [===================>..........] - ETA: 1s - loss: 0.1584 - accuracy: 0.8920 - jacard_coef: 0.02747/9 [======================>.......] - ETA: 0s - loss: 0.1579 - accuracy: 0.8914 - jacard_coef: 0.02828/9 [=========================>....] - ETA: 0s - loss: 0.1586 - accuracy: 0.8856 - jacard_coef: 0.03159/9 [==============================] - 3s 374ms/step - loss: 0.1586 - accuracy: 0.8834 - jacard_coef: 0.0281 - val_loss: 0.1203 - val_accuracy: 0.9302 - val_jacard_coef: 4.0787e-04 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1592 - accuracy: 0.8853 - jacard_coef: 0.04702/9 [=====>........................] - ETA: 2s - loss: 0.1570 - accuracy: 0.8971 - jacard_coef: 0.03283/9 [=========>....................] - ETA: 2s - loss: 0.1560 - accuracy: 0.9045 - jacard_coef: 0.03124/9 [============>.................] - ETA: 2s - loss: 0.1558 - accuracy: 0.9061 - jacard_coef: 0.02925/9 [===============>..............] - ETA: 1s - loss: 0.1558 - accuracy: 0.9059 - jacard_coef: 0.02466/9 [===================>..........] - ETA: 1s - loss: 0.1557 - accuracy: 0.9039 - jacard_coef: 0.02327/9 [======================>.......] - ETA: 0s - loss: 0.1557 - accuracy: 0.9060 - jacard_coef: 0.02068/9 [=========================>....] - ETA: 0s - loss: 0.1560 - accuracy: 0.9050 - jacard_coef: 0.02029/9 [==============================] - 3s 374ms/step - loss: 0.1561 - accuracy: 0.9023 - jacard_coef: 0.0225 - val_loss: 0.1573 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0721 (epoch 4)
  Final Val Loss: 0.1573
  Training Time: 0:02:02.548692
  Stability (std): 0.4369

Results saved to: hyperparameter_optimization_20250926_165036/exp_35_Attention_ResUNet_lr5e-3_bs16/Attention_ResUNet_lr0.005_bs16_results.json

Experiment 35 completed in 139s
Progress: 35/36 completed
Estimated remaining time: 2 minutes

ðŸ”¬ EXPERIMENT 36/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758881360.290862 1235617 service.cc:145] XLA service 0x14b7056d8cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758881360.290883 1235617 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758881360.429930 1235617 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:39 - loss: 0.3316 - accuracy: 0.4854 - jacard_coef: 0.06412/5 [===========>..................] - ETA: 46s - loss: 0.2811 - accuracy: 0.4151 - jacard_coef: 0.0885 3/5 [=================>............] - ETA: 16s - loss: 0.2567 - accuracy: 0.3606 - jacard_coef: 0.08304/5 [=======================>......] - ETA: 5s - loss: 0.2425 - accuracy: 0.3349 - jacard_coef: 0.0831 5/5 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.3334 - jacard_coef: 0.07795/5 [==============================] - 96s 7s/step - loss: 0.2422 - accuracy: 0.3334 - jacard_coef: 0.0779 - val_loss: 1.0947 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1962 - accuracy: 0.1083 - jacard_coef: 0.08012/5 [===========>..................] - ETA: 2s - loss: 0.1958 - accuracy: 0.1116 - jacard_coef: 0.08393/5 [=================>............] - ETA: 1s - loss: 0.1946 - accuracy: 0.1191 - jacard_coef: 0.08284/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.1271 - jacard_coef: 0.08375/5 [==============================] - 3s 653ms/step - loss: 0.1942 - accuracy: 0.1278 - jacard_coef: 0.0976 - val_loss: 1.1183 - val_accuracy: 0.9298 - val_jacard_coef: 4.3452e-04 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1891 - accuracy: 0.1895 - jacard_coef: 0.08522/5 [===========>..................] - ETA: 2s - loss: 0.1894 - accuracy: 0.2383 - jacard_coef: 0.07763/5 [=================>............] - ETA: 1s - loss: 0.1884 - accuracy: 0.2648 - jacard_coef: 0.07694/5 [=======================>......] - ETA: 0s - loss: 0.1882 - accuracy: 0.2685 - jacard_coef: 0.08395/5 [==============================] - 3s 639ms/step - loss: 0.1882 - accuracy: 0.2681 - jacard_coef: 0.0812 - val_loss: 1.1054 - val_accuracy: 0.9303 - val_jacard_coef: 1.4592e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1816 - accuracy: 0.2208 - jacard_coef: 0.07712/5 [===========>..................] - ETA: 2s - loss: 0.1801 - accuracy: 0.2865 - jacard_coef: 0.08763/5 [=================>............] - ETA: 1s - loss: 0.1793 - accuracy: 0.3350 - jacard_coef: 0.08924/5 [=======================>......] - ETA: 0s - loss: 0.1786 - accuracy: 0.3665 - jacard_coef: 0.08495/5 [==============================] - 3s 653ms/step - loss: 0.1786 - accuracy: 0.3671 - jacard_coef: 0.0931 - val_loss: 0.2400 - val_accuracy: 0.8862 - val_jacard_coef: 0.0187 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1751 - accuracy: 0.5564 - jacard_coef: 0.06922/5 [===========>..................] - ETA: 2s - loss: 0.1750 - accuracy: 0.5369 - jacard_coef: 0.08093/5 [=================>............] - ETA: 1s - loss: 0.1750 - accuracy: 0.5248 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1748 - accuracy: 0.5204 - jacard_coef: 0.08005/5 [==============================] - 3s 639ms/step - loss: 0.1748 - accuracy: 0.5201 - jacard_coef: 0.0640 - val_loss: 1.0997 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1734 - accuracy: 0.4330 - jacard_coef: 0.09342/5 [===========>..................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4597 - jacard_coef: 0.09073/5 [=================>............] - ETA: 1s - loss: 0.1719 - accuracy: 0.4763 - jacard_coef: 0.08684/5 [=======================>......] - ETA: 0s - loss: 0.1714 - accuracy: 0.4799 - jacard_coef: 0.08605/5 [==============================] - 3s 652ms/step - loss: 0.1717 - accuracy: 0.4784 - jacard_coef: 0.0824 - val_loss: 7.4904 - val_accuracy: 0.0930 - val_jacard_coef: 0.0703 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1725 - accuracy: 0.6042 - jacard_coef: 0.06122/5 [===========>..................] - ETA: 2s - loss: 0.1729 - accuracy: 0.6359 - jacard_coef: 0.06403/5 [=================>............] - ETA: 1s - loss: 0.1729 - accuracy: 0.6586 - jacard_coef: 0.05974/5 [=======================>......] - ETA: 0s - loss: 0.1727 - accuracy: 0.6712 - jacard_coef: 0.05845/5 [==============================] - 3s 639ms/step - loss: 0.1726 - accuracy: 0.6718 - jacard_coef: 0.0619 - val_loss: 12.6217 - val_accuracy: 0.0733 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1699 - accuracy: 0.7268 - jacard_coef: 0.06132/5 [===========>..................] - ETA: 2s - loss: 0.1689 - accuracy: 0.7385 - jacard_coef: 0.05403/5 [=================>............] - ETA: 1s - loss: 0.1683 - accuracy: 0.7496 - jacard_coef: 0.05484/5 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.6943 - jacard_coef: 0.05955/5 [==============================] - 3s 640ms/step - loss: 0.1690 - accuracy: 0.6930 - jacard_coef: 0.0828 - val_loss: 0.2566 - val_accuracy: 0.9303 - val_jacard_coef: 1.4594e-12 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1733 - accuracy: 0.5744 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 2s - loss: 0.1730 - accuracy: 0.5455 - jacard_coef: 0.07883/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.5311 - jacard_coef: 0.07504/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.5237 - jacard_coef: 0.07445/5 [==============================] - 3s 639ms/step - loss: 0.1740 - accuracy: 0.5236 - jacard_coef: 0.0789 - val_loss: 1.0665 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1747 - accuracy: 0.4936 - jacard_coef: 0.06392/5 [===========>..................] - ETA: 2s - loss: 0.1734 - accuracy: 0.5034 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1722 - accuracy: 0.5050 - jacard_coef: 0.07744/5 [=======================>......] - ETA: 0s - loss: 0.1733 - accuracy: 0.4808 - jacard_coef: 0.08125/5 [==============================] - 3s 640ms/step - loss: 0.1733 - accuracy: 0.4803 - jacard_coef: 0.0834 - val_loss: 0.7421 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1717 - accuracy: 0.5895 - jacard_coef: 0.09552/5 [===========>..................] - ETA: 2s - loss: 0.1732 - accuracy: 0.5809 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1737 - accuracy: 0.5689 - jacard_coef: 0.08324/5 [=======================>......] - ETA: 0s - loss: 0.1742 - accuracy: 0.5604 - jacard_coef: 0.08345/5 [==============================] - 3s 639ms/step - loss: 0.1742 - accuracy: 0.5600 - jacard_coef: 0.0675 - val_loss: 2.7031 - val_accuracy: 0.0801 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1750 - accuracy: 0.5517 - jacard_coef: 0.07232/5 [===========>..................] - ETA: 2s - loss: 0.1750 - accuracy: 0.5545 - jacard_coef: 0.07963/5 [=================>............] - ETA: 1s - loss: 0.1751 - accuracy: 0.5520 - jacard_coef: 0.07664/5 [=======================>......] - ETA: 0s - loss: 0.1748 - accuracy: 0.5548 - jacard_coef: 0.07905/5 [==============================] - 3s 639ms/step - loss: 0.1748 - accuracy: 0.5550 - jacard_coef: 0.1018 - val_loss: 2.6489 - val_accuracy: 0.0744 - val_jacard_coef: 0.0698 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1743 - accuracy: 0.5901 - jacard_coef: 0.07932/5 [===========>..................] - ETA: 2s - loss: 0.1734 - accuracy: 0.5869 - jacard_coef: 0.07973/5 [=================>............] - ETA: 1s - loss: 0.1735 - accuracy: 0.5885 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.5930 - jacard_coef: 0.08035/5 [==============================] - 3s 640ms/step - loss: 0.1729 - accuracy: 0.5938 - jacard_coef: 0.0679 - val_loss: 1.2184 - val_accuracy: 0.0809 - val_jacard_coef: 0.0702 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1707 - accuracy: 0.6565 - jacard_coef: 0.10052/5 [===========>..................] - ETA: 2s - loss: 0.1706 - accuracy: 0.6523 - jacard_coef: 0.08283/5 [=================>............] - ETA: 1s - loss: 0.1708 - accuracy: 0.6595 - jacard_coef: 0.08804/5 [=======================>......] - ETA: 0s - loss: 0.1706 - accuracy: 0.6706 - jacard_coef: 0.07895/5 [==============================] - 3s 652ms/step - loss: 0.1705 - accuracy: 0.6724 - jacard_coef: 0.0651 - val_loss: 0.2481 - val_accuracy: 0.1580 - val_jacard_coef: 0.0734 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1700 - accuracy: 0.7785 - jacard_coef: 0.07642/5 [===========>..................] - ETA: 2s - loss: 0.1694 - accuracy: 0.7906 - jacard_coef: 0.06683/5 [=================>............] - ETA: 1s - loss: 0.1689 - accuracy: 0.7999 - jacard_coef: 0.06014/5 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.8083 - jacard_coef: 0.05855/5 [==============================] - 3s 639ms/step - loss: 0.1685 - accuracy: 0.8077 - jacard_coef: 0.0625 - val_loss: 0.1273 - val_accuracy: 0.8965 - val_jacard_coef: 0.0256 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1672 - accuracy: 0.8583 - jacard_coef: 0.05542/5 [===========>..................] - ETA: 2s - loss: 0.1669 - accuracy: 0.8655 - jacard_coef: 0.04493/5 [=================>............] - ETA: 1s - loss: 0.1667 - accuracy: 0.8745 - jacard_coef: 0.03904/5 [=======================>......] - ETA: 0s - loss: 0.1663 - accuracy: 0.8808 - jacard_coef: 0.03515/5 [==============================] - 3s 639ms/step - loss: 0.1663 - accuracy: 0.8813 - jacard_coef: 0.0331 - val_loss: 0.1453 - val_accuracy: 0.8959 - val_jacard_coef: 0.0312 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1639 - accuracy: 0.9127 - jacard_coef: 0.00992/5 [===========>..................] - ETA: 2s - loss: 0.1634 - accuracy: 0.9125 - jacard_coef: 0.00753/5 [=================>............] - ETA: 1s - loss: 0.1627 - accuracy: 0.9135 - jacard_coef: 0.00594/5 [=======================>......] - ETA: 0s - loss: 0.1620 - accuracy: 0.9125 - jacard_coef: 0.00575/5 [==============================] - 3s 653ms/step - loss: 0.1620 - accuracy: 0.9126 - jacard_coef: 0.0046 - val_loss: 0.1739 - val_accuracy: 0.3758 - val_jacard_coef: 0.0805 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1589 - accuracy: 0.8384 - jacard_coef: 0.04882/5 [===========>..................] - ETA: 2s - loss: 0.1591 - accuracy: 0.7914 - jacard_coef: 0.06353/5 [=================>............] - ETA: 1s - loss: 0.1591 - accuracy: 0.8006 - jacard_coef: 0.06704/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.7977 - jacard_coef: 0.06155/5 [==============================] - 3s 639ms/step - loss: 0.1600 - accuracy: 0.7953 - jacard_coef: 0.0493 - val_loss: 0.1412 - val_accuracy: 0.9010 - val_jacard_coef: 0.0294 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1600 - accuracy: 0.6302 - jacard_coef: 0.10362/5 [===========>..................] - ETA: 2s - loss: 0.1601 - accuracy: 0.6861 - jacard_coef: 0.07933/5 [=================>............] - ETA: 1s - loss: 0.1605 - accuracy: 0.7041 - jacard_coef: 0.07494/5 [=======================>......] - ETA: 0s - loss: 0.1605 - accuracy: 0.7110 - jacard_coef: 0.07735/5 [==============================] - 3s 640ms/step - loss: 0.1605 - accuracy: 0.7122 - jacard_coef: 0.0618 - val_loss: 0.0868 - val_accuracy: 0.9255 - val_jacard_coef: 0.0022 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1603 - accuracy: 0.7695 - jacard_coef: 0.06252/5 [===========>..................] - ETA: 2s - loss: 0.1610 - accuracy: 0.7640 - jacard_coef: 0.06143/5 [=================>............] - ETA: 1s - loss: 0.1609 - accuracy: 0.7758 - jacard_coef: 0.06204/5 [=======================>......] - ETA: 0s - loss: 0.1608 - accuracy: 0.7874 - jacard_coef: 0.06245/5 [==============================] - 3s 640ms/step - loss: 0.1608 - accuracy: 0.7877 - jacard_coef: 0.0662 - val_loss: 0.0863 - val_accuracy: 0.9186 - val_jacard_coef: 0.0020 - lr: 5.0000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1599 - accuracy: 0.8633 - jacard_coef: 0.04262/5 [===========>..................] - ETA: 2s - loss: 0.1596 - accuracy: 0.8649 - jacard_coef: 0.04873/5 [=================>............] - ETA: 1s - loss: 0.1595 - accuracy: 0.8623 - jacard_coef: 0.04444/5 [=======================>......] - ETA: 0s - loss: 0.1593 - accuracy: 0.8665 - jacard_coef: 0.03815/5 [==============================] - 3s 639ms/step - loss: 0.1593 - accuracy: 0.8672 - jacard_coef: 0.0409 - val_loss: 0.1373 - val_accuracy: 0.9044 - val_jacard_coef: 0.0119 - lr: 5.0000e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1584 - accuracy: 0.8938 - jacard_coef: 0.01732/5 [===========>..................] - ETA: 2s - loss: 0.1583 - accuracy: 0.8981 - jacard_coef: 0.01153/5 [=================>............] - ETA: 1s - loss: 0.1580 - accuracy: 0.9036 - jacard_coef: 0.00874/5 [=======================>......] - ETA: 0s - loss: 0.1578 - accuracy: 0.9087 - jacard_coef: 0.00785/5 [==============================] - 3s 639ms/step - loss: 0.1578 - accuracy: 0.9085 - jacard_coef: 0.0074 - val_loss: 0.1701 - val_accuracy: 0.8314 - val_jacard_coef: 0.0246 - lr: 5.0000e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1570 - accuracy: 0.9104 - jacard_coef: 0.00582/5 [===========>..................] - ETA: 2s - loss: 0.1569 - accuracy: 0.9110 - jacard_coef: 0.00663/5 [=================>............] - ETA: 1s - loss: 0.1568 - accuracy: 0.9124 - jacard_coef: 0.00574/5 [=======================>......] - ETA: 0s - loss: 0.1570 - accuracy: 0.9080 - jacard_coef: 0.00535/5 [==============================] - 3s 639ms/step - loss: 0.1570 - accuracy: 0.9078 - jacard_coef: 0.0046 - val_loss: 0.1686 - val_accuracy: 0.8488 - val_jacard_coef: 0.0162 - lr: 2.5000e-04
Epoch 24/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1568 - accuracy: 0.9077 - jacard_coef: 0.00352/5 [===========>..................] - ETA: 2s - loss: 0.1570 - accuracy: 0.9010 - jacard_coef: 0.00383/5 [=================>............] - ETA: 1s - loss: 0.1568 - accuracy: 0.9063 - jacard_coef: 0.00454/5 [=======================>......] - ETA: 0s - loss: 0.1565 - accuracy: 0.9106 - jacard_coef: 0.00435/5 [==============================] - 3s 640ms/step - loss: 0.1566 - accuracy: 0.9101 - jacard_coef: 0.0034 - val_loss: 0.1601 - val_accuracy: 0.8959 - val_jacard_coef: 0.0105 - lr: 2.5000e-04
Epoch 25/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1562 - accuracy: 0.9118 - jacard_coef: 0.00362/5 [===========>..................] - ETA: 2s - loss: 0.1563 - accuracy: 0.9092 - jacard_coef: 0.00303/5 [=================>............] - ETA: 1s - loss: 0.1562 - accuracy: 0.9140 - jacard_coef: 0.00264/5 [=======================>......] - ETA: 0s - loss: 0.1561 - accuracy: 0.9128 - jacard_coef: 0.00255/5 [==============================] - 3s 639ms/step - loss: 0.1562 - accuracy: 0.9126 - jacard_coef: 0.0038 - val_loss: 0.1624 - val_accuracy: 0.8987 - val_jacard_coef: 0.0118 - lr: 2.5000e-04
Epoch 26/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1559 - accuracy: 0.9138 - jacard_coef: 0.00262/5 [===========>..................] - ETA: 2s - loss: 0.1555 - accuracy: 0.9196 - jacard_coef: 0.00233/5 [=================>............] - ETA: 1s - loss: 0.1557 - accuracy: 0.9148 - jacard_coef: 0.00224/5 [=======================>......] - ETA: 0s - loss: 0.1557 - accuracy: 0.9140 - jacard_coef: 0.00245/5 [==============================] - 3s 639ms/step - loss: 0.1557 - accuracy: 0.9137 - jacard_coef: 0.0019 - val_loss: 0.1647 - val_accuracy: 0.8960 - val_jacard_coef: 0.0138 - lr: 2.5000e-04
Epoch 27/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1554 - accuracy: 0.9161 - jacard_coef: 0.00212/5 [===========>..................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9237 - jacard_coef: 0.00373/5 [=================>............] - ETA: 1s - loss: 0.1553 - accuracy: 0.9156 - jacard_coef: 0.00364/5 [=======================>......] - ETA: 0s - loss: 0.1554 - accuracy: 0.9130 - jacard_coef: 0.00325/5 [==============================] - 3s 639ms/step - loss: 0.1554 - accuracy: 0.9127 - jacard_coef: 0.0026 - val_loss: 0.1673 - val_accuracy: 0.8722 - val_jacard_coef: 0.0232 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0805 (epoch 17)
  Final Val Loss: 0.1673
  Training Time: 0:03:04.093981
  Stability (std): 0.0308

Results saved to: hyperparameter_optimization_20250926_165036/exp_36_Attention_ResUNet_lr5e-3_bs32/Attention_ResUNet_lr0.005_bs32_results.json

Experiment 36 completed in 200s
Progress: 36/36 completed
Estimated remaining time: 0 minutes

ðŸ“Š GENERATING RESULTS ANALYSIS
==============================
Running hyperparameter analysis...
No successful experiments found!

ðŸ“‹ HYPERPARAMETER OPTIMIZATION COMPLETE
=======================================
Job finished on Fri Sep 26 06:11:41 PM +08 2025

ðŸ“ ALL RESULTS SAVED IN: hyperparameter_optimization_20250926_165036

ðŸ“Š Generated Files:
  - hyperparameter_summary.csv (raw results)
  - hyperparameter_summary_report.txt (detailed analysis)
  - hyperparameter_heatmaps.png (performance heatmaps)
  - architecture_comparisons.png (comparative analysis)

ðŸ† QUICK RESULTS PREVIEW:
========================
No successful experiments found.

ðŸ” Next Steps:
  1. Review hyperparameter_summary_report.txt for detailed analysis
  2. Examine heatmaps for hyperparameter trends
  3. Select best configurations for production training
  4. Consider extended training with optimal hyperparameters

âœ… Hyperparameter optimization job complete!
total 1240
drwxr-x--- 38 phyzxi svuusers 1775 Sep 26 18:08 .
drwxr-xr-x  9 phyzxi svuusers 1797 Sep 26 16:56 ..
drwxr-x---  2 phyzxi svuusers  172 Sep 26 17:08 exp_10_UNet_lr5e-3_bs8
drwxr-x---  2 phyzxi svuusers  175 Sep 26 17:10 exp_11_UNet_lr5e-3_bs16
drwxr-x---  2 phyzxi svuusers  175 Sep 26 17:12 exp_12_UNet_lr5e-3_bs32
drwxr-x---  2 phyzxi svuusers  205 Sep 26 17:14 exp_13_Attention_UNet_lr1e-4_bs8
drwxr-x---  2 phyzxi svuusers  208 Sep 26 17:17 exp_14_Attention_UNet_lr1e-4_bs16
drwxr-x---  2 phyzxi svuusers  208 Sep 26 17:19 exp_15_Attention_UNet_lr1e-4_bs32
drwxr-x---  2 phyzxi svuusers  205 Sep 26 17:21 exp_16_Attention_UNet_lr5e-4_bs8
drwxr-x---  2 phyzxi svuusers  208 Sep 26 17:23 exp_17_Attention_UNet_lr5e-4_bs16
drwxr-x---  2 phyzxi svuusers  208 Sep 26 17:26 exp_18_Attention_UNet_lr5e-4_bs32
drwxr-x---  2 phyzxi svuusers  202 Sep 26 17:28 exp_19_Attention_UNet_lr1e-3_bs8
drwxr-x---  2 phyzxi svuusers  175 Sep 26 16:52 exp_1_UNet_lr1e-4_bs8
drwxr-x---  2 phyzxi svuusers  205 Sep 26 17:30 exp_20_Attention_UNet_lr1e-3_bs16
drwxr-x---  2 phyzxi svuusers  205 Sep 26 17:33 exp_21_Attention_UNet_lr1e-3_bs32
drwxr-x---  2 phyzxi svuusers  202 Sep 26 17:35 exp_22_Attention_UNet_lr5e-3_bs8
drwxr-x---  2 phyzxi svuusers  205 Sep 26 17:38 exp_23_Attention_UNet_lr5e-3_bs16
drwxr-x---  2 phyzxi svuusers  205 Sep 26 17:40 exp_24_Attention_UNet_lr5e-3_bs32
drwxr-x---  2 phyzxi svuusers  214 Sep 26 17:42 exp_25_Attention_ResUNet_lr1e-4_bs8
drwxr-x---  2 phyzxi svuusers  217 Sep 26 17:45 exp_26_Attention_ResUNet_lr1e-4_bs16
drwxr-x---  2 phyzxi svuusers  217 Sep 26 17:47 exp_27_Attention_ResUNet_lr1e-4_bs32
drwxr-x---  2 phyzxi svuusers  214 Sep 26 17:49 exp_28_Attention_ResUNet_lr5e-4_bs8
drwxr-x---  2 phyzxi svuusers  217 Sep 26 17:52 exp_29_Attention_ResUNet_lr5e-4_bs16
drwxr-x---  2 phyzxi svuusers  178 Sep 26 16:54 exp_2_UNet_lr1e-4_bs16
drwxr-x---  2 phyzxi svuusers  217 Sep 26 17:55 exp_30_Attention_ResUNet_lr5e-4_bs32
drwxr-x---  2 phyzxi svuusers  211 Sep 26 17:58 exp_31_Attention_ResUNet_lr1e-3_bs8
drwxr-x---  2 phyzxi svuusers  214 Sep 26 18:00 exp_32_Attention_ResUNet_lr1e-3_bs16
drwxr-x---  2 phyzxi svuusers  214 Sep 26 18:03 exp_33_Attention_ResUNet_lr1e-3_bs32
drwxr-x---  2 phyzxi svuusers  211 Sep 26 18:05 exp_34_Attention_ResUNet_lr5e-3_bs8
drwxr-x---  2 phyzxi svuusers  214 Sep 26 18:08 exp_35_Attention_ResUNet_lr5e-3_bs16
drwxr-x---  2 phyzxi svuusers  214 Sep 26 18:11 exp_36_Attention_ResUNet_lr5e-3_bs32
drwxr-x---  2 phyzxi svuusers  178 Sep 26 16:56 exp_3_UNet_lr1e-4_bs32
drwxr-x---  2 phyzxi svuusers  175 Sep 26 16:57 exp_4_UNet_lr5e-4_bs8
drwxr-x---  2 phyzxi svuusers  178 Sep 26 16:59 exp_5_UNet_lr5e-4_bs16
drwxr-x---  2 phyzxi svuusers  178 Sep 26 17:01 exp_6_UNet_lr5e-4_bs32
drwxr-x---  2 phyzxi svuusers  172 Sep 26 17:03 exp_7_UNet_lr1e-3_bs8
drwxr-x---  2 phyzxi svuusers  175 Sep 26 17:05 exp_8_UNet_lr1e-3_bs16
drwxr-x---  2 phyzxi svuusers  175 Sep 26 17:07 exp_9_UNet_lr1e-3_bs32
-rw-r-----  1 phyzxi svuusers 3017 Sep 26 18:11 hyperparameter_summary.csv

=======================================================================
HYPERPARAMETER OPTIMIZATION FINISHED
Results directory: hyperparameter_optimization_20250926_165036
=======================================================================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-09-26 18:11:43.338713:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 229354.stdct-mgmt-02
	Project: personal-phyzxi
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(36), Used(36)
	CPU Time Used: 01:17:11
	Memory: Requested(240gb), Used(21781432kb)
	Vmem Used: 64359816kb
	Walltime: Requested(48:00:00), Used(01:21:07)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (GN-A40-007[1]:ncpus=36:ngpus=1:mem=251658240kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	No GPU-related information available for this job.
