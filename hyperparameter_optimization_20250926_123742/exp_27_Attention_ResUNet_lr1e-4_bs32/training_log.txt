✓ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
✓ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758865506.370376 3317814 service.cc:145] XLA service 0x150fa545d420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758865506.370413 3317814 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758865506.746322 3317814 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:56 - loss: 0.3419 - accuracy: 0.4979 - jacard_coef: 0.06262/5 [===========>..................] - ETA: 46s - loss: 0.3146 - accuracy: 0.5502 - jacard_coef: 0.0717 3/5 [=================>............] - ETA: 16s - loss: 0.2870 - accuracy: 0.4668 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 5s - loss: 0.2698 - accuracy: 0.4035 - jacard_coef: 0.0781 5/5 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.4023 - jacard_coef: 0.07575/5 [==============================] - 101s 7s/step - loss: 0.2695 - accuracy: 0.4023 - jacard_coef: 0.0757 - val_loss: 1.1153 - val_accuracy: 0.9304 - val_jacard_coef: 4.2127e-05 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1977 - accuracy: 0.2937 - jacard_coef: 0.07852/5 [===========>..................] - ETA: 2s - loss: 0.1944 - accuracy: 0.2812 - jacard_coef: 0.07563/5 [=================>............] - ETA: 1s - loss: 0.1907 - accuracy: 0.3085 - jacard_coef: 0.07374/5 [=======================>......] - ETA: 0s - loss: 0.1874 - accuracy: 0.3377 - jacard_coef: 0.07605/5 [==============================] - 3s 647ms/step - loss: 0.1879 - accuracy: 0.3380 - jacard_coef: 0.0951 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1927 - accuracy: 0.1428 - jacard_coef: 0.08102/5 [===========>..................] - ETA: 2s - loss: 0.1967 - accuracy: 0.1391 - jacard_coef: 0.08163/5 [=================>............] - ETA: 1s - loss: 0.1963 - accuracy: 0.1378 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 0s - loss: 0.1960 - accuracy: 0.1369 - jacard_coef: 0.07725/5 [==============================] - 3s 647ms/step - loss: 0.1961 - accuracy: 0.1374 - jacard_coef: 0.0866 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1894 - accuracy: 0.1574 - jacard_coef: 0.07732/5 [===========>..................] - ETA: 2s - loss: 0.1898 - accuracy: 0.1715 - jacard_coef: 0.07553/5 [=================>............] - ETA: 1s - loss: 0.1895 - accuracy: 0.1641 - jacard_coef: 0.07874/5 [=======================>......] - ETA: 0s - loss: 0.1913 - accuracy: 0.1543 - jacard_coef: 0.07655/5 [==============================] - 3s 647ms/step - loss: 0.1913 - accuracy: 0.1548 - jacard_coef: 0.0904 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1934 - accuracy: 0.1336 - jacard_coef: 0.08092/5 [===========>..................] - ETA: 2s - loss: 0.1901 - accuracy: 0.1336 - jacard_coef: 0.07753/5 [=================>............] - ETA: 1s - loss: 0.1899 - accuracy: 0.1345 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.1416 - jacard_coef: 0.07715/5 [==============================] - 3s 669ms/step - loss: 0.1888 - accuracy: 0.1416 - jacard_coef: 0.0692 - val_loss: 1.1772 - val_accuracy: 0.9268 - val_jacard_coef: 0.0013 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1849 - accuracy: 0.1936 - jacard_coef: 0.08582/5 [===========>..................] - ETA: 2s - loss: 0.1837 - accuracy: 0.1886 - jacard_coef: 0.07783/5 [=================>............] - ETA: 1s - loss: 0.1835 - accuracy: 0.1904 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1824 - accuracy: 0.1974 - jacard_coef: 0.07685/5 [==============================] - 3s 668ms/step - loss: 0.1826 - accuracy: 0.1980 - jacard_coef: 0.0781 - val_loss: 14.8395 - val_accuracy: 0.0767 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1788 - accuracy: 0.3314 - jacard_coef: 0.08062/5 [===========>..................] - ETA: 2s - loss: 0.1764 - accuracy: 0.4821 - jacard_coef: 0.07203/5 [=================>............] - ETA: 1s - loss: 0.1776 - accuracy: 0.4427 - jacard_coef: 0.08084/5 [=======================>......] - ETA: 0s - loss: 0.1767 - accuracy: 0.5360 - jacard_coef: 0.07675/5 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.5370 - jacard_coef: 0.06915/5 [==============================] - 3s 658ms/step - loss: 0.1767 - accuracy: 0.5370 - jacard_coef: 0.0691 - val_loss: 14.9161 - val_accuracy: 0.0697 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1765 - accuracy: 0.7910 - jacard_coef: 0.07072/5 [===========>..................] - ETA: 2s - loss: 0.1753 - accuracy: 0.7072 - jacard_coef: 0.07443/5 [=================>............] - ETA: 1s - loss: 0.1745 - accuracy: 0.6081 - jacard_coef: 0.07364/5 [=======================>......] - ETA: 0s - loss: 0.1739 - accuracy: 0.5534 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.5523 - jacard_coef: 0.08595/5 [==============================] - 3s 660ms/step - loss: 0.1739 - accuracy: 0.5523 - jacard_coef: 0.0859 - val_loss: 1.0407 - val_accuracy: 0.9304 - val_jacard_coef: 0.0019 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1715 - accuracy: 0.3687 - jacard_coef: 0.08922/5 [===========>..................] - ETA: 2s - loss: 0.1711 - accuracy: 0.3640 - jacard_coef: 0.08073/5 [=================>............] - ETA: 1s - loss: 0.1708 - accuracy: 0.3651 - jacard_coef: 0.07744/5 [=======================>......] - ETA: 0s - loss: 0.1708 - accuracy: 0.3666 - jacard_coef: 0.07715/5 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.3660 - jacard_coef: 0.07235/5 [==============================] - 4s 697ms/step - loss: 0.1708 - accuracy: 0.3660 - jacard_coef: 0.0723 - val_loss: 9.1947 - val_accuracy: 0.0762 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1669 - accuracy: 0.7621 - jacard_coef: 0.07482/5 [===========>..................] - ETA: 2s - loss: 0.1667 - accuracy: 0.7801 - jacard_coef: 0.08273/5 [=================>............] - ETA: 1s - loss: 0.1668 - accuracy: 0.7976 - jacard_coef: 0.07544/5 [=======================>......] - ETA: 0s - loss: 0.1668 - accuracy: 0.8023 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.8021 - jacard_coef: 0.06955/5 [==============================] - 3s 660ms/step - loss: 0.1668 - accuracy: 0.8021 - jacard_coef: 0.0695 - val_loss: 13.1315 - val_accuracy: 0.0735 - val_jacard_coef: 0.0700 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1649 - accuracy: 0.7368 - jacard_coef: 0.07812/5 [===========>..................] - ETA: 2s - loss: 0.1639 - accuracy: 0.7928 - jacard_coef: 0.06933/5 [=================>............] - ETA: 1s - loss: 0.1643 - accuracy: 0.7538 - jacard_coef: 0.07274/5 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.7660 - jacard_coef: 0.07555/5 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.7649 - jacard_coef: 0.09065/5 [==============================] - 4s 705ms/step - loss: 0.1640 - accuracy: 0.7649 - jacard_coef: 0.0906 - val_loss: 9.1058 - val_accuracy: 0.0749 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1626 - accuracy: 0.7559 - jacard_coef: 0.07962/5 [===========>..................] - ETA: 2s - loss: 0.1625 - accuracy: 0.7654 - jacard_coef: 0.07683/5 [=================>............] - ETA: 1s - loss: 0.1624 - accuracy: 0.7588 - jacard_coef: 0.07524/5 [=======================>......] - ETA: 0s - loss: 0.1623 - accuracy: 0.7505 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.7505 - jacard_coef: 0.07455/5 [==============================] - 3s 661ms/step - loss: 0.1623 - accuracy: 0.7505 - jacard_coef: 0.0745 - val_loss: 2.8285 - val_accuracy: 0.0916 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1628 - accuracy: 0.7063 - jacard_coef: 0.07672/5 [===========>..................] - ETA: 2s - loss: 0.1619 - accuracy: 0.7582 - jacard_coef: 0.07403/5 [=================>............] - ETA: 1s - loss: 0.1615 - accuracy: 0.7928 - jacard_coef: 0.07044/5 [=======================>......] - ETA: 0s - loss: 0.1614 - accuracy: 0.8052 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.8030 - jacard_coef: 0.08645/5 [==============================] - 3s 660ms/step - loss: 0.1620 - accuracy: 0.8030 - jacard_coef: 0.0864 - val_loss: 1.4956 - val_accuracy: 0.1154 - val_jacard_coef: 0.0689 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1645 - accuracy: 0.8796 - jacard_coef: 0.08362/5 [===========>..................] - ETA: 2s - loss: 0.1656 - accuracy: 0.8806 - jacard_coef: 0.07813/5 [=================>............] - ETA: 1s - loss: 0.1667 - accuracy: 0.8750 - jacard_coef: 0.07834/5 [=======================>......] - ETA: 0s - loss: 0.1674 - accuracy: 0.8762 - jacard_coef: 0.07545/5 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.8754 - jacard_coef: 0.08795/5 [==============================] - 3s 660ms/step - loss: 0.1675 - accuracy: 0.8754 - jacard_coef: 0.0879 - val_loss: 0.2586 - val_accuracy: 0.9213 - val_jacard_coef: 0.0264 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1699 - accuracy: 0.8741 - jacard_coef: 0.07772/5 [===========>..................] - ETA: 2s - loss: 0.1705 - accuracy: 0.8620 - jacard_coef: 0.08003/5 [=================>............] - ETA: 1s - loss: 0.1710 - accuracy: 0.8618 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 0s - loss: 0.1707 - accuracy: 0.8634 - jacard_coef: 0.07625/5 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.8589 - jacard_coef: 0.08045/5 [==============================] - 3s 659ms/step - loss: 0.1708 - accuracy: 0.8589 - jacard_coef: 0.0804 - val_loss: 0.2272 - val_accuracy: 0.9303 - val_jacard_coef: 0.0342 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1700 - accuracy: 0.9017 - jacard_coef: 0.06342/5 [===========>..................] - ETA: 2s - loss: 0.1698 - accuracy: 0.8350 - jacard_coef: 0.07653/5 [=================>............] - ETA: 1s - loss: 0.1694 - accuracy: 0.8291 - jacard_coef: 0.07834/5 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.8393 - jacard_coef: 0.07605/5 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.8392 - jacard_coef: 0.08685/5 [==============================] - 3s 660ms/step - loss: 0.1690 - accuracy: 0.8392 - jacard_coef: 0.0868 - val_loss: 0.1214 - val_accuracy: 0.9288 - val_jacard_coef: 0.0550 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1668 - accuracy: 0.8821 - jacard_coef: 0.07842/5 [===========>..................] - ETA: 2s - loss: 0.1665 - accuracy: 0.8952 - jacard_coef: 0.07263/5 [=================>............] - ETA: 1s - loss: 0.1663 - accuracy: 0.8919 - jacard_coef: 0.07344/5 [=======================>......] - ETA: 0s - loss: 0.1666 - accuracy: 0.8897 - jacard_coef: 0.07615/5 [==============================] - 3s 660ms/step - loss: 0.1666 - accuracy: 0.8862 - jacard_coef: 0.0862 - val_loss: 0.0905 - val_accuracy: 0.9248 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1661 - accuracy: 0.9064 - jacard_coef: 0.07822/5 [===========>..................] - ETA: 2s - loss: 0.1652 - accuracy: 0.9027 - jacard_coef: 0.08223/5 [=================>............] - ETA: 1s - loss: 0.1647 - accuracy: 0.9027 - jacard_coef: 0.08314/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.9108 - jacard_coef: 0.07685/5 [==============================] - 3s 660ms/step - loss: 0.1642 - accuracy: 0.9114 - jacard_coef: 0.0631 - val_loss: 0.1274 - val_accuracy: 0.9224 - val_jacard_coef: 0.0644 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1629 - accuracy: 0.9061 - jacard_coef: 0.08112/5 [===========>..................] - ETA: 2s - loss: 0.1620 - accuracy: 0.9126 - jacard_coef: 0.07593/5 [=================>............] - ETA: 1s - loss: 0.1619 - accuracy: 0.9119 - jacard_coef: 0.07604/5 [=======================>......] - ETA: 0s - loss: 0.1617 - accuracy: 0.9125 - jacard_coef: 0.07585/5 [==============================] - 3s 659ms/step - loss: 0.1617 - accuracy: 0.9119 - jacard_coef: 0.0873 - val_loss: 0.1564 - val_accuracy: 0.9130 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1611 - accuracy: 0.9053 - jacard_coef: 0.08182/5 [===========>..................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8950 - jacard_coef: 0.09063/5 [=================>............] - ETA: 1s - loss: 0.1604 - accuracy: 0.9074 - jacard_coef: 0.08044/5 [=======================>......] - ETA: 0s - loss: 0.1599 - accuracy: 0.9125 - jacard_coef: 0.07615/5 [==============================] - 3s 660ms/step - loss: 0.1600 - accuracy: 0.9085 - jacard_coef: 0.0713 - val_loss: 0.1662 - val_accuracy: 0.8855 - val_jacard_coef: 0.0650 - lr: 2.5000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1597 - accuracy: 0.9061 - jacard_coef: 0.08182/5 [===========>..................] - ETA: 2s - loss: 0.1591 - accuracy: 0.9158 - jacard_coef: 0.07373/5 [=================>............] - ETA: 1s - loss: 0.1588 - accuracy: 0.9185 - jacard_coef: 0.07134/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.9135 - jacard_coef: 0.07545/5 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.9126 - jacard_coef: 0.09025/5 [==============================] - 3s 659ms/step - loss: 0.1589 - accuracy: 0.9126 - jacard_coef: 0.0902 - val_loss: 0.1692 - val_accuracy: 0.8959 - val_jacard_coef: 0.0651 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

✓ Training completed successfully!
  Best Val Jaccard: 0.0701 (epoch 11)
  Final Val Loss: 0.1692
  Training Time: 0:02:51.137730
  Stability (std): 0.8540

Results saved to: hyperparameter_optimization_20250926_123742/exp_27_Attention_ResUNet_lr1e-4_bs32/Attention_ResUNet_lr0.0001_bs32_results.json
