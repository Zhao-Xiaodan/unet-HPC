âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758863514.335187 3238564 service.cc:145] XLA service 0x14bcc98f0510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758863514.335226 3238564 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758863514.727322 3238564 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:36 - loss: 0.3423 - accuracy: 0.5014 - jacard_coef: 0.08172/5 [===========>..................] - ETA: 46s - loss: 0.3116 - accuracy: 0.4190 - jacard_coef: 0.0788 3/5 [=================>............] - ETA: 16s - loss: 0.2822 - accuracy: 0.3351 - jacard_coef: 0.07574/5 [=======================>......] - ETA: 5s - loss: 0.2698 - accuracy: 0.2953 - jacard_coef: 0.0777 5/5 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.2940 - jacard_coef: 0.07405/5 [==============================] - 95s 6s/step - loss: 0.2693 - accuracy: 0.2940 - jacard_coef: 0.0740 - val_loss: 0.6129 - val_accuracy: 0.9303 - val_jacard_coef: 0.0116 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1952 - accuracy: 0.2002 - jacard_coef: 0.08842/5 [===========>..................] - ETA: 2s - loss: 0.1958 - accuracy: 0.1696 - jacard_coef: 0.07473/5 [=================>............] - ETA: 1s - loss: 0.1953 - accuracy: 0.1594 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.1564 - jacard_coef: 0.07655/5 [==============================] - 3s 566ms/step - loss: 0.1941 - accuracy: 0.1569 - jacard_coef: 0.0875 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1866 - accuracy: 0.1669 - jacard_coef: 0.07512/5 [===========>..................] - ETA: 2s - loss: 0.1841 - accuracy: 0.1703 - jacard_coef: 0.07213/5 [=================>............] - ETA: 1s - loss: 0.1825 - accuracy: 0.1872 - jacard_coef: 0.07484/5 [=======================>......] - ETA: 0s - loss: 0.1814 - accuracy: 0.2152 - jacard_coef: 0.07635/5 [==============================] - 3s 567ms/step - loss: 0.1813 - accuracy: 0.2164 - jacard_coef: 0.0807 - val_loss: 1.1221 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1741 - accuracy: 0.4125 - jacard_coef: 0.05922/5 [===========>..................] - ETA: 2s - loss: 0.1736 - accuracy: 0.4047 - jacard_coef: 0.06693/5 [=================>............] - ETA: 1s - loss: 0.1730 - accuracy: 0.4261 - jacard_coef: 0.07284/5 [=======================>......] - ETA: 0s - loss: 0.1727 - accuracy: 0.4478 - jacard_coef: 0.07575/5 [==============================] - 3s 566ms/step - loss: 0.1727 - accuracy: 0.4473 - jacard_coef: 0.0850 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1705 - accuracy: 0.6090 - jacard_coef: 0.06622/5 [===========>..................] - ETA: 2s - loss: 0.1704 - accuracy: 0.5732 - jacard_coef: 0.07913/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.5400 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 0s - loss: 0.1700 - accuracy: 0.5473 - jacard_coef: 0.07675/5 [==============================] - 3s 568ms/step - loss: 0.1704 - accuracy: 0.5457 - jacard_coef: 0.0619 - val_loss: 1.1275 - val_accuracy: 0.9295 - val_jacard_coef: 9.5950e-04 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1917 - accuracy: 0.1241 - jacard_coef: 0.08052/5 [===========>..................] - ETA: 2s - loss: 0.1914 - accuracy: 0.1562 - jacard_coef: 0.07903/5 [=================>............] - ETA: 1s - loss: 0.1905 - accuracy: 0.2162 - jacard_coef: 0.07884/5 [=======================>......] - ETA: 0s - loss: 0.1890 - accuracy: 0.2468 - jacard_coef: 0.07735/5 [==============================] - 3s 583ms/step - loss: 0.1891 - accuracy: 0.2473 - jacard_coef: 0.0684 - val_loss: 1.3476 - val_accuracy: 0.9146 - val_jacard_coef: 0.0192 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1891 - accuracy: 0.0993 - jacard_coef: 0.07382/5 [===========>..................] - ETA: 2s - loss: 0.1898 - accuracy: 0.1051 - jacard_coef: 0.07613/5 [=================>............] - ETA: 1s - loss: 0.1930 - accuracy: 0.1064 - jacard_coef: 0.07734/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.1060 - jacard_coef: 0.07675/5 [==============================] - 3s 566ms/step - loss: 0.1941 - accuracy: 0.1062 - jacard_coef: 0.0721 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1786 - accuracy: 0.3280 - jacard_coef: 0.08272/5 [===========>..................] - ETA: 2s - loss: 0.1823 - accuracy: 0.5423 - jacard_coef: 0.07873/5 [=================>............] - ETA: 1s - loss: 0.1819 - accuracy: 0.6129 - jacard_coef: 0.07424/5 [=======================>......] - ETA: 0s - loss: 0.1799 - accuracy: 0.5983 - jacard_coef: 0.07555/5 [==============================] - 3s 567ms/step - loss: 0.1800 - accuracy: 0.5956 - jacard_coef: 0.0875 - val_loss: 1.1144 - val_accuracy: 0.9304 - val_jacard_coef: 1.4497e-04 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1872 - accuracy: 0.2044 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 2s - loss: 0.1857 - accuracy: 0.2075 - jacard_coef: 0.07923/5 [=================>............] - ETA: 1s - loss: 0.1839 - accuracy: 0.2086 - jacard_coef: 0.07724/5 [=======================>......] - ETA: 0s - loss: 0.1829 - accuracy: 0.2135 - jacard_coef: 0.07795/5 [==============================] - 3s 587ms/step - loss: 0.1828 - accuracy: 0.2129 - jacard_coef: 0.0632 - val_loss: 8.1040 - val_accuracy: 0.2113 - val_jacard_coef: 0.0702 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1762 - accuracy: 0.3260 - jacard_coef: 0.07872/5 [===========>..................] - ETA: 2s - loss: 0.1756 - accuracy: 0.3498 - jacard_coef: 0.07143/5 [=================>............] - ETA: 1s - loss: 0.1749 - accuracy: 0.3914 - jacard_coef: 0.08094/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.4361 - jacard_coef: 0.07585/5 [==============================] - 3s 569ms/step - loss: 0.1740 - accuracy: 0.4357 - jacard_coef: 0.0841 - val_loss: 0.2484 - val_accuracy: 0.5930 - val_jacard_coef: 0.0644 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1676 - accuracy: 0.7183 - jacard_coef: 0.07132/5 [===========>..................] - ETA: 2s - loss: 0.1684 - accuracy: 0.6872 - jacard_coef: 0.07393/5 [=================>............] - ETA: 1s - loss: 0.1684 - accuracy: 0.6585 - jacard_coef: 0.07534/5 [=======================>......] - ETA: 0s - loss: 0.1684 - accuracy: 0.6516 - jacard_coef: 0.07575/5 [==============================] - 3s 569ms/step - loss: 0.1685 - accuracy: 0.6490 - jacard_coef: 0.0875 - val_loss: 0.7962 - val_accuracy: 0.9252 - val_jacard_coef: 0.0223 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1704 - accuracy: 0.5352 - jacard_coef: 0.09192/5 [===========>..................] - ETA: 2s - loss: 0.1712 - accuracy: 0.4401 - jacard_coef: 0.07493/5 [=================>............] - ETA: 1s - loss: 0.1750 - accuracy: 0.3669 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 0s - loss: 0.1751 - accuracy: 0.3422 - jacard_coef: 0.07575/5 [==============================] - 3s 571ms/step - loss: 0.1751 - accuracy: 0.3424 - jacard_coef: 0.0917 - val_loss: 0.8995 - val_accuracy: 0.9303 - val_jacard_coef: 0.0117 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1722 - accuracy: 0.3672 - jacard_coef: 0.07232/5 [===========>..................] - ETA: 2s - loss: 0.1708 - accuracy: 0.4019 - jacard_coef: 0.07453/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.4400 - jacard_coef: 0.07714/5 [=======================>......] - ETA: 0s - loss: 0.1695 - accuracy: 0.4849 - jacard_coef: 0.07695/5 [==============================] - 3s 581ms/step - loss: 0.1695 - accuracy: 0.4840 - jacard_coef: 0.0633 - val_loss: 0.6061 - val_accuracy: 0.9237 - val_jacard_coef: 0.0137 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1660 - accuracy: 0.6564 - jacard_coef: 0.06772/5 [===========>..................] - ETA: 2s - loss: 0.1660 - accuracy: 0.6697 - jacard_coef: 0.07043/5 [=================>............] - ETA: 1s - loss: 0.1656 - accuracy: 0.6783 - jacard_coef: 0.07764/5 [=======================>......] - ETA: 0s - loss: 0.1653 - accuracy: 0.6802 - jacard_coef: 0.07525/5 [==============================] - 3s 581ms/step - loss: 0.1656 - accuracy: 0.6787 - jacard_coef: 0.0909 - val_loss: 1.0418 - val_accuracy: 0.9302 - val_jacard_coef: 0.0028 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.5857 - jacard_coef: 0.06882/5 [===========>..................] - ETA: 2s - loss: 0.1685 - accuracy: 0.5848 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1686 - accuracy: 0.5819 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 0s - loss: 0.1687 - accuracy: 0.5713 - jacard_coef: 0.07555/5 [==============================] - 3s 581ms/step - loss: 0.1687 - accuracy: 0.5699 - jacard_coef: 0.0912 - val_loss: 0.9450 - val_accuracy: 0.9263 - val_jacard_coef: 0.0159 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.6671 - jacard_coef: 0.08252/5 [===========>..................] - ETA: 2s - loss: 0.1687 - accuracy: 0.6815 - jacard_coef: 0.07873/5 [=================>............] - ETA: 1s - loss: 0.1688 - accuracy: 0.6296 - jacard_coef: 0.07574/5 [=======================>......] - ETA: 0s - loss: 0.1689 - accuracy: 0.6749 - jacard_coef: 0.07665/5 [==============================] - 3s 579ms/step - loss: 0.1689 - accuracy: 0.6731 - jacard_coef: 0.0718 - val_loss: 0.6218 - val_accuracy: 0.9085 - val_jacard_coef: 0.0373 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1684 - accuracy: 0.7954 - jacard_coef: 0.09302/5 [===========>..................] - ETA: 2s - loss: 0.1674 - accuracy: 0.8071 - jacard_coef: 0.08083/5 [=================>............] - ETA: 1s - loss: 0.1681 - accuracy: 0.7821 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1678 - accuracy: 0.7797 - jacard_coef: 0.07685/5 [==============================] - 3s 580ms/step - loss: 0.1678 - accuracy: 0.7792 - jacard_coef: 0.0623 - val_loss: 0.5456 - val_accuracy: 0.9169 - val_jacard_coef: 0.0345 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.7756 - jacard_coef: 0.08122/5 [===========>..................] - ETA: 2s - loss: 0.1658 - accuracy: 0.8075 - jacard_coef: 0.07223/5 [=================>............] - ETA: 1s - loss: 0.1661 - accuracy: 0.8077 - jacard_coef: 0.07254/5 [=======================>......] - ETA: 0s - loss: 0.1659 - accuracy: 0.8111 - jacard_coef: 0.07595/5 [==============================] - 3s 580ms/step - loss: 0.1659 - accuracy: 0.8093 - jacard_coef: 0.0868 - val_loss: 0.1676 - val_accuracy: 0.9047 - val_jacard_coef: 0.0518 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8523 - jacard_coef: 0.08062/5 [===========>..................] - ETA: 2s - loss: 0.1651 - accuracy: 0.8624 - jacard_coef: 0.07783/5 [=================>............] - ETA: 1s - loss: 0.1645 - accuracy: 0.8761 - jacard_coef: 0.07464/5 [=======================>......] - ETA: 0s - loss: 0.1643 - accuracy: 0.8807 - jacard_coef: 0.07595/5 [==============================] - 3s 580ms/step - loss: 0.1644 - accuracy: 0.8780 - jacard_coef: 0.0857 - val_loss: 0.1169 - val_accuracy: 0.8736 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0702 (epoch 9)
  Final Val Loss: 0.1169
  Training Time: 0:02:29.685945
  Stability (std): 0.3146

Results saved to: hyperparameter_optimization_20250926_123742/exp_15_Attention_UNet_lr1e-4_bs32/Attention_UNet_lr0.0001_bs32_results.json
