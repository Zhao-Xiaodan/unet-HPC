âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758866595.273009 3364772 service.cc:145] XLA service 0x14cac18836b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758866595.273044 3364772 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758866595.652814 3364772 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 5:02 - loss: 0.3205 - accuracy: 0.4754 - jacard_coef: 0.08712/5 [===========>..................] - ETA: 46s - loss: 0.2764 - accuracy: 0.3624 - jacard_coef: 0.0849 3/5 [=================>............] - ETA: 16s - loss: 0.2570 - accuracy: 0.3323 - jacard_coef: 0.07584/5 [=======================>......] - ETA: 5s - loss: 0.2439 - accuracy: 0.2893 - jacard_coef: 0.0775 5/5 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.2882 - jacard_coef: 0.07445/5 [==============================] - 102s 7s/step - loss: 0.2435 - accuracy: 0.2882 - jacard_coef: 0.0744 - val_loss: 0.1400 - val_accuracy: 0.9299 - val_jacard_coef: 0.0629 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1965 - accuracy: 0.2142 - jacard_coef: 0.08632/5 [===========>..................] - ETA: 2s - loss: 0.1964 - accuracy: 0.2558 - jacard_coef: 0.08403/5 [=================>............] - ETA: 1s - loss: 0.1950 - accuracy: 0.2369 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1944 - accuracy: 0.2244 - jacard_coef: 0.07735/5 [==============================] - 3s 667ms/step - loss: 0.1943 - accuracy: 0.2234 - jacard_coef: 0.0623 - val_loss: 0.4444 - val_accuracy: 0.0974 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1914 - accuracy: 0.1510 - jacard_coef: 0.08362/5 [===========>..................] - ETA: 2s - loss: 0.1906 - accuracy: 0.1393 - jacard_coef: 0.07803/5 [=================>............] - ETA: 1s - loss: 0.1907 - accuracy: 0.1352 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1904 - accuracy: 0.1366 - jacard_coef: 0.07675/5 [==============================] - 3s 668ms/step - loss: 0.1904 - accuracy: 0.1367 - jacard_coef: 0.0738 - val_loss: 0.4188 - val_accuracy: 0.1092 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1879 - accuracy: 0.1835 - jacard_coef: 0.07342/5 [===========>..................] - ETA: 2s - loss: 0.1883 - accuracy: 0.2008 - jacard_coef: 0.07173/5 [=================>............] - ETA: 1s - loss: 0.1883 - accuracy: 0.2169 - jacard_coef: 0.07084/5 [=======================>......] - ETA: 0s - loss: 0.1883 - accuracy: 0.2418 - jacard_coef: 0.07625/5 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.2423 - jacard_coef: 0.09005/5 [==============================] - 3s 659ms/step - loss: 0.1883 - accuracy: 0.2423 - jacard_coef: 0.0900 - val_loss: 0.3088 - val_accuracy: 0.1075 - val_jacard_coef: 0.0670 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1873 - accuracy: 0.2954 - jacard_coef: 0.07772/5 [===========>..................] - ETA: 2s - loss: 0.1858 - accuracy: 0.3010 - jacard_coef: 0.07983/5 [=================>............] - ETA: 1s - loss: 0.1858 - accuracy: 0.3024 - jacard_coef: 0.07674/5 [=======================>......] - ETA: 0s - loss: 0.1860 - accuracy: 0.2973 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.2980 - jacard_coef: 0.08925/5 [==============================] - 3s 659ms/step - loss: 0.1860 - accuracy: 0.2980 - jacard_coef: 0.0892 - val_loss: 0.1933 - val_accuracy: 0.1973 - val_jacard_coef: 0.0653 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1837 - accuracy: 0.3137 - jacard_coef: 0.07102/5 [===========>..................] - ETA: 2s - loss: 0.1823 - accuracy: 0.3307 - jacard_coef: 0.07433/5 [=================>............] - ETA: 1s - loss: 0.1816 - accuracy: 0.3397 - jacard_coef: 0.07834/5 [=======================>......] - ETA: 0s - loss: 0.1812 - accuracy: 0.3467 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.3469 - jacard_coef: 0.08585/5 [==============================] - 3s 660ms/step - loss: 0.1812 - accuracy: 0.3469 - jacard_coef: 0.0858 - val_loss: 0.2372 - val_accuracy: 0.1155 - val_jacard_coef: 0.0663 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1764 - accuracy: 0.3822 - jacard_coef: 0.08142/5 [===========>..................] - ETA: 2s - loss: 0.1755 - accuracy: 0.3757 - jacard_coef: 0.07743/5 [=================>............] - ETA: 1s - loss: 0.1746 - accuracy: 0.3815 - jacard_coef: 0.07214/5 [=======================>......] - ETA: 0s - loss: 0.1739 - accuracy: 0.4161 - jacard_coef: 0.07665/5 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.4149 - jacard_coef: 0.06855/5 [==============================] - 3s 661ms/step - loss: 0.1741 - accuracy: 0.4149 - jacard_coef: 0.0685 - val_loss: 0.1525 - val_accuracy: 0.8618 - val_jacard_coef: 0.0643 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1790 - accuracy: 0.3280 - jacard_coef: 0.07302/5 [===========>..................] - ETA: 2s - loss: 0.1756 - accuracy: 0.4461 - jacard_coef: 0.07033/5 [=================>............] - ETA: 1s - loss: 0.1747 - accuracy: 0.4926 - jacard_coef: 0.07284/5 [=======================>......] - ETA: 0s - loss: 0.1745 - accuracy: 0.5086 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.5095 - jacard_coef: 0.09085/5 [==============================] - 3s 660ms/step - loss: 0.1745 - accuracy: 0.5095 - jacard_coef: 0.0908 - val_loss: 0.1095 - val_accuracy: 0.8870 - val_jacard_coef: 0.0617 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1754 - accuracy: 0.5032 - jacard_coef: 0.08572/5 [===========>..................] - ETA: 2s - loss: 0.1755 - accuracy: 0.5025 - jacard_coef: 0.08563/5 [=================>............] - ETA: 1s - loss: 0.1754 - accuracy: 0.5029 - jacard_coef: 0.07994/5 [=======================>......] - ETA: 0s - loss: 0.1755 - accuracy: 0.5066 - jacard_coef: 0.07705/5 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.5065 - jacard_coef: 0.06205/5 [==============================] - 3s 661ms/step - loss: 0.1755 - accuracy: 0.5065 - jacard_coef: 0.0620 - val_loss: 0.0993 - val_accuracy: 0.9111 - val_jacard_coef: 0.0606 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1734 - accuracy: 0.5536 - jacard_coef: 0.07992/5 [===========>..................] - ETA: 2s - loss: 0.1743 - accuracy: 0.5476 - jacard_coef: 0.07393/5 [=================>............] - ETA: 1s - loss: 0.1740 - accuracy: 0.5549 - jacard_coef: 0.07624/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.5627 - jacard_coef: 0.07645/5 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.5625 - jacard_coef: 0.08015/5 [==============================] - 3s 659ms/step - loss: 0.1741 - accuracy: 0.5625 - jacard_coef: 0.0801 - val_loss: 0.1496 - val_accuracy: 0.8604 - val_jacard_coef: 0.0645 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1713 - accuracy: 0.6273 - jacard_coef: 0.08942/5 [===========>..................] - ETA: 2s - loss: 0.1715 - accuracy: 0.6277 - jacard_coef: 0.08113/5 [=================>............] - ETA: 1s - loss: 0.1717 - accuracy: 0.6351 - jacard_coef: 0.07774/5 [=======================>......] - ETA: 0s - loss: 0.1716 - accuracy: 0.6401 - jacard_coef: 0.07705/5 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.6402 - jacard_coef: 0.06205/5 [==============================] - 3s 660ms/step - loss: 0.1716 - accuracy: 0.6402 - jacard_coef: 0.0620 - val_loss: 0.2286 - val_accuracy: 0.1294 - val_jacard_coef: 0.0665 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1699 - accuracy: 0.6780 - jacard_coef: 0.08302/5 [===========>..................] - ETA: 2s - loss: 0.1696 - accuracy: 0.6863 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1694 - accuracy: 0.6945 - jacard_coef: 0.07454/5 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.6931 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.6928 - jacard_coef: 0.09025/5 [==============================] - 3s 659ms/step - loss: 0.1694 - accuracy: 0.6928 - jacard_coef: 0.0902 - val_loss: 0.1509 - val_accuracy: 0.8601 - val_jacard_coef: 0.0644 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1678 - accuracy: 0.7154 - jacard_coef: 0.07412/5 [===========>..................] - ETA: 2s - loss: 0.1679 - accuracy: 0.7165 - jacard_coef: 0.07463/5 [=================>............] - ETA: 1s - loss: 0.1676 - accuracy: 0.7200 - jacard_coef: 0.07454/5 [=======================>......] - ETA: 0s - loss: 0.1673 - accuracy: 0.7194 - jacard_coef: 0.07665/5 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.7193 - jacard_coef: 0.07275/5 [==============================] - 3s 660ms/step - loss: 0.1673 - accuracy: 0.7193 - jacard_coef: 0.0727 - val_loss: 0.1373 - val_accuracy: 0.8885 - val_jacard_coef: 0.0641 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0682 (epoch 3)
  Final Val Loss: 0.1373
  Training Time: 0:02:24.876700
  Stability (std): 0.0617

Results saved to: hyperparameter_optimization_20250926_123742/exp_33_Attention_ResUNet_lr1e-3_bs32/Attention_ResUNet_lr0.001_bs32_results.json
