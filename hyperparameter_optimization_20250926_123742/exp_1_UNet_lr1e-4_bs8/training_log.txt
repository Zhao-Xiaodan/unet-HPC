âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758861534.496847 3175547 service.cc:145] XLA service 0x147055ca9170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758861534.496916 3175547 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758861535.011625 3175547 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 11:22 - loss: 0.3500 - accuracy: 0.5060 - jacard_coef: 0.0629 2/17 [==>...........................] - ETA: 56s - loss: 0.3504 - accuracy: 0.5342 - jacard_coef: 0.0707   3/17 [====>.........................] - ETA: 34s - loss: 0.3368 - accuracy: 0.5475 - jacard_coef: 0.0684 4/17 [======>.......................] - ETA: 25s - loss: 0.3214 - accuracy: 0.5731 - jacard_coef: 0.0650 5/17 [=======>......................] - ETA: 18s - loss: 0.3046 - accuracy: 0.5904 - jacard_coef: 0.0733 6/17 [=========>....................] - ETA: 13s - loss: 0.2908 - accuracy: 0.6225 - jacard_coef: 0.0692 7/17 [===========>..................] - ETA: 10s - loss: 0.2882 - accuracy: 0.6484 - jacard_coef: 0.0681 8/17 [=============>................] - ETA: 8s - loss: 0.2818 - accuracy: 0.6644 - jacard_coef: 0.0703  9/17 [==============>...............] - ETA: 6s - loss: 0.2741 - accuracy: 0.6762 - jacard_coef: 0.070910/17 [================>.............] - ETA: 5s - loss: 0.2683 - accuracy: 0.6721 - jacard_coef: 0.071811/17 [==================>...........] - ETA: 4s - loss: 0.2649 - accuracy: 0.6760 - jacard_coef: 0.072512/17 [====================>.........] - ETA: 3s - loss: 0.2613 - accuracy: 0.6716 - jacard_coef: 0.073913/17 [=====================>........] - ETA: 2s - loss: 0.2578 - accuracy: 0.6696 - jacard_coef: 0.073814/17 [=======================>......] - ETA: 1s - loss: 0.2541 - accuracy: 0.6660 - jacard_coef: 0.072615/17 [=========================>....] - ETA: 1s - loss: 0.2508 - accuracy: 0.6624 - jacard_coef: 0.073516/17 [===========================>..] - ETA: 0s - loss: 0.2482 - accuracy: 0.6599 - jacard_coef: 0.074817/17 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.6597 - jacard_coef: 0.076617/17 [==============================] - 57s 911ms/step - loss: 0.2486 - accuracy: 0.6597 - jacard_coef: 0.0766 - val_loss: 14.9608 - val_accuracy: 0.0696 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1964 - accuracy: 0.4187 - jacard_coef: 0.0918 2/17 [==>...........................] - ETA: 1s - loss: 0.2021 - accuracy: 0.4370 - jacard_coef: 0.1067 3/17 [====>.........................] - ETA: 1s - loss: 0.2114 - accuracy: 0.4665 - jacard_coef: 0.0944 4/17 [======>.......................] - ETA: 1s - loss: 0.2027 - accuracy: 0.5043 - jacard_coef: 0.0896 5/17 [=======>......................] - ETA: 1s - loss: 0.2045 - accuracy: 0.5530 - jacard_coef: 0.0886 6/17 [=========>....................] - ETA: 1s - loss: 0.2009 - accuracy: 0.5955 - jacard_coef: 0.0926 7/17 [===========>..................] - ETA: 1s - loss: 0.2022 - accuracy: 0.6318 - jacard_coef: 0.0866 8/17 [=============>................] - ETA: 1s - loss: 0.2008 - accuracy: 0.6531 - jacard_coef: 0.0858 9/17 [==============>...............] - ETA: 1s - loss: 0.1991 - accuracy: 0.6680 - jacard_coef: 0.082610/17 [================>.............] - ETA: 0s - loss: 0.1987 - accuracy: 0.6776 - jacard_coef: 0.083711/17 [==================>...........] - ETA: 0s - loss: 0.1978 - accuracy: 0.6868 - jacard_coef: 0.080812/17 [====================>.........] - ETA: 0s - loss: 0.1967 - accuracy: 0.6890 - jacard_coef: 0.081813/17 [=====================>........] - ETA: 0s - loss: 0.1949 - accuracy: 0.6906 - jacard_coef: 0.080814/17 [=======================>......] - ETA: 0s - loss: 0.1948 - accuracy: 0.6953 - jacard_coef: 0.077315/17 [=========================>....] - ETA: 0s - loss: 0.1936 - accuracy: 0.6996 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1930 - accuracy: 0.7047 - jacard_coef: 0.075817/17 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.7061 - jacard_coef: 0.075017/17 [==============================] - 2s 137ms/step - loss: 0.1930 - accuracy: 0.7061 - jacard_coef: 0.0750 - val_loss: 14.0259 - val_accuracy: 0.0746 - val_jacard_coef: 0.0676 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1929 - accuracy: 0.7856 - jacard_coef: 0.0861 2/17 [==>...........................] - ETA: 1s - loss: 0.1856 - accuracy: 0.7639 - jacard_coef: 0.0827 3/17 [====>.........................] - ETA: 1s - loss: 0.1804 - accuracy: 0.7735 - jacard_coef: 0.0795 4/17 [======>.......................] - ETA: 1s - loss: 0.1796 - accuracy: 0.7765 - jacard_coef: 0.0825 5/17 [=======>......................] - ETA: 1s - loss: 0.1794 - accuracy: 0.7851 - jacard_coef: 0.0739 6/17 [=========>....................] - ETA: 1s - loss: 0.1794 - accuracy: 0.7729 - jacard_coef: 0.0725 7/17 [===========>..................] - ETA: 1s - loss: 0.1777 - accuracy: 0.7722 - jacard_coef: 0.0717 8/17 [=============>................] - ETA: 1s - loss: 0.1775 - accuracy: 0.7536 - jacard_coef: 0.0725 9/17 [==============>...............] - ETA: 1s - loss: 0.1787 - accuracy: 0.7412 - jacard_coef: 0.070610/17 [================>.............] - ETA: 0s - loss: 0.1775 - accuracy: 0.7317 - jacard_coef: 0.072411/17 [==================>...........] - ETA: 0s - loss: 0.1773 - accuracy: 0.7321 - jacard_coef: 0.074812/17 [====================>.........] - ETA: 0s - loss: 0.1771 - accuracy: 0.7329 - jacard_coef: 0.075113/17 [=====================>........] - ETA: 0s - loss: 0.1778 - accuracy: 0.7289 - jacard_coef: 0.075314/17 [=======================>......] - ETA: 0s - loss: 0.1777 - accuracy: 0.7335 - jacard_coef: 0.077915/17 [=========================>....] - ETA: 0s - loss: 0.1775 - accuracy: 0.7364 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1776 - accuracy: 0.7406 - jacard_coef: 0.076217/17 [==============================] - 2s 137ms/step - loss: 0.1777 - accuracy: 0.7416 - jacard_coef: 0.0718 - val_loss: 14.4513 - val_accuracy: 0.0731 - val_jacard_coef: 0.0683 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1726 - accuracy: 0.8016 - jacard_coef: 0.0762 2/17 [==>...........................] - ETA: 1s - loss: 0.1709 - accuracy: 0.8253 - jacard_coef: 0.0594 3/17 [====>.........................] - ETA: 1s - loss: 0.1702 - accuracy: 0.8142 - jacard_coef: 0.0612 4/17 [======>.......................] - ETA: 1s - loss: 0.1930 - accuracy: 0.8062 - jacard_coef: 0.0671 5/17 [=======>......................] - ETA: 1s - loss: 0.2018 - accuracy: 0.8055 - jacard_coef: 0.0721 6/17 [=========>....................] - ETA: 1s - loss: 0.2051 - accuracy: 0.7957 - jacard_coef: 0.0745 7/17 [===========>..................] - ETA: 1s - loss: 0.2060 - accuracy: 0.7737 - jacard_coef: 0.0780 8/17 [=============>................] - ETA: 1s - loss: 0.2058 - accuracy: 0.7793 - jacard_coef: 0.0774 9/17 [==============>...............] - ETA: 1s - loss: 0.2099 - accuracy: 0.7852 - jacard_coef: 0.078710/17 [================>.............] - ETA: 0s - loss: 0.2127 - accuracy: 0.7925 - jacard_coef: 0.077211/17 [==================>...........] - ETA: 0s - loss: 0.2120 - accuracy: 0.7999 - jacard_coef: 0.077312/17 [====================>.........] - ETA: 0s - loss: 0.2112 - accuracy: 0.8055 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.2111 - accuracy: 0.8115 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.2112 - accuracy: 0.8205 - jacard_coef: 0.073715/17 [=========================>....] - ETA: 0s - loss: 0.2095 - accuracy: 0.8254 - jacard_coef: 0.073816/17 [===========================>..] - ETA: 0s - loss: 0.2088 - accuracy: 0.8293 - jacard_coef: 0.074317/17 [==============================] - 2s 136ms/step - loss: 0.2087 - accuracy: 0.8290 - jacard_coef: 0.0789 - val_loss: 11.7496 - val_accuracy: 0.1140 - val_jacard_coef: 0.0688 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1924 - accuracy: 0.8538 - jacard_coef: 0.0805 2/17 [==>...........................] - ETA: 1s - loss: 0.1902 - accuracy: 0.7807 - jacard_coef: 0.0740 3/17 [====>.........................] - ETA: 1s - loss: 0.1875 - accuracy: 0.7756 - jacard_coef: 0.0817 4/17 [======>.......................] - ETA: 1s - loss: 0.1845 - accuracy: 0.7525 - jacard_coef: 0.0823 5/17 [=======>......................] - ETA: 1s - loss: 0.1833 - accuracy: 0.7265 - jacard_coef: 0.0832 6/17 [=========>....................] - ETA: 1s - loss: 0.1811 - accuracy: 0.7350 - jacard_coef: 0.0832 7/17 [===========>..................] - ETA: 1s - loss: 0.1817 - accuracy: 0.7261 - jacard_coef: 0.0770 8/17 [=============>................] - ETA: 1s - loss: 0.1803 - accuracy: 0.7503 - jacard_coef: 0.0758 9/17 [==============>...............] - ETA: 1s - loss: 0.1800 - accuracy: 0.7693 - jacard_coef: 0.074710/17 [================>.............] - ETA: 0s - loss: 0.1797 - accuracy: 0.7618 - jacard_coef: 0.077611/17 [==================>...........] - ETA: 0s - loss: 0.1793 - accuracy: 0.7573 - jacard_coef: 0.079512/17 [====================>.........] - ETA: 0s - loss: 0.1785 - accuracy: 0.7590 - jacard_coef: 0.076713/17 [=====================>........] - ETA: 0s - loss: 0.1767 - accuracy: 0.7573 - jacard_coef: 0.074914/17 [=======================>......] - ETA: 0s - loss: 0.1755 - accuracy: 0.7689 - jacard_coef: 0.074315/17 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.7744 - jacard_coef: 0.074416/17 [===========================>..] - ETA: 0s - loss: 0.1764 - accuracy: 0.7822 - jacard_coef: 0.074717/17 [==============================] - 2s 136ms/step - loss: 0.1765 - accuracy: 0.7826 - jacard_coef: 0.0773 - val_loss: 0.4235 - val_accuracy: 0.0856 - val_jacard_coef: 0.0645 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1667 - accuracy: 0.8251 - jacard_coef: 0.0647 2/17 [==>...........................] - ETA: 2s - loss: 0.1715 - accuracy: 0.7759 - jacard_coef: 0.0955 3/17 [====>.........................] - ETA: 1s - loss: 0.1691 - accuracy: 0.7763 - jacard_coef: 0.0852 4/17 [======>.......................] - ETA: 1s - loss: 0.1666 - accuracy: 0.7798 - jacard_coef: 0.0865 5/17 [=======>......................] - ETA: 1s - loss: 0.1648 - accuracy: 0.7873 - jacard_coef: 0.0770 6/17 [=========>....................] - ETA: 1s - loss: 0.1681 - accuracy: 0.7729 - jacard_coef: 0.0806 7/17 [===========>..................] - ETA: 1s - loss: 0.1676 - accuracy: 0.7851 - jacard_coef: 0.0774 8/17 [=============>................] - ETA: 1s - loss: 0.1672 - accuracy: 0.7914 - jacard_coef: 0.0734 9/17 [==============>...............] - ETA: 1s - loss: 0.1683 - accuracy: 0.7958 - jacard_coef: 0.074110/17 [================>.............] - ETA: 0s - loss: 0.1680 - accuracy: 0.8019 - jacard_coef: 0.074611/17 [==================>...........] - ETA: 0s - loss: 0.1686 - accuracy: 0.8063 - jacard_coef: 0.076212/17 [====================>.........] - ETA: 0s - loss: 0.1694 - accuracy: 0.8052 - jacard_coef: 0.079013/17 [=====================>........] - ETA: 0s - loss: 0.1692 - accuracy: 0.8100 - jacard_coef: 0.079514/17 [=======================>......] - ETA: 0s - loss: 0.1683 - accuracy: 0.8178 - jacard_coef: 0.076215/17 [=========================>....] - ETA: 0s - loss: 0.1681 - accuracy: 0.8195 - jacard_coef: 0.075116/17 [===========================>..] - ETA: 0s - loss: 0.1687 - accuracy: 0.8135 - jacard_coef: 0.074617/17 [==============================] - 2s 137ms/step - loss: 0.1688 - accuracy: 0.8135 - jacard_coef: 0.0734 - val_loss: 0.1893 - val_accuracy: 0.4937 - val_jacard_coef: 0.0644 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9277 - jacard_coef: 0.0623 2/17 [==>...........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.9200 - jacard_coef: 0.0694 3/17 [====>.........................] - ETA: 1s - loss: 0.1606 - accuracy: 0.9180 - jacard_coef: 0.0703 4/17 [======>.......................] - ETA: 1s - loss: 0.1613 - accuracy: 0.9208 - jacard_coef: 0.0671 5/17 [=======>......................] - ETA: 1s - loss: 0.1623 - accuracy: 0.9189 - jacard_coef: 0.0666 6/17 [=========>....................] - ETA: 1s - loss: 0.1619 - accuracy: 0.9086 - jacard_coef: 0.0675 7/17 [===========>..................] - ETA: 1s - loss: 0.1644 - accuracy: 0.8855 - jacard_coef: 0.0754 8/17 [=============>................] - ETA: 1s - loss: 0.1639 - accuracy: 0.8902 - jacard_coef: 0.0744 9/17 [==============>...............] - ETA: 1s - loss: 0.1630 - accuracy: 0.8975 - jacard_coef: 0.070710/17 [================>.............] - ETA: 0s - loss: 0.1637 - accuracy: 0.8988 - jacard_coef: 0.071511/17 [==================>...........] - ETA: 0s - loss: 0.1647 - accuracy: 0.9015 - jacard_coef: 0.070312/17 [====================>.........] - ETA: 0s - loss: 0.1659 - accuracy: 0.8985 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 0s - loss: 0.1665 - accuracy: 0.8994 - jacard_coef: 0.073514/17 [=======================>......] - ETA: 0s - loss: 0.1665 - accuracy: 0.8977 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1653 - accuracy: 0.8990 - jacard_coef: 0.075216/17 [===========================>..] - ETA: 0s - loss: 0.1652 - accuracy: 0.8974 - jacard_coef: 0.075517/17 [==============================] - 2s 137ms/step - loss: 0.1654 - accuracy: 0.8978 - jacard_coef: 0.0713 - val_loss: 0.1952 - val_accuracy: 0.1079 - val_jacard_coef: 0.0638 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1690 - accuracy: 0.8279 - jacard_coef: 0.0473 2/17 [==>...........................] - ETA: 2s - loss: 0.1628 - accuracy: 0.8429 - jacard_coef: 0.0652 3/17 [====>.........................] - ETA: 1s - loss: 0.1592 - accuracy: 0.8627 - jacard_coef: 0.0677 4/17 [======>.......................] - ETA: 1s - loss: 0.1576 - accuracy: 0.8712 - jacard_coef: 0.0693 5/17 [=======>......................] - ETA: 1s - loss: 0.1573 - accuracy: 0.8765 - jacard_coef: 0.0717 6/17 [=========>....................] - ETA: 1s - loss: 0.1569 - accuracy: 0.8713 - jacard_coef: 0.0756 7/17 [===========>..................] - ETA: 1s - loss: 0.1565 - accuracy: 0.8706 - jacard_coef: 0.0733 8/17 [=============>................] - ETA: 1s - loss: 0.1559 - accuracy: 0.8709 - jacard_coef: 0.0740 9/17 [==============>...............] - ETA: 1s - loss: 0.1579 - accuracy: 0.8632 - jacard_coef: 0.072210/17 [================>.............] - ETA: 0s - loss: 0.1578 - accuracy: 0.8675 - jacard_coef: 0.073211/17 [==================>...........] - ETA: 0s - loss: 0.1567 - accuracy: 0.8729 - jacard_coef: 0.072812/17 [====================>.........] - ETA: 0s - loss: 0.1570 - accuracy: 0.8751 - jacard_coef: 0.074113/17 [=====================>........] - ETA: 0s - loss: 0.1567 - accuracy: 0.8789 - jacard_coef: 0.073714/17 [=======================>......] - ETA: 0s - loss: 0.1572 - accuracy: 0.8801 - jacard_coef: 0.074915/17 [=========================>....] - ETA: 0s - loss: 0.1574 - accuracy: 0.8831 - jacard_coef: 0.074016/17 [===========================>..] - ETA: 0s - loss: 0.1581 - accuracy: 0.8763 - jacard_coef: 0.075317/17 [==============================] - 2s 137ms/step - loss: 0.1582 - accuracy: 0.8768 - jacard_coef: 0.0724 - val_loss: 0.1400 - val_accuracy: 0.9303 - val_jacard_coef: 0.0626 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1578 - accuracy: 0.9117 - jacard_coef: 0.0678 2/17 [==>...........................] - ETA: 2s - loss: 0.1663 - accuracy: 0.8877 - jacard_coef: 0.0804 3/17 [====>.........................] - ETA: 1s - loss: 0.1594 - accuracy: 0.8985 - jacard_coef: 0.0769 4/17 [======>.......................] - ETA: 1s - loss: 0.1595 - accuracy: 0.8846 - jacard_coef: 0.0669 5/17 [=======>......................] - ETA: 1s - loss: 0.1582 - accuracy: 0.8848 - jacard_coef: 0.0705 6/17 [=========>....................] - ETA: 1s - loss: 0.1591 - accuracy: 0.8766 - jacard_coef: 0.0771 7/17 [===========>..................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8580 - jacard_coef: 0.0774 8/17 [=============>................] - ETA: 1s - loss: 0.1609 - accuracy: 0.8470 - jacard_coef: 0.0785 9/17 [==============>...............] - ETA: 1s - loss: 0.1609 - accuracy: 0.8404 - jacard_coef: 0.078410/17 [================>.............] - ETA: 0s - loss: 0.1615 - accuracy: 0.8367 - jacard_coef: 0.077311/17 [==================>...........] - ETA: 0s - loss: 0.1614 - accuracy: 0.8381 - jacard_coef: 0.074512/17 [====================>.........] - ETA: 0s - loss: 0.1613 - accuracy: 0.8423 - jacard_coef: 0.073513/17 [=====================>........] - ETA: 0s - loss: 0.1611 - accuracy: 0.8445 - jacard_coef: 0.074614/17 [=======================>......] - ETA: 0s - loss: 0.1607 - accuracy: 0.8470 - jacard_coef: 0.075315/17 [=========================>....] - ETA: 0s - loss: 0.1603 - accuracy: 0.8496 - jacard_coef: 0.075816/17 [===========================>..] - ETA: 0s - loss: 0.1595 - accuracy: 0.8532 - jacard_coef: 0.075017/17 [==============================] - 2s 137ms/step - loss: 0.1593 - accuracy: 0.8538 - jacard_coef: 0.0742 - val_loss: 0.1218 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1632 - accuracy: 0.8626 - jacard_coef: 0.0720 2/17 [==>...........................] - ETA: 2s - loss: 0.1553 - accuracy: 0.8784 - jacard_coef: 0.0706 3/17 [====>.........................] - ETA: 1s - loss: 0.1515 - accuracy: 0.8944 - jacard_coef: 0.0671 4/17 [======>.......................] - ETA: 1s - loss: 0.1481 - accuracy: 0.8995 - jacard_coef: 0.0697 5/17 [=======>......................] - ETA: 1s - loss: 0.1476 - accuracy: 0.9001 - jacard_coef: 0.0731 6/17 [=========>....................] - ETA: 1s - loss: 0.1489 - accuracy: 0.9049 - jacard_coef: 0.0715 7/17 [===========>..................] - ETA: 1s - loss: 0.1482 - accuracy: 0.9117 - jacard_coef: 0.0675 8/17 [=============>................] - ETA: 1s - loss: 0.1478 - accuracy: 0.9178 - jacard_coef: 0.0636 9/17 [==============>...............] - ETA: 1s - loss: 0.1480 - accuracy: 0.9141 - jacard_coef: 0.066710/17 [================>.............] - ETA: 0s - loss: 0.1483 - accuracy: 0.9143 - jacard_coef: 0.067411/17 [==================>...........] - ETA: 0s - loss: 0.1483 - accuracy: 0.9115 - jacard_coef: 0.070512/17 [====================>.........] - ETA: 0s - loss: 0.1483 - accuracy: 0.9088 - jacard_coef: 0.073313/17 [=====================>........] - ETA: 0s - loss: 0.1479 - accuracy: 0.9092 - jacard_coef: 0.072614/17 [=======================>......] - ETA: 0s - loss: 0.1479 - accuracy: 0.9063 - jacard_coef: 0.073715/17 [=========================>....] - ETA: 0s - loss: 0.1530 - accuracy: 0.8967 - jacard_coef: 0.075316/17 [===========================>..] - ETA: 0s - loss: 0.1524 - accuracy: 0.8976 - jacard_coef: 0.075517/17 [==============================] - 2s 137ms/step - loss: 0.1525 - accuracy: 0.8979 - jacard_coef: 0.0713 - val_loss: 0.1242 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1482 - accuracy: 0.9159 - jacard_coef: 0.0758 2/17 [==>...........................] - ETA: 2s - loss: 0.1497 - accuracy: 0.9110 - jacard_coef: 0.0796 3/17 [====>.........................] - ETA: 1s - loss: 0.1488 - accuracy: 0.9270 - jacard_coef: 0.0654 4/17 [======>.......................] - ETA: 1s - loss: 0.1494 - accuracy: 0.9179 - jacard_coef: 0.0727 5/17 [=======>......................] - ETA: 1s - loss: 0.1496 - accuracy: 0.9156 - jacard_coef: 0.0749 6/17 [=========>....................] - ETA: 1s - loss: 0.1490 - accuracy: 0.9209 - jacard_coef: 0.0704 7/17 [===========>..................] - ETA: 1s - loss: 0.1490 - accuracy: 0.9222 - jacard_coef: 0.0694 8/17 [=============>................] - ETA: 1s - loss: 0.1515 - accuracy: 0.9226 - jacard_coef: 0.0687 9/17 [==============>...............] - ETA: 1s - loss: 0.1525 - accuracy: 0.9211 - jacard_coef: 0.069710/17 [================>.............] - ETA: 0s - loss: 0.1526 - accuracy: 0.9195 - jacard_coef: 0.071011/17 [==================>...........] - ETA: 0s - loss: 0.1536 - accuracy: 0.9147 - jacard_coef: 0.074612/17 [====================>.........] - ETA: 0s - loss: 0.1533 - accuracy: 0.9128 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.1531 - accuracy: 0.9133 - jacard_coef: 0.075514/17 [=======================>......] - ETA: 0s - loss: 0.1529 - accuracy: 0.9133 - jacard_coef: 0.075215/17 [=========================>....] - ETA: 0s - loss: 0.1521 - accuracy: 0.9133 - jacard_coef: 0.075316/17 [===========================>..] - ETA: 0s - loss: 0.1520 - accuracy: 0.9137 - jacard_coef: 0.074917/17 [==============================] - 2s 137ms/step - loss: 0.1520 - accuracy: 0.9138 - jacard_coef: 0.0737 - val_loss: 0.1234 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1466 - accuracy: 0.8973 - jacard_coef: 0.0884 2/17 [==>...........................] - ETA: 2s - loss: 0.1465 - accuracy: 0.9100 - jacard_coef: 0.0783 3/17 [====>.........................] - ETA: 1s - loss: 0.1460 - accuracy: 0.9175 - jacard_coef: 0.0725 4/17 [======>.......................] - ETA: 1s - loss: 0.1456 - accuracy: 0.9174 - jacard_coef: 0.0731 5/17 [=======>......................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9167 - jacard_coef: 0.0740 6/17 [=========>....................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9131 - jacard_coef: 0.0771 7/17 [===========>..................] - ETA: 1s - loss: 0.1453 - accuracy: 0.9092 - jacard_coef: 0.0753 8/17 [=============>................] - ETA: 1s - loss: 0.1453 - accuracy: 0.9081 - jacard_coef: 0.0769 9/17 [==============>...............] - ETA: 1s - loss: 0.1448 - accuracy: 0.9089 - jacard_coef: 0.077010/17 [================>.............] - ETA: 0s - loss: 0.1445 - accuracy: 0.9097 - jacard_coef: 0.076811/17 [==================>...........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9088 - jacard_coef: 0.077812/17 [====================>.........] - ETA: 0s - loss: 0.1441 - accuracy: 0.9110 - jacard_coef: 0.075713/17 [=====================>........] - ETA: 0s - loss: 0.1439 - accuracy: 0.9102 - jacard_coef: 0.075614/17 [=======================>......] - ETA: 0s - loss: 0.1440 - accuracy: 0.9097 - jacard_coef: 0.075715/17 [=========================>....] - ETA: 0s - loss: 0.1438 - accuracy: 0.9096 - jacard_coef: 0.076016/17 [===========================>..] - ETA: 0s - loss: 0.1435 - accuracy: 0.9108 - jacard_coef: 0.075217/17 [==============================] - 2s 137ms/step - loss: 0.1436 - accuracy: 0.9096 - jacard_coef: 0.0741 - val_loss: 0.1163 - val_accuracy: 0.9304 - val_jacard_coef: 0.0620 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1423 - accuracy: 0.9348 - jacard_coef: 0.0599 2/17 [==>...........................] - ETA: 2s - loss: 0.1426 - accuracy: 0.9166 - jacard_coef: 0.0752 3/17 [====>.........................] - ETA: 1s - loss: 0.1424 - accuracy: 0.9183 - jacard_coef: 0.0738 4/17 [======>.......................] - ETA: 1s - loss: 0.1442 - accuracy: 0.9164 - jacard_coef: 0.0753 5/17 [=======>......................] - ETA: 1s - loss: 0.1438 - accuracy: 0.9199 - jacard_coef: 0.0722 6/17 [=========>....................] - ETA: 1s - loss: 0.1443 - accuracy: 0.9195 - jacard_coef: 0.0727 7/17 [===========>..................] - ETA: 1s - loss: 0.1453 - accuracy: 0.9182 - jacard_coef: 0.0737 8/17 [=============>................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9185 - jacard_coef: 0.0735 9/17 [==============>...............] - ETA: 1s - loss: 0.1458 - accuracy: 0.9168 - jacard_coef: 0.074810/17 [================>.............] - ETA: 0s - loss: 0.1455 - accuracy: 0.9153 - jacard_coef: 0.076111/17 [==================>...........] - ETA: 0s - loss: 0.1456 - accuracy: 0.9156 - jacard_coef: 0.075812/17 [====================>.........] - ETA: 0s - loss: 0.1451 - accuracy: 0.9150 - jacard_coef: 0.076413/17 [=====================>........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9162 - jacard_coef: 0.075414/17 [=======================>......] - ETA: 0s - loss: 0.1446 - accuracy: 0.9144 - jacard_coef: 0.076815/17 [=========================>....] - ETA: 0s - loss: 0.1441 - accuracy: 0.9152 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9170 - jacard_coef: 0.074417/17 [==============================] - 2s 137ms/step - loss: 0.1437 - accuracy: 0.9162 - jacard_coef: 0.0785 - val_loss: 0.1182 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1375 - accuracy: 0.9217 - jacard_coef: 0.0670 2/17 [==>...........................] - ETA: 2s - loss: 0.1380 - accuracy: 0.9110 - jacard_coef: 0.0698 3/17 [====>.........................] - ETA: 1s - loss: 0.1381 - accuracy: 0.9048 - jacard_coef: 0.0736 4/17 [======>.......................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9002 - jacard_coef: 0.0796 5/17 [=======>......................] - ETA: 1s - loss: 0.1395 - accuracy: 0.8999 - jacard_coef: 0.0815 6/17 [=========>....................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9057 - jacard_coef: 0.0772 7/17 [===========>..................] - ETA: 1s - loss: 0.1384 - accuracy: 0.9067 - jacard_coef: 0.0728 8/17 [=============>................] - ETA: 1s - loss: 0.1386 - accuracy: 0.9056 - jacard_coef: 0.0746 9/17 [==============>...............] - ETA: 1s - loss: 0.1381 - accuracy: 0.9097 - jacard_coef: 0.072310/17 [================>.............] - ETA: 0s - loss: 0.1380 - accuracy: 0.9102 - jacard_coef: 0.072411/17 [==================>...........] - ETA: 0s - loss: 0.1378 - accuracy: 0.9125 - jacard_coef: 0.071112/17 [====================>.........] - ETA: 0s - loss: 0.1380 - accuracy: 0.9097 - jacard_coef: 0.071113/17 [=====================>........] - ETA: 0s - loss: 0.1381 - accuracy: 0.9090 - jacard_coef: 0.072514/17 [=======================>......] - ETA: 0s - loss: 0.1378 - accuracy: 0.9119 - jacard_coef: 0.070715/17 [=========================>....] - ETA: 0s - loss: 0.1379 - accuracy: 0.9106 - jacard_coef: 0.072316/17 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9091 - jacard_coef: 0.074017/17 [==============================] - 2s 137ms/step - loss: 0.1383 - accuracy: 0.9083 - jacard_coef: 0.0787 - val_loss: 0.1170 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0688 (epoch 4)
  Final Val Loss: 0.1170
  Training Time: 0:01:28.369425
  Stability (std): 0.0900

Results saved to: hyperparameter_optimization_20250926_123742/exp_1_UNet_lr1e-4_bs8/UNet_lr0.0001_bs8_results.json
