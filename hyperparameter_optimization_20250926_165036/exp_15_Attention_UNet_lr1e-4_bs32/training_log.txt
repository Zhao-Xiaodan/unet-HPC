✓ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
✓ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758878292.987658 1085220 service.cc:145] XLA service 0x14875456d230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758878292.987682 1085220 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758878293.126460 1085220 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:02 - loss: 0.3366 - accuracy: 0.4936 - jacard_coef: 0.08892/5 [===========>..................] - ETA: 46s - loss: 0.2957 - accuracy: 0.3688 - jacard_coef: 0.0785 3/5 [=================>............] - ETA: 16s - loss: 0.2722 - accuracy: 0.3131 - jacard_coef: 0.07994/5 [=======================>......] - ETA: 5s - loss: 0.2563 - accuracy: 0.2670 - jacard_coef: 0.0809 5/5 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.2667 - jacard_coef: 0.09935/5 [==============================] - 86s 6s/step - loss: 0.2558 - accuracy: 0.2667 - jacard_coef: 0.0993 - val_loss: 1.1135 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.2021 - accuracy: 0.1435 - jacard_coef: 0.06582/5 [===========>..................] - ETA: 2s - loss: 0.2000 - accuracy: 0.1520 - jacard_coef: 0.07413/5 [=================>............] - ETA: 1s - loss: 0.1981 - accuracy: 0.1530 - jacard_coef: 0.08244/5 [=======================>......] - ETA: 0s - loss: 0.1961 - accuracy: 0.1494 - jacard_coef: 0.08095/5 [==============================] - 3s 558ms/step - loss: 0.1960 - accuracy: 0.1498 - jacard_coef: 0.0949 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1844 - accuracy: 0.1744 - jacard_coef: 0.08052/5 [===========>..................] - ETA: 2s - loss: 0.1831 - accuracy: 0.1888 - jacard_coef: 0.08773/5 [=================>............] - ETA: 1s - loss: 0.1825 - accuracy: 0.1976 - jacard_coef: 0.08304/5 [=======================>......] - ETA: 0s - loss: 0.1810 - accuracy: 0.2304 - jacard_coef: 0.08235/5 [==============================] - 3s 557ms/step - loss: 0.1816 - accuracy: 0.2313 - jacard_coef: 0.0920 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1793 - accuracy: 0.3469 - jacard_coef: 0.08922/5 [===========>..................] - ETA: 2s - loss: 0.1805 - accuracy: 0.3103 - jacard_coef: 0.09003/5 [=================>............] - ETA: 1s - loss: 0.1813 - accuracy: 0.2741 - jacard_coef: 0.08534/5 [=======================>......] - ETA: 0s - loss: 0.1815 - accuracy: 0.2535 - jacard_coef: 0.08055/5 [==============================] - 3s 557ms/step - loss: 0.1816 - accuracy: 0.2522 - jacard_coef: 0.0649 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1830 - accuracy: 0.1894 - jacard_coef: 0.06752/5 [===========>..................] - ETA: 2s - loss: 0.1818 - accuracy: 0.2182 - jacard_coef: 0.07923/5 [=================>............] - ETA: 1s - loss: 0.1810 - accuracy: 0.2402 - jacard_coef: 0.08434/5 [=======================>......] - ETA: 0s - loss: 0.1803 - accuracy: 0.2701 - jacard_coef: 0.08105/5 [==============================] - 3s 556ms/step - loss: 0.1803 - accuracy: 0.2709 - jacard_coef: 0.0862 - val_loss: 1.1202 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1754 - accuracy: 0.5342 - jacard_coef: 0.06922/5 [===========>..................] - ETA: 2s - loss: 0.1745 - accuracy: 0.5558 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1751 - accuracy: 0.5131 - jacard_coef: 0.08054/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.5359 - jacard_coef: 0.07915/5 [==============================] - 3s 570ms/step - loss: 0.1740 - accuracy: 0.5383 - jacard_coef: 0.0872 - val_loss: 0.6372 - val_accuracy: 0.8724 - val_jacard_coef: 0.0468 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1733 - accuracy: 0.8247 - jacard_coef: 0.04862/5 [===========>..................] - ETA: 2s - loss: 0.1763 - accuracy: 0.6494 - jacard_coef: 0.07343/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.7146 - jacard_coef: 0.06034/5 [=======================>......] - ETA: 0s - loss: 0.1722 - accuracy: 0.7473 - jacard_coef: 0.05955/5 [==============================] - 3s 557ms/step - loss: 0.1721 - accuracy: 0.7476 - jacard_coef: 0.0483 - val_loss: 0.3185 - val_accuracy: 0.9226 - val_jacard_coef: 0.0063 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1683 - accuracy: 0.6996 - jacard_coef: 0.05462/5 [===========>..................] - ETA: 2s - loss: 0.1678 - accuracy: 0.7852 - jacard_coef: 0.03793/5 [=================>............] - ETA: 1s - loss: 0.1674 - accuracy: 0.8188 - jacard_coef: 0.03584/5 [=======================>......] - ETA: 0s - loss: 0.1674 - accuracy: 0.8307 - jacard_coef: 0.03605/5 [==============================] - 3s 558ms/step - loss: 0.1677 - accuracy: 0.8261 - jacard_coef: 0.0366 - val_loss: 0.2064 - val_accuracy: 0.8104 - val_jacard_coef: 0.0421 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.8316 - jacard_coef: 0.03922/5 [===========>..................] - ETA: 2s - loss: 0.1668 - accuracy: 0.8197 - jacard_coef: 0.04953/5 [=================>............] - ETA: 1s - loss: 0.1670 - accuracy: 0.7986 - jacard_coef: 0.05524/5 [=======================>......] - ETA: 0s - loss: 0.1670 - accuracy: 0.7979 - jacard_coef: 0.05975/5 [==============================] - 3s 558ms/step - loss: 0.1674 - accuracy: 0.7945 - jacard_coef: 0.0477 - val_loss: 1.0291 - val_accuracy: 0.9255 - val_jacard_coef: 0.0013 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1665 - accuracy: 0.8509 - jacard_coef: 0.04572/5 [===========>..................] - ETA: 2s - loss: 0.1664 - accuracy: 0.8613 - jacard_coef: 0.04423/5 [=================>............] - ETA: 1s - loss: 0.1665 - accuracy: 0.8595 - jacard_coef: 0.04054/5 [=======================>......] - ETA: 0s - loss: 0.1662 - accuracy: 0.8655 - jacard_coef: 0.03805/5 [==============================] - 3s 558ms/step - loss: 0.1663 - accuracy: 0.8644 - jacard_coef: 0.0416 - val_loss: 1.0956 - val_accuracy: 0.9302 - val_jacard_coef: 2.4757e-04 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1651 - accuracy: 0.8773 - jacard_coef: 0.04202/5 [===========>..................] - ETA: 2s - loss: 0.1655 - accuracy: 0.8692 - jacard_coef: 0.02883/5 [=================>............] - ETA: 1s - loss: 0.1655 - accuracy: 0.8777 - jacard_coef: 0.02564/5 [=======================>......] - ETA: 0s - loss: 0.1652 - accuracy: 0.8897 - jacard_coef: 0.02295/5 [==============================] - 3s 558ms/step - loss: 0.1652 - accuracy: 0.8901 - jacard_coef: 0.0241 - val_loss: 1.1079 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1645 - accuracy: 0.9029 - jacard_coef: 0.01042/5 [===========>..................] - ETA: 2s - loss: 0.1645 - accuracy: 0.8986 - jacard_coef: 0.01003/5 [=================>............] - ETA: 1s - loss: 0.1642 - accuracy: 0.9070 - jacard_coef: 0.01264/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.9054 - jacard_coef: 0.01245/5 [==============================] - 3s 558ms/step - loss: 0.1641 - accuracy: 0.9059 - jacard_coef: 0.0134 - val_loss: 1.0928 - val_accuracy: 0.9304 - val_jacard_coef: 1.4609e-12 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1634 - accuracy: 0.9003 - jacard_coef: 0.01372/5 [===========>..................] - ETA: 2s - loss: 0.1635 - accuracy: 0.8897 - jacard_coef: 0.01653/5 [=================>............] - ETA: 1s - loss: 0.1633 - accuracy: 0.8894 - jacard_coef: 0.01824/5 [=======================>......] - ETA: 0s - loss: 0.1630 - accuracy: 0.8923 - jacard_coef: 0.02285/5 [==============================] - 3s 557ms/step - loss: 0.1630 - accuracy: 0.8918 - jacard_coef: 0.0271 - val_loss: 0.9834 - val_accuracy: 0.9303 - val_jacard_coef: 1.4587e-12 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1621 - accuracy: 0.8858 - jacard_coef: 0.03472/5 [===========>..................] - ETA: 2s - loss: 0.1623 - accuracy: 0.8724 - jacard_coef: 0.03443/5 [=================>............] - ETA: 1s - loss: 0.1621 - accuracy: 0.8756 - jacard_coef: 0.03364/5 [=======================>......] - ETA: 0s - loss: 0.1621 - accuracy: 0.8739 - jacard_coef: 0.03315/5 [==============================] - 3s 558ms/step - loss: 0.1621 - accuracy: 0.8743 - jacard_coef: 0.0341 - val_loss: 0.6692 - val_accuracy: 0.9301 - val_jacard_coef: 1.4563e-12 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1617 - accuracy: 0.8739 - jacard_coef: 0.02982/5 [===========>..................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8792 - jacard_coef: 0.02533/5 [=================>............] - ETA: 1s - loss: 0.1612 - accuracy: 0.8888 - jacard_coef: 0.02714/5 [=======================>......] - ETA: 0s - loss: 0.1612 - accuracy: 0.8877 - jacard_coef: 0.02545/5 [==============================] - 3s 558ms/step - loss: 0.1613 - accuracy: 0.8872 - jacard_coef: 0.0203 - val_loss: 0.5243 - val_accuracy: 0.9301 - val_jacard_coef: 1.4559e-12 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1612 - accuracy: 0.8911 - jacard_coef: 0.00292/5 [===========>..................] - ETA: 2s - loss: 0.1608 - accuracy: 0.9048 - jacard_coef: 0.00423/5 [=================>............] - ETA: 1s - loss: 0.1607 - accuracy: 0.9082 - jacard_coef: 0.00434/5 [=======================>......] - ETA: 0s - loss: 0.1605 - accuracy: 0.9120 - jacard_coef: 0.00415/5 [==============================] - 3s 558ms/step - loss: 0.1605 - accuracy: 0.9119 - jacard_coef: 0.0051 - val_loss: 0.3181 - val_accuracy: 0.9299 - val_jacard_coef: 1.4514e-12 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

✓ Training completed successfully!
  Best Val Jaccard: 0.0468 (epoch 6)
  Final Val Loss: 0.3181
  Training Time: 0:02:09.985396
  Stability (std): 0.3494

Results saved to: hyperparameter_optimization_20250926_165036/exp_15_Attention_UNet_lr1e-4_bs32/Attention_UNet_lr0.0001_bs32_results.json
