âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758877219.674443 1043399 service.cc:145] XLA service 0x147831b452e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758877219.674468 1043399 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758877219.813095 1043399 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 2:59 - loss: 0.3430 - accuracy: 0.4908 - jacard_coef: 0.07712/5 [===========>..................] - ETA: 42s - loss: 0.3131 - accuracy: 0.4808 - jacard_coef: 0.0894 3/5 [=================>............] - ETA: 17s - loss: 0.2857 - accuracy: 0.4312 - jacard_coef: 0.08354/5 [=======================>......] - ETA: 6s - loss: 0.2656 - accuracy: 0.3728 - jacard_coef: 0.0821 5/5 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.3713 - jacard_coef: 0.07895/5 [==============================] - 71s 6s/step - loss: 0.2650 - accuracy: 0.3713 - jacard_coef: 0.0789 - val_loss: 0.1367 - val_accuracy: 0.9209 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1952 - accuracy: 0.1350 - jacard_coef: 0.07542/5 [===========>..................] - ETA: 1s - loss: 0.1939 - accuracy: 0.1409 - jacard_coef: 0.07963/5 [=================>............] - ETA: 0s - loss: 0.1933 - accuracy: 0.1449 - jacard_coef: 0.08054/5 [=======================>......] - ETA: 0s - loss: 0.1928 - accuracy: 0.1473 - jacard_coef: 0.08315/5 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.1471 - jacard_coef: 0.07535/5 [==============================] - 2s 422ms/step - loss: 0.1928 - accuracy: 0.1471 - jacard_coef: 0.0753 - val_loss: 0.1599 - val_accuracy: 0.7479 - val_jacard_coef: 0.0484 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1902 - accuracy: 0.1882 - jacard_coef: 0.08512/5 [===========>..................] - ETA: 1s - loss: 0.1903 - accuracy: 0.2139 - jacard_coef: 0.07873/5 [=================>............] - ETA: 0s - loss: 0.1895 - accuracy: 0.2239 - jacard_coef: 0.07984/5 [=======================>......] - ETA: 0s - loss: 0.1895 - accuracy: 0.2145 - jacard_coef: 0.08165/5 [==============================] - 2s 387ms/step - loss: 0.1895 - accuracy: 0.2145 - jacard_coef: 0.1005 - val_loss: 0.2496 - val_accuracy: 0.9226 - val_jacard_coef: 0.0072 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1861 - accuracy: 0.1649 - jacard_coef: 0.07982/5 [===========>..................] - ETA: 1s - loss: 0.1874 - accuracy: 0.1766 - jacard_coef: 0.07593/5 [=================>............] - ETA: 0s - loss: 0.1866 - accuracy: 0.1903 - jacard_coef: 0.07854/5 [=======================>......] - ETA: 0s - loss: 0.1865 - accuracy: 0.1989 - jacard_coef: 0.08255/5 [==============================] - 2s 387ms/step - loss: 0.1864 - accuracy: 0.1996 - jacard_coef: 0.0961 - val_loss: 0.3658 - val_accuracy: 0.9257 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1846 - accuracy: 0.1868 - jacard_coef: 0.06142/5 [===========>..................] - ETA: 1s - loss: 0.1847 - accuracy: 0.1911 - jacard_coef: 0.07393/5 [=================>............] - ETA: 0s - loss: 0.1847 - accuracy: 0.1889 - jacard_coef: 0.07764/5 [=======================>......] - ETA: 0s - loss: 0.1843 - accuracy: 0.1872 - jacard_coef: 0.08325/5 [==============================] - 2s 386ms/step - loss: 0.1843 - accuracy: 0.1864 - jacard_coef: 0.0683 - val_loss: 0.2325 - val_accuracy: 0.9192 - val_jacard_coef: 0.0071 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1815 - accuracy: 0.1457 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 1s - loss: 0.1815 - accuracy: 0.1541 - jacard_coef: 0.08713/5 [=================>............] - ETA: 0s - loss: 0.1812 - accuracy: 0.1653 - jacard_coef: 0.08454/5 [=======================>......] - ETA: 0s - loss: 0.1811 - accuracy: 0.1864 - jacard_coef: 0.08235/5 [==============================] - 2s 387ms/step - loss: 0.1811 - accuracy: 0.1878 - jacard_coef: 0.1014 - val_loss: 0.1189 - val_accuracy: 0.9075 - val_jacard_coef: 0.0237 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1791 - accuracy: 0.3496 - jacard_coef: 0.05842/5 [===========>..................] - ETA: 1s - loss: 0.1781 - accuracy: 0.3812 - jacard_coef: 0.07283/5 [=================>............] - ETA: 0s - loss: 0.1778 - accuracy: 0.4139 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1772 - accuracy: 0.4394 - jacard_coef: 0.07785/5 [==============================] - 2s 387ms/step - loss: 0.1772 - accuracy: 0.4399 - jacard_coef: 0.0763 - val_loss: 0.1136 - val_accuracy: 0.9125 - val_jacard_coef: 0.0217 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1745 - accuracy: 0.5919 - jacard_coef: 0.08572/5 [===========>..................] - ETA: 1s - loss: 0.1738 - accuracy: 0.6026 - jacard_coef: 0.08723/5 [=================>............] - ETA: 0s - loss: 0.1736 - accuracy: 0.6081 - jacard_coef: 0.07924/5 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.6164 - jacard_coef: 0.07985/5 [==============================] - 2s 398ms/step - loss: 0.1732 - accuracy: 0.6170 - jacard_coef: 0.0642 - val_loss: 0.1951 - val_accuracy: 0.2330 - val_jacard_coef: 0.0718 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1757 - accuracy: 0.3762 - jacard_coef: 0.08142/5 [===========>..................] - ETA: 1s - loss: 0.1734 - accuracy: 0.5269 - jacard_coef: 0.07613/5 [=================>............] - ETA: 0s - loss: 0.1730 - accuracy: 0.5848 - jacard_coef: 0.07424/5 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.6164 - jacard_coef: 0.07745/5 [==============================] - 2s 388ms/step - loss: 0.1729 - accuracy: 0.6167 - jacard_coef: 0.0853 - val_loss: 0.2073 - val_accuracy: 0.3072 - val_jacard_coef: 0.0711 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1735 - accuracy: 0.6480 - jacard_coef: 0.07522/5 [===========>..................] - ETA: 1s - loss: 0.1728 - accuracy: 0.6366 - jacard_coef: 0.07233/5 [=================>............] - ETA: 0s - loss: 0.1726 - accuracy: 0.6125 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1728 - accuracy: 0.5942 - jacard_coef: 0.07985/5 [==============================] - 2s 387ms/step - loss: 0.1729 - accuracy: 0.5935 - jacard_coef: 0.0868 - val_loss: 0.1512 - val_accuracy: 0.9066 - val_jacard_coef: 0.0328 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1727 - accuracy: 0.5504 - jacard_coef: 0.06632/5 [===========>..................] - ETA: 1s - loss: 0.1723 - accuracy: 0.5819 - jacard_coef: 0.07643/5 [=================>............] - ETA: 0s - loss: 0.1724 - accuracy: 0.6234 - jacard_coef: 0.07554/5 [=======================>......] - ETA: 0s - loss: 0.1720 - accuracy: 0.6517 - jacard_coef: 0.07725/5 [==============================] - 2s 387ms/step - loss: 0.1720 - accuracy: 0.6523 - jacard_coef: 0.0757 - val_loss: 0.1872 - val_accuracy: 0.6985 - val_jacard_coef: 0.0584 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1711 - accuracy: 0.7200 - jacard_coef: 0.08212/5 [===========>..................] - ETA: 1s - loss: 0.1708 - accuracy: 0.7271 - jacard_coef: 0.07803/5 [=================>............] - ETA: 0s - loss: 0.1706 - accuracy: 0.7180 - jacard_coef: 0.07984/5 [=======================>......] - ETA: 0s - loss: 0.1703 - accuracy: 0.7107 - jacard_coef: 0.07835/5 [==============================] - 2s 388ms/step - loss: 0.1703 - accuracy: 0.7097 - jacard_coef: 0.0631 - val_loss: 0.1601 - val_accuracy: 0.8918 - val_jacard_coef: 0.0244 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1688 - accuracy: 0.6768 - jacard_coef: 0.08692/5 [===========>..................] - ETA: 1s - loss: 0.1685 - accuracy: 0.6968 - jacard_coef: 0.07873/5 [=================>............] - ETA: 0s - loss: 0.1680 - accuracy: 0.7072 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 0s - loss: 0.1678 - accuracy: 0.7158 - jacard_coef: 0.07275/5 [==============================] - 2s 387ms/step - loss: 0.1680 - accuracy: 0.7130 - jacard_coef: 0.0653 - val_loss: 0.1723 - val_accuracy: 0.8183 - val_jacard_coef: 0.0510 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1661 - accuracy: 0.8298 - jacard_coef: 0.06082/5 [===========>..................] - ETA: 1s - loss: 0.1680 - accuracy: 0.7781 - jacard_coef: 0.07233/5 [=================>............] - ETA: 0s - loss: 0.1682 - accuracy: 0.7665 - jacard_coef: 0.07014/5 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.7364 - jacard_coef: 0.07305/5 [==============================] - 2s 387ms/step - loss: 0.1686 - accuracy: 0.7326 - jacard_coef: 0.0601 - val_loss: 0.1557 - val_accuracy: 0.9096 - val_jacard_coef: 0.0092 - lr: 2.5000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1683 - accuracy: 0.8292 - jacard_coef: 0.05532/5 [===========>..................] - ETA: 1s - loss: 0.1677 - accuracy: 0.8234 - jacard_coef: 0.05483/5 [=================>............] - ETA: 0s - loss: 0.1680 - accuracy: 0.8224 - jacard_coef: 0.06464/5 [=======================>......] - ETA: 0s - loss: 0.1678 - accuracy: 0.8321 - jacard_coef: 0.06105/5 [==============================] - 2s 388ms/step - loss: 0.1678 - accuracy: 0.8291 - jacard_coef: 0.0764 - val_loss: 0.1662 - val_accuracy: 0.8643 - val_jacard_coef: 0.0345 - lr: 2.5000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1659 - accuracy: 0.8817 - jacard_coef: 0.04322/5 [===========>..................] - ETA: 1s - loss: 0.1662 - accuracy: 0.8648 - jacard_coef: 0.04993/5 [=================>............] - ETA: 0s - loss: 0.1660 - accuracy: 0.8534 - jacard_coef: 0.04934/5 [=======================>......] - ETA: 0s - loss: 0.1660 - accuracy: 0.8387 - jacard_coef: 0.05215/5 [==============================] - 2s 396ms/step - loss: 0.1660 - accuracy: 0.8384 - jacard_coef: 0.0473 - val_loss: 0.1724 - val_accuracy: 0.5695 - val_jacard_coef: 0.0761 - lr: 2.5000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1655 - accuracy: 0.7809 - jacard_coef: 0.05602/5 [===========>..................] - ETA: 1s - loss: 0.1650 - accuracy: 0.7912 - jacard_coef: 0.06523/5 [=================>............] - ETA: 0s - loss: 0.1649 - accuracy: 0.7972 - jacard_coef: 0.06214/5 [=======================>......] - ETA: 0s - loss: 0.1651 - accuracy: 0.8010 - jacard_coef: 0.06145/5 [==============================] - 2s 387ms/step - loss: 0.1655 - accuracy: 0.7978 - jacard_coef: 0.0606 - val_loss: 0.1697 - val_accuracy: 0.8171 - val_jacard_coef: 0.0495 - lr: 2.5000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1648 - accuracy: 0.8468 - jacard_coef: 0.07102/5 [===========>..................] - ETA: 1s - loss: 0.1657 - accuracy: 0.8331 - jacard_coef: 0.06203/5 [=================>............] - ETA: 0s - loss: 0.1656 - accuracy: 0.8245 - jacard_coef: 0.06004/5 [=======================>......] - ETA: 0s - loss: 0.1656 - accuracy: 0.8109 - jacard_coef: 0.06655/5 [==============================] - 2s 387ms/step - loss: 0.1656 - accuracy: 0.8096 - jacard_coef: 0.0781 - val_loss: 0.1613 - val_accuracy: 0.9286 - val_jacard_coef: 1.4255e-12 - lr: 2.5000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1659 - accuracy: 0.7103 - jacard_coef: 0.08172/5 [===========>..................] - ETA: 1s - loss: 0.1662 - accuracy: 0.6968 - jacard_coef: 0.08153/5 [=================>............] - ETA: 0s - loss: 0.1658 - accuracy: 0.6958 - jacard_coef: 0.08224/5 [=======================>......] - ETA: 0s - loss: 0.1656 - accuracy: 0.6979 - jacard_coef: 0.07865/5 [==============================] - 2s 388ms/step - loss: 0.1656 - accuracy: 0.6975 - jacard_coef: 0.0636 - val_loss: 0.1578 - val_accuracy: 0.9293 - val_jacard_coef: 1.4386e-12 - lr: 2.5000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1658 - accuracy: 0.6900 - jacard_coef: 0.08022/5 [===========>..................] - ETA: 1s - loss: 0.1651 - accuracy: 0.6962 - jacard_coef: 0.07243/5 [=================>............] - ETA: 0s - loss: 0.1649 - accuracy: 0.7050 - jacard_coef: 0.07484/5 [=======================>......] - ETA: 0s - loss: 0.1649 - accuracy: 0.7127 - jacard_coef: 0.07565/5 [==============================] - 2s 387ms/step - loss: 0.1650 - accuracy: 0.7102 - jacard_coef: 0.0707 - val_loss: 0.1600 - val_accuracy: 0.9275 - val_jacard_coef: 0.0030 - lr: 2.5000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1641 - accuracy: 0.8268 - jacard_coef: 0.05712/5 [===========>..................] - ETA: 1s - loss: 0.1642 - accuracy: 0.8386 - jacard_coef: 0.05693/5 [=================>............] - ETA: 0s - loss: 0.1643 - accuracy: 0.8382 - jacard_coef: 0.05714/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.8431 - jacard_coef: 0.05835/5 [==============================] - 2s 388ms/step - loss: 0.1643 - accuracy: 0.8404 - jacard_coef: 0.0812 - val_loss: 0.1634 - val_accuracy: 0.9252 - val_jacard_coef: 0.0030 - lr: 2.5000e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1643 - accuracy: 0.8311 - jacard_coef: 0.05792/5 [===========>..................] - ETA: 1s - loss: 0.1642 - accuracy: 0.8334 - jacard_coef: 0.05743/5 [=================>............] - ETA: 0s - loss: 0.1641 - accuracy: 0.8309 - jacard_coef: 0.05784/5 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.8288 - jacard_coef: 0.06335/5 [==============================] - 2s 387ms/step - loss: 0.1640 - accuracy: 0.8285 - jacard_coef: 0.0554 - val_loss: 0.1633 - val_accuracy: 0.9269 - val_jacard_coef: 0.0023 - lr: 1.2500e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1635 - accuracy: 0.8371 - jacard_coef: 0.07602/5 [===========>..................] - ETA: 1s - loss: 0.1637 - accuracy: 0.8234 - jacard_coef: 0.06963/5 [=================>............] - ETA: 0s - loss: 0.1637 - accuracy: 0.8279 - jacard_coef: 0.06264/5 [=======================>......] - ETA: 0s - loss: 0.1637 - accuracy: 0.8289 - jacard_coef: 0.06145/5 [==============================] - 2s 388ms/step - loss: 0.1637 - accuracy: 0.8293 - jacard_coef: 0.0496 - val_loss: 0.1628 - val_accuracy: 0.9277 - val_jacard_coef: 0.0010 - lr: 1.2500e-04
Epoch 24/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1633 - accuracy: 0.8484 - jacard_coef: 0.07212/5 [===========>..................] - ETA: 1s - loss: 0.1633 - accuracy: 0.8455 - jacard_coef: 0.06673/5 [=================>............] - ETA: 0s - loss: 0.1634 - accuracy: 0.8469 - jacard_coef: 0.06324/5 [=======================>......] - ETA: 0s - loss: 0.1633 - accuracy: 0.8518 - jacard_coef: 0.05875/5 [==============================] - 2s 387ms/step - loss: 0.1633 - accuracy: 0.8522 - jacard_coef: 0.0475 - val_loss: 0.1621 - val_accuracy: 0.9274 - val_jacard_coef: 0.0014 - lr: 1.2500e-04
Epoch 25/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8714 - jacard_coef: 0.06592/5 [===========>..................] - ETA: 1s - loss: 0.1631 - accuracy: 0.8719 - jacard_coef: 0.05663/5 [=================>............] - ETA: 0s - loss: 0.1626 - accuracy: 0.8795 - jacard_coef: 0.06304/5 [=======================>......] - ETA: 0s - loss: 0.1629 - accuracy: 0.8798 - jacard_coef: 0.05385/5 [==============================] - 2s 387ms/step - loss: 0.1629 - accuracy: 0.8803 - jacard_coef: 0.0440 - val_loss: 0.1620 - val_accuracy: 0.9255 - val_jacard_coef: 0.0012 - lr: 1.2500e-04
Epoch 26/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1625 - accuracy: 0.8802 - jacard_coef: 0.04332/5 [===========>..................] - ETA: 1s - loss: 0.1624 - accuracy: 0.8909 - jacard_coef: 0.05873/5 [=================>............] - ETA: 0s - loss: 0.1622 - accuracy: 0.8942 - jacard_coef: 0.04974/5 [=======================>......] - ETA: 0s - loss: 0.1625 - accuracy: 0.8880 - jacard_coef: 0.04995/5 [==============================] - 2s 388ms/step - loss: 0.1625 - accuracy: 0.8863 - jacard_coef: 0.0697 - val_loss: 0.1624 - val_accuracy: 0.9249 - val_jacard_coef: 0.0028 - lr: 1.2500e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0761 (epoch 16)
  Final Val Loss: 0.1624
  Training Time: 0:02:01.987647
  Stability (std): 0.0029

Results saved to: hyperparameter_optimization_20250926_165036/exp_6_UNet_lr5e-4_bs32/UNet_lr0.0005_bs32_results.json
