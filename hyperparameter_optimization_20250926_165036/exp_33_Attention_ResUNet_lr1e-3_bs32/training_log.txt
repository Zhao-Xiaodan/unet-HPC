âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880911.039746 1212661 service.cc:145] XLA service 0x14e5a15a38e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880911.039767 1212661 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880911.179421 1212661 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:38 - loss: 0.3403 - accuracy: 0.5118 - jacard_coef: 0.07862/5 [===========>..................] - ETA: 46s - loss: 0.2932 - accuracy: 0.4657 - jacard_coef: 0.0817 3/5 [=================>............] - ETA: 16s - loss: 0.2781 - accuracy: 0.4312 - jacard_coef: 0.07784/5 [=======================>......] - ETA: 5s - loss: 0.2612 - accuracy: 0.3907 - jacard_coef: 0.0762 5/5 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.3892 - jacard_coef: 0.07185/5 [==============================] - 96s 7s/step - loss: 0.2607 - accuracy: 0.3892 - jacard_coef: 0.0718 - val_loss: 0.1066 - val_accuracy: 0.9135 - val_jacard_coef: 0.0173 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1970 - accuracy: 0.3558 - jacard_coef: 0.08232/5 [===========>..................] - ETA: 2s - loss: 0.1941 - accuracy: 0.3637 - jacard_coef: 0.08943/5 [=================>............] - ETA: 1s - loss: 0.1921 - accuracy: 0.3409 - jacard_coef: 0.08914/5 [=======================>......] - ETA: 0s - loss: 0.1908 - accuracy: 0.3219 - jacard_coef: 0.08515/5 [==============================] - 3s 639ms/step - loss: 0.1908 - accuracy: 0.3219 - jacard_coef: 0.0741 - val_loss: 0.4288 - val_accuracy: 0.9175 - val_jacard_coef: 0.0093 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1812 - accuracy: 0.3636 - jacard_coef: 0.08442/5 [===========>..................] - ETA: 2s - loss: 0.1816 - accuracy: 0.3712 - jacard_coef: 0.07533/5 [=================>............] - ETA: 1s - loss: 0.1801 - accuracy: 0.4020 - jacard_coef: 0.08204/5 [=======================>......] - ETA: 0s - loss: 0.1787 - accuracy: 0.4242 - jacard_coef: 0.08265/5 [==============================] - 3s 640ms/step - loss: 0.1786 - accuracy: 0.4244 - jacard_coef: 0.0741 - val_loss: 1.0837 - val_accuracy: 0.8935 - val_jacard_coef: 0.0126 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1727 - accuracy: 0.4441 - jacard_coef: 0.08162/5 [===========>..................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4823 - jacard_coef: 0.08473/5 [=================>............] - ETA: 1s - loss: 0.1722 - accuracy: 0.5063 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1719 - accuracy: 0.5198 - jacard_coef: 0.07935/5 [==============================] - 3s 654ms/step - loss: 0.1719 - accuracy: 0.5202 - jacard_coef: 0.0877 - val_loss: 13.3118 - val_accuracy: 0.0737 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1704 - accuracy: 0.5248 - jacard_coef: 0.10912/5 [===========>..................] - ETA: 2s - loss: 0.1704 - accuracy: 0.5218 - jacard_coef: 0.09393/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.5329 - jacard_coef: 0.09194/5 [=======================>......] - ETA: 0s - loss: 0.1701 - accuracy: 0.5435 - jacard_coef: 0.08625/5 [==============================] - 3s 640ms/step - loss: 0.1701 - accuracy: 0.5441 - jacard_coef: 0.0690 - val_loss: 12.9488 - val_accuracy: 0.0731 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1688 - accuracy: 0.6660 - jacard_coef: 0.08452/5 [===========>..................] - ETA: 2s - loss: 0.1687 - accuracy: 0.6742 - jacard_coef: 0.09223/5 [=================>............] - ETA: 1s - loss: 0.1685 - accuracy: 0.6635 - jacard_coef: 0.09004/5 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.6495 - jacard_coef: 0.08495/5 [==============================] - 3s 641ms/step - loss: 0.1697 - accuracy: 0.6475 - jacard_coef: 0.0720 - val_loss: 1.0713 - val_accuracy: 0.9235 - val_jacard_coef: 0.0045 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1716 - accuracy: 0.4545 - jacard_coef: 0.08142/5 [===========>..................] - ETA: 2s - loss: 0.1725 - accuracy: 0.4534 - jacard_coef: 0.07953/5 [=================>............] - ETA: 1s - loss: 0.1725 - accuracy: 0.4539 - jacard_coef: 0.08644/5 [=======================>......] - ETA: 0s - loss: 0.1730 - accuracy: 0.4493 - jacard_coef: 0.08425/5 [==============================] - 3s 640ms/step - loss: 0.1731 - accuracy: 0.4491 - jacard_coef: 0.1023 - val_loss: 1.1193 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1727 - accuracy: 0.4153 - jacard_coef: 0.08672/5 [===========>..................] - ETA: 2s - loss: 0.1719 - accuracy: 0.4050 - jacard_coef: 0.08733/5 [=================>............] - ETA: 1s - loss: 0.1716 - accuracy: 0.4014 - jacard_coef: 0.08464/5 [=======================>......] - ETA: 0s - loss: 0.1715 - accuracy: 0.4190 - jacard_coef: 0.08575/5 [==============================] - 3s 639ms/step - loss: 0.1714 - accuracy: 0.4208 - jacard_coef: 0.0870 - val_loss: 1.0728 - val_accuracy: 0.9304 - val_jacard_coef: 1.4611e-12 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1707 - accuracy: 0.6752 - jacard_coef: 0.08052/5 [===========>..................] - ETA: 2s - loss: 0.1701 - accuracy: 0.6862 - jacard_coef: 0.07433/5 [=================>............] - ETA: 1s - loss: 0.1692 - accuracy: 0.6939 - jacard_coef: 0.06664/5 [=======================>......] - ETA: 0s - loss: 0.1779 - accuracy: 0.6134 - jacard_coef: 0.07015/5 [==============================] - 3s 654ms/step - loss: 0.1778 - accuracy: 0.6150 - jacard_coef: 0.0847 - val_loss: 9.9415 - val_accuracy: 0.0749 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1705 - accuracy: 0.8126 - jacard_coef: 0.04452/5 [===========>..................] - ETA: 2s - loss: 0.1723 - accuracy: 0.7927 - jacard_coef: 0.04643/5 [=================>............] - ETA: 1s - loss: 0.1734 - accuracy: 0.7873 - jacard_coef: 0.04664/5 [=======================>......] - ETA: 0s - loss: 0.1738 - accuracy: 0.7740 - jacard_coef: 0.04945/5 [==============================] - 3s 654ms/step - loss: 0.1738 - accuracy: 0.7732 - jacard_coef: 0.0563 - val_loss: 8.8215 - val_accuracy: 0.0768 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1705 - accuracy: 0.7706 - jacard_coef: 0.05332/5 [===========>..................] - ETA: 2s - loss: 0.1694 - accuracy: 0.7836 - jacard_coef: 0.04503/5 [=================>............] - ETA: 1s - loss: 0.1684 - accuracy: 0.7993 - jacard_coef: 0.04544/5 [=======================>......] - ETA: 0s - loss: 0.1677 - accuracy: 0.8059 - jacard_coef: 0.04775/5 [==============================] - 3s 639ms/step - loss: 0.1677 - accuracy: 0.8060 - jacard_coef: 0.0442 - val_loss: 0.6266 - val_accuracy: 0.9259 - val_jacard_coef: 2.3330e-04 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1645 - accuracy: 0.7920 - jacard_coef: 0.05402/5 [===========>..................] - ETA: 2s - loss: 0.1645 - accuracy: 0.7621 - jacard_coef: 0.05753/5 [=================>............] - ETA: 1s - loss: 0.1642 - accuracy: 0.7497 - jacard_coef: 0.06964/5 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.7383 - jacard_coef: 0.07365/5 [==============================] - 3s 639ms/step - loss: 0.1640 - accuracy: 0.7386 - jacard_coef: 0.1124 - val_loss: 0.5084 - val_accuracy: 0.9264 - val_jacard_coef: 1.3829e-12 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1631 - accuracy: 0.6663 - jacard_coef: 0.07852/5 [===========>..................] - ETA: 2s - loss: 0.1625 - accuracy: 0.6681 - jacard_coef: 0.08553/5 [=================>............] - ETA: 1s - loss: 0.1622 - accuracy: 0.6653 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 0s - loss: 0.1622 - accuracy: 0.6601 - jacard_coef: 0.08095/5 [==============================] - 3s 640ms/step - loss: 0.1622 - accuracy: 0.6593 - jacard_coef: 0.0772 - val_loss: 0.1664 - val_accuracy: 0.9247 - val_jacard_coef: 1.3515e-05 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1610 - accuracy: 0.6747 - jacard_coef: 0.08562/5 [===========>..................] - ETA: 2s - loss: 0.1606 - accuracy: 0.6921 - jacard_coef: 0.09373/5 [=================>............] - ETA: 1s - loss: 0.1604 - accuracy: 0.7007 - jacard_coef: 0.08404/5 [=======================>......] - ETA: 0s - loss: 0.1603 - accuracy: 0.7182 - jacard_coef: 0.07905/5 [==============================] - 3s 640ms/step - loss: 0.1603 - accuracy: 0.7183 - jacard_coef: 0.0703 - val_loss: 0.1195 - val_accuracy: 0.8874 - val_jacard_coef: 0.0157 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1595 - accuracy: 0.8783 - jacard_coef: 0.02672/5 [===========>..................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8869 - jacard_coef: 0.02283/5 [=================>............] - ETA: 1s - loss: 0.1592 - accuracy: 0.8860 - jacard_coef: 0.02334/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.8928 - jacard_coef: 0.02665/5 [==============================] - 3s 640ms/step - loss: 0.1588 - accuracy: 0.8934 - jacard_coef: 0.0212 - val_loss: 0.1528 - val_accuracy: 0.8576 - val_jacard_coef: 0.0191 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1589 - accuracy: 0.8769 - jacard_coef: 0.03082/5 [===========>..................] - ETA: 2s - loss: 0.1585 - accuracy: 0.8809 - jacard_coef: 0.03303/5 [=================>............] - ETA: 1s - loss: 0.1583 - accuracy: 0.8841 - jacard_coef: 0.02954/5 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8831 - jacard_coef: 0.03265/5 [==============================] - 3s 639ms/step - loss: 0.1583 - accuracy: 0.8826 - jacard_coef: 0.0273 - val_loss: 0.1473 - val_accuracy: 0.9102 - val_jacard_coef: 0.0063 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1584 - accuracy: 0.8856 - jacard_coef: 0.02972/5 [===========>..................] - ETA: 2s - loss: 0.1578 - accuracy: 0.9008 - jacard_coef: 0.02443/5 [=================>............] - ETA: 1s - loss: 0.1578 - accuracy: 0.9011 - jacard_coef: 0.01994/5 [=======================>......] - ETA: 0s - loss: 0.1576 - accuracy: 0.9045 - jacard_coef: 0.01655/5 [==============================] - 3s 640ms/step - loss: 0.1576 - accuracy: 0.9048 - jacard_coef: 0.0153 - val_loss: 0.1383 - val_accuracy: 0.9204 - val_jacard_coef: 0.0023 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1570 - accuracy: 0.9192 - jacard_coef: 0.00382/5 [===========>..................] - ETA: 2s - loss: 0.1570 - accuracy: 0.9183 - jacard_coef: 0.00653/5 [=================>............] - ETA: 1s - loss: 0.1570 - accuracy: 0.9141 - jacard_coef: 0.00674/5 [=======================>......] - ETA: 0s - loss: 0.1570 - accuracy: 0.9130 - jacard_coef: 0.00605/5 [==============================] - 3s 640ms/step - loss: 0.1570 - accuracy: 0.9134 - jacard_coef: 0.0048 - val_loss: 0.1362 - val_accuracy: 0.9272 - val_jacard_coef: 0.0038 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1568 - accuracy: 0.9075 - jacard_coef: 0.00702/5 [===========>..................] - ETA: 2s - loss: 0.1565 - accuracy: 0.9130 - jacard_coef: 0.00613/5 [=================>............] - ETA: 1s - loss: 0.1566 - accuracy: 0.9113 - jacard_coef: 0.00554/5 [=======================>......] - ETA: 0s - loss: 0.1564 - accuracy: 0.9146 - jacard_coef: 0.00465/5 [==============================] - 3s 640ms/step - loss: 0.1564 - accuracy: 0.9148 - jacard_coef: 0.0039 - val_loss: 0.1399 - val_accuracy: 0.9282 - val_jacard_coef: 0.0034 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1566 - accuracy: 0.8955 - jacard_coef: 0.00652/5 [===========>..................] - ETA: 2s - loss: 0.1559 - accuracy: 0.9097 - jacard_coef: 0.00733/5 [=================>............] - ETA: 1s - loss: 0.1558 - accuracy: 0.9135 - jacard_coef: 0.00874/5 [=======================>......] - ETA: 0s - loss: 0.1558 - accuracy: 0.9112 - jacard_coef: 0.00935/5 [==============================] - 3s 640ms/step - loss: 0.1558 - accuracy: 0.9105 - jacard_coef: 0.0183 - val_loss: 0.1440 - val_accuracy: 0.9263 - val_jacard_coef: 0.0015 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0698 (epoch 10)
  Final Val Loss: 0.1440
  Training Time: 0:02:40.490381
  Stability (std): 0.1722

Results saved to: hyperparameter_optimization_20250926_165036/exp_33_Attention_ResUNet_lr1e-3_bs32/Attention_ResUNet_lr0.001_bs32_results.json
