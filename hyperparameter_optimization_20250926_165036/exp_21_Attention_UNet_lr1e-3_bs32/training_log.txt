✓ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
✓ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879096.360586 1123403 service.cc:145] XLA service 0x14c959becc20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879096.360609 1123403 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879096.499540 1123403 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:01 - loss: 0.3392 - accuracy: 0.5464 - jacard_coef: 0.06952/5 [===========>..................] - ETA: 46s - loss: 0.2834 - accuracy: 0.5771 - jacard_coef: 0.0732 3/5 [=================>............] - ETA: 16s - loss: 0.2606 - accuracy: 0.5898 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 5s - loss: 0.2485 - accuracy: 0.6316 - jacard_coef: 0.0727 5/5 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.6328 - jacard_coef: 0.06265/5 [==============================] - 86s 6s/step - loss: 0.2482 - accuracy: 0.6328 - jacard_coef: 0.0626 - val_loss: 0.1687 - val_accuracy: 0.9159 - val_jacard_coef: 0.0163 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1877 - accuracy: 0.7205 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 2s - loss: 0.1876 - accuracy: 0.7323 - jacard_coef: 0.07013/5 [=================>............] - ETA: 1s - loss: 0.1841 - accuracy: 0.7193 - jacard_coef: 0.07304/5 [=======================>......] - ETA: 0s - loss: 0.1830 - accuracy: 0.7059 - jacard_coef: 0.07515/5 [==============================] - 3s 571ms/step - loss: 0.1837 - accuracy: 0.7052 - jacard_coef: 0.0702 - val_loss: 0.2438 - val_accuracy: 0.0845 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1795 - accuracy: 0.5219 - jacard_coef: 0.07322/5 [===========>..................] - ETA: 2s - loss: 0.1797 - accuracy: 0.5666 - jacard_coef: 0.06943/5 [=================>............] - ETA: 1s - loss: 0.1805 - accuracy: 0.6213 - jacard_coef: 0.06984/5 [=======================>......] - ETA: 0s - loss: 0.1798 - accuracy: 0.6316 - jacard_coef: 0.07275/5 [==============================] - 3s 558ms/step - loss: 0.1798 - accuracy: 0.6312 - jacard_coef: 0.0643 - val_loss: 0.0943 - val_accuracy: 0.8913 - val_jacard_coef: 0.0135 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1713 - accuracy: 0.6562 - jacard_coef: 0.07952/5 [===========>..................] - ETA: 2s - loss: 0.1708 - accuracy: 0.6928 - jacard_coef: 0.06163/5 [=================>............] - ETA: 1s - loss: 0.1705 - accuracy: 0.7012 - jacard_coef: 0.06754/5 [=======================>......] - ETA: 0s - loss: 0.1704 - accuracy: 0.7052 - jacard_coef: 0.06715/5 [==============================] - 3s 558ms/step - loss: 0.1704 - accuracy: 0.7052 - jacard_coef: 0.0841 - val_loss: 4.7568 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1687 - accuracy: 0.7246 - jacard_coef: 0.06892/5 [===========>..................] - ETA: 2s - loss: 0.1684 - accuracy: 0.7353 - jacard_coef: 0.07713/5 [=================>............] - ETA: 1s - loss: 0.1682 - accuracy: 0.7407 - jacard_coef: 0.07094/5 [=======================>......] - ETA: 0s - loss: 0.1681 - accuracy: 0.7457 - jacard_coef: 0.07475/5 [==============================] - 3s 558ms/step - loss: 0.1681 - accuracy: 0.7466 - jacard_coef: 0.0598 - val_loss: 0.4211 - val_accuracy: 0.9061 - val_jacard_coef: 0.0144 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1670 - accuracy: 0.8538 - jacard_coef: 0.04852/5 [===========>..................] - ETA: 2s - loss: 0.1670 - accuracy: 0.8569 - jacard_coef: 0.04723/5 [=================>............] - ETA: 1s - loss: 0.1669 - accuracy: 0.8603 - jacard_coef: 0.04334/5 [=======================>......] - ETA: 0s - loss: 0.1667 - accuracy: 0.8685 - jacard_coef: 0.04265/5 [==============================] - 3s 570ms/step - loss: 0.1667 - accuracy: 0.8685 - jacard_coef: 0.0497 - val_loss: 2.4437 - val_accuracy: 0.0740 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1655 - accuracy: 0.8603 - jacard_coef: 0.06902/5 [===========>..................] - ETA: 2s - loss: 0.1659 - accuracy: 0.8244 - jacard_coef: 0.05293/5 [=================>............] - ETA: 1s - loss: 0.1657 - accuracy: 0.8143 - jacard_coef: 0.05514/5 [=======================>......] - ETA: 0s - loss: 0.1655 - accuracy: 0.8116 - jacard_coef: 0.05935/5 [==============================] - 3s 558ms/step - loss: 0.1655 - accuracy: 0.8119 - jacard_coef: 0.1084 - val_loss: 13.4941 - val_accuracy: 0.0696 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1645 - accuracy: 0.7821 - jacard_coef: 0.07122/5 [===========>..................] - ETA: 2s - loss: 0.1648 - accuracy: 0.7724 - jacard_coef: 0.06393/5 [=================>............] - ETA: 1s - loss: 0.1647 - accuracy: 0.7722 - jacard_coef: 0.06864/5 [=======================>......] - ETA: 0s - loss: 0.1647 - accuracy: 0.7712 - jacard_coef: 0.06805/5 [==============================] - 3s 558ms/step - loss: 0.1647 - accuracy: 0.7711 - jacard_coef: 0.0544 - val_loss: 3.6969 - val_accuracy: 0.0697 - val_jacard_coef: 0.0696 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1640 - accuracy: 0.7825 - jacard_coef: 0.06462/5 [===========>..................] - ETA: 2s - loss: 0.1640 - accuracy: 0.7850 - jacard_coef: 0.06823/5 [=================>............] - ETA: 1s - loss: 0.1640 - accuracy: 0.7858 - jacard_coef: 0.07414/5 [=======================>......] - ETA: 0s - loss: 0.1638 - accuracy: 0.7873 - jacard_coef: 0.07485/5 [==============================] - 3s 558ms/step - loss: 0.1638 - accuracy: 0.7871 - jacard_coef: 0.0779 - val_loss: 0.4655 - val_accuracy: 0.9155 - val_jacard_coef: 0.0069 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1632 - accuracy: 0.7824 - jacard_coef: 0.08862/5 [===========>..................] - ETA: 2s - loss: 0.1633 - accuracy: 0.7769 - jacard_coef: 0.06843/5 [=================>............] - ETA: 1s - loss: 0.1632 - accuracy: 0.7830 - jacard_coef: 0.08214/5 [=======================>......] - ETA: 0s - loss: 0.1631 - accuracy: 0.7880 - jacard_coef: 0.08345/5 [==============================] - 3s 558ms/step - loss: 0.1631 - accuracy: 0.7877 - jacard_coef: 0.1092 - val_loss: 0.4930 - val_accuracy: 0.9196 - val_jacard_coef: 0.0060 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1623 - accuracy: 0.7956 - jacard_coef: 0.08912/5 [===========>..................] - ETA: 2s - loss: 0.1625 - accuracy: 0.7811 - jacard_coef: 0.09363/5 [=================>............] - ETA: 1s - loss: 0.1627 - accuracy: 0.7674 - jacard_coef: 0.08204/5 [=======================>......] - ETA: 0s - loss: 0.1626 - accuracy: 0.7660 - jacard_coef: 0.08145/5 [==============================] - 3s 558ms/step - loss: 0.1626 - accuracy: 0.7659 - jacard_coef: 0.0667 - val_loss: 0.2933 - val_accuracy: 0.9191 - val_jacard_coef: 0.0064 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8019 - jacard_coef: 0.07382/5 [===========>..................] - ETA: 2s - loss: 0.1619 - accuracy: 0.8075 - jacard_coef: 0.06593/5 [=================>............] - ETA: 1s - loss: 0.1620 - accuracy: 0.8207 - jacard_coef: 0.07564/5 [=======================>......] - ETA: 0s - loss: 0.1619 - accuracy: 0.8364 - jacard_coef: 0.06995/5 [==============================] - 3s 559ms/step - loss: 0.1618 - accuracy: 0.8369 - jacard_coef: 0.0560 - val_loss: 0.1584 - val_accuracy: 0.7727 - val_jacard_coef: 0.0673 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1616 - accuracy: 0.8879 - jacard_coef: 0.03572/5 [===========>..................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8873 - jacard_coef: 0.03373/5 [=================>............] - ETA: 1s - loss: 0.1614 - accuracy: 0.8918 - jacard_coef: 0.03504/5 [=======================>......] - ETA: 0s - loss: 0.1613 - accuracy: 0.8930 - jacard_coef: 0.03355/5 [==============================] - 3s 570ms/step - loss: 0.1613 - accuracy: 0.8935 - jacard_coef: 0.0268 - val_loss: 0.2766 - val_accuracy: 0.1164 - val_jacard_coef: 0.0708 - lr: 2.5000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.9002 - jacard_coef: 0.03162/5 [===========>..................] - ETA: 2s - loss: 0.1609 - accuracy: 0.8997 - jacard_coef: 0.04023/5 [=================>............] - ETA: 1s - loss: 0.1611 - accuracy: 0.8917 - jacard_coef: 0.03644/5 [=======================>......] - ETA: 0s - loss: 0.1610 - accuracy: 0.8933 - jacard_coef: 0.03785/5 [==============================] - 3s 559ms/step - loss: 0.1610 - accuracy: 0.8931 - jacard_coef: 0.0380 - val_loss: 0.1569 - val_accuracy: 0.7770 - val_jacard_coef: 0.0656 - lr: 2.5000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1604 - accuracy: 0.9083 - jacard_coef: 0.02512/5 [===========>..................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8920 - jacard_coef: 0.04353/5 [=================>............] - ETA: 1s - loss: 0.1607 - accuracy: 0.8933 - jacard_coef: 0.04014/5 [=======================>......] - ETA: 0s - loss: 0.1607 - accuracy: 0.8932 - jacard_coef: 0.04845/5 [==============================] - 3s 557ms/step - loss: 0.1607 - accuracy: 0.8931 - jacard_coef: 0.0429 - val_loss: 0.1119 - val_accuracy: 0.8751 - val_jacard_coef: 0.0409 - lr: 2.5000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8917 - jacard_coef: 0.06502/5 [===========>..................] - ETA: 2s - loss: 0.1605 - accuracy: 0.8845 - jacard_coef: 0.05873/5 [=================>............] - ETA: 1s - loss: 0.1604 - accuracy: 0.8854 - jacard_coef: 0.05784/5 [=======================>......] - ETA: 0s - loss: 0.1604 - accuracy: 0.8843 - jacard_coef: 0.06345/5 [==============================] - 3s 557ms/step - loss: 0.1604 - accuracy: 0.8847 - jacard_coef: 0.0508 - val_loss: 0.1025 - val_accuracy: 0.8973 - val_jacard_coef: 0.0273 - lr: 2.5000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1604 - accuracy: 0.8805 - jacard_coef: 0.03822/5 [===========>..................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8789 - jacard_coef: 0.04283/5 [=================>............] - ETA: 1s - loss: 0.1601 - accuracy: 0.8818 - jacard_coef: 0.05384/5 [=======================>......] - ETA: 0s - loss: 0.1601 - accuracy: 0.8814 - jacard_coef: 0.06515/5 [==============================] - 3s 558ms/step - loss: 0.1601 - accuracy: 0.8819 - jacard_coef: 0.0702 - val_loss: 0.1196 - val_accuracy: 0.8899 - val_jacard_coef: 0.0270 - lr: 2.5000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1600 - accuracy: 0.8787 - jacard_coef: 0.07822/5 [===========>..................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8696 - jacard_coef: 0.05263/5 [=================>............] - ETA: 1s - loss: 0.1601 - accuracy: 0.8754 - jacard_coef: 0.06334/5 [=======================>......] - ETA: 0s - loss: 0.1598 - accuracy: 0.8833 - jacard_coef: 0.06605/5 [==============================] - 3s 559ms/step - loss: 0.1598 - accuracy: 0.8837 - jacard_coef: 0.0567 - val_loss: 0.1246 - val_accuracy: 0.8949 - val_jacard_coef: 0.0227 - lr: 2.5000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1593 - accuracy: 0.8866 - jacard_coef: 0.03772/5 [===========>..................] - ETA: 2s - loss: 0.1595 - accuracy: 0.8806 - jacard_coef: 0.05563/5 [=================>............] - ETA: 1s - loss: 0.1595 - accuracy: 0.8788 - jacard_coef: 0.06004/5 [=======================>......] - ETA: 0s - loss: 0.1595 - accuracy: 0.8809 - jacard_coef: 0.06505/5 [==============================] - 3s 557ms/step - loss: 0.1595 - accuracy: 0.8806 - jacard_coef: 0.0539 - val_loss: 0.1288 - val_accuracy: 0.8979 - val_jacard_coef: 0.0230 - lr: 1.2500e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8700 - jacard_coef: 0.06272/5 [===========>..................] - ETA: 2s - loss: 0.1596 - accuracy: 0.8721 - jacard_coef: 0.06353/5 [=================>............] - ETA: 1s - loss: 0.1595 - accuracy: 0.8705 - jacard_coef: 0.06914/5 [=======================>......] - ETA: 0s - loss: 0.1593 - accuracy: 0.8739 - jacard_coef: 0.06895/5 [==============================] - 3s 558ms/step - loss: 0.1593 - accuracy: 0.8730 - jacard_coef: 0.0718 - val_loss: 0.1312 - val_accuracy: 0.8925 - val_jacard_coef: 0.0274 - lr: 1.2500e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8806 - jacard_coef: 0.08832/5 [===========>..................] - ETA: 2s - loss: 0.1590 - accuracy: 0.8657 - jacard_coef: 0.08183/5 [=================>............] - ETA: 1s - loss: 0.1590 - accuracy: 0.8654 - jacard_coef: 0.07694/5 [=======================>......] - ETA: 0s - loss: 0.1592 - accuracy: 0.8573 - jacard_coef: 0.07735/5 [==============================] - 3s 556ms/step - loss: 0.1592 - accuracy: 0.8569 - jacard_coef: 0.0785 - val_loss: 0.1341 - val_accuracy: 0.8759 - val_jacard_coef: 0.0562 - lr: 1.2500e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1585 - accuracy: 0.8649 - jacard_coef: 0.07062/5 [===========>..................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8414 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1592 - accuracy: 0.8388 - jacard_coef: 0.08334/5 [=======================>......] - ETA: 0s - loss: 0.1590 - accuracy: 0.8395 - jacard_coef: 0.08545/5 [==============================] - 3s 556ms/step - loss: 0.1590 - accuracy: 0.8390 - jacard_coef: 0.0964 - val_loss: 0.1420 - val_accuracy: 0.8457 - val_jacard_coef: 0.0648 - lr: 1.2500e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.8227 - jacard_coef: 0.09262/5 [===========>..................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8150 - jacard_coef: 0.10303/5 [=================>............] - ETA: 1s - loss: 0.1590 - accuracy: 0.8140 - jacard_coef: 0.09714/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.8108 - jacard_coef: 0.09775/5 [==============================] - 3s 569ms/step - loss: 0.1589 - accuracy: 0.8113 - jacard_coef: 0.0974 - val_loss: 0.1442 - val_accuracy: 0.7954 - val_jacard_coef: 0.0855 - lr: 1.2500e-04
Epoch 24/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8063 - jacard_coef: 0.09562/5 [===========>..................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8048 - jacard_coef: 0.09623/5 [=================>............] - ETA: 1s - loss: 0.1587 - accuracy: 0.7998 - jacard_coef: 0.09624/5 [=======================>......] - ETA: 0s - loss: 0.1587 - accuracy: 0.7991 - jacard_coef: 0.10415/5 [==============================] - 3s 567ms/step - loss: 0.1587 - accuracy: 0.7985 - jacard_coef: 0.1083 - val_loss: 0.1458 - val_accuracy: 0.7605 - val_jacard_coef: 0.0869 - lr: 1.2500e-04
Epoch 25/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1587 - accuracy: 0.8089 - jacard_coef: 0.08172/5 [===========>..................] - ETA: 2s - loss: 0.1588 - accuracy: 0.8067 - jacard_coef: 0.09703/5 [=================>............] - ETA: 1s - loss: 0.1587 - accuracy: 0.8086 - jacard_coef: 0.10184/5 [=======================>......] - ETA: 0s - loss: 0.1586 - accuracy: 0.8111 - jacard_coef: 0.10055/5 [==============================] - 3s 566ms/step - loss: 0.1586 - accuracy: 0.8117 - jacard_coef: 0.0804 - val_loss: 0.1441 - val_accuracy: 0.7631 - val_jacard_coef: 0.0872 - lr: 1.2500e-04
Epoch 26/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1587 - accuracy: 0.8115 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8195 - jacard_coef: 0.09503/5 [=================>............] - ETA: 1s - loss: 0.1585 - accuracy: 0.8277 - jacard_coef: 0.09204/5 [=======================>......] - ETA: 0s - loss: 0.1584 - accuracy: 0.8316 - jacard_coef: 0.09455/5 [==============================] - 3s 566ms/step - loss: 0.1584 - accuracy: 0.8313 - jacard_coef: 0.1059 - val_loss: 0.1469 - val_accuracy: 0.7604 - val_jacard_coef: 0.0873 - lr: 1.2500e-04
Epoch 27/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8374 - jacard_coef: 0.10982/5 [===========>..................] - ETA: 2s - loss: 0.1581 - accuracy: 0.8397 - jacard_coef: 0.09973/5 [=================>............] - ETA: 1s - loss: 0.1582 - accuracy: 0.8323 - jacard_coef: 0.09594/5 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.8305 - jacard_coef: 0.10055/5 [==============================] - 3s 568ms/step - loss: 0.1583 - accuracy: 0.8304 - jacard_coef: 0.0833 - val_loss: 0.1517 - val_accuracy: 0.7532 - val_jacard_coef: 0.0875 - lr: 1.2500e-04
Epoch 28/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1579 - accuracy: 0.8313 - jacard_coef: 0.09632/5 [===========>..................] - ETA: 2s - loss: 0.1578 - accuracy: 0.8310 - jacard_coef: 0.10183/5 [=================>............] - ETA: 1s - loss: 0.1582 - accuracy: 0.8220 - jacard_coef: 0.09444/5 [=======================>......] - ETA: 0s - loss: 0.1582 - accuracy: 0.8209 - jacard_coef: 0.09485/5 [==============================] - 3s 567ms/step - loss: 0.1582 - accuracy: 0.8211 - jacard_coef: 0.1454 - val_loss: 0.1536 - val_accuracy: 0.7629 - val_jacard_coef: 0.0882 - lr: 1.2500e-04
Epoch 29/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1575 - accuracy: 0.8309 - jacard_coef: 0.13082/5 [===========>..................] - ETA: 2s - loss: 0.1579 - accuracy: 0.8186 - jacard_coef: 0.10503/5 [=================>............] - ETA: 1s - loss: 0.1580 - accuracy: 0.8154 - jacard_coef: 0.09964/5 [=======================>......] - ETA: 0s - loss: 0.1581 - accuracy: 0.8112 - jacard_coef: 0.09595/5 [==============================] - 3s 556ms/step - loss: 0.1581 - accuracy: 0.8109 - jacard_coef: 0.0768 - val_loss: 0.1591 - val_accuracy: 0.6959 - val_jacard_coef: 0.0703 - lr: 1.2500e-04
Epoch 30/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1576 - accuracy: 0.8332 - jacard_coef: 0.09542/5 [===========>..................] - ETA: 2s - loss: 0.1579 - accuracy: 0.8252 - jacard_coef: 0.08423/5 [=================>............] - ETA: 1s - loss: 0.1577 - accuracy: 0.8330 - jacard_coef: 0.09784/5 [=======================>......] - ETA: 0s - loss: 0.1579 - accuracy: 0.8315 - jacard_coef: 0.09745/5 [==============================] - 3s 558ms/step - loss: 0.1578 - accuracy: 0.8319 - jacard_coef: 0.0779 - val_loss: 0.1652 - val_accuracy: 0.6162 - val_jacard_coef: 0.0785 - lr: 1.2500e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

✓ Training completed successfully!
  Best Val Jaccard: 0.0882 (epoch 28)
  Final Val Loss: 0.1652
  Training Time: 0:02:51.139547
  Stability (std): 0.0085

Results saved to: hyperparameter_optimization_20250926_165036/exp_21_Attention_UNet_lr1e-3_bs32/Attention_UNet_lr0.001_bs32_results.json
