✓ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
✓ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758880423.017874 1189018 service.cc:145] XLA service 0x1458e1e443a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758880423.017894 1189018 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758880423.156565 1189018 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:39 - loss: 0.3391 - accuracy: 0.4947 - jacard_coef: 0.09022/5 [===========>..................] - ETA: 46s - loss: 0.3190 - accuracy: 0.4212 - jacard_coef: 0.0869 3/5 [=================>............] - ETA: 16s - loss: 0.2936 - accuracy: 0.3588 - jacard_coef: 0.08204/5 [=======================>......] - ETA: 5s - loss: 0.2780 - accuracy: 0.3287 - jacard_coef: 0.0818 5/5 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.3273 - jacard_coef: 0.07325/5 [==============================] - 96s 7s/step - loss: 0.2775 - accuracy: 0.3273 - jacard_coef: 0.0732 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2159 - accuracy: 0.2215 - jacard_coef: 0.08992/5 [===========>..................] - ETA: 2s - loss: 0.2069 - accuracy: 0.2110 - jacard_coef: 0.07793/5 [=================>............] - ETA: 1s - loss: 0.2043 - accuracy: 0.2434 - jacard_coef: 0.07584/5 [=======================>......] - ETA: 0s - loss: 0.2000 - accuracy: 0.2939 - jacard_coef: 0.07915/5 [==============================] - 3s 640ms/step - loss: 0.2004 - accuracy: 0.2939 - jacard_coef: 0.0679 - val_loss: 1.1240 - val_accuracy: 0.9302 - val_jacard_coef: 1.4581e-12 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2290 - accuracy: 0.1366 - jacard_coef: 0.06702/5 [===========>..................] - ETA: 2s - loss: 0.2220 - accuracy: 0.1486 - jacard_coef: 0.08683/5 [=================>............] - ETA: 1s - loss: 0.2130 - accuracy: 0.1544 - jacard_coef: 0.08604/5 [=======================>......] - ETA: 0s - loss: 0.2099 - accuracy: 0.1498 - jacard_coef: 0.08265/5 [==============================] - 3s 639ms/step - loss: 0.2097 - accuracy: 0.1505 - jacard_coef: 0.1017 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1935 - accuracy: 0.2017 - jacard_coef: 0.09512/5 [===========>..................] - ETA: 2s - loss: 0.1930 - accuracy: 0.2004 - jacard_coef: 0.08553/5 [=================>............] - ETA: 1s - loss: 0.1907 - accuracy: 0.2159 - jacard_coef: 0.08584/5 [=======================>......] - ETA: 0s - loss: 0.1902 - accuracy: 0.2341 - jacard_coef: 0.08215/5 [==============================] - 3s 649ms/step - loss: 0.1903 - accuracy: 0.2344 - jacard_coef: 0.0794 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1878 - accuracy: 0.4199 - jacard_coef: 0.09062/5 [===========>..................] - ETA: 2s - loss: 0.1860 - accuracy: 0.3933 - jacard_coef: 0.08653/5 [=================>............] - ETA: 1s - loss: 0.1848 - accuracy: 0.3773 - jacard_coef: 0.08404/5 [=======================>......] - ETA: 0s - loss: 0.1843 - accuracy: 0.3622 - jacard_coef: 0.08105/5 [==============================] - 3s 639ms/step - loss: 0.1843 - accuracy: 0.3619 - jacard_coef: 0.0652 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1794 - accuracy: 0.3515 - jacard_coef: 0.08602/5 [===========>..................] - ETA: 2s - loss: 0.1800 - accuracy: 0.3802 - jacard_coef: 0.08303/5 [=================>............] - ETA: 1s - loss: 0.1851 - accuracy: 0.3504 - jacard_coef: 0.07924/5 [=======================>......] - ETA: 0s - loss: 0.1855 - accuracy: 0.3577 - jacard_coef: 0.07975/5 [==============================] - 3s 638ms/step - loss: 0.1856 - accuracy: 0.3562 - jacard_coef: 0.0675 - val_loss: 1.1202 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1827 - accuracy: 0.3255 - jacard_coef: 0.07142/5 [===========>..................] - ETA: 2s - loss: 0.1871 - accuracy: 0.2831 - jacard_coef: 0.07563/5 [=================>............] - ETA: 1s - loss: 0.1849 - accuracy: 0.2784 - jacard_coef: 0.08034/5 [=======================>......] - ETA: 0s - loss: 0.1835 - accuracy: 0.2812 - jacard_coef: 0.08345/5 [==============================] - 3s 639ms/step - loss: 0.1836 - accuracy: 0.2803 - jacard_coef: 0.0675 - val_loss: 1.1199 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-12 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1777 - accuracy: 0.3790 - jacard_coef: 0.08222/5 [===========>..................] - ETA: 2s - loss: 0.1814 - accuracy: 0.3957 - jacard_coef: 0.08733/5 [=================>............] - ETA: 1s - loss: 0.1805 - accuracy: 0.3826 - jacard_coef: 0.08694/5 [=======================>......] - ETA: 0s - loss: 0.1794 - accuracy: 0.4030 - jacard_coef: 0.08415/5 [==============================] - 3s 655ms/step - loss: 0.1794 - accuracy: 0.4013 - jacard_coef: 0.0685 - val_loss: 1.1016 - val_accuracy: 0.9303 - val_jacard_coef: 6.7043e-04 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1747 - accuracy: 0.5537 - jacard_coef: 0.07782/5 [===========>..................] - ETA: 2s - loss: 0.1743 - accuracy: 0.5528 - jacard_coef: 0.07743/5 [=================>............] - ETA: 1s - loss: 0.1752 - accuracy: 0.5333 - jacard_coef: 0.07754/5 [=======================>......] - ETA: 0s - loss: 0.1748 - accuracy: 0.5264 - jacard_coef: 0.08485/5 [==============================] - 3s 641ms/step - loss: 0.1749 - accuracy: 0.5260 - jacard_coef: 0.0776 - val_loss: 1.0931 - val_accuracy: 0.9293 - val_jacard_coef: 4.3156e-04 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1717 - accuracy: 0.5468 - jacard_coef: 0.08882/5 [===========>..................] - ETA: 2s - loss: 0.1717 - accuracy: 0.5190 - jacard_coef: 0.09123/5 [=================>............] - ETA: 1s - loss: 0.1718 - accuracy: 0.5113 - jacard_coef: 0.08664/5 [=======================>......] - ETA: 0s - loss: 0.1722 - accuracy: 0.5176 - jacard_coef: 0.08245/5 [==============================] - 3s 638ms/step - loss: 0.1724 - accuracy: 0.5173 - jacard_coef: 0.0825 - val_loss: 1.1155 - val_accuracy: 0.9301 - val_jacard_coef: 2.4728e-04 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1712 - accuracy: 0.4695 - jacard_coef: 0.08602/5 [===========>..................] - ETA: 2s - loss: 0.1723 - accuracy: 0.4175 - jacard_coef: 0.08943/5 [=================>............] - ETA: 1s - loss: 0.1751 - accuracy: 0.3777 - jacard_coef: 0.08244/5 [=======================>......] - ETA: 0s - loss: 0.1751 - accuracy: 0.3595 - jacard_coef: 0.08005/5 [==============================] - 3s 639ms/step - loss: 0.1757 - accuracy: 0.3583 - jacard_coef: 0.0849 - val_loss: 1.0813 - val_accuracy: 0.9271 - val_jacard_coef: 2.9305e-04 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1742 - accuracy: 0.2950 - jacard_coef: 0.10662/5 [===========>..................] - ETA: 2s - loss: 0.1775 - accuracy: 0.2814 - jacard_coef: 0.08873/5 [=================>............] - ETA: 1s - loss: 0.1761 - accuracy: 0.2866 - jacard_coef: 0.08524/5 [=======================>......] - ETA: 0s - loss: 0.1757 - accuracy: 0.2840 - jacard_coef: 0.08155/5 [==============================] - 3s 651ms/step - loss: 0.1757 - accuracy: 0.2836 - jacard_coef: 0.0941 - val_loss: 0.3510 - val_accuracy: 0.5818 - val_jacard_coef: 0.0773 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1776 - accuracy: 0.4937 - jacard_coef: 0.08402/5 [===========>..................] - ETA: 2s - loss: 0.1752 - accuracy: 0.5458 - jacard_coef: 0.06793/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.5766 - jacard_coef: 0.06954/5 [=======================>......] - ETA: 0s - loss: 0.1751 - accuracy: 0.5719 - jacard_coef: 0.07305/5 [==============================] - 3s 652ms/step - loss: 0.1754 - accuracy: 0.5705 - jacard_coef: 0.0765 - val_loss: 0.1902 - val_accuracy: 0.4898 - val_jacard_coef: 0.0776 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1718 - accuracy: 0.7516 - jacard_coef: 0.06042/5 [===========>..................] - ETA: 2s - loss: 0.1723 - accuracy: 0.7600 - jacard_coef: 0.05943/5 [=================>............] - ETA: 1s - loss: 0.1729 - accuracy: 0.7524 - jacard_coef: 0.05444/5 [=======================>......] - ETA: 0s - loss: 0.1726 - accuracy: 0.7504 - jacard_coef: 0.06105/5 [==============================] - 3s 638ms/step - loss: 0.1726 - accuracy: 0.7495 - jacard_coef: 0.0491 - val_loss: 0.1667 - val_accuracy: 0.7261 - val_jacard_coef: 0.0567 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1680 - accuracy: 0.7309 - jacard_coef: 0.05742/5 [===========>..................] - ETA: 2s - loss: 0.1679 - accuracy: 0.7232 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1681 - accuracy: 0.7047 - jacard_coef: 0.07004/5 [=======================>......] - ETA: 0s - loss: 0.1680 - accuracy: 0.6963 - jacard_coef: 0.07255/5 [==============================] - 3s 638ms/step - loss: 0.1681 - accuracy: 0.6948 - jacard_coef: 0.0592 - val_loss: 0.1514 - val_accuracy: 0.8635 - val_jacard_coef: 0.0428 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1681 - accuracy: 0.6677 - jacard_coef: 0.08282/5 [===========>..................] - ETA: 2s - loss: 0.1671 - accuracy: 0.7197 - jacard_coef: 0.07803/5 [=================>............] - ETA: 1s - loss: 0.1662 - accuracy: 0.7815 - jacard_coef: 0.05774/5 [=======================>......] - ETA: 0s - loss: 0.1661 - accuracy: 0.7647 - jacard_coef: 0.05875/5 [==============================] - 3s 640ms/step - loss: 0.1663 - accuracy: 0.7616 - jacard_coef: 0.0808 - val_loss: 0.1609 - val_accuracy: 0.8122 - val_jacard_coef: 0.0402 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1962 - accuracy: 0.3702 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 2s - loss: 0.1852 - accuracy: 0.4126 - jacard_coef: 0.06923/5 [=================>............] - ETA: 1s - loss: 0.1817 - accuracy: 0.4936 - jacard_coef: 0.07364/5 [=======================>......] - ETA: 0s - loss: 0.1794 - accuracy: 0.5356 - jacard_coef: 0.08035/5 [==============================] - 3s 641ms/step - loss: 0.1794 - accuracy: 0.5342 - jacard_coef: 0.0720 - val_loss: 0.1357 - val_accuracy: 0.9139 - val_jacard_coef: 0.0219 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1727 - accuracy: 0.6331 - jacard_coef: 0.08132/5 [===========>..................] - ETA: 2s - loss: 0.1726 - accuracy: 0.6975 - jacard_coef: 0.06833/5 [=================>............] - ETA: 1s - loss: 0.1735 - accuracy: 0.6213 - jacard_coef: 0.08024/5 [=======================>......] - ETA: 0s - loss: 0.1735 - accuracy: 0.6678 - jacard_coef: 0.07645/5 [==============================] - 3s 638ms/step - loss: 0.1735 - accuracy: 0.6644 - jacard_coef: 0.0616 - val_loss: 0.1063 - val_accuracy: 0.9210 - val_jacard_coef: 0.0106 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1689 - accuracy: 0.8360 - jacard_coef: 0.04972/5 [===========>..................] - ETA: 2s - loss: 0.1709 - accuracy: 0.7694 - jacard_coef: 0.06183/5 [=================>............] - ETA: 1s - loss: 0.1709 - accuracy: 0.7790 - jacard_coef: 0.06124/5 [=======================>......] - ETA: 0s - loss: 0.1706 - accuracy: 0.7787 - jacard_coef: 0.05765/5 [==============================] - 3s 639ms/step - loss: 0.1706 - accuracy: 0.7738 - jacard_coef: 0.0489 - val_loss: 0.1036 - val_accuracy: 0.9195 - val_jacard_coef: 0.0141 - lr: 2.5000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1700 - accuracy: 0.8094 - jacard_coef: 0.04972/5 [===========>..................] - ETA: 2s - loss: 0.1689 - accuracy: 0.8097 - jacard_coef: 0.04643/5 [=================>............] - ETA: 1s - loss: 0.1695 - accuracy: 0.7802 - jacard_coef: 0.05844/5 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.7928 - jacard_coef: 0.05555/5 [==============================] - 3s 638ms/step - loss: 0.1695 - accuracy: 0.7880 - jacard_coef: 0.0553 - val_loss: 0.1056 - val_accuracy: 0.9213 - val_jacard_coef: 0.0111 - lr: 2.5000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1678 - accuracy: 0.8048 - jacard_coef: 0.06092/5 [===========>..................] - ETA: 2s - loss: 0.1676 - accuracy: 0.8076 - jacard_coef: 0.05383/5 [=================>............] - ETA: 1s - loss: 0.1675 - accuracy: 0.8158 - jacard_coef: 0.05074/5 [=======================>......] - ETA: 0s - loss: 0.1675 - accuracy: 0.8099 - jacard_coef: 0.05335/5 [==============================] - 3s 638ms/step - loss: 0.1676 - accuracy: 0.8064 - jacard_coef: 0.0441 - val_loss: 0.1124 - val_accuracy: 0.9247 - val_jacard_coef: 0.0068 - lr: 2.5000e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1661 - accuracy: 0.8373 - jacard_coef: 0.06322/5 [===========>..................] - ETA: 2s - loss: 0.1657 - accuracy: 0.8391 - jacard_coef: 0.05613/5 [=================>............] - ETA: 1s - loss: 0.1664 - accuracy: 0.8165 - jacard_coef: 0.05594/5 [=======================>......] - ETA: 0s - loss: 0.1664 - accuracy: 0.8065 - jacard_coef: 0.05585/5 [==============================] - 3s 639ms/step - loss: 0.1665 - accuracy: 0.8037 - jacard_coef: 0.0459 - val_loss: 0.1216 - val_accuracy: 0.9271 - val_jacard_coef: 0.0041 - lr: 2.5000e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1642 - accuracy: 0.8541 - jacard_coef: 0.06422/5 [===========>..................] - ETA: 2s - loss: 0.1653 - accuracy: 0.8171 - jacard_coef: 0.06713/5 [=================>............] - ETA: 1s - loss: 0.1664 - accuracy: 0.7388 - jacard_coef: 0.06574/5 [=======================>......] - ETA: 0s - loss: 0.1663 - accuracy: 0.7605 - jacard_coef: 0.06485/5 [==============================] - 3s 639ms/step - loss: 0.1663 - accuracy: 0.7580 - jacard_coef: 0.0841 - val_loss: 0.1346 - val_accuracy: 0.9278 - val_jacard_coef: 0.0033 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

✓ Training completed successfully!
  Best Val Jaccard: 0.0776 (epoch 13)
  Final Val Loss: 0.1346
  Training Time: 0:02:50.585235
  Stability (std): 0.0225

Results saved to: hyperparameter_optimization_20250926_165036/exp_30_Attention_ResUNet_lr5e-4_bs32/Attention_ResUNet_lr0.0005_bs32_results.json
