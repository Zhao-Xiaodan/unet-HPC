âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758879553.809687 1143929 service.cc:145] XLA service 0x14e25e3152b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758879553.809710 1143929 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758879553.948135 1143929 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:02 - loss: 0.3393 - accuracy: 0.4962 - jacard_coef: 0.07132/5 [===========>..................] - ETA: 45s - loss: 0.3009 - accuracy: 0.4405 - jacard_coef: 0.0723 3/5 [=================>............] - ETA: 15s - loss: 0.2728 - accuracy: 0.4257 - jacard_coef: 0.07374/5 [=======================>......] - ETA: 5s - loss: 0.2552 - accuracy: 0.3901 - jacard_coef: 0.0752 5/5 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.3881 - jacard_coef: 0.06685/5 [==============================] - 85s 6s/step - loss: 0.2547 - accuracy: 0.3881 - jacard_coef: 0.0668 - val_loss: 0.0929 - val_accuracy: 0.9290 - val_jacard_coef: 0.0025 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1909 - accuracy: 0.1498 - jacard_coef: 0.08622/5 [===========>..................] - ETA: 2s - loss: 0.1899 - accuracy: 0.1452 - jacard_coef: 0.08653/5 [=================>............] - ETA: 1s - loss: 0.1904 - accuracy: 0.1426 - jacard_coef: 0.08664/5 [=======================>......] - ETA: 0s - loss: 0.1892 - accuracy: 0.1481 - jacard_coef: 0.08345/5 [==============================] - 3s 572ms/step - loss: 0.1894 - accuracy: 0.1492 - jacard_coef: 0.0775 - val_loss: 1.0548 - val_accuracy: 0.9201 - val_jacard_coef: 0.0075 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1988 - accuracy: 0.2835 - jacard_coef: 0.06962/5 [===========>..................] - ETA: 2s - loss: 0.1973 - accuracy: 0.2325 - jacard_coef: 0.07313/5 [=================>............] - ETA: 1s - loss: 0.1974 - accuracy: 0.2065 - jacard_coef: 0.07514/5 [=======================>......] - ETA: 0s - loss: 0.1968 - accuracy: 0.1907 - jacard_coef: 0.08165/5 [==============================] - 3s 557ms/step - loss: 0.1967 - accuracy: 0.1904 - jacard_coef: 0.0877 - val_loss: 1.1446 - val_accuracy: 0.9269 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1907 - accuracy: 0.1643 - jacard_coef: 0.06712/5 [===========>..................] - ETA: 2s - loss: 0.1900 - accuracy: 0.2104 - jacard_coef: 0.08163/5 [=================>............] - ETA: 1s - loss: 0.1901 - accuracy: 0.2552 - jacard_coef: 0.07994/5 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.2875 - jacard_coef: 0.08315/5 [==============================] - 3s 558ms/step - loss: 0.1888 - accuracy: 0.2877 - jacard_coef: 0.0667 - val_loss: 1.1368 - val_accuracy: 0.9269 - val_jacard_coef: 0.0029 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1853 - accuracy: 0.2870 - jacard_coef: 0.06532/5 [===========>..................] - ETA: 2s - loss: 0.1853 - accuracy: 0.2738 - jacard_coef: 0.08243/5 [=================>............] - ETA: 1s - loss: 0.1849 - accuracy: 0.2547 - jacard_coef: 0.08214/5 [=======================>......] - ETA: 0s - loss: 0.1849 - accuracy: 0.2403 - jacard_coef: 0.08475/5 [==============================] - 3s 558ms/step - loss: 0.1849 - accuracy: 0.2402 - jacard_coef: 0.0972 - val_loss: 1.1233 - val_accuracy: 0.9265 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1821 - accuracy: 0.1733 - jacard_coef: 0.09332/5 [===========>..................] - ETA: 2s - loss: 0.1829 - accuracy: 0.1715 - jacard_coef: 0.09233/5 [=================>............] - ETA: 1s - loss: 0.1825 - accuracy: 0.1720 - jacard_coef: 0.08424/5 [=======================>......] - ETA: 0s - loss: 0.1820 - accuracy: 0.1822 - jacard_coef: 0.08395/5 [==============================] - 3s 557ms/step - loss: 0.1820 - accuracy: 0.1823 - jacard_coef: 0.0740 - val_loss: 0.6648 - val_accuracy: 0.9163 - val_jacard_coef: 0.0073 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1801 - accuracy: 0.2392 - jacard_coef: 0.09842/5 [===========>..................] - ETA: 2s - loss: 0.1789 - accuracy: 0.2607 - jacard_coef: 0.09283/5 [=================>............] - ETA: 1s - loss: 0.1777 - accuracy: 0.3127 - jacard_coef: 0.08194/5 [=======================>......] - ETA: 0s - loss: 0.1766 - accuracy: 0.3492 - jacard_coef: 0.08115/5 [==============================] - 3s 572ms/step - loss: 0.1766 - accuracy: 0.3492 - jacard_coef: 0.0656 - val_loss: 0.2835 - val_accuracy: 0.9002 - val_jacard_coef: 0.0149 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1724 - accuracy: 0.4866 - jacard_coef: 0.07322/5 [===========>..................] - ETA: 2s - loss: 0.1732 - accuracy: 0.4495 - jacard_coef: 0.07163/5 [=================>............] - ETA: 1s - loss: 0.1733 - accuracy: 0.4469 - jacard_coef: 0.08194/5 [=======================>......] - ETA: 0s - loss: 0.1733 - accuracy: 0.4444 - jacard_coef: 0.08455/5 [==============================] - 3s 569ms/step - loss: 0.1733 - accuracy: 0.4441 - jacard_coef: 0.0688 - val_loss: 0.1406 - val_accuracy: 0.9073 - val_jacard_coef: 0.0161 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1706 - accuracy: 0.6372 - jacard_coef: 0.07232/5 [===========>..................] - ETA: 2s - loss: 0.1703 - accuracy: 0.6803 - jacard_coef: 0.07843/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.6998 - jacard_coef: 0.07344/5 [=======================>......] - ETA: 0s - loss: 0.1702 - accuracy: 0.7081 - jacard_coef: 0.07025/5 [==============================] - 3s 559ms/step - loss: 0.1702 - accuracy: 0.7073 - jacard_coef: 0.0740 - val_loss: 0.4146 - val_accuracy: 0.9192 - val_jacard_coef: 0.0084 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.7156 - jacard_coef: 0.05472/5 [===========>..................] - ETA: 2s - loss: 0.1688 - accuracy: 0.6793 - jacard_coef: 0.05953/5 [=================>............] - ETA: 1s - loss: 0.1680 - accuracy: 0.7021 - jacard_coef: 0.06184/5 [=======================>......] - ETA: 0s - loss: 0.1675 - accuracy: 0.7162 - jacard_coef: 0.06225/5 [==============================] - 3s 559ms/step - loss: 0.1675 - accuracy: 0.7166 - jacard_coef: 0.0497 - val_loss: 0.2315 - val_accuracy: 0.9191 - val_jacard_coef: 0.0082 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1647 - accuracy: 0.8494 - jacard_coef: 0.03562/5 [===========>..................] - ETA: 2s - loss: 0.1646 - accuracy: 0.8692 - jacard_coef: 0.02963/5 [=================>............] - ETA: 1s - loss: 0.1643 - accuracy: 0.8851 - jacard_coef: 0.02484/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.8882 - jacard_coef: 0.02355/5 [==============================] - 3s 558ms/step - loss: 0.1644 - accuracy: 0.8841 - jacard_coef: 0.0289 - val_loss: 0.1388 - val_accuracy: 0.9222 - val_jacard_coef: 0.0047 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1677 - accuracy: 0.8854 - jacard_coef: 0.01512/5 [===========>..................] - ETA: 2s - loss: 0.1684 - accuracy: 0.8909 - jacard_coef: 0.01733/5 [=================>............] - ETA: 1s - loss: 0.1696 - accuracy: 0.8753 - jacard_coef: 0.02634/5 [=======================>......] - ETA: 0s - loss: 0.1702 - accuracy: 0.8458 - jacard_coef: 0.03605/5 [==============================] - 3s 558ms/step - loss: 0.1703 - accuracy: 0.8442 - jacard_coef: 0.0573 - val_loss: 0.1056 - val_accuracy: 0.9208 - val_jacard_coef: 0.0097 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1737 - accuracy: 0.4949 - jacard_coef: 0.07972/5 [===========>..................] - ETA: 2s - loss: 0.1735 - accuracy: 0.4478 - jacard_coef: 0.08023/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.4305 - jacard_coef: 0.07964/5 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.4326 - jacard_coef: 0.07655/5 [==============================] - 3s 558ms/step - loss: 0.1732 - accuracy: 0.4351 - jacard_coef: 0.0614 - val_loss: 0.1647 - val_accuracy: 0.9280 - val_jacard_coef: 0.0018 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1700 - accuracy: 0.7618 - jacard_coef: 0.05322/5 [===========>..................] - ETA: 2s - loss: 0.1695 - accuracy: 0.7848 - jacard_coef: 0.05973/5 [=================>............] - ETA: 1s - loss: 0.1687 - accuracy: 0.8110 - jacard_coef: 0.05894/5 [=======================>......] - ETA: 0s - loss: 0.1679 - accuracy: 0.8290 - jacard_coef: 0.04895/5 [==============================] - 3s 557ms/step - loss: 0.1679 - accuracy: 0.8296 - jacard_coef: 0.0392 - val_loss: 0.1351 - val_accuracy: 0.9279 - val_jacard_coef: 0.0021 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1633 - accuracy: 0.9144 - jacard_coef: 0.00872/5 [===========>..................] - ETA: 2s - loss: 0.1629 - accuracy: 0.9063 - jacard_coef: 0.00683/5 [=================>............] - ETA: 1s - loss: 0.1624 - accuracy: 0.9115 - jacard_coef: 0.00604/5 [=======================>......] - ETA: 0s - loss: 0.1621 - accuracy: 0.9099 - jacard_coef: 0.00545/5 [==============================] - 3s 557ms/step - loss: 0.1621 - accuracy: 0.9103 - jacard_coef: 0.0043 - val_loss: 0.0970 - val_accuracy: 0.9285 - val_jacard_coef: 0.0011 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1596 - accuracy: 0.9055 - jacard_coef: 5.3986e-042/5 [===========>..................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8688 - jacard_coef: 0.0205    3/5 [=================>............] - ETA: 1s - loss: 0.1594 - accuracy: 0.8626 - jacard_coef: 0.04174/5 [=======================>......] - ETA: 0s - loss: 0.1594 - accuracy: 0.8764 - jacard_coef: 0.03155/5 [==============================] - 3s 557ms/step - loss: 0.1601 - accuracy: 0.8718 - jacard_coef: 0.0321 - val_loss: 0.0902 - val_accuracy: 0.9275 - val_jacard_coef: 0.0015 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1602 - accuracy: 0.9111 - jacard_coef: 0.00512/5 [===========>..................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9075 - jacard_coef: 0.00653/5 [=================>............] - ETA: 1s - loss: 0.1605 - accuracy: 0.9069 - jacard_coef: 0.00684/5 [=======================>......] - ETA: 0s - loss: 0.1604 - accuracy: 0.9082 - jacard_coef: 0.00705/5 [==============================] - 3s 558ms/step - loss: 0.1604 - accuracy: 0.9083 - jacard_coef: 0.0115 - val_loss: 0.0840 - val_accuracy: 0.9145 - val_jacard_coef: 0.0136 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1602 - accuracy: 0.9004 - jacard_coef: 0.01272/5 [===========>..................] - ETA: 2s - loss: 0.1605 - accuracy: 0.8956 - jacard_coef: 0.01413/5 [=================>............] - ETA: 1s - loss: 0.1602 - accuracy: 0.9003 - jacard_coef: 0.01374/5 [=======================>......] - ETA: 0s - loss: 0.1601 - accuracy: 0.9008 - jacard_coef: 0.01455/5 [==============================] - 3s 558ms/step - loss: 0.1601 - accuracy: 0.9001 - jacard_coef: 0.0198 - val_loss: 0.1445 - val_accuracy: 0.9218 - val_jacard_coef: 0.0078 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0161 (epoch 8)
  Final Val Loss: 0.1445
  Training Time: 0:02:15.649909
  Stability (std): 0.0943

Results saved to: hyperparameter_optimization_20250926_165036/exp_24_Attention_UNet_lr5e-3_bs32/Attention_UNet_lr0.005_bs32_results.json
