âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758881360.290862 1235617 service.cc:145] XLA service 0x14b7056d8cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758881360.290883 1235617 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758881360.429930 1235617 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:39 - loss: 0.3316 - accuracy: 0.4854 - jacard_coef: 0.06412/5 [===========>..................] - ETA: 46s - loss: 0.2811 - accuracy: 0.4151 - jacard_coef: 0.0885 3/5 [=================>............] - ETA: 16s - loss: 0.2567 - accuracy: 0.3606 - jacard_coef: 0.08304/5 [=======================>......] - ETA: 5s - loss: 0.2425 - accuracy: 0.3349 - jacard_coef: 0.0831 5/5 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.3334 - jacard_coef: 0.07795/5 [==============================] - 96s 7s/step - loss: 0.2422 - accuracy: 0.3334 - jacard_coef: 0.0779 - val_loss: 1.0947 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1962 - accuracy: 0.1083 - jacard_coef: 0.08012/5 [===========>..................] - ETA: 2s - loss: 0.1958 - accuracy: 0.1116 - jacard_coef: 0.08393/5 [=================>............] - ETA: 1s - loss: 0.1946 - accuracy: 0.1191 - jacard_coef: 0.08284/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.1271 - jacard_coef: 0.08375/5 [==============================] - 3s 653ms/step - loss: 0.1942 - accuracy: 0.1278 - jacard_coef: 0.0976 - val_loss: 1.1183 - val_accuracy: 0.9298 - val_jacard_coef: 4.3452e-04 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1891 - accuracy: 0.1895 - jacard_coef: 0.08522/5 [===========>..................] - ETA: 2s - loss: 0.1894 - accuracy: 0.2383 - jacard_coef: 0.07763/5 [=================>............] - ETA: 1s - loss: 0.1884 - accuracy: 0.2648 - jacard_coef: 0.07694/5 [=======================>......] - ETA: 0s - loss: 0.1882 - accuracy: 0.2685 - jacard_coef: 0.08395/5 [==============================] - 3s 639ms/step - loss: 0.1882 - accuracy: 0.2681 - jacard_coef: 0.0812 - val_loss: 1.1054 - val_accuracy: 0.9303 - val_jacard_coef: 1.4592e-12 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1816 - accuracy: 0.2208 - jacard_coef: 0.07712/5 [===========>..................] - ETA: 2s - loss: 0.1801 - accuracy: 0.2865 - jacard_coef: 0.08763/5 [=================>............] - ETA: 1s - loss: 0.1793 - accuracy: 0.3350 - jacard_coef: 0.08924/5 [=======================>......] - ETA: 0s - loss: 0.1786 - accuracy: 0.3665 - jacard_coef: 0.08495/5 [==============================] - 3s 653ms/step - loss: 0.1786 - accuracy: 0.3671 - jacard_coef: 0.0931 - val_loss: 0.2400 - val_accuracy: 0.8862 - val_jacard_coef: 0.0187 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1751 - accuracy: 0.5564 - jacard_coef: 0.06922/5 [===========>..................] - ETA: 2s - loss: 0.1750 - accuracy: 0.5369 - jacard_coef: 0.08093/5 [=================>............] - ETA: 1s - loss: 0.1750 - accuracy: 0.5248 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1748 - accuracy: 0.5204 - jacard_coef: 0.08005/5 [==============================] - 3s 639ms/step - loss: 0.1748 - accuracy: 0.5201 - jacard_coef: 0.0640 - val_loss: 1.0997 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-12 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1734 - accuracy: 0.4330 - jacard_coef: 0.09342/5 [===========>..................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4597 - jacard_coef: 0.09073/5 [=================>............] - ETA: 1s - loss: 0.1719 - accuracy: 0.4763 - jacard_coef: 0.08684/5 [=======================>......] - ETA: 0s - loss: 0.1714 - accuracy: 0.4799 - jacard_coef: 0.08605/5 [==============================] - 3s 652ms/step - loss: 0.1717 - accuracy: 0.4784 - jacard_coef: 0.0824 - val_loss: 7.4904 - val_accuracy: 0.0930 - val_jacard_coef: 0.0703 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1725 - accuracy: 0.6042 - jacard_coef: 0.06122/5 [===========>..................] - ETA: 2s - loss: 0.1729 - accuracy: 0.6359 - jacard_coef: 0.06403/5 [=================>............] - ETA: 1s - loss: 0.1729 - accuracy: 0.6586 - jacard_coef: 0.05974/5 [=======================>......] - ETA: 0s - loss: 0.1727 - accuracy: 0.6712 - jacard_coef: 0.05845/5 [==============================] - 3s 639ms/step - loss: 0.1726 - accuracy: 0.6718 - jacard_coef: 0.0619 - val_loss: 12.6217 - val_accuracy: 0.0733 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1699 - accuracy: 0.7268 - jacard_coef: 0.06132/5 [===========>..................] - ETA: 2s - loss: 0.1689 - accuracy: 0.7385 - jacard_coef: 0.05403/5 [=================>............] - ETA: 1s - loss: 0.1683 - accuracy: 0.7496 - jacard_coef: 0.05484/5 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.6943 - jacard_coef: 0.05955/5 [==============================] - 3s 640ms/step - loss: 0.1690 - accuracy: 0.6930 - jacard_coef: 0.0828 - val_loss: 0.2566 - val_accuracy: 0.9303 - val_jacard_coef: 1.4594e-12 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1733 - accuracy: 0.5744 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 2s - loss: 0.1730 - accuracy: 0.5455 - jacard_coef: 0.07883/5 [=================>............] - ETA: 1s - loss: 0.1738 - accuracy: 0.5311 - jacard_coef: 0.07504/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.5237 - jacard_coef: 0.07445/5 [==============================] - 3s 639ms/step - loss: 0.1740 - accuracy: 0.5236 - jacard_coef: 0.0789 - val_loss: 1.0665 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1747 - accuracy: 0.4936 - jacard_coef: 0.06392/5 [===========>..................] - ETA: 2s - loss: 0.1734 - accuracy: 0.5034 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1722 - accuracy: 0.5050 - jacard_coef: 0.07744/5 [=======================>......] - ETA: 0s - loss: 0.1733 - accuracy: 0.4808 - jacard_coef: 0.08125/5 [==============================] - 3s 640ms/step - loss: 0.1733 - accuracy: 0.4803 - jacard_coef: 0.0834 - val_loss: 0.7421 - val_accuracy: 0.9304 - val_jacard_coef: 1.4616e-12 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1717 - accuracy: 0.5895 - jacard_coef: 0.09552/5 [===========>..................] - ETA: 2s - loss: 0.1732 - accuracy: 0.5809 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1737 - accuracy: 0.5689 - jacard_coef: 0.08324/5 [=======================>......] - ETA: 0s - loss: 0.1742 - accuracy: 0.5604 - jacard_coef: 0.08345/5 [==============================] - 3s 639ms/step - loss: 0.1742 - accuracy: 0.5600 - jacard_coef: 0.0675 - val_loss: 2.7031 - val_accuracy: 0.0801 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1750 - accuracy: 0.5517 - jacard_coef: 0.07232/5 [===========>..................] - ETA: 2s - loss: 0.1750 - accuracy: 0.5545 - jacard_coef: 0.07963/5 [=================>............] - ETA: 1s - loss: 0.1751 - accuracy: 0.5520 - jacard_coef: 0.07664/5 [=======================>......] - ETA: 0s - loss: 0.1748 - accuracy: 0.5548 - jacard_coef: 0.07905/5 [==============================] - 3s 639ms/step - loss: 0.1748 - accuracy: 0.5550 - jacard_coef: 0.1018 - val_loss: 2.6489 - val_accuracy: 0.0744 - val_jacard_coef: 0.0698 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1743 - accuracy: 0.5901 - jacard_coef: 0.07932/5 [===========>..................] - ETA: 2s - loss: 0.1734 - accuracy: 0.5869 - jacard_coef: 0.07973/5 [=================>............] - ETA: 1s - loss: 0.1735 - accuracy: 0.5885 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.5930 - jacard_coef: 0.08035/5 [==============================] - 3s 640ms/step - loss: 0.1729 - accuracy: 0.5938 - jacard_coef: 0.0679 - val_loss: 1.2184 - val_accuracy: 0.0809 - val_jacard_coef: 0.0702 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1707 - accuracy: 0.6565 - jacard_coef: 0.10052/5 [===========>..................] - ETA: 2s - loss: 0.1706 - accuracy: 0.6523 - jacard_coef: 0.08283/5 [=================>............] - ETA: 1s - loss: 0.1708 - accuracy: 0.6595 - jacard_coef: 0.08804/5 [=======================>......] - ETA: 0s - loss: 0.1706 - accuracy: 0.6706 - jacard_coef: 0.07895/5 [==============================] - 3s 652ms/step - loss: 0.1705 - accuracy: 0.6724 - jacard_coef: 0.0651 - val_loss: 0.2481 - val_accuracy: 0.1580 - val_jacard_coef: 0.0734 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1700 - accuracy: 0.7785 - jacard_coef: 0.07642/5 [===========>..................] - ETA: 2s - loss: 0.1694 - accuracy: 0.7906 - jacard_coef: 0.06683/5 [=================>............] - ETA: 1s - loss: 0.1689 - accuracy: 0.7999 - jacard_coef: 0.06014/5 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.8083 - jacard_coef: 0.05855/5 [==============================] - 3s 639ms/step - loss: 0.1685 - accuracy: 0.8077 - jacard_coef: 0.0625 - val_loss: 0.1273 - val_accuracy: 0.8965 - val_jacard_coef: 0.0256 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1672 - accuracy: 0.8583 - jacard_coef: 0.05542/5 [===========>..................] - ETA: 2s - loss: 0.1669 - accuracy: 0.8655 - jacard_coef: 0.04493/5 [=================>............] - ETA: 1s - loss: 0.1667 - accuracy: 0.8745 - jacard_coef: 0.03904/5 [=======================>......] - ETA: 0s - loss: 0.1663 - accuracy: 0.8808 - jacard_coef: 0.03515/5 [==============================] - 3s 639ms/step - loss: 0.1663 - accuracy: 0.8813 - jacard_coef: 0.0331 - val_loss: 0.1453 - val_accuracy: 0.8959 - val_jacard_coef: 0.0312 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1639 - accuracy: 0.9127 - jacard_coef: 0.00992/5 [===========>..................] - ETA: 2s - loss: 0.1634 - accuracy: 0.9125 - jacard_coef: 0.00753/5 [=================>............] - ETA: 1s - loss: 0.1627 - accuracy: 0.9135 - jacard_coef: 0.00594/5 [=======================>......] - ETA: 0s - loss: 0.1620 - accuracy: 0.9125 - jacard_coef: 0.00575/5 [==============================] - 3s 653ms/step - loss: 0.1620 - accuracy: 0.9126 - jacard_coef: 0.0046 - val_loss: 0.1739 - val_accuracy: 0.3758 - val_jacard_coef: 0.0805 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1589 - accuracy: 0.8384 - jacard_coef: 0.04882/5 [===========>..................] - ETA: 2s - loss: 0.1591 - accuracy: 0.7914 - jacard_coef: 0.06353/5 [=================>............] - ETA: 1s - loss: 0.1591 - accuracy: 0.8006 - jacard_coef: 0.06704/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.7977 - jacard_coef: 0.06155/5 [==============================] - 3s 639ms/step - loss: 0.1600 - accuracy: 0.7953 - jacard_coef: 0.0493 - val_loss: 0.1412 - val_accuracy: 0.9010 - val_jacard_coef: 0.0294 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1600 - accuracy: 0.6302 - jacard_coef: 0.10362/5 [===========>..................] - ETA: 2s - loss: 0.1601 - accuracy: 0.6861 - jacard_coef: 0.07933/5 [=================>............] - ETA: 1s - loss: 0.1605 - accuracy: 0.7041 - jacard_coef: 0.07494/5 [=======================>......] - ETA: 0s - loss: 0.1605 - accuracy: 0.7110 - jacard_coef: 0.07735/5 [==============================] - 3s 640ms/step - loss: 0.1605 - accuracy: 0.7122 - jacard_coef: 0.0618 - val_loss: 0.0868 - val_accuracy: 0.9255 - val_jacard_coef: 0.0022 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1603 - accuracy: 0.7695 - jacard_coef: 0.06252/5 [===========>..................] - ETA: 2s - loss: 0.1610 - accuracy: 0.7640 - jacard_coef: 0.06143/5 [=================>............] - ETA: 1s - loss: 0.1609 - accuracy: 0.7758 - jacard_coef: 0.06204/5 [=======================>......] - ETA: 0s - loss: 0.1608 - accuracy: 0.7874 - jacard_coef: 0.06245/5 [==============================] - 3s 640ms/step - loss: 0.1608 - accuracy: 0.7877 - jacard_coef: 0.0662 - val_loss: 0.0863 - val_accuracy: 0.9186 - val_jacard_coef: 0.0020 - lr: 5.0000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1599 - accuracy: 0.8633 - jacard_coef: 0.04262/5 [===========>..................] - ETA: 2s - loss: 0.1596 - accuracy: 0.8649 - jacard_coef: 0.04873/5 [=================>............] - ETA: 1s - loss: 0.1595 - accuracy: 0.8623 - jacard_coef: 0.04444/5 [=======================>......] - ETA: 0s - loss: 0.1593 - accuracy: 0.8665 - jacard_coef: 0.03815/5 [==============================] - 3s 639ms/step - loss: 0.1593 - accuracy: 0.8672 - jacard_coef: 0.0409 - val_loss: 0.1373 - val_accuracy: 0.9044 - val_jacard_coef: 0.0119 - lr: 5.0000e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1584 - accuracy: 0.8938 - jacard_coef: 0.01732/5 [===========>..................] - ETA: 2s - loss: 0.1583 - accuracy: 0.8981 - jacard_coef: 0.01153/5 [=================>............] - ETA: 1s - loss: 0.1580 - accuracy: 0.9036 - jacard_coef: 0.00874/5 [=======================>......] - ETA: 0s - loss: 0.1578 - accuracy: 0.9087 - jacard_coef: 0.00785/5 [==============================] - 3s 639ms/step - loss: 0.1578 - accuracy: 0.9085 - jacard_coef: 0.0074 - val_loss: 0.1701 - val_accuracy: 0.8314 - val_jacard_coef: 0.0246 - lr: 5.0000e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1570 - accuracy: 0.9104 - jacard_coef: 0.00582/5 [===========>..................] - ETA: 2s - loss: 0.1569 - accuracy: 0.9110 - jacard_coef: 0.00663/5 [=================>............] - ETA: 1s - loss: 0.1568 - accuracy: 0.9124 - jacard_coef: 0.00574/5 [=======================>......] - ETA: 0s - loss: 0.1570 - accuracy: 0.9080 - jacard_coef: 0.00535/5 [==============================] - 3s 639ms/step - loss: 0.1570 - accuracy: 0.9078 - jacard_coef: 0.0046 - val_loss: 0.1686 - val_accuracy: 0.8488 - val_jacard_coef: 0.0162 - lr: 2.5000e-04
Epoch 24/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1568 - accuracy: 0.9077 - jacard_coef: 0.00352/5 [===========>..................] - ETA: 2s - loss: 0.1570 - accuracy: 0.9010 - jacard_coef: 0.00383/5 [=================>............] - ETA: 1s - loss: 0.1568 - accuracy: 0.9063 - jacard_coef: 0.00454/5 [=======================>......] - ETA: 0s - loss: 0.1565 - accuracy: 0.9106 - jacard_coef: 0.00435/5 [==============================] - 3s 640ms/step - loss: 0.1566 - accuracy: 0.9101 - jacard_coef: 0.0034 - val_loss: 0.1601 - val_accuracy: 0.8959 - val_jacard_coef: 0.0105 - lr: 2.5000e-04
Epoch 25/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1562 - accuracy: 0.9118 - jacard_coef: 0.00362/5 [===========>..................] - ETA: 2s - loss: 0.1563 - accuracy: 0.9092 - jacard_coef: 0.00303/5 [=================>............] - ETA: 1s - loss: 0.1562 - accuracy: 0.9140 - jacard_coef: 0.00264/5 [=======================>......] - ETA: 0s - loss: 0.1561 - accuracy: 0.9128 - jacard_coef: 0.00255/5 [==============================] - 3s 639ms/step - loss: 0.1562 - accuracy: 0.9126 - jacard_coef: 0.0038 - val_loss: 0.1624 - val_accuracy: 0.8987 - val_jacard_coef: 0.0118 - lr: 2.5000e-04
Epoch 26/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1559 - accuracy: 0.9138 - jacard_coef: 0.00262/5 [===========>..................] - ETA: 2s - loss: 0.1555 - accuracy: 0.9196 - jacard_coef: 0.00233/5 [=================>............] - ETA: 1s - loss: 0.1557 - accuracy: 0.9148 - jacard_coef: 0.00224/5 [=======================>......] - ETA: 0s - loss: 0.1557 - accuracy: 0.9140 - jacard_coef: 0.00245/5 [==============================] - 3s 639ms/step - loss: 0.1557 - accuracy: 0.9137 - jacard_coef: 0.0019 - val_loss: 0.1647 - val_accuracy: 0.8960 - val_jacard_coef: 0.0138 - lr: 2.5000e-04
Epoch 27/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1554 - accuracy: 0.9161 - jacard_coef: 0.00212/5 [===========>..................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9237 - jacard_coef: 0.00373/5 [=================>............] - ETA: 1s - loss: 0.1553 - accuracy: 0.9156 - jacard_coef: 0.00364/5 [=======================>......] - ETA: 0s - loss: 0.1554 - accuracy: 0.9130 - jacard_coef: 0.00325/5 [==============================] - 3s 639ms/step - loss: 0.1554 - accuracy: 0.9127 - jacard_coef: 0.0026 - val_loss: 0.1673 - val_accuracy: 0.8722 - val_jacard_coef: 0.0232 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0805 (epoch 17)
  Final Val Loss: 0.1673
  Training Time: 0:03:04.093981
  Stability (std): 0.0308

Results saved to: hyperparameter_optimization_20250926_165036/exp_36_Attention_ResUNet_lr5e-3_bs32/Attention_ResUNet_lr0.005_bs32_results.json
