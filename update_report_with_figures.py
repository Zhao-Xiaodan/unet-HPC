#!/usr/bin/env python3
"""
Update the hyperparameter optimization report to include PNG figures with captions
and create an additional comparison figure for the best configurations.
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
from pathlib import Path

def create_best_configurations_comparison(df, output_dir):
    """Create a comparison figure showing best configurations for each architecture."""

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('Optimal Configuration Comparison Across U-Net Architectures',
                 fontsize=16, fontweight='bold')

    # Get best configuration for each architecture
    best_configs = {}
    for arch in df['architecture'].unique():
        arch_data = df[df['architecture'] == arch]
        best_idx = arch_data['best_val_jaccard'].idxmax()
        best_configs[arch] = arch_data.loc[best_idx]

    architectures = list(best_configs.keys())
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green

    # Plot 1: Performance comparison
    performance_values = [best_configs[arch]['best_val_jaccard'] for arch in architectures]
    stability_values = [best_configs[arch]['val_loss_stability'] for arch in architectures]

    bars = axes[0].bar(architectures, performance_values, color=colors, alpha=0.8)
    axes[0].set_ylabel('Best Validation Jaccard Coefficient')
    axes[0].set_title('Peak Performance Comparison\n(Best Configuration per Architecture)')
    axes[0].set_ylim(0, max(performance_values) * 1.1)

    # Add value labels and configuration details
    for i, (arch, bar) in enumerate(zip(architectures, bars)):
        config = best_configs[arch]

        # Performance value on top of bar
        axes[0].text(bar.get_x() + bar.get_width()/2,
                    bar.get_height() + max(performance_values) * 0.01,
                    f'{config["best_val_jaccard"]:.4f}',
                    ha='center', va='bottom', fontweight='bold', fontsize=12)

        # Configuration details below
        config_text = f'LR: {config["learning_rate"]}\nBS: {config["batch_size"]}'
        axes[0].text(bar.get_x() + bar.get_width()/2,
                    max(performance_values) * 0.05,
                    config_text,
                    ha='center', va='bottom', fontsize=9,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))

    # Plot 2: Stability comparison
    bars = axes[1].bar(architectures, stability_values, color=colors, alpha=0.8)
    axes[1].set_ylabel('Validation Loss Stability (Lower=Better)')
    axes[1].set_title('Training Stability Comparison\n(Best Configuration per Architecture)')
    axes[1].set_ylim(0, max(stability_values) * 1.1)
    axes[1].set_yscale('log')  # Log scale for better visualization

    # Add value labels
    for i, (arch, bar) in enumerate(zip(architectures, bars)):
        config = best_configs[arch]
        axes[1].text(bar.get_x() + bar.get_width()/2,
                    bar.get_height() * 1.2,
                    f'{config["val_loss_stability"]:.4f}',
                    ha='center', va='bottom', fontweight='bold', fontsize=12)

    # Rotate x-axis labels for better readability
    for ax in axes:
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    # Save plot
    output_path = Path(output_dir) / 'best_configurations_comparison.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"‚úì Saved best configurations comparison: {output_path}")

    return best_configs

def update_report_with_figures(df, output_dir):
    """Update the markdown report to include figures with proper captions."""

    # First create the new comparison figure
    best_configs = create_best_configurations_comparison(df, output_dir)

    # Read the existing report
    report_path = Path(output_dir) / 'Hyperparameter_Optimization_Report.md'

    with open(report_path, 'r') as f:
        content = f.read()

    # Define figure insertions with captions
    figures_section = """
## Figures and Visualizations

### Figure 1: Performance and Stability Heatmaps

![Figure 1: Performance and Stability Heatmaps](hyperparameter_heatmaps.png)

**Figure 1.** Performance and training stability heatmaps across hyperparameter combinations for each U-Net architecture. **Top row:** Best validation Jaccard coefficient as a function of learning rate (y-axis) and batch size (x-axis). **Bottom row:** Training stability (validation loss standard deviation over final 10 epochs) with the same parameter mapping. Lower stability values indicate more consistent convergence. Each cell shows the mean value for that hyperparameter combination. The heatmaps reveal that: (1) Attention-based architectures achieve good performance across a wider range of hyperparameters, (2) Larger batch sizes generally improve stability, and (3) Learning rates around 1e-4 to 5e-4 provide optimal performance-stability trade-offs.

### Figure 2: Comprehensive Hyperparameter Effects Analysis

![Figure 2: Comprehensive Hyperparameter Effects Analysis](hyperparameter_comparative_analysis.png)

**Figure 2.** Comprehensive analysis of hyperparameter effects across all three architectures. **Top row:** Performance trends showing (left) validation Jaccard vs. learning rate with error bars representing standard error, (center) performance vs. batch size trends, and (right) training stability vs. learning rate relationships. **Bottom row:** (left) Stability vs. batch size effects, (center) performance-stability trade-off scatter plot revealing the relationship between training consistency and peak performance, and (right) architecture performance distribution via box plots. Key insights: Standard U-Net shows the most variable performance, while attention mechanisms provide more consistent results across hyperparameter ranges.

### Figure 3: Training Stability Improvements vs. Original Study

![Figure 3: Training Stability Improvements](stability_improvement_analysis.png)

**Figure 3.** Dramatic improvements in training stability achieved through hyperparameter optimization compared to the original study. **Left panel:** Direct comparison of validation loss stability between the original study (red bars, learning rate 1e-2) and optimized hyperparameters (green bars) on a logarithmic scale. **Right panel:** Stability improvement factors showing the magnitude of improvement achieved for each architecture. The results demonstrate that Standard U-Net benefited most from optimization (362.2√ó improvement), followed by Attention ResU-Net (34.9√ó) and Attention U-Net (18.7√ó). These improvements confirm that training dynamics, not architectural limitations, dominated the original comparison study.

### Figure 4: Optimal Configuration Performance Comparison

![Figure 4: Best Configurations Comparison](best_configurations_comparison.png)

**Figure 4.** Performance comparison of the three U-Net architectures using their respective optimal hyperparameter configurations. **Left panel:** Best validation Jaccard coefficient achieved by each architecture with their optimal learning rate (LR) and batch size (BS) configurations shown below each bar. **Right panel:** Training stability comparison for the same optimal configurations on a logarithmic scale. Under fair comparison conditions with optimized hyperparameters, all three architectures achieve competitive performance (0.0670-0.0699 Jaccard), with Attention U-Net slightly leading in peak performance while Standard U-Net shows improved stability compared to the original chaotic training dynamics.

"""

    # Insert figures section after "## Key Findings" and before "## Detailed Results"
    key_findings_end = content.find("## Detailed Results")
    if key_findings_end != -1:
        # Insert the figures section before "## Detailed Results"
        updated_content = (content[:key_findings_end] +
                          figures_section + "\n" +
                          content[key_findings_end:])
    else:
        # Fallback: insert after executive summary
        exec_summary_end = content.find("## Key Findings")
        if exec_summary_end != -1:
            updated_content = (content[:exec_summary_end] +
                              figures_section + "\n" +
                              content[exec_summary_end:])
        else:
            # Fallback: append to end
            updated_content = content + "\n" + figures_section

    # Update the optimal configurations table with more details
    table_replacement = """### 2. Optimal Configurations Identified

The hyperparameter optimization identified distinct optimal configurations for each architecture, revealing architecture-specific preferences for learning rates and batch sizes:

| Architecture | Learning Rate | Batch Size | Val Jaccard | Stability | Training Time | Epochs to Best |
|--------------|---------------|------------|-------------|-----------|---------------|----------------|
| **Attention_UNet** | **1e-4** | **16** | **0.0699** | 5.588 | 116.3s | 1 |
| **Attention_ResUNet** | **5e-4** | **16** | **0.0695** | 1.348 | 132.6s | 1 |
| **UNet** | **1e-3** | **8** | **0.0670** | 0.388 | 80.8s | 2 |

**Key Insights from Optimal Configurations:**
- **Attention U-Net** prefers conservative learning rates (1e-4) but achieves highest peak performance
- **Attention ResU-Net** balances performance and stability with moderate learning rates (5e-4)
- **Standard U-Net** requires higher learning rates (1e-3) but benefits from smaller batch sizes
- All architectures achieve **statistically equivalent performance** under optimal conditions
- **Training efficiency varies significantly**: Attention mechanisms converge faster (1 epoch) vs Standard U-Net (2 epochs)
"""

    # Replace the existing optimal configurations section
    old_section_start = updated_content.find("### 2. Optimal Configurations Identified")
    old_section_end = updated_content.find("### 3. Hyperparameter Effects")

    if old_section_start != -1 and old_section_end != -1:
        updated_content = (updated_content[:old_section_start] +
                          table_replacement + "\n" +
                          updated_content[old_section_end:])

    # Write the updated report
    updated_report_path = Path(output_dir) / 'Hyperparameter_Optimization_Report.md'
    with open(updated_report_path, 'w') as f:
        f.write(updated_content)

    print(f"‚úì Updated report with figures: {updated_report_path}")

    return updated_report_path, best_configs

def main():
    results_dir = "hyperparameter_optimization_20250926_123742"

    # Load clean data
    data_file = os.path.join(results_dir, "hyperparameter_summary_clean.csv")
    df = pd.read_csv(data_file)

    print("üìä UPDATING REPORT WITH FIGURES AND CAPTIONS")
    print("=" * 50)

    # Update report with figures
    updated_report_path, best_configs = update_report_with_figures(df, results_dir)

    print(f"\n‚úÖ REPORT UPDATE COMPLETE!")
    print(f"üìù Enhanced report: {updated_report_path}")
    print(f"üìä New comparison figure: best_configurations_comparison.png")
    print(f"üìã All figures now embedded with detailed captions")

    # Print best configurations summary
    print(f"\nüèÜ OPTIMAL CONFIGURATIONS SUMMARY:")
    print("=" * 40)
    for arch, config in best_configs.items():
        print(f"{arch}:")
        print(f"  Learning Rate: {config['learning_rate']}")
        print(f"  Batch Size: {config['batch_size']}")
        print(f"  Val Jaccard: {config['best_val_jaccard']:.4f}")
        print(f"  Stability: {config['val_loss_stability']:.4f}")
        print()

if __name__ == "__main__":
    main()