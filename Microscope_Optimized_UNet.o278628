=======================================================================
OPTIMIZED MITOCHONDRIA SEGMENTATION - MICROSCOPE DATASET
=======================================================================
Dataset: dataset_microscope/
Models: UNet, Attention UNet, Attention ResUNet
Using optimized hyperparameters from optimization study

Job started on Wed Oct  8 03:47:46 PM +08 2025
Running on node: GN-A40-076
Job ID: 278628.stdct-mgmt-02
Available GPUs: GPU-d742d0ca-2c4b-cdde-aa99-848a73ee1f5e
Memory: 503Gi, CPUs: 36

=== TRAINING CONFIGURATION ===
Dataset: ./dataset_microscope/
  Images: ./dataset_microscope/images/
  Masks: ./dataset_microscope/masks/
Image Size: 256x256

Model Configurations (Optimized):
  1. UNet:
     - Learning Rate: 1e-3
     - Batch Size: 8
     - Expected Val Jaccard: ~0.0670

  2. Attention UNet (Best Performer):
     - Learning Rate: 1e-4
     - Batch Size: 16
     - Expected Val Jaccard: ~0.0699

  3. Attention ResUNet:
     - Learning Rate: 5e-4
     - Batch Size: 16
     - Expected Val Jaccard: ~0.0695

All models use:
  - Binary Focal Loss (gamma=2)
  - Gradient Clipping (clipnorm=1.0)
  - Early Stopping (patience=15)
  - Adaptive LR Reduction
  - Maximum 100 epochs per model
==============================

TensorFlow Container: /app1/common/singularity-img/hopper/tensorflow/tensorflow_2.16.1-cuda_12.5.0_24.06.sif

=== PRE-EXECUTION CHECKS ===
1. Checking dataset structure...
   âœ“ Dataset directories found
   âœ“ Images found: 73 files
   âœ“ Masks found: 73 files

2. Checking Python files...
   âœ“ Optimized training script found
   âœ“ models.py found
==========================

=== TENSORFLOW & GPU STATUS ===
Python version: 3.10.12
TensorFlow version: 2.16.1
CUDA built support: True
Physical GPUs found: 1
  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
âœ“ GPU memory growth enabled
âœ“ Basic GPU operation successful

Checking dependencies:
  âœ“ cv2
  âœ“ PIL
  âœ“ matplotlib
  âœ“ numpy
  âœ“ sklearn
  âœ“ pandas
  âœ“ focal_loss
===============================

=== INSTALLING DEPENDENCIES ===
âœ“ focal_loss already available
===============================

ðŸš€ STARTING OPTIMIZED MITOCHONDRIA SEGMENTATION TRAINING
==============================================
Training 3 models sequentially with optimized hyperparameters:
1. UNet (LR=1e-3, BS=8, up to 100 epochs)
2. Attention UNet (LR=1e-4, BS=16, up to 100 epochs) - BEST
3. Attention ResUNet (LR=5e-4, BS=16, up to 100 epochs)

Early stopping enabled (patience=15)
Expected total time: 6-12 hours (depending on convergence)
Results will be saved in timestamped directory
==============================================

======================================================================
OPTIMIZED MITOCHONDRIA SEGMENTATION - MICROSCOPE DATASET
======================================================================
Dataset: dataset_microscope/images/
Image size: 256x256

Loading images...
Loaded 73 images
Loading masks...
Loaded 73 masks
Image dataset shape: (73, 256, 256, 3)
Mask dataset shape: (73, 256, 256, 1)
Training samples: 65
Testing samples: 8

Saved dataset sample visualization: dataset_sample_check.png

======================================================================
MODEL CONFIGURATION
======================================================================
Input shape: (256, 256, 3)

Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
Output directory: microscope_training_20251008_074915

======================================================================
TRAINING MODEL 1/3: STANDARD U-NET
======================================================================
Optimized Hyperparameters:
  Learning Rate: 1e-3
  Batch Size: 8
  Expected Val Jaccard: ~0.0670

Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
Epoch 1/100
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1759909789.363838 3241085 service.cc:145] XLA service 0x1552b4ce2ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1759909789.363880 3241085 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1759909789.830202 3241085 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 5:35 - loss: 0.3243 - accuracy: 0.4896 - jacard_coef: 0.09412/9 [=====>........................] - ETA: 26s - loss: 0.2841 - accuracy: 0.5727 - jacard_coef: 0.1459 3/9 [=========>....................] - ETA: 14s - loss: 0.2589 - accuracy: 0.6248 - jacard_coef: 0.16624/9 [============>.................] - ETA: 9s - loss: 0.2437 - accuracy: 0.6751 - jacard_coef: 0.2091 5/9 [===============>..............] - ETA: 5s - loss: 0.2329 - accuracy: 0.6882 - jacard_coef: 0.24896/9 [===================>..........] - ETA: 3s - loss: 0.2210 - accuracy: 0.7033 - jacard_coef: 0.26837/9 [======================>.......] - ETA: 2s - loss: 0.2058 - accuracy: 0.7206 - jacard_coef: 0.30438/9 [=========================>....] - ETA: 0s - loss: 0.1936 - accuracy: 0.7382 - jacard_coef: 0.32979/9 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.7397 - jacard_coef: 0.3664
Epoch 1: val_jacard_coef improved from -inf to 0.12728, saving model to microscope_training_20251008_074915/best_unet_model.hdf5
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
9/9 [==============================] - 57s 2s/step - loss: 0.1920 - accuracy: 0.7397 - jacard_coef: 0.3664 - val_loss: 11.6323 - val_accuracy: 0.0816 - val_jacard_coef: 0.1273 - lr: 0.0010
Epoch 2/100
1/9 [==>...........................] - ETA: 1s - loss: 0.1746 - accuracy: 0.8102 - jacard_coef: 0.37892/9 [=====>........................] - ETA: 0s - loss: 0.1491 - accuracy: 0.8068 - jacard_coef: 0.46263/9 [=========>....................] - ETA: 0s - loss: 0.1412 - accuracy: 0.8115 - jacard_coef: 0.48644/9 [============>.................] - ETA: 0s - loss: 0.1491 - accuracy: 0.8040 - jacard_coef: 0.43345/9 [===============>..............] - ETA: 0s - loss: 0.1414 - accuracy: 0.8114 - jacard_coef: 0.44366/9 [===================>..........] - ETA: 0s - loss: 0.1377 - accuracy: 0.8127 - jacard_coef: 0.45467/9 [======================>.......] - ETA: 0s - loss: 0.1371 - accuracy: 0.8161 - jacard_coef: 0.43028/9 [=========================>....] - ETA: 0s - loss: 0.1320 - accuracy: 0.8222 - jacard_coef: 0.44059/9 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.8231 - jacard_coef: 0.4545
Epoch 2: val_jacard_coef improved from 0.12728 to 0.12728, saving model to microscope_training_20251008_074915/best_unet_model.hdf5
9/9 [==============================] - 4s 535ms/step - loss: 0.1313 - accuracy: 0.8231 - jacard_coef: 0.4545 - val_loss: 13.0110 - val_accuracy: 0.0814 - val_jacard_coef: 0.1273 - lr: 0.0010
Epoch 3/100
1/9 [==>...........................] - ETA: 1s - loss: 0.1502 - accuracy: 0.8059 - jacard_coef: 0.46502/9 [=====>........................] - ETA: 0s - loss: 0.1253 - accuracy: 0.8307 - jacard_coef: 0.47213/9 [=========>....................] - ETA: 0s - loss: 0.1328 - accuracy: 0.8422 - jacard_coef: 0.46274/9 [============>.................] - ETA: 0s - loss: 0.1293 - accuracy: 0.8449 - jacard_coef: 0.48445/9 [===============>..............] - ETA: 0s - loss: 0.1326 - accuracy: 0.8433 - jacard_coef: 0.48016/9 [===================>..........] - ETA: 0s - loss: 0.1312 - accuracy: 0.8488 - jacard_coef: 0.48117/9 [======================>.......] - ETA: 0s - loss: 0.1250 - accuracy: 0.8466 - jacard_coef: 0.50458/9 [=========================>....] - ETA: 0s - loss: 0.1211 - accuracy: 0.8495 - jacard_coef: 0.5082
Epoch 3: val_jacard_coef did not improve from 0.12728
9/9 [==============================] - 1s 127ms/step - loss: 0.1213 - accuracy: 0.8495 - jacard_coef: 0.4703 - val_loss: 12.0459 - val_accuracy: 0.0816 - val_jacard_coef: 0.1273 - lr: 0.0010
Epoch 4/100
1/9 [==>...........................] - ETA: 1s - loss: 0.1059 - accuracy: 0.8564 - jacard_coef: 0.46162/9 [=====>........................] - ETA: 0s - loss: 0.1031 - accuracy: 0.8547 - jacard_coef: 0.49753/9 [=========>....................] - ETA: 0s - loss: 0.1117 - accuracy: 0.8584 - jacard_coef: 0.46484/9 [============>.................] - ETA: 0s - loss: 0.1071 - accuracy: 0.8636 - jacard_coef: 0.49895/9 [===============>..............] - ETA: 0s - loss: 0.1045 - accuracy: 0.8623 - jacard_coef: 0.50706/9 [===================>..........] - ETA: 0s - loss: 0.1035 - accuracy: 0.8685 - jacard_coef: 0.50987/9 [======================>.......] - ETA: 0s - loss: 0.1011 - accuracy: 0.8664 - jacard_coef: 0.52488/9 [=========================>....] - ETA: 0s - loss: 0.1007 - accuracy: 0.8601 - jacard_coef: 0.5311
Epoch 4: val_jacard_coef improved from 0.12728 to 0.12754, saving model to microscope_training_20251008_074915/best_unet_model.hdf5
9/9 [==============================] - 4s 494ms/step - loss: 0.1028 - accuracy: 0.8575 - jacard_coef: 0.5004 - val_loss: 3.3205 - val_accuracy: 0.0965 - val_jacard_coef: 0.1275 - lr: 0.0010
Epoch 5/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0911 - accuracy: 0.8351 - jacard_coef: 0.52982/9 [=====>........................] - ETA: 0s - loss: 0.1143 - accuracy: 0.8497 - jacard_coef: 0.43013/9 [=========>....................] - ETA: 0s - loss: 0.1110 - accuracy: 0.8297 - jacard_coef: 0.46894/9 [============>.................] - ETA: 0s - loss: 0.1090 - accuracy: 0.8350 - jacard_coef: 0.45485/9 [===============>..............] - ETA: 0s - loss: 0.1064 - accuracy: 0.8419 - jacard_coef: 0.47556/9 [===================>..........] - ETA: 0s - loss: 0.1048 - accuracy: 0.8372 - jacard_coef: 0.48977/9 [======================>.......] - ETA: 0s - loss: 0.1033 - accuracy: 0.8443 - jacard_coef: 0.50128/9 [=========================>....] - ETA: 0s - loss: 0.1026 - accuracy: 0.8504 - jacard_coef: 0.5142
Epoch 5: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 126ms/step - loss: 0.1045 - accuracy: 0.8490 - jacard_coef: 0.4586 - val_loss: 0.5110 - val_accuracy: 0.1856 - val_jacard_coef: 0.1264 - lr: 0.0010
Epoch 6/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0917 - accuracy: 0.8891 - jacard_coef: 0.51372/9 [=====>........................] - ETA: 0s - loss: 0.0924 - accuracy: 0.8674 - jacard_coef: 0.52813/9 [=========>....................] - ETA: 0s - loss: 0.0968 - accuracy: 0.8653 - jacard_coef: 0.50914/9 [============>.................] - ETA: 0s - loss: 0.1040 - accuracy: 0.8696 - jacard_coef: 0.49355/9 [===============>..............] - ETA: 0s - loss: 0.1056 - accuracy: 0.8674 - jacard_coef: 0.49316/9 [===================>..........] - ETA: 0s - loss: 0.1050 - accuracy: 0.8536 - jacard_coef: 0.51187/9 [======================>.......] - ETA: 0s - loss: 0.1069 - accuracy: 0.8605 - jacard_coef: 0.47838/9 [=========================>....] - ETA: 0s - loss: 0.1054 - accuracy: 0.8548 - jacard_coef: 0.4925
Epoch 6: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 125ms/step - loss: 0.1050 - accuracy: 0.8547 - jacard_coef: 0.5057 - val_loss: 0.1438 - val_accuracy: 0.7811 - val_jacard_coef: 0.0384 - lr: 0.0010
Epoch 7/100
1/9 [==>...........................] - ETA: 1s - loss: 0.1240 - accuracy: 0.8289 - jacard_coef: 0.43092/9 [=====>........................] - ETA: 0s - loss: 0.1100 - accuracy: 0.8523 - jacard_coef: 0.48803/9 [=========>....................] - ETA: 0s - loss: 0.1083 - accuracy: 0.8655 - jacard_coef: 0.47864/9 [============>.................] - ETA: 0s - loss: 0.1038 - accuracy: 0.8676 - jacard_coef: 0.51845/9 [===============>..............] - ETA: 0s - loss: 0.1005 - accuracy: 0.8699 - jacard_coef: 0.53966/9 [===================>..........] - ETA: 0s - loss: 0.1014 - accuracy: 0.8661 - jacard_coef: 0.51447/9 [======================>.......] - ETA: 0s - loss: 0.1029 - accuracy: 0.8555 - jacard_coef: 0.50688/9 [=========================>....] - ETA: 0s - loss: 0.1002 - accuracy: 0.8550 - jacard_coef: 0.5213
Epoch 7: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 126ms/step - loss: 0.1000 - accuracy: 0.8553 - jacard_coef: 0.5207 - val_loss: 0.3475 - val_accuracy: 0.1153 - val_jacard_coef: 0.1274 - lr: 0.0010
Epoch 8/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0962 - accuracy: 0.8032 - jacard_coef: 0.45802/9 [=====>........................] - ETA: 0s - loss: 0.0949 - accuracy: 0.8173 - jacard_coef: 0.47963/9 [=========>....................] - ETA: 0s - loss: 0.0929 - accuracy: 0.8380 - jacard_coef: 0.50414/9 [============>.................] - ETA: 0s - loss: 0.0896 - accuracy: 0.8250 - jacard_coef: 0.52965/9 [===============>..............] - ETA: 0s - loss: 0.0883 - accuracy: 0.8357 - jacard_coef: 0.54466/9 [===================>..........] - ETA: 0s - loss: 0.0893 - accuracy: 0.8430 - jacard_coef: 0.53317/9 [======================>.......] - ETA: 0s - loss: 0.0920 - accuracy: 0.8449 - jacard_coef: 0.51058/9 [=========================>....] - ETA: 0s - loss: 0.0930 - accuracy: 0.8463 - jacard_coef: 0.4990
Epoch 8: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 127ms/step - loss: 0.0931 - accuracy: 0.8474 - jacard_coef: 0.4910 - val_loss: 0.1844 - val_accuracy: 0.2263 - val_jacard_coef: 0.1250 - lr: 0.0010
Epoch 9/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0827 - accuracy: 0.8855 - jacard_coef: 0.62272/9 [=====>........................] - ETA: 0s - loss: 0.0911 - accuracy: 0.8995 - jacard_coef: 0.57673/9 [=========>....................] - ETA: 0s - loss: 0.0985 - accuracy: 0.8903 - jacard_coef: 0.50344/9 [============>.................] - ETA: 0s - loss: 0.0938 - accuracy: 0.8744 - jacard_coef: 0.51605/9 [===============>..............] - ETA: 0s - loss: 0.0965 - accuracy: 0.8778 - jacard_coef: 0.49276/9 [===================>..........] - ETA: 0s - loss: 0.0931 - accuracy: 0.8704 - jacard_coef: 0.50417/9 [======================>.......] - ETA: 0s - loss: 0.0905 - accuracy: 0.8663 - jacard_coef: 0.52408/9 [=========================>....] - ETA: 0s - loss: 0.0908 - accuracy: 0.8602 - jacard_coef: 0.5203
Epoch 9: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 127ms/step - loss: 0.0904 - accuracy: 0.8599 - jacard_coef: 0.5365 - val_loss: 0.1292 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 10/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0816 - accuracy: 0.8556 - jacard_coef: 0.49402/9 [=====>........................] - ETA: 0s - loss: 0.0813 - accuracy: 0.8537 - jacard_coef: 0.50803/9 [=========>....................] - ETA: 0s - loss: 0.0819 - accuracy: 0.8577 - jacard_coef: 0.50714/9 [============>.................] - ETA: 0s - loss: 0.0798 - accuracy: 0.8576 - jacard_coef: 0.54375/9 [===============>..............] - ETA: 0s - loss: 0.0807 - accuracy: 0.8543 - jacard_coef: 0.53276/9 [===================>..........] - ETA: 0s - loss: 0.0786 - accuracy: 0.8544 - jacard_coef: 0.55267/9 [======================>.......] - ETA: 0s - loss: 0.0801 - accuracy: 0.8598 - jacard_coef: 0.54598/9 [=========================>....] - ETA: 0s - loss: 0.0832 - accuracy: 0.8603 - jacard_coef: 0.5249
Epoch 10: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 127ms/step - loss: 0.0849 - accuracy: 0.8570 - jacard_coef: 0.5065 - val_loss: 0.1227 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 11/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0628 - accuracy: 0.8348 - jacard_coef: 0.69602/9 [=====>........................] - ETA: 0s - loss: 0.0866 - accuracy: 0.8528 - jacard_coef: 0.51423/9 [=========>....................] - ETA: 0s - loss: 0.0916 - accuracy: 0.8328 - jacard_coef: 0.50214/9 [============>.................] - ETA: 0s - loss: 0.0871 - accuracy: 0.8434 - jacard_coef: 0.53385/9 [===============>..............] - ETA: 0s - loss: 0.0881 - accuracy: 0.8502 - jacard_coef: 0.51396/9 [===================>..........] - ETA: 0s - loss: 0.0858 - accuracy: 0.8476 - jacard_coef: 0.51627/9 [======================>.......] - ETA: 0s - loss: 0.0845 - accuracy: 0.8483 - jacard_coef: 0.52318/9 [=========================>....] - ETA: 0s - loss: 0.0858 - accuracy: 0.8545 - jacard_coef: 0.5189
Epoch 11: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 126ms/step - loss: 0.0878 - accuracy: 0.8530 - jacard_coef: 0.4622 - val_loss: 0.1266 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 12/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0793 - accuracy: 0.8738 - jacard_coef: 0.56082/9 [=====>........................] - ETA: 0s - loss: 0.0832 - accuracy: 0.8590 - jacard_coef: 0.54853/9 [=========>....................] - ETA: 0s - loss: 0.0818 - accuracy: 0.8660 - jacard_coef: 0.56174/9 [============>.................] - ETA: 0s - loss: 0.0849 - accuracy: 0.8556 - jacard_coef: 0.56135/9 [===============>..............] - ETA: 0s - loss: 0.0830 - accuracy: 0.8609 - jacard_coef: 0.57116/9 [===================>..........] - ETA: 0s - loss: 0.0999 - accuracy: 0.8502 - jacard_coef: 0.53097/9 [======================>.......] - ETA: 0s - loss: 0.1004 - accuracy: 0.8504 - jacard_coef: 0.53248/9 [=========================>....] - ETA: 0s - loss: 0.0979 - accuracy: 0.8567 - jacard_coef: 0.5491
Epoch 12: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 127ms/step - loss: 0.0985 - accuracy: 0.8573 - jacard_coef: 0.4921 - val_loss: 0.1330 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 13/100
1/9 [==>...........................] - ETA: 1s - loss: 0.1027 - accuracy: 0.8595 - jacard_coef: 0.53692/9 [=====>........................] - ETA: 0s - loss: 0.0945 - accuracy: 0.8870 - jacard_coef: 0.53603/9 [=========>....................] - ETA: 0s - loss: 0.1010 - accuracy: 0.8938 - jacard_coef: 0.48554/9 [============>.................] - ETA: 0s - loss: 0.1002 - accuracy: 0.9060 - jacard_coef: 0.52165/9 [===============>..............] - ETA: 0s - loss: 0.0980 - accuracy: 0.8798 - jacard_coef: 0.54246/9 [===================>..........] - ETA: 0s - loss: 0.0947 - accuracy: 0.8753 - jacard_coef: 0.55517/9 [======================>.......] - ETA: 0s - loss: 0.0924 - accuracy: 0.8742 - jacard_coef: 0.55498/9 [=========================>....] - ETA: 0s - loss: 0.0905 - accuracy: 0.8729 - jacard_coef: 0.5549
Epoch 13: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 128ms/step - loss: 0.0901 - accuracy: 0.8725 - jacard_coef: 0.5614 - val_loss: 0.1270 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 14/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0915 - accuracy: 0.8876 - jacard_coef: 0.58502/9 [=====>........................] - ETA: 0s - loss: 0.0941 - accuracy: 0.8463 - jacard_coef: 0.52363/9 [=========>....................] - ETA: 0s - loss: 0.0890 - accuracy: 0.8510 - jacard_coef: 0.51704/9 [============>.................] - ETA: 0s - loss: 0.0847 - accuracy: 0.8449 - jacard_coef: 0.51885/9 [===============>..............] - ETA: 0s - loss: 0.0823 - accuracy: 0.8515 - jacard_coef: 0.54036/9 [===================>..........] - ETA: 0s - loss: 0.0801 - accuracy: 0.8471 - jacard_coef: 0.54107/9 [======================>.......] - ETA: 0s - loss: 0.0793 - accuracy: 0.8483 - jacard_coef: 0.53798/9 [=========================>....] - ETA: 0s - loss: 0.0810 - accuracy: 0.8548 - jacard_coef: 0.5261
Epoch 14: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 127ms/step - loss: 0.0821 - accuracy: 0.8537 - jacard_coef: 0.4694 - val_loss: 0.1236 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 15/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0923 - accuracy: 0.8905 - jacard_coef: 0.45792/9 [=====>........................] - ETA: 0s - loss: 0.0862 - accuracy: 0.8959 - jacard_coef: 0.50003/9 [=========>....................] - ETA: 0s - loss: 0.0795 - accuracy: 0.8915 - jacard_coef: 0.54344/9 [============>.................] - ETA: 0s - loss: 0.0833 - accuracy: 0.8822 - jacard_coef: 0.53005/9 [===============>..............] - ETA: 0s - loss: 0.0830 - accuracy: 0.8754 - jacard_coef: 0.54056/9 [===================>..........] - ETA: 0s - loss: 0.0938 - accuracy: 0.8722 - jacard_coef: 0.49857/9 [======================>.......] - ETA: 0s - loss: 0.0908 - accuracy: 0.8764 - jacard_coef: 0.52008/9 [=========================>....] - ETA: 0s - loss: 0.0908 - accuracy: 0.8608 - jacard_coef: 0.5300
Epoch 15: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 128ms/step - loss: 0.0914 - accuracy: 0.8611 - jacard_coef: 0.4732 - val_loss: 0.1447 - val_accuracy: 0.8189 - val_jacard_coef: 0.0038 - lr: 0.0010
Epoch 16/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0763 - accuracy: 0.9090 - jacard_coef: 0.60742/9 [=====>........................] - ETA: 0s - loss: 0.0743 - accuracy: 0.8927 - jacard_coef: 0.62393/9 [=========>....................] - ETA: 0s - loss: 0.0738 - accuracy: 0.8794 - jacard_coef: 0.63594/9 [============>.................] - ETA: 0s - loss: 0.0741 - accuracy: 0.8691 - jacard_coef: 0.62305/9 [===============>..............] - ETA: 0s - loss: 0.0797 - accuracy: 0.8623 - jacard_coef: 0.57206/9 [===================>..........] - ETA: 0s - loss: 0.0812 - accuracy: 0.8715 - jacard_coef: 0.56087/9 [======================>.......] - ETA: 0s - loss: 0.0832 - accuracy: 0.8651 - jacard_coef: 0.53578/9 [=========================>....] - ETA: 0s - loss: 0.0816 - accuracy: 0.8590 - jacard_coef: 0.5381
Epoch 16: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 128ms/step - loss: 0.0823 - accuracy: 0.8600 - jacard_coef: 0.4827 - val_loss: 0.1236 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 17/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0780 - accuracy: 0.8558 - jacard_coef: 0.49852/9 [=====>........................] - ETA: 0s - loss: 0.0805 - accuracy: 0.8707 - jacard_coef: 0.48603/9 [=========>....................] - ETA: 0s - loss: 0.0800 - accuracy: 0.8750 - jacard_coef: 0.54574/9 [============>.................] - ETA: 0s - loss: 0.0815 - accuracy: 0.8807 - jacard_coef: 0.52345/9 [===============>..............] - ETA: 0s - loss: 0.0805 - accuracy: 0.8855 - jacard_coef: 0.55136/9 [===================>..........] - ETA: 0s - loss: 0.0794 - accuracy: 0.8875 - jacard_coef: 0.56787/9 [======================>.......] - ETA: 0s - loss: 0.0781 - accuracy: 0.8752 - jacard_coef: 0.56638/9 [=========================>....] - ETA: 0s - loss: 0.0765 - accuracy: 0.8706 - jacard_coef: 0.5687
Epoch 17: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 128ms/step - loss: 0.0774 - accuracy: 0.8704 - jacard_coef: 0.5073 - val_loss: 0.1289 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 18/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0767 - accuracy: 0.8614 - jacard_coef: 0.47482/9 [=====>........................] - ETA: 0s - loss: 0.0727 - accuracy: 0.8740 - jacard_coef: 0.56213/9 [=========>....................] - ETA: 0s - loss: 0.0748 - accuracy: 0.8566 - jacard_coef: 0.53594/9 [============>.................] - ETA: 0s - loss: 0.0760 - accuracy: 0.8736 - jacard_coef: 0.55905/9 [===============>..............] - ETA: 0s - loss: 0.0732 - accuracy: 0.8762 - jacard_coef: 0.58166/9 [===================>..........] - ETA: 0s - loss: 0.0713 - accuracy: 0.8783 - jacard_coef: 0.59427/9 [======================>.......] - ETA: 0s - loss: 0.0703 - accuracy: 0.8812 - jacard_coef: 0.60038/9 [=========================>....] - ETA: 0s - loss: 0.0805 - accuracy: 0.8654 - jacard_coef: 0.5741
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

Epoch 18: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 129ms/step - loss: 0.0802 - accuracy: 0.8657 - jacard_coef: 0.5786 - val_loss: 0.1284 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 0.0010
Epoch 19/100
1/9 [==>...........................] - ETA: 1s - loss: 0.0627 - accuracy: 0.9043 - jacard_coef: 0.63512/9 [=====>........................] - ETA: 0s - loss: 0.0610 - accuracy: 0.8971 - jacard_coef: 0.64493/9 [=========>....................] - ETA: 0s - loss: 0.0702 - accuracy: 0.8866 - jacard_coef: 0.59644/9 [============>.................] - ETA: 0s - loss: 0.0715 - accuracy: 0.8809 - jacard_coef: 0.58415/9 [===============>..............] - ETA: 0s - loss: 0.0773 - accuracy: 0.8692 - jacard_coef: 0.55006/9 [===================>..........] - ETA: 0s - loss: 0.0761 - accuracy: 0.8778 - jacard_coef: 0.56447/9 [======================>.......] - ETA: 0s - loss: 0.0775 - accuracy: 0.8711 - jacard_coef: 0.56298/9 [=========================>....] - ETA: 0s - loss: 0.0760 - accuracy: 0.8660 - jacard_coef: 0.5667
Epoch 19: val_jacard_coef did not improve from 0.12754
9/9 [==============================] - 1s 127ms/step - loss: 0.0763 - accuracy: 0.8675 - jacard_coef: 0.5584 - val_loss: 0.1205 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 5.0000e-04
Epoch 19: early stopping
Restoring model weights from the end of the best epoch: 4.
âœ“ UNet training completed in 0:01:24.897692
Best Val Jaccard: 0.1275

======================================================================
TRAINING MODEL 2/3: ATTENTION U-NET (BEST PERFORMER)
======================================================================
Optimized Hyperparameters:
  Learning Rate: 1e-4
  Batch Size: 16
  Expected Val Jaccard: ~0.0699 (highest)

Epoch 1/100
1/5 [=====>........................] - ETA: 2:14 - loss: 0.3161 - accuracy: 0.4980 - jacard_coef: 0.15132/5 [===========>..................] - ETA: 1s - loss: 0.2749 - accuracy: 0.5534 - jacard_coef: 0.1741  3/5 [=================>............] - ETA: 0s - loss: 0.2523 - accuracy: 0.5916 - jacard_coef: 0.20164/5 [=======================>......] - ETA: 0s - loss: 0.2263 - accuracy: 0.6137 - jacard_coef: 0.21285/5 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.6137 - jacard_coef: 0.1779
Epoch 1: val_jacard_coef improved from -inf to 0.11046, saving model to microscope_training_20251008_074915/best_attention_unet_model.hdf5
5/5 [==============================] - 43s 2s/step - loss: 0.2256 - accuracy: 0.6137 - jacard_coef: 0.1779 - val_loss: 0.1744 - val_accuracy: 0.0822 - val_jacard_coef: 0.1105 - lr: 1.0000e-04
Epoch 2/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1523 - accuracy: 0.7719 - jacard_coef: 0.30162/5 [===========>..................] - ETA: 1s - loss: 0.1420 - accuracy: 0.7776 - jacard_coef: 0.34253/5 [=================>............] - ETA: 0s - loss: 0.1383 - accuracy: 0.7808 - jacard_coef: 0.35874/5 [=======================>......] - ETA: 0s - loss: 0.1331 - accuracy: 0.7800 - jacard_coef: 0.3769
Epoch 2: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 299ms/step - loss: 0.1333 - accuracy: 0.7790 - jacard_coef: 0.3159 - val_loss: 0.1719 - val_accuracy: 0.8071 - val_jacard_coef: 0.0110 - lr: 1.0000e-04
Epoch 3/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1123 - accuracy: 0.8293 - jacard_coef: 0.44882/5 [===========>..................] - ETA: 1s - loss: 0.1255 - accuracy: 0.8100 - jacard_coef: 0.36673/5 [=================>............] - ETA: 0s - loss: 0.1257 - accuracy: 0.8057 - jacard_coef: 0.36984/5 [=======================>......] - ETA: 0s - loss: 0.1197 - accuracy: 0.8078 - jacard_coef: 0.4147
Epoch 3: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 299ms/step - loss: 0.1195 - accuracy: 0.8086 - jacard_coef: 0.4264 - val_loss: 0.1694 - val_accuracy: 0.8126 - val_jacard_coef: 0.0063 - lr: 1.0000e-04
Epoch 4/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1042 - accuracy: 0.8328 - jacard_coef: 0.50502/5 [===========>..................] - ETA: 1s - loss: 0.1071 - accuracy: 0.8402 - jacard_coef: 0.48193/5 [=================>............] - ETA: 0s - loss: 0.1082 - accuracy: 0.8335 - jacard_coef: 0.46134/5 [=======================>......] - ETA: 0s - loss: 0.1086 - accuracy: 0.8270 - jacard_coef: 0.4620
Epoch 4: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 299ms/step - loss: 0.1092 - accuracy: 0.8258 - jacard_coef: 0.3813 - val_loss: 0.1661 - val_accuracy: 0.8151 - val_jacard_coef: 0.0041 - lr: 1.0000e-04
Epoch 5/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1054 - accuracy: 0.8489 - jacard_coef: 0.44682/5 [===========>..................] - ETA: 1s - loss: 0.1051 - accuracy: 0.8454 - jacard_coef: 0.45213/5 [=================>............] - ETA: 0s - loss: 0.1050 - accuracy: 0.8524 - jacard_coef: 0.46144/5 [=======================>......] - ETA: 0s - loss: 0.1068 - accuracy: 0.8379 - jacard_coef: 0.4789
Epoch 5: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 308ms/step - loss: 0.1067 - accuracy: 0.8387 - jacard_coef: 0.4911 - val_loss: 0.1628 - val_accuracy: 0.8158 - val_jacard_coef: 0.0035 - lr: 1.0000e-04
Epoch 6/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1009 - accuracy: 0.8630 - jacard_coef: 0.51492/5 [===========>..................] - ETA: 1s - loss: 0.0978 - accuracy: 0.8476 - jacard_coef: 0.54003/5 [=================>............] - ETA: 0s - loss: 0.1006 - accuracy: 0.8452 - jacard_coef: 0.50444/5 [=======================>......] - ETA: 0s - loss: 0.0995 - accuracy: 0.8478 - jacard_coef: 0.5083
Epoch 6: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 310ms/step - loss: 0.1001 - accuracy: 0.8471 - jacard_coef: 0.4287 - val_loss: 0.1598 - val_accuracy: 0.8158 - val_jacard_coef: 0.0034 - lr: 1.0000e-04
Epoch 7/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1092 - accuracy: 0.8597 - jacard_coef: 0.39872/5 [===========>..................] - ETA: 1s - loss: 0.1051 - accuracy: 0.8560 - jacard_coef: 0.44123/5 [=================>............] - ETA: 0s - loss: 0.1005 - accuracy: 0.8456 - jacard_coef: 0.49094/5 [=======================>......] - ETA: 0s - loss: 0.0993 - accuracy: 0.8493 - jacard_coef: 0.5014
Epoch 7: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 309ms/step - loss: 0.0991 - accuracy: 0.8494 - jacard_coef: 0.5301 - val_loss: 0.1578 - val_accuracy: 0.8159 - val_jacard_coef: 0.0032 - lr: 1.0000e-04
Epoch 8/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1243 - accuracy: 0.8177 - jacard_coef: 0.40972/5 [===========>..................] - ETA: 1s - loss: 0.1122 - accuracy: 0.8472 - jacard_coef: 0.46323/5 [=================>............] - ETA: 0s - loss: 0.1063 - accuracy: 0.8508 - jacard_coef: 0.49824/5 [=======================>......] - ETA: 0s - loss: 0.1037 - accuracy: 0.8482 - jacard_coef: 0.5030
Epoch 8: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 309ms/step - loss: 0.1035 - accuracy: 0.8487 - jacard_coef: 0.5217 - val_loss: 0.1550 - val_accuracy: 0.8181 - val_jacard_coef: 6.6906e-04 - lr: 1.0000e-04
Epoch 9/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0973 - accuracy: 0.8172 - jacard_coef: 0.51532/5 [===========>..................] - ETA: 1s - loss: 0.1025 - accuracy: 0.8323 - jacard_coef: 0.46273/5 [=================>............] - ETA: 0s - loss: 0.0985 - accuracy: 0.8431 - jacard_coef: 0.50094/5 [=======================>......] - ETA: 0s - loss: 0.0972 - accuracy: 0.8468 - jacard_coef: 0.5065
Epoch 9: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 308ms/step - loss: 0.0979 - accuracy: 0.8461 - jacard_coef: 0.4214 - val_loss: 0.1521 - val_accuracy: 0.8187 - val_jacard_coef: 4.5358e-05 - lr: 1.0000e-04
Epoch 10/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1026 - accuracy: 0.8261 - jacard_coef: 0.42192/5 [===========>..................] - ETA: 1s - loss: 0.0996 - accuracy: 0.8474 - jacard_coef: 0.46193/5 [=================>............] - ETA: 0s - loss: 0.0958 - accuracy: 0.8483 - jacard_coef: 0.50154/5 [=======================>......] - ETA: 0s - loss: 0.0935 - accuracy: 0.8516 - jacard_coef: 0.5196
Epoch 10: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 308ms/step - loss: 0.0938 - accuracy: 0.8523 - jacard_coef: 0.4870 - val_loss: 0.1507 - val_accuracy: 0.8187 - val_jacard_coef: 3.0376e-05 - lr: 1.0000e-04
Epoch 11/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0881 - accuracy: 0.8754 - jacard_coef: 0.58942/5 [===========>..................] - ETA: 1s - loss: 0.0868 - accuracy: 0.8506 - jacard_coef: 0.59113/5 [=================>............] - ETA: 0s - loss: 0.0901 - accuracy: 0.8504 - jacard_coef: 0.55994/5 [=======================>......] - ETA: 0s - loss: 0.0981 - accuracy: 0.8479 - jacard_coef: 0.5015
Epoch 11: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 309ms/step - loss: 0.0978 - accuracy: 0.8481 - jacard_coef: 0.5274 - val_loss: 0.1476 - val_accuracy: 0.8187 - val_jacard_coef: 1.6511e-05 - lr: 1.0000e-04
Epoch 12/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0929 - accuracy: 0.8191 - jacard_coef: 0.53112/5 [===========>..................] - ETA: 1s - loss: 0.0958 - accuracy: 0.8560 - jacard_coef: 0.51343/5 [=================>............] - ETA: 0s - loss: 0.0953 - accuracy: 0.8563 - jacard_coef: 0.52094/5 [=======================>......] - ETA: 0s - loss: 0.0956 - accuracy: 0.8531 - jacard_coef: 0.5186
Epoch 12: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 309ms/step - loss: 0.0962 - accuracy: 0.8530 - jacard_coef: 0.4473 - val_loss: 0.1460 - val_accuracy: 0.8187 - val_jacard_coef: 1.6511e-05 - lr: 1.0000e-04
Epoch 13/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0976 - accuracy: 0.8387 - jacard_coef: 0.49962/5 [===========>..................] - ETA: 1s - loss: 0.1000 - accuracy: 0.8546 - jacard_coef: 0.48283/5 [=================>............] - ETA: 0s - loss: 0.0947 - accuracy: 0.8508 - jacard_coef: 0.52074/5 [=======================>......] - ETA: 0s - loss: 0.0947 - accuracy: 0.8542 - jacard_coef: 0.5194
Epoch 13: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 310ms/step - loss: 0.0952 - accuracy: 0.8548 - jacard_coef: 0.4625 - val_loss: 0.1458 - val_accuracy: 0.8187 - val_jacard_coef: 1.6511e-05 - lr: 1.0000e-04
Epoch 14/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0849 - accuracy: 0.8720 - jacard_coef: 0.61852/5 [===========>..................] - ETA: 1s - loss: 0.0986 - accuracy: 0.8681 - jacard_coef: 0.48703/5 [=================>............] - ETA: 0s - loss: 0.0924 - accuracy: 0.8641 - jacard_coef: 0.52964/5 [=======================>......] - ETA: 0s - loss: 0.1061 - accuracy: 0.8454 - jacard_coef: 0.5023
Epoch 14: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 308ms/step - loss: 0.1058 - accuracy: 0.8462 - jacard_coef: 0.5252 - val_loss: 0.1439 - val_accuracy: 0.8187 - val_jacard_coef: 1.6511e-05 - lr: 1.0000e-04
Epoch 15/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0849 - accuracy: 0.8498 - jacard_coef: 0.58122/5 [===========>..................] - ETA: 1s - loss: 0.0889 - accuracy: 0.8491 - jacard_coef: 0.54623/5 [=================>............] - ETA: 0s - loss: 0.0873 - accuracy: 0.8517 - jacard_coef: 0.55784/5 [=======================>......] - ETA: 0s - loss: 0.0925 - accuracy: 0.8564 - jacard_coef: 0.5169
Epoch 15: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 309ms/step - loss: 0.0942 - accuracy: 0.8534 - jacard_coef: 0.4821 - val_loss: 0.1412 - val_accuracy: 0.8187 - val_jacard_coef: 1.6511e-05 - lr: 1.0000e-04
Epoch 16/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0777 - accuracy: 0.8269 - jacard_coef: 0.62552/5 [===========>..................] - ETA: 1s - loss: 0.0952 - accuracy: 0.8164 - jacard_coef: 0.52303/5 [=================>............] - ETA: 0s - loss: 0.0935 - accuracy: 0.8347 - jacard_coef: 0.53564/5 [=======================>......] - ETA: 0s - loss: 0.1025 - accuracy: 0.8325 - jacard_coef: 0.4528
Epoch 16: val_jacard_coef did not improve from 0.11046
5/5 [==============================] - 2s 310ms/step - loss: 0.1025 - accuracy: 0.8307 - jacard_coef: 0.4885 - val_loss: 0.1396 - val_accuracy: 0.8187 - val_jacard_coef: 1.4985e-12 - lr: 1.0000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 1.
âœ“ Attention UNet training completed in 0:01:07.266522
Best Val Jaccard: 0.1105

======================================================================
TRAINING MODEL 3/3: ATTENTION RESIDUAL U-NET
======================================================================
Optimized Hyperparameters:
  Learning Rate: 5e-4
  Batch Size: 16
  Expected Val Jaccard: ~0.0695

Epoch 1/100
1/5 [=====>........................] - ETA: 2:09 - loss: 0.3649 - accuracy: 0.3409 - jacard_coef: 0.08452/5 [===========>..................] - ETA: 1s - loss: 0.2948 - accuracy: 0.4246 - jacard_coef: 0.1367  3/5 [=================>............] - ETA: 0s - loss: 0.2572 - accuracy: 0.4556 - jacard_coef: 0.15244/5 [=======================>......] - ETA: 0s - loss: 0.2339 - accuracy: 0.5197 - jacard_coef: 0.18485/5 [==============================] - ETA: 0s - loss: 0.2328 - accuracy: 0.5213 - jacard_coef: 0.2283
Epoch 1: val_jacard_coef improved from -inf to 0.14268, saving model to microscope_training_20251008_074915/best_attention_resunet_model.hdf5
5/5 [==============================] - 43s 3s/step - loss: 0.2328 - accuracy: 0.5213 - jacard_coef: 0.2283 - val_loss: 0.1713 - val_accuracy: 0.4386 - val_jacard_coef: 0.1427 - lr: 5.0000e-04
Epoch 2/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1317 - accuracy: 0.7773 - jacard_coef: 0.37672/5 [===========>..................] - ETA: 1s - loss: 0.1261 - accuracy: 0.7896 - jacard_coef: 0.39633/5 [=================>............] - ETA: 0s - loss: 0.1188 - accuracy: 0.8066 - jacard_coef: 0.41294/5 [=======================>......] - ETA: 0s - loss: 0.1256 - accuracy: 0.8029 - jacard_coef: 0.4224
Epoch 2: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 342ms/step - loss: 0.1271 - accuracy: 0.8003 - jacard_coef: 0.3470 - val_loss: 0.3324 - val_accuracy: 0.0965 - val_jacard_coef: 0.1276 - lr: 5.0000e-04
Epoch 3/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1223 - accuracy: 0.8133 - jacard_coef: 0.53332/5 [===========>..................] - ETA: 1s - loss: 0.1247 - accuracy: 0.8135 - jacard_coef: 0.47913/5 [=================>............] - ETA: 0s - loss: 0.1183 - accuracy: 0.8271 - jacard_coef: 0.49464/5 [=======================>......] - ETA: 0s - loss: 0.1198 - accuracy: 0.8324 - jacard_coef: 0.4789
Epoch 3: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 342ms/step - loss: 0.1202 - accuracy: 0.8321 - jacard_coef: 0.4054 - val_loss: 0.4776 - val_accuracy: 0.0937 - val_jacard_coef: 0.1277 - lr: 5.0000e-04
Epoch 4/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1029 - accuracy: 0.8575 - jacard_coef: 0.57812/5 [===========>..................] - ETA: 1s - loss: 0.1063 - accuracy: 0.8610 - jacard_coef: 0.55703/5 [=================>............] - ETA: 0s - loss: 0.1044 - accuracy: 0.8658 - jacard_coef: 0.56794/5 [=======================>......] - ETA: 0s - loss: 0.1059 - accuracy: 0.8664 - jacard_coef: 0.55145/5 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.8668 - jacard_coef: 0.5496
Epoch 4: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 351ms/step - loss: 0.1059 - accuracy: 0.8668 - jacard_coef: 0.5496 - val_loss: 0.5741 - val_accuracy: 0.1369 - val_jacard_coef: 0.1269 - lr: 5.0000e-04
Epoch 5/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0972 - accuracy: 0.8461 - jacard_coef: 0.57812/5 [===========>..................] - ETA: 1s - loss: 0.0963 - accuracy: 0.8430 - jacard_coef: 0.59603/5 [=================>............] - ETA: 0s - loss: 0.0983 - accuracy: 0.8489 - jacard_coef: 0.56204/5 [=======================>......] - ETA: 0s - loss: 0.1051 - accuracy: 0.8529 - jacard_coef: 0.50125/5 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.8507 - jacard_coef: 0.4548
Epoch 5: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 351ms/step - loss: 0.1071 - accuracy: 0.8507 - jacard_coef: 0.4548 - val_loss: 2.8099 - val_accuracy: 0.0858 - val_jacard_coef: 0.1273 - lr: 5.0000e-04
Epoch 6/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0984 - accuracy: 0.8669 - jacard_coef: 0.53382/5 [===========>..................] - ETA: 1s - loss: 0.0980 - accuracy: 0.8606 - jacard_coef: 0.51573/5 [=================>............] - ETA: 0s - loss: 0.1042 - accuracy: 0.8443 - jacard_coef: 0.49034/5 [=======================>......] - ETA: 0s - loss: 0.1023 - accuracy: 0.8390 - jacard_coef: 0.49845/5 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.8369 - jacard_coef: 0.4013
Epoch 6: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 350ms/step - loss: 0.1034 - accuracy: 0.8369 - jacard_coef: 0.4013 - val_loss: 2.4374 - val_accuracy: 0.0847 - val_jacard_coef: 0.1273 - lr: 5.0000e-04
Epoch 7/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0979 - accuracy: 0.8484 - jacard_coef: 0.51972/5 [===========>..................] - ETA: 1s - loss: 0.1031 - accuracy: 0.8518 - jacard_coef: 0.47843/5 [=================>............] - ETA: 0s - loss: 0.1002 - accuracy: 0.8513 - jacard_coef: 0.51974/5 [=======================>......] - ETA: 0s - loss: 0.0998 - accuracy: 0.8561 - jacard_coef: 0.53095/5 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.8566 - jacard_coef: 0.4666
Epoch 7: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 351ms/step - loss: 0.1002 - accuracy: 0.8566 - jacard_coef: 0.4666 - val_loss: 1.3828 - val_accuracy: 0.0929 - val_jacard_coef: 0.1274 - lr: 5.0000e-04
Epoch 8/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0935 - accuracy: 0.8700 - jacard_coef: 0.58032/5 [===========>..................] - ETA: 1s - loss: 0.0942 - accuracy: 0.8614 - jacard_coef: 0.57293/5 [=================>............] - ETA: 0s - loss: 0.0941 - accuracy: 0.8588 - jacard_coef: 0.55824/5 [=======================>......] - ETA: 0s - loss: 0.0947 - accuracy: 0.8602 - jacard_coef: 0.54535/5 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.8575 - jacard_coef: 0.4376
Epoch 8: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 351ms/step - loss: 0.0959 - accuracy: 0.8575 - jacard_coef: 0.4376 - val_loss: 1.1644 - val_accuracy: 0.0936 - val_jacard_coef: 0.1274 - lr: 5.0000e-04
Epoch 9/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1012 - accuracy: 0.8671 - jacard_coef: 0.47092/5 [===========>..................] - ETA: 1s - loss: 0.0991 - accuracy: 0.8571 - jacard_coef: 0.48173/5 [=================>............] - ETA: 0s - loss: 0.0936 - accuracy: 0.8505 - jacard_coef: 0.53614/5 [=======================>......] - ETA: 0s - loss: 0.0953 - accuracy: 0.8564 - jacard_coef: 0.52575/5 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.8573 - jacard_coef: 0.5150
Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.

Epoch 9: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 352ms/step - loss: 0.0955 - accuracy: 0.8573 - jacard_coef: 0.5150 - val_loss: 0.9910 - val_accuracy: 0.0941 - val_jacard_coef: 0.1274 - lr: 5.0000e-04
Epoch 10/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0892 - accuracy: 0.8626 - jacard_coef: 0.57052/5 [===========>..................] - ETA: 1s - loss: 0.0970 - accuracy: 0.8690 - jacard_coef: 0.51153/5 [=================>............] - ETA: 0s - loss: 0.0961 - accuracy: 0.8510 - jacard_coef: 0.53334/5 [=======================>......] - ETA: 0s - loss: 0.0956 - accuracy: 0.8583 - jacard_coef: 0.5276
Epoch 10: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 350ms/step - loss: 0.0955 - accuracy: 0.8588 - jacard_coef: 0.5499 - val_loss: 0.7335 - val_accuracy: 0.1038 - val_jacard_coef: 0.1272 - lr: 2.5000e-04
Epoch 11/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0945 - accuracy: 0.8802 - jacard_coef: 0.51872/5 [===========>..................] - ETA: 1s - loss: 0.0962 - accuracy: 0.8822 - jacard_coef: 0.53303/5 [=================>............] - ETA: 0s - loss: 0.0950 - accuracy: 0.8690 - jacard_coef: 0.53864/5 [=======================>......] - ETA: 0s - loss: 0.0986 - accuracy: 0.8568 - jacard_coef: 0.5360
Epoch 11: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 350ms/step - loss: 0.0989 - accuracy: 0.8578 - jacard_coef: 0.4896 - val_loss: 0.6017 - val_accuracy: 0.1195 - val_jacard_coef: 0.1269 - lr: 2.5000e-04
Epoch 12/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0872 - accuracy: 0.8647 - jacard_coef: 0.54042/5 [===========>..................] - ETA: 1s - loss: 0.0946 - accuracy: 0.8650 - jacard_coef: 0.49343/5 [=================>............] - ETA: 0s - loss: 0.0934 - accuracy: 0.8507 - jacard_coef: 0.50724/5 [=======================>......] - ETA: 0s - loss: 0.0925 - accuracy: 0.8536 - jacard_coef: 0.52545/5 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.8524 - jacard_coef: 0.4237
Epoch 12: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 353ms/step - loss: 0.0935 - accuracy: 0.8524 - jacard_coef: 0.4237 - val_loss: 0.3451 - val_accuracy: 0.1568 - val_jacard_coef: 0.1267 - lr: 2.5000e-04
Epoch 13/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1053 - accuracy: 0.8715 - jacard_coef: 0.44052/5 [===========>..................] - ETA: 1s - loss: 0.1021 - accuracy: 0.8778 - jacard_coef: 0.48533/5 [=================>............] - ETA: 0s - loss: 0.1005 - accuracy: 0.8668 - jacard_coef: 0.48114/5 [=======================>......] - ETA: 0s - loss: 0.0983 - accuracy: 0.8521 - jacard_coef: 0.51045/5 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.8525 - jacard_coef: 0.5330
Epoch 13: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 351ms/step - loss: 0.0981 - accuracy: 0.8525 - jacard_coef: 0.5330 - val_loss: 0.2423 - val_accuracy: 0.2135 - val_jacard_coef: 0.1265 - lr: 2.5000e-04
Epoch 14/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0808 - accuracy: 0.8624 - jacard_coef: 0.66552/5 [===========>..................] - ETA: 1s - loss: 0.0959 - accuracy: 0.8429 - jacard_coef: 0.52923/5 [=================>............] - ETA: 0s - loss: 0.0999 - accuracy: 0.8497 - jacard_coef: 0.48204/5 [=======================>......] - ETA: 0s - loss: 0.0958 - accuracy: 0.8527 - jacard_coef: 0.5192
Epoch 14: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 351ms/step - loss: 0.0960 - accuracy: 0.8537 - jacard_coef: 0.5116 - val_loss: 0.1879 - val_accuracy: 0.3123 - val_jacard_coef: 0.1241 - lr: 2.5000e-04
Epoch 15/100
1/5 [=====>........................] - ETA: 1s - loss: 0.0911 - accuracy: 0.8740 - jacard_coef: 0.51532/5 [===========>..................] - ETA: 1s - loss: 0.0958 - accuracy: 0.8691 - jacard_coef: 0.48773/5 [=================>............] - ETA: 0s - loss: 0.0901 - accuracy: 0.8622 - jacard_coef: 0.54374/5 [=======================>......] - ETA: 0s - loss: 0.0919 - accuracy: 0.8531 - jacard_coef: 0.52085/5 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.8527 - jacard_coef: 0.4202
Epoch 15: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 350ms/step - loss: 0.0929 - accuracy: 0.8527 - jacard_coef: 0.4202 - val_loss: 0.1670 - val_accuracy: 0.4907 - val_jacard_coef: 0.1203 - lr: 2.5000e-04
Epoch 16/100
1/5 [=====>........................] - ETA: 1s - loss: 0.1042 - accuracy: 0.8971 - jacard_coef: 0.43362/5 [===========>..................] - ETA: 1s - loss: 0.0932 - accuracy: 0.8674 - jacard_coef: 0.50633/5 [=================>............] - ETA: 0s - loss: 0.0916 - accuracy: 0.8577 - jacard_coef: 0.50464/5 [=======================>......] - ETA: 0s - loss: 0.0899 - accuracy: 0.8580 - jacard_coef: 0.52305/5 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.8585 - jacard_coef: 0.5421
Epoch 16: val_jacard_coef did not improve from 0.14268
5/5 [==============================] - 2s 351ms/step - loss: 0.0898 - accuracy: 0.8585 - jacard_coef: 0.5421 - val_loss: 0.1537 - val_accuracy: 0.8187 - val_jacard_coef: 2.2463e-04 - lr: 2.5000e-04
Epoch 16: early stopping
Restoring model weights from the end of the best epoch: 1.
âœ“ Attention ResUNet training completed in 0:01:10.186647
Best Val Jaccard: 0.1427

======================================================================
GENERATING VISUALIZATIONS
======================================================================
âœ“ Saved training curves: microscope_training_20251008_074915/training_curves_comparison.png
âœ“ Saved performance summary: microscope_training_20251008_074915/performance_summary.png

======================================================================
TRAINING COMPLETE - SUMMARY REPORT
======================================================================

            model     lr  batch_size  best_val_jacard  training_time
             UNet 0.0010           8         0.127539 0:01:24.897692
   Attention_UNet 0.0001          16         0.110458 0:01:07.266522
Attention_ResUNet 0.0005          16         0.142679 0:01:10.186647

Output Files:
  Directory: microscope_training_20251008_074915/
  - Model files: best_*.hdf5, final_*.hdf5
  - Training histories: *_history.csv
  - Visualizations: training_curves_comparison.png, performance_summary.png

ðŸ† BEST MODEL:
  Architecture: Attention_ResUNet
  Learning Rate: 5e-04
  Batch Size: 16
  Best Val Jaccard: 0.1427
  Training Time: 0:01:10.186647

======================================================================
All training completed successfully!
======================================================================

=======================================================================
OPTIMIZED MICROSCOPE TRAINING COMPLETED
=======================================================================
Job finished on Wed Oct  8 03:53:19 PM +08 2025
Exit code: 0 âœ“ SUCCESS

âœ“ Training completed successfully!

Output directory not found. Results may be in current directory.
Looking for generated files...
total 2.9G
-rw------- 1 phyzxi svuusers 2.0K Oct  8 15:53 attention_resunet_history.csv
-rw------- 1 phyzxi svuusers 2.1K Oct  8 15:51 attention_unet_history.csv
-rw------- 1 phyzxi svuusers 448M Oct  8 15:52 best_attention_resunet_model.hdf5
-rw------- 1 phyzxi svuusers 428M Oct  8 15:51 best_attention_unet_model.hdf5
-rw------- 1 phyzxi svuusers 360M Oct  8 15:50 best_unet_model.hdf5
-rw------- 1 phyzxi svuusers 448M Oct  8 15:53 final_attention_resunet_model.hdf5
-rw------- 1 phyzxi svuusers 428M Oct  8 15:51 final_attention_unet_model.hdf5
-rw------- 1 phyzxi svuusers 360M Oct  8 15:50 final_unet_model.hdf5
-rw------- 1 phyzxi svuusers 140K Oct  8 15:53 performance_summary.png
-rw------- 1 phyzxi svuusers 518K Oct  8 15:53 training_curves_comparison.png
-rw------- 1 phyzxi svuusers 2.4K Oct  8 15:50 unet_history.csv

ðŸ“ CONSOLE LOG: microscope_training_20251008_154844.log

=======================================
MICROSCOPE OPTIMIZED TRAINING COMPLETE
Dataset: dataset_microscope/
Models: UNet, Attention UNet, Attention ResUNet
Optimized Hyperparameters Applied
=======================================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-08 15:53:20.177040:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 278628.stdct-mgmt-02
	Project: personal-phyzxi
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(36), Used(36)
	CPU Time Used: 00:05:23
	Memory: Requested(240gb), Used(9889988kb)
	Vmem Used: 75423816kb
	Walltime: Requested(24:00:00), Used(00:05:35)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (GN-A40-076[0]:ncpus=36:ngpus=1:mem=251658240kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	No GPU-related information available for this job.
