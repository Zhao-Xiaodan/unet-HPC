=======================================================================
CONVNEXT-UNET DEDICATED TRAINING - MITOCHONDRIA SEGMENTATION
=======================================================================
Model: ConvNeXt-UNet (Modern CNN with improved efficiency)
Task: Mitochondria semantic segmentation
Framework: TensorFlow/Keras with enhanced dataset management
Expected Training Time: 8-12 hours (optimized)

Job started on Thu Oct  2 01:26:45 PM +08 2025
Running on node: GN-A40-102
Job ID: 239689.stdct-mgmt-02
Available GPUs: GPU-d9b88488-5332-9c08-bdb5-4fe9ea40587c
Memory: 503Gi, CPUs: 36

=== CONVNEXT-UNET TRAINING CONFIGURATION ===
Dataset Images: ./dataset_full_stack/images/ (1980 patches - REQUIRED)
Dataset Masks: ./dataset_full_stack/masks/ (1980 patches - REQUIRED)
Alternative: ./dataset/images/ and ./dataset/masks/
Image Size: 256x256x3
Batch Size: 6 (optimized for faster training)
Learning Rate: 2e-4 (Adam optimizer, optimized)
Epochs: 80 (with early stopping, optimized)
Loss Function: Binary Focal Loss
Special Features: Enhanced dataset cache management + TF compatibility fixes
==============================================

TensorFlow Container: /app1/common/singularity-img/hopper/tensorflow/tensorflow_2.16.1-cuda_12.5.0_24.06.sif

=== AGGRESSIVE CACHE CLEARING ===
Performing comprehensive cache clearing to prevent dataset conflicts...
Clearing TensorFlow dataset caches...
Clearing Python cache...
Clearing previous model checkpoints...
Unique session ID: convnext_1759382806_1835693
âœ“ Aggressive cache clearing completed
==================================

=== PRE-EXECUTION CHECKS ===
1. Checking dataset structure...
   âœ“ Full stack dataset directories found (PREFERRED)
   âœ“ Images found: 1980 files
   âœ“ Masks found: 1980 files

2. Checking Python files...
   âœ“ convnext_unet_training.py found
   âœ“ modern_unet_models.py found
==========================

=== TENSORFLOW & GPU STATUS ===
Python version: 3.10.12
TensorFlow version: 2.16.1
CUDA built support: True
Physical GPUs found: 1
  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
âœ“ GPU memory growth enabled
âœ“ GPU operation test successful

ConvNeXt-UNet memory requirements:
- Expected GPU memory: 8-12 GB
- Batch size: 4 (optimized)
- Model parameters: ~15-25M
===============================

=== CONVNEXT-UNET MODEL VALIDATION ===
Testing ConvNeXt-UNet creation to validate implementation...
2025-10-02 05:27:01.118542: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
Testing ConvNeXt-UNet model creation...
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ ConvNeXt-UNet: 34,590,913 parameters
âœ“ Model building successful
âœ“ Forward pass successful: (1, 64, 64, 1)
âœ“ ConvNeXt-UNet validation completed successfully
âœ“ ConvNeXt-UNet validation passed
=====================================

ðŸš€ STARTING CONVNEXT-UNET TRAINING
=============================================
Training ConvNeXt-UNet with enhanced dataset management

Training Configuration:
- Architecture: ConvNeXt-UNet (Modern CNN)
- Learning Rate: 1e-4 (Adam optimizer)
- Batch Size: 4 (memory optimized)
- Max Epochs: 100 (with early stopping)
- Loss: Binary Focal Loss (gamma=2)
- Special: No tf.data.Dataset caching

Expected timeline: 8-12 hours (optimized + compatibility fixes)
Expected performance: 93-95% Jaccard
Recent fixes: TensorFlow compatibility issues resolved
=============================================
Output directory: convnext_unet_training_20251002_132711

Starting ConvNeXt-UNet training execution...
======================================================================
CONVNEXT-UNET DEDICATED TRAINING FOR MITOCHONDRIA SEGMENTATION
======================================================================
Model: ConvNeXt-UNet (Modern CNN with improved efficiency)
Task: Mitochondria semantic segmentation
Framework: TensorFlow/Keras with enhanced dataset management

âœ“ GPU memory growth enabled for 1 GPUs
Output directory: convnext_unet_training_20251002_052719
=== LOADING DATASET FOR CONVNEXT-UNET ===
Using dataset: dataset_full_stack/images/ and dataset_full_stack/masks/
âœ“ Loaded 1980 images and 1980 masks
Training set: 1782 samples
Validation set: 198 samples

Training Configuration:
  Learning Rate: 0.0002
  Batch Size: 6
  Max Epochs: 80
  Input Shape: (256, 256, 3)

============================================================
TRAINING: ConvNeXt-UNet (Dedicated)
Learning Rate: 0.0002, Batch Size: 6, Max Epochs: 80
TensorFlow Version: 2.16.1
============================================================
ðŸ§¹ Performing aggressive cache clearing for ConvNeXt-UNet...
âœ“ Aggressive cache clearing completed: 3 items cleared
âœ“ Unique session ID: 50a34c97
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
Creating ConvNeXt-UNet model...
Model parameters: 34,590,913
âœ“ Loss scaling optimizer enabled for mixed precision
Starting ConvNeXt-UNet training...
Applying ConvNeXt-specific optimizations...
âœ“ TF32 enabled
âœ“ Mixed precision (float16) enabled
âœ“ XLA JIT compilation enabled
Epoch 1/80
WARNING:tensorflow:AutoGraph could not transform <function create_autocast_variable at 0x14fe5f9f27a0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: <gast.gast.Expr object at 0x14fa46348f70>
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1759382904.180021 1836613 service.cc:145] XLA service 0x14faa4013fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1759382904.180059 1836613 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1759382917.869798 1836613 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_81__6', 168 bytes spill stores, 168 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_74__6', 168 bytes spill stores, 168 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_69__6', 172 bytes spill stores, 172 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_66__8', 336 bytes spill stores, 336 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_62__4', 40 bytes spill stores, 40 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_reduce_transpose_fusion_2__1', 16 bytes spill stores, 16 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_60__6', 172 bytes spill stores, 172 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_55__6', 172 bytes spill stores, 172 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_50__6', 180 bytes spill stores, 180 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_47__6', 168 bytes spill stores, 168 bytes spill loads

I0000 00:00:1759382917.907171 1836613 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  1/297 [..............................] - ETA: 5:55:42 - loss: 0.2013 - accuracy: 0.4916 - jacard_coef: 0.0653  2/297 [..............................] - ETA: 17s - loss: 0.1740 - accuracy: 0.6023 - jacard_coef: 0.0650      3/297 [..............................] - ETA: 16s - loss: 0.1528 - accuracy: 0.6925 - jacard_coef: 0.0551  4/297 [..............................] - ETA: 16s - loss: 0.1336 - accuracy: 0.7575 - jacard_coef: 0.0434  5/297 [..............................] - ETA: 15s - loss: 0.1230 - accuracy: 0.7904 - jacard_coef: 0.0347  7/297 [..............................] - ETA: 15s - loss: 0.1068 - accuracy: 0.8346 - jacard_coef: 0.0248  9/297 [..............................] - ETA: 14s - loss: 0.0986 - accuracy: 0.8587 - jacard_coef: 0.0193 11/297 [>.............................] - ETA: 14s - loss: 0.0939 - accuracy: 0.8733 - jacard_coef: 0.0158 13/297 [>.............................] - ETA: 14s - loss: 0.0888 - accuracy: 0.8848 - jacard_coef: 0.0133 15/297 [>.............................] - ETA: 14s - loss: 0.0852 - accuracy: 0.8928 - jacard_coef: 0.0116 17/297 [>.............................] - ETA: 14s - loss: 0.0813 - accuracy: 0.9004 - jacard_coef: 0.0102 19/297 [>.............................] - ETA: 13s - loss: 0.0800 - accuracy: 0.9038 - jacard_coef: 0.0091 21/297 [=>............................] - ETA: 13s - loss: 0.0791 - accuracy: 0.9065 - jacard_coef: 0.0083 23/297 [=>............................] - ETA: 13s - loss: 0.0786 - accuracy: 0.9083 - jacard_coef: 0.0075 25/297 [=>............................] - ETA: 13s - loss: 0.0769 - accuracy: 0.9116 - jacard_coef: 0.0069 27/297 [=>............................] - ETA: 13s - loss: 0.0756 - accuracy: 0.9142 - jacard_coef: 0.0064 29/297 [=>............................] - ETA: 13s - loss: 0.0736 - accuracy: 0.9176 - jacard_coef: 0.0060 31/297 [==>...........................] - ETA: 13s - loss: 0.0727 - accuracy: 0.9194 - jacard_coef: 0.0056 33/297 [==>...........................] - ETA: 13s - loss: 0.0722 - accuracy: 0.9207 - jacard_coef: 0.0053 35/297 [==>...........................] - ETA: 12s - loss: 0.0707 - accuracy: 0.9231 - jacard_coef: 0.0050 37/297 [==>...........................] - ETA: 12s - loss: 0.0699 - accuracy: 0.9246 - jacard_coef: 0.0047 39/297 [==>...........................] - ETA: 12s - loss: 0.0695 - accuracy: 0.9255 - jacard_coef: 0.0044 41/297 [===>..........................] - ETA: 12s - loss: 0.0691 - accuracy: 0.9264 - jacard_coef: 0.0042 43/297 [===>..........................] - ETA: 12s - loss: 0.0685 - accuracy: 0.9275 - jacard_coef: 0.0040 45/297 [===>..........................] - ETA: 12s - loss: 0.0679 - accuracy: 0.9286 - jacard_coef: 0.0039 47/297 [===>..........................] - ETA: 12s - loss: 0.0671 - accuracy: 0.9299 - jacard_coef: 0.0037 49/297 [===>..........................] - ETA: 12s - loss: 0.0661 - accuracy: 0.9314 - jacard_coef: 0.0035 51/297 [====>.........................] - ETA: 12s - loss: 0.0667 - accuracy: 0.9309 - jacard_coef: 0.0034 53/297 [====>.........................] - ETA: 12s - loss: 0.0668 - accuracy: 0.9310 - jacard_coef: 0.0033 55/297 [====>.........................] - ETA: 11s - loss: 0.0672 - accuracy: 0.9306 - jacard_coef: 0.0032 57/297 [====>.........................] - ETA: 11s - loss: 0.0676 - accuracy: 0.9301 - jacard_coef: 0.0030 59/297 [====>.........................] - ETA: 11s - loss: 0.0673 - accuracy: 0.9309 - jacard_coef: 0.0029 61/297 [=====>........................] - ETA: 11s - loss: 0.0673 - accuracy: 0.9311 - jacard_coef: 0.0028 63/297 [=====>........................] - ETA: 11s - loss: 0.0675 - accuracy: 0.9309 - jacard_coef: 0.0028 65/297 [=====>........................] - ETA: 11s - loss: 0.0672 - accuracy: 0.9313 - jacard_coef: 0.0027 67/297 [=====>........................] - ETA: 11s - loss: 0.0668 - accuracy: 0.9320 - jacard_coef: 0.0026 69/297 [=====>........................] - ETA: 11s - loss: 0.0663 - accuracy: 0.9328 - jacard_coef: 0.0025 71/297 [======>.......................] - ETA: 11s - loss: 0.0660 - accuracy: 0.9333 - jacard_coef: 0.0024 73/297 [======>.......................] - ETA: 11s - loss: 0.0656 - accuracy: 0.9339 - jacard_coef: 0.0024 75/297 [======>.......................] - ETA: 10s - loss: 0.0651 - accuracy: 0.9347 - jacard_coef: 0.0023 77/297 [======>.......................] - ETA: 10s - loss: 0.0648 - accuracy: 0.9351 - jacard_coef: 0.0023 79/297 [======>.......................] - ETA: 10s - loss: 0.0646 - accuracy: 0.9355 - jacard_coef: 0.0022 81/297 [=======>......................] - ETA: 10s - loss: 0.0644 - accuracy: 0.9359 - jacard_coef: 0.0021 83/297 [=======>......................] - ETA: 10s - loss: 0.0647 - accuracy: 0.9356 - jacard_coef: 0.0021 85/297 [=======>......................] - ETA: 10s - loss: 0.0640 - accuracy: 0.9366 - jacard_coef: 0.0020 87/297 [=======>......................] - ETA: 10s - loss: 0.0641 - accuracy: 0.9365 - jacard_coef: 0.0020 89/297 [=======>......................] - ETA: 10s - loss: 0.0639 - accuracy: 0.9368 - jacard_coef: 0.0019 91/297 [========>.....................] - ETA: 10s - loss: 0.0640 - accuracy: 0.9368 - jacard_coef: 0.0019 93/297 [========>.....................] - ETA: 10s - loss: 0.0636 - accuracy: 0.9373 - jacard_coef: 0.0019 95/297 [========>.....................] - ETA: 9s - loss: 0.0642 - accuracy: 0.9367 - jacard_coef: 0.0018  97/297 [========>.....................] - ETA: 9s - loss: 0.0649 - accuracy: 0.9357 - jacard_coef: 0.0018 99/297 [=========>....................] - ETA: 9s - loss: 0.0649 - accuracy: 0.9357 - jacard_coef: 0.0018101/297 [=========>....................] - ETA: 9s - loss: 0.0650 - accuracy: 0.9357 - jacard_coef: 0.0017103/297 [=========>....................] - ETA: 9s - loss: 0.0649 - accuracy: 0.9359 - jacard_coef: 0.0017105/297 [=========>....................] - ETA: 9s - loss: 0.0650 - accuracy: 0.9358 - jacard_coef: 0.0017107/297 [=========>....................] - ETA: 9s - loss: 0.0648 - accuracy: 0.9362 - jacard_coef: 0.0016109/297 [==========>...................] - ETA: 9s - loss: 0.0649 - accuracy: 0.9361 - jacard_coef: 0.0016111/297 [==========>...................] - ETA: 9s - loss: 0.0651 - accuracy: 0.9358 - jacard_coef: 0.0016113/297 [==========>...................] - ETA: 9s - loss: 0.0651 - accuracy: 0.9359 - jacard_coef: 0.0015115/297 [==========>...................] - ETA: 8s - loss: 0.0652 - accuracy: 0.9358 - jacard_coef: 0.0015117/297 [==========>...................] - ETA: 8s - loss: 0.0652 - accuracy: 0.9359 - jacard_coef: 0.0015119/297 [===========>..................] - ETA: 8s - loss: 0.0649 - accuracy: 0.9363 - jacard_coef: 0.0015121/297 [===========>..................] - ETA: 8s - loss: 0.0652 - accuracy: 0.9358 - jacard_coef: 0.0014123/297 [===========>..................] - ETA: 8s - loss: 0.0653 - accuracy: 0.9357 - jacard_coef: 0.0014125/297 [===========>..................] - ETA: 8s - loss: 0.0651 - accuracy: 0.9361 - jacard_coef: 0.0014127/297 [===========>..................] - ETA: 8s - loss: 0.0647 - accuracy: 0.9366 - jacard_coef: 0.0014129/297 [============>.................] - ETA: 8s - loss: 0.0646 - accuracy: 0.9368 - jacard_coef: 0.0013131/297 [============>.................] - ETA: 8s - loss: 0.0646 - accuracy: 0.9368 - jacard_coef: 0.0013133/297 [============>.................] - ETA: 8s - loss: 0.0647 - accuracy: 0.9367 - jacard_coef: 0.0013135/297 [============>.................] - ETA: 7s - loss: 0.0646 - accuracy: 0.9369 - jacard_coef: 0.0013137/297 [============>.................] - ETA: 7s - loss: 0.0642 - accuracy: 0.9375 - jacard_coef: 0.0013139/297 [=============>................] - ETA: 7s - loss: 0.0641 - accuracy: 0.9376 - jacard_coef: 0.0012141/297 [=============>................] - ETA: 7s - loss: 0.0639 - accuracy: 0.9378 - jacard_coef: 0.0012143/297 [=============>................] - ETA: 7s - loss: 0.0638 - accuracy: 0.9381 - jacard_coef: 0.0012145/297 [=============>................] - ETA: 7s - loss: 0.0635 - accuracy: 0.9384 - jacard_coef: 0.0012147/297 [=============>................] - ETA: 7s - loss: 0.0633 - accuracy: 0.9387 - jacard_coef: 0.0012149/297 [==============>...............] - ETA: 7s - loss: 0.0632 - accuracy: 0.9389 - jacard_coef: 0.0012151/297 [==============>...............] - ETA: 7s - loss: 0.0633 - accuracy: 0.9388 - jacard_coef: 0.0011153/297 [==============>...............] - ETA: 7s - loss: 0.0634 - accuracy: 0.9387 - jacard_coef: 0.0011155/297 [==============>...............] - ETA: 6s - loss: 0.0632 - accuracy: 0.9389 - jacard_coef: 0.0011157/297 [==============>...............] - ETA: 6s - loss: 0.0634 - accuracy: 0.9386 - jacard_coef: 0.0011159/297 [===============>..............] - ETA: 6s - loss: 0.0635 - accuracy: 0.9385 - jacard_coef: 0.0011161/297 [===============>..............] - ETA: 6s - loss: 0.0637 - accuracy: 0.9382 - jacard_coef: 0.0011163/297 [===============>..............] - ETA: 6s - loss: 0.0637 - accuracy: 0.9381 - jacard_coef: 0.0011165/297 [===============>..............] - ETA: 6s - loss: 0.0637 - accuracy: 0.9382 - jacard_coef: 0.0011167/297 [===============>..............] - ETA: 6s - loss: 0.0635 - accuracy: 0.9386 - jacard_coef: 0.0010169/297 [================>.............] - ETA: 6s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 0.0010171/297 [================>.............] - ETA: 6s - loss: 0.0634 - accuracy: 0.9388 - jacard_coef: 0.0010173/297 [================>.............] - ETA: 6s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 0.0010175/297 [================>.............] - ETA: 5s - loss: 0.0632 - accuracy: 0.9391 - jacard_coef: 9.9090e-04177/297 [================>.............] - ETA: 5s - loss: 0.0632 - accuracy: 0.9391 - jacard_coef: 9.7971e-04179/297 [=================>............] - ETA: 5s - loss: 0.0630 - accuracy: 0.9393 - jacard_coef: 9.6876e-04181/297 [=================>............] - ETA: 5s - loss: 0.0631 - accuracy: 0.9392 - jacard_coef: 9.5806e-04183/297 [=================>............] - ETA: 5s - loss: 0.0631 - accuracy: 0.9393 - jacard_coef: 9.4758e-04185/297 [=================>............] - ETA: 5s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 9.3734e-04187/297 [=================>............] - ETA: 5s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 9.2732e-04189/297 [==================>...........] - ETA: 5s - loss: 0.0633 - accuracy: 0.9390 - jacard_coef: 9.1750e-04191/297 [==================>...........] - ETA: 5s - loss: 0.0632 - accuracy: 0.9391 - jacard_coef: 9.0790e-04193/297 [==================>...........] - ETA: 5s - loss: 0.0632 - accuracy: 0.9392 - jacard_coef: 8.9849e-04195/297 [==================>...........] - ETA: 4s - loss: 0.0632 - accuracy: 0.9391 - jacard_coef: 8.8927e-04197/297 [==================>...........] - ETA: 4s - loss: 0.0631 - accuracy: 0.9393 - jacard_coef: 8.8024e-04199/297 [===================>..........] - ETA: 4s - loss: 0.0632 - accuracy: 0.9391 - jacard_coef: 8.7140e-04201/297 [===================>..........] - ETA: 4s - loss: 0.0632 - accuracy: 0.9392 - jacard_coef: 8.6273e-04203/297 [===================>..........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9390 - jacard_coef: 8.5423e-04205/297 [===================>..........] - ETA: 4s - loss: 0.0632 - accuracy: 0.9391 - jacard_coef: 8.4589e-04207/297 [===================>..........] - ETA: 4s - loss: 0.0634 - accuracy: 0.9389 - jacard_coef: 8.3772e-04209/297 [====================>.........] - ETA: 4s - loss: 0.0634 - accuracy: 0.9389 - jacard_coef: 8.2970e-04211/297 [====================>.........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 8.2184e-04213/297 [====================>.........] - ETA: 4s - loss: 0.0632 - accuracy: 0.9390 - jacard_coef: 8.1412e-04215/297 [====================>.........] - ETA: 4s - loss: 0.0634 - accuracy: 0.9388 - jacard_coef: 8.0655e-04217/297 [====================>.........] - ETA: 3s - loss: 0.0633 - accuracy: 0.9390 - jacard_coef: 7.9912e-04219/297 [=====================>........] - ETA: 3s - loss: 0.0633 - accuracy: 0.9390 - jacard_coef: 7.9182e-04221/297 [=====================>........] - ETA: 3s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 7.8465e-04223/297 [=====================>........] - ETA: 3s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 7.7761e-04225/297 [=====================>........] - ETA: 3s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 7.7070e-04227/297 [=====================>........] - ETA: 3s - loss: 0.0634 - accuracy: 0.9388 - jacard_coef: 7.6391e-04229/297 [======================>.......] - ETA: 3s - loss: 0.0634 - accuracy: 0.9388 - jacard_coef: 7.5724e-04231/297 [======================>.......] - ETA: 3s - loss: 0.0632 - accuracy: 0.9390 - jacard_coef: 7.5068e-04233/297 [======================>.......] - ETA: 3s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 7.4424e-04235/297 [======================>.......] - ETA: 3s - loss: 0.0633 - accuracy: 0.9389 - jacard_coef: 7.3791e-04237/297 [======================>.......] - ETA: 2s - loss: 0.0632 - accuracy: 0.9390 - jacard_coef: 7.3168e-04239/297 [=======================>......] - ETA: 2s - loss: 0.0632 - accuracy: 0.9389 - jacard_coef: 7.2556e-04241/297 [=======================>......] - ETA: 2s - loss: 0.0631 - accuracy: 0.9391 - jacard_coef: 7.1954e-04243/297 [=======================>......] - ETA: 2s - loss: 0.0631 - accuracy: 0.9390 - jacard_coef: 7.1361e-04245/297 [=======================>......] - ETA: 2s - loss: 0.0630 - accuracy: 0.9392 - jacard_coef: 7.0779e-04247/297 [=======================>......] - ETA: 2s - loss: 0.0630 - accuracy: 0.9392 - jacard_coef: 7.0206e-04249/297 [========================>.....] - ETA: 2s - loss: 0.0627 - accuracy: 0.9395 - jacard_coef: 6.9642e-04251/297 [========================>.....] - ETA: 2s - loss: 0.0628 - accuracy: 0.9395 - jacard_coef: 6.9087e-04253/297 [========================>.....] - ETA: 2s - loss: 0.0627 - accuracy: 0.9396 - jacard_coef: 6.8541e-04255/297 [========================>.....] - ETA: 2s - loss: 0.0626 - accuracy: 0.9397 - jacard_coef: 6.8003e-04257/297 [========================>.....] - ETA: 1s - loss: 0.0624 - accuracy: 0.9399 - jacard_coef: 6.7474e-04259/297 [=========================>....] - ETA: 1s - loss: 0.0624 - accuracy: 0.9399 - jacard_coef: 6.6953e-04261/297 [=========================>....] - ETA: 1s - loss: 0.0623 - accuracy: 0.9399 - jacard_coef: 6.6440e-04263/297 [=========================>....] - ETA: 1s - loss: 0.0622 - accuracy: 0.9401 - jacard_coef: 6.5935e-04265/297 [=========================>....] - ETA: 1s - loss: 0.0622 - accuracy: 0.9401 - jacard_coef: 6.5437e-04267/297 [=========================>....] - ETA: 1s - loss: 0.0621 - accuracy: 0.9401 - jacard_coef: 6.4947e-04269/297 [==========================>...] - ETA: 1s - loss: 0.0621 - accuracy: 0.9402 - jacard_coef: 6.4464e-04271/297 [==========================>...] - ETA: 1s - loss: 0.0619 - accuracy: 0.9404 - jacard_coef: 6.3988e-04273/297 [==========================>...] - ETA: 1s - loss: 0.0619 - accuracy: 0.9404 - jacard_coef: 6.3519e-04275/297 [==========================>...] - ETA: 1s - loss: 0.0619 - accuracy: 0.9404 - jacard_coef: 6.3057e-04277/297 [==========================>...] - ETA: 0s - loss: 0.0618 - accuracy: 0.9405 - jacard_coef: 6.2625e-04279/297 [===========================>..] - ETA: 0s - loss: 0.0618 - accuracy: 0.9405 - jacard_coef: 6.2176e-04281/297 [===========================>..] - ETA: 0s - loss: 0.0618 - accuracy: 0.9405 - jacard_coef: 6.1733e-04283/297 [===========================>..] - ETA: 0s - loss: 0.0619 - accuracy: 0.9403 - jacard_coef: 6.1334e-04285/297 [===========================>..] - ETA: 0s - loss: 0.0619 - accuracy: 0.9403 - jacard_coef: 6.0904e-04287/297 [===========================>..] - ETA: 0s - loss: 0.0617 - accuracy: 0.9404 - jacard_coef: 6.0479e-04289/297 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9406 - jacard_coef: 6.0061e-04291/297 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9407 - jacard_coef: 5.9648e-04293/297 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9408 - jacard_coef: 5.9241e-04295/297 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9408 - jacard_coef: 5.8839e-04297/297 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9408 - jacard_coef: 5.8952e-04
Epoch 1: val_jacard_coef improved from -inf to 0.00004, saving model to convnext_unet_training_20251002_052719/ConvNeXt_UNet_lr0.0002_bs6_50a34c97_model.hdf5
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
Traceback (most recent call last):
  File "/scratch/phyzxi/unet-HPC/convnext_unet_training.py", line 357, in train_convnext_unet
    history = model.fit(
  File "/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py", line 183, in create_dataset
    dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)
  File "/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py", line 163, in make_new_dset
    dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5d.pyx", line 137, in h5py.h5d.create
ValueError: Unable to synchronously create dataset (name already exists)

âœ— ConvNeXt-UNet training failed: Unable to synchronously create dataset (name already exists)
ðŸ§¹ Performing aggressive cache clearing for ConvNeXt-UNet...
âœ“ Aggressive cache clearing completed: 3 items cleared
âœ“ Unique session ID: 77991edb

âœ— ConvNeXt-UNet training failed!

======================================================================
CONVNEXT-UNET DEDICATED TRAINING COMPLETED
======================================================================

=======================================================================
CONVNEXT-UNET TRAINING COMPLETED
=======================================================================
Job finished on Thu Oct  2 01:29:18 PM +08 2025
Exit code: 0 âœ“ SUCCESS

âœ“ ConvNeXt-UNet training completed successfully!

Generated files:
ðŸ“ Training directory:
convnext_unet_training_20251001_153225/:
total 64
drwxr-x---  2 phyzxi svuusers    0 Oct  1 23:53 .
drwxr-xr-x 31 phyzxi svuusers 5997 Oct  2 13:27 ..

convnext_unet_training_20251001_155440/:
total 64
drwxr-x---  2 phyzxi svuusers    0 Oct  2 11:33 .
drwxr-xr-x 31 phyzxi svuusers 5997 Oct  2 13:27 ..

convnext_unet_training_20251002_033437/:
total 64
drwxr-x---  2 phyzxi svuusers    0 Oct  2 11:34 .
drwxr-xr-x 31 phyzxi svuusers 5997 Oct  2 13:27 ..

convnext_unet_training_20251002_052719/:
total 170672
drwxr-x---  2 phyzxi svuusers        64 Oct  2 13:29 .
drwxr-xr-x 31 phyzxi svuusers      5997 Oct  2 13:27 ..
-rw-r-----  1 phyzxi svuusers 139447040 Oct  2 13:29 ConvNeXt_UNet_lr0.0002_bs6_50a34c97_model.hdf5

ðŸ“Š Model and results:
-rw-r----- 1 phyzxi svuusers 139447040 Oct  2 13:29 convnext_unet_training_20251002_052719/ConvNeXt_UNet_lr0.0002_bs6_50a34c97_model.hdf5

ðŸŽ¯ CONVNEXT-UNET PERFORMANCE SUMMARY:
======================================
No ConvNeXt-UNet results found.

ðŸ“ CONSOLE LOG SAVED: convnext_unet_training_20251002_132711_console.log

ðŸ”— NEXT STEPS:
=============
1. Analyze ConvNeXt-UNet training results
2. Compare with Swin-UNet performance (93.57%)
3. Train CoAtNet-UNet separately if needed
4. Consider ensemble methods for best performance

=========================================
CONVNEXT-UNET TRAINING JOB COMPLETE
Model: ConvNeXt-UNet (Modern CNN)
Framework: TensorFlow/Keras + Enhanced Dataset Management
========================================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-02 13:29:20.285911:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 239689.stdct-mgmt-02
	Project: personal-phyzxi
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(36), Used(36)
	CPU Time Used: 00:02:53
	Memory: Requested(240gb), Used(14884056kb)
	Vmem Used: 43848996kb
	Walltime: Requested(18:00:00), Used(00:02:35)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (GN-A40-102[0]:ncpus=36:ngpus=1:mem=251658240kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	No GPU-related information available for this job.
