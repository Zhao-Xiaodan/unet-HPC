=======================================================================
U-NET HYPERPARAMETER OPTIMIZATION - GRID SEARCH
=======================================================================
Testing Learning Rates: [1e-4, 5e-4, 1e-3, 5e-3]
Testing Batch Sizes: [8, 16, 32]
Testing Architectures: [UNet, Attention_UNet, Attention_ResUNet]
Epochs per experiment: 30 (reduced for grid search)
Total experiments: 4 LR Ã— 3 BS Ã— 3 Arch = 36 experiments
Estimated total time: 36-48 hours

Job started on Fri Sep 26 12:37:42 PM +08 2025
Running on node: GN-A40-097
Job ID: 229134.stdct-mgmt-02
Available GPUs: GPU-33961ece-d144-6477-790d-f2e9b0695a10
Memory: 503Gi, CPUs: 36

TensorFlow Container: /app1/common/singularity-img/hopper/tensorflow/tensorflow_2.16.1-cuda_12.5.0_24.06.sif

Main results directory: hyperparameter_optimization_20250926_123742

Creating hyperparameter training script...
âœ“ Hyperparameter training script created

ðŸš€ STARTING HYPERPARAMETER GRID SEARCH
======================================
Total experiments to run: 36


ðŸ”¬ EXPERIMENT 1/36
================================================
Architecture: UNet
Learning Rate: 1e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758861534.496847 3175547 service.cc:145] XLA service 0x147055ca9170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758861534.496916 3175547 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758861535.011625 3175547 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 11:22 - loss: 0.3500 - accuracy: 0.5060 - jacard_coef: 0.0629 2/17 [==>...........................] - ETA: 56s - loss: 0.3504 - accuracy: 0.5342 - jacard_coef: 0.0707   3/17 [====>.........................] - ETA: 34s - loss: 0.3368 - accuracy: 0.5475 - jacard_coef: 0.0684 4/17 [======>.......................] - ETA: 25s - loss: 0.3214 - accuracy: 0.5731 - jacard_coef: 0.0650 5/17 [=======>......................] - ETA: 18s - loss: 0.3046 - accuracy: 0.5904 - jacard_coef: 0.0733 6/17 [=========>....................] - ETA: 13s - loss: 0.2908 - accuracy: 0.6225 - jacard_coef: 0.0692 7/17 [===========>..................] - ETA: 10s - loss: 0.2882 - accuracy: 0.6484 - jacard_coef: 0.0681 8/17 [=============>................] - ETA: 8s - loss: 0.2818 - accuracy: 0.6644 - jacard_coef: 0.0703  9/17 [==============>...............] - ETA: 6s - loss: 0.2741 - accuracy: 0.6762 - jacard_coef: 0.070910/17 [================>.............] - ETA: 5s - loss: 0.2683 - accuracy: 0.6721 - jacard_coef: 0.071811/17 [==================>...........] - ETA: 4s - loss: 0.2649 - accuracy: 0.6760 - jacard_coef: 0.072512/17 [====================>.........] - ETA: 3s - loss: 0.2613 - accuracy: 0.6716 - jacard_coef: 0.073913/17 [=====================>........] - ETA: 2s - loss: 0.2578 - accuracy: 0.6696 - jacard_coef: 0.073814/17 [=======================>......] - ETA: 1s - loss: 0.2541 - accuracy: 0.6660 - jacard_coef: 0.072615/17 [=========================>....] - ETA: 1s - loss: 0.2508 - accuracy: 0.6624 - jacard_coef: 0.073516/17 [===========================>..] - ETA: 0s - loss: 0.2482 - accuracy: 0.6599 - jacard_coef: 0.074817/17 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.6597 - jacard_coef: 0.076617/17 [==============================] - 57s 911ms/step - loss: 0.2486 - accuracy: 0.6597 - jacard_coef: 0.0766 - val_loss: 14.9608 - val_accuracy: 0.0696 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1964 - accuracy: 0.4187 - jacard_coef: 0.0918 2/17 [==>...........................] - ETA: 1s - loss: 0.2021 - accuracy: 0.4370 - jacard_coef: 0.1067 3/17 [====>.........................] - ETA: 1s - loss: 0.2114 - accuracy: 0.4665 - jacard_coef: 0.0944 4/17 [======>.......................] - ETA: 1s - loss: 0.2027 - accuracy: 0.5043 - jacard_coef: 0.0896 5/17 [=======>......................] - ETA: 1s - loss: 0.2045 - accuracy: 0.5530 - jacard_coef: 0.0886 6/17 [=========>....................] - ETA: 1s - loss: 0.2009 - accuracy: 0.5955 - jacard_coef: 0.0926 7/17 [===========>..................] - ETA: 1s - loss: 0.2022 - accuracy: 0.6318 - jacard_coef: 0.0866 8/17 [=============>................] - ETA: 1s - loss: 0.2008 - accuracy: 0.6531 - jacard_coef: 0.0858 9/17 [==============>...............] - ETA: 1s - loss: 0.1991 - accuracy: 0.6680 - jacard_coef: 0.082610/17 [================>.............] - ETA: 0s - loss: 0.1987 - accuracy: 0.6776 - jacard_coef: 0.083711/17 [==================>...........] - ETA: 0s - loss: 0.1978 - accuracy: 0.6868 - jacard_coef: 0.080812/17 [====================>.........] - ETA: 0s - loss: 0.1967 - accuracy: 0.6890 - jacard_coef: 0.081813/17 [=====================>........] - ETA: 0s - loss: 0.1949 - accuracy: 0.6906 - jacard_coef: 0.080814/17 [=======================>......] - ETA: 0s - loss: 0.1948 - accuracy: 0.6953 - jacard_coef: 0.077315/17 [=========================>....] - ETA: 0s - loss: 0.1936 - accuracy: 0.6996 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1930 - accuracy: 0.7047 - jacard_coef: 0.075817/17 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.7061 - jacard_coef: 0.075017/17 [==============================] - 2s 137ms/step - loss: 0.1930 - accuracy: 0.7061 - jacard_coef: 0.0750 - val_loss: 14.0259 - val_accuracy: 0.0746 - val_jacard_coef: 0.0676 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1929 - accuracy: 0.7856 - jacard_coef: 0.0861 2/17 [==>...........................] - ETA: 1s - loss: 0.1856 - accuracy: 0.7639 - jacard_coef: 0.0827 3/17 [====>.........................] - ETA: 1s - loss: 0.1804 - accuracy: 0.7735 - jacard_coef: 0.0795 4/17 [======>.......................] - ETA: 1s - loss: 0.1796 - accuracy: 0.7765 - jacard_coef: 0.0825 5/17 [=======>......................] - ETA: 1s - loss: 0.1794 - accuracy: 0.7851 - jacard_coef: 0.0739 6/17 [=========>....................] - ETA: 1s - loss: 0.1794 - accuracy: 0.7729 - jacard_coef: 0.0725 7/17 [===========>..................] - ETA: 1s - loss: 0.1777 - accuracy: 0.7722 - jacard_coef: 0.0717 8/17 [=============>................] - ETA: 1s - loss: 0.1775 - accuracy: 0.7536 - jacard_coef: 0.0725 9/17 [==============>...............] - ETA: 1s - loss: 0.1787 - accuracy: 0.7412 - jacard_coef: 0.070610/17 [================>.............] - ETA: 0s - loss: 0.1775 - accuracy: 0.7317 - jacard_coef: 0.072411/17 [==================>...........] - ETA: 0s - loss: 0.1773 - accuracy: 0.7321 - jacard_coef: 0.074812/17 [====================>.........] - ETA: 0s - loss: 0.1771 - accuracy: 0.7329 - jacard_coef: 0.075113/17 [=====================>........] - ETA: 0s - loss: 0.1778 - accuracy: 0.7289 - jacard_coef: 0.075314/17 [=======================>......] - ETA: 0s - loss: 0.1777 - accuracy: 0.7335 - jacard_coef: 0.077915/17 [=========================>....] - ETA: 0s - loss: 0.1775 - accuracy: 0.7364 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1776 - accuracy: 0.7406 - jacard_coef: 0.076217/17 [==============================] - 2s 137ms/step - loss: 0.1777 - accuracy: 0.7416 - jacard_coef: 0.0718 - val_loss: 14.4513 - val_accuracy: 0.0731 - val_jacard_coef: 0.0683 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1726 - accuracy: 0.8016 - jacard_coef: 0.0762 2/17 [==>...........................] - ETA: 1s - loss: 0.1709 - accuracy: 0.8253 - jacard_coef: 0.0594 3/17 [====>.........................] - ETA: 1s - loss: 0.1702 - accuracy: 0.8142 - jacard_coef: 0.0612 4/17 [======>.......................] - ETA: 1s - loss: 0.1930 - accuracy: 0.8062 - jacard_coef: 0.0671 5/17 [=======>......................] - ETA: 1s - loss: 0.2018 - accuracy: 0.8055 - jacard_coef: 0.0721 6/17 [=========>....................] - ETA: 1s - loss: 0.2051 - accuracy: 0.7957 - jacard_coef: 0.0745 7/17 [===========>..................] - ETA: 1s - loss: 0.2060 - accuracy: 0.7737 - jacard_coef: 0.0780 8/17 [=============>................] - ETA: 1s - loss: 0.2058 - accuracy: 0.7793 - jacard_coef: 0.0774 9/17 [==============>...............] - ETA: 1s - loss: 0.2099 - accuracy: 0.7852 - jacard_coef: 0.078710/17 [================>.............] - ETA: 0s - loss: 0.2127 - accuracy: 0.7925 - jacard_coef: 0.077211/17 [==================>...........] - ETA: 0s - loss: 0.2120 - accuracy: 0.7999 - jacard_coef: 0.077312/17 [====================>.........] - ETA: 0s - loss: 0.2112 - accuracy: 0.8055 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.2111 - accuracy: 0.8115 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.2112 - accuracy: 0.8205 - jacard_coef: 0.073715/17 [=========================>....] - ETA: 0s - loss: 0.2095 - accuracy: 0.8254 - jacard_coef: 0.073816/17 [===========================>..] - ETA: 0s - loss: 0.2088 - accuracy: 0.8293 - jacard_coef: 0.074317/17 [==============================] - 2s 136ms/step - loss: 0.2087 - accuracy: 0.8290 - jacard_coef: 0.0789 - val_loss: 11.7496 - val_accuracy: 0.1140 - val_jacard_coef: 0.0688 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1924 - accuracy: 0.8538 - jacard_coef: 0.0805 2/17 [==>...........................] - ETA: 1s - loss: 0.1902 - accuracy: 0.7807 - jacard_coef: 0.0740 3/17 [====>.........................] - ETA: 1s - loss: 0.1875 - accuracy: 0.7756 - jacard_coef: 0.0817 4/17 [======>.......................] - ETA: 1s - loss: 0.1845 - accuracy: 0.7525 - jacard_coef: 0.0823 5/17 [=======>......................] - ETA: 1s - loss: 0.1833 - accuracy: 0.7265 - jacard_coef: 0.0832 6/17 [=========>....................] - ETA: 1s - loss: 0.1811 - accuracy: 0.7350 - jacard_coef: 0.0832 7/17 [===========>..................] - ETA: 1s - loss: 0.1817 - accuracy: 0.7261 - jacard_coef: 0.0770 8/17 [=============>................] - ETA: 1s - loss: 0.1803 - accuracy: 0.7503 - jacard_coef: 0.0758 9/17 [==============>...............] - ETA: 1s - loss: 0.1800 - accuracy: 0.7693 - jacard_coef: 0.074710/17 [================>.............] - ETA: 0s - loss: 0.1797 - accuracy: 0.7618 - jacard_coef: 0.077611/17 [==================>...........] - ETA: 0s - loss: 0.1793 - accuracy: 0.7573 - jacard_coef: 0.079512/17 [====================>.........] - ETA: 0s - loss: 0.1785 - accuracy: 0.7590 - jacard_coef: 0.076713/17 [=====================>........] - ETA: 0s - loss: 0.1767 - accuracy: 0.7573 - jacard_coef: 0.074914/17 [=======================>......] - ETA: 0s - loss: 0.1755 - accuracy: 0.7689 - jacard_coef: 0.074315/17 [=========================>....] - ETA: 0s - loss: 0.1764 - accuracy: 0.7744 - jacard_coef: 0.074416/17 [===========================>..] - ETA: 0s - loss: 0.1764 - accuracy: 0.7822 - jacard_coef: 0.074717/17 [==============================] - 2s 136ms/step - loss: 0.1765 - accuracy: 0.7826 - jacard_coef: 0.0773 - val_loss: 0.4235 - val_accuracy: 0.0856 - val_jacard_coef: 0.0645 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1667 - accuracy: 0.8251 - jacard_coef: 0.0647 2/17 [==>...........................] - ETA: 2s - loss: 0.1715 - accuracy: 0.7759 - jacard_coef: 0.0955 3/17 [====>.........................] - ETA: 1s - loss: 0.1691 - accuracy: 0.7763 - jacard_coef: 0.0852 4/17 [======>.......................] - ETA: 1s - loss: 0.1666 - accuracy: 0.7798 - jacard_coef: 0.0865 5/17 [=======>......................] - ETA: 1s - loss: 0.1648 - accuracy: 0.7873 - jacard_coef: 0.0770 6/17 [=========>....................] - ETA: 1s - loss: 0.1681 - accuracy: 0.7729 - jacard_coef: 0.0806 7/17 [===========>..................] - ETA: 1s - loss: 0.1676 - accuracy: 0.7851 - jacard_coef: 0.0774 8/17 [=============>................] - ETA: 1s - loss: 0.1672 - accuracy: 0.7914 - jacard_coef: 0.0734 9/17 [==============>...............] - ETA: 1s - loss: 0.1683 - accuracy: 0.7958 - jacard_coef: 0.074110/17 [================>.............] - ETA: 0s - loss: 0.1680 - accuracy: 0.8019 - jacard_coef: 0.074611/17 [==================>...........] - ETA: 0s - loss: 0.1686 - accuracy: 0.8063 - jacard_coef: 0.076212/17 [====================>.........] - ETA: 0s - loss: 0.1694 - accuracy: 0.8052 - jacard_coef: 0.079013/17 [=====================>........] - ETA: 0s - loss: 0.1692 - accuracy: 0.8100 - jacard_coef: 0.079514/17 [=======================>......] - ETA: 0s - loss: 0.1683 - accuracy: 0.8178 - jacard_coef: 0.076215/17 [=========================>....] - ETA: 0s - loss: 0.1681 - accuracy: 0.8195 - jacard_coef: 0.075116/17 [===========================>..] - ETA: 0s - loss: 0.1687 - accuracy: 0.8135 - jacard_coef: 0.074617/17 [==============================] - 2s 137ms/step - loss: 0.1688 - accuracy: 0.8135 - jacard_coef: 0.0734 - val_loss: 0.1893 - val_accuracy: 0.4937 - val_jacard_coef: 0.0644 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9277 - jacard_coef: 0.0623 2/17 [==>...........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.9200 - jacard_coef: 0.0694 3/17 [====>.........................] - ETA: 1s - loss: 0.1606 - accuracy: 0.9180 - jacard_coef: 0.0703 4/17 [======>.......................] - ETA: 1s - loss: 0.1613 - accuracy: 0.9208 - jacard_coef: 0.0671 5/17 [=======>......................] - ETA: 1s - loss: 0.1623 - accuracy: 0.9189 - jacard_coef: 0.0666 6/17 [=========>....................] - ETA: 1s - loss: 0.1619 - accuracy: 0.9086 - jacard_coef: 0.0675 7/17 [===========>..................] - ETA: 1s - loss: 0.1644 - accuracy: 0.8855 - jacard_coef: 0.0754 8/17 [=============>................] - ETA: 1s - loss: 0.1639 - accuracy: 0.8902 - jacard_coef: 0.0744 9/17 [==============>...............] - ETA: 1s - loss: 0.1630 - accuracy: 0.8975 - jacard_coef: 0.070710/17 [================>.............] - ETA: 0s - loss: 0.1637 - accuracy: 0.8988 - jacard_coef: 0.071511/17 [==================>...........] - ETA: 0s - loss: 0.1647 - accuracy: 0.9015 - jacard_coef: 0.070312/17 [====================>.........] - ETA: 0s - loss: 0.1659 - accuracy: 0.8985 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 0s - loss: 0.1665 - accuracy: 0.8994 - jacard_coef: 0.073514/17 [=======================>......] - ETA: 0s - loss: 0.1665 - accuracy: 0.8977 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1653 - accuracy: 0.8990 - jacard_coef: 0.075216/17 [===========================>..] - ETA: 0s - loss: 0.1652 - accuracy: 0.8974 - jacard_coef: 0.075517/17 [==============================] - 2s 137ms/step - loss: 0.1654 - accuracy: 0.8978 - jacard_coef: 0.0713 - val_loss: 0.1952 - val_accuracy: 0.1079 - val_jacard_coef: 0.0638 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1690 - accuracy: 0.8279 - jacard_coef: 0.0473 2/17 [==>...........................] - ETA: 2s - loss: 0.1628 - accuracy: 0.8429 - jacard_coef: 0.0652 3/17 [====>.........................] - ETA: 1s - loss: 0.1592 - accuracy: 0.8627 - jacard_coef: 0.0677 4/17 [======>.......................] - ETA: 1s - loss: 0.1576 - accuracy: 0.8712 - jacard_coef: 0.0693 5/17 [=======>......................] - ETA: 1s - loss: 0.1573 - accuracy: 0.8765 - jacard_coef: 0.0717 6/17 [=========>....................] - ETA: 1s - loss: 0.1569 - accuracy: 0.8713 - jacard_coef: 0.0756 7/17 [===========>..................] - ETA: 1s - loss: 0.1565 - accuracy: 0.8706 - jacard_coef: 0.0733 8/17 [=============>................] - ETA: 1s - loss: 0.1559 - accuracy: 0.8709 - jacard_coef: 0.0740 9/17 [==============>...............] - ETA: 1s - loss: 0.1579 - accuracy: 0.8632 - jacard_coef: 0.072210/17 [================>.............] - ETA: 0s - loss: 0.1578 - accuracy: 0.8675 - jacard_coef: 0.073211/17 [==================>...........] - ETA: 0s - loss: 0.1567 - accuracy: 0.8729 - jacard_coef: 0.072812/17 [====================>.........] - ETA: 0s - loss: 0.1570 - accuracy: 0.8751 - jacard_coef: 0.074113/17 [=====================>........] - ETA: 0s - loss: 0.1567 - accuracy: 0.8789 - jacard_coef: 0.073714/17 [=======================>......] - ETA: 0s - loss: 0.1572 - accuracy: 0.8801 - jacard_coef: 0.074915/17 [=========================>....] - ETA: 0s - loss: 0.1574 - accuracy: 0.8831 - jacard_coef: 0.074016/17 [===========================>..] - ETA: 0s - loss: 0.1581 - accuracy: 0.8763 - jacard_coef: 0.075317/17 [==============================] - 2s 137ms/step - loss: 0.1582 - accuracy: 0.8768 - jacard_coef: 0.0724 - val_loss: 0.1400 - val_accuracy: 0.9303 - val_jacard_coef: 0.0626 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1578 - accuracy: 0.9117 - jacard_coef: 0.0678 2/17 [==>...........................] - ETA: 2s - loss: 0.1663 - accuracy: 0.8877 - jacard_coef: 0.0804 3/17 [====>.........................] - ETA: 1s - loss: 0.1594 - accuracy: 0.8985 - jacard_coef: 0.0769 4/17 [======>.......................] - ETA: 1s - loss: 0.1595 - accuracy: 0.8846 - jacard_coef: 0.0669 5/17 [=======>......................] - ETA: 1s - loss: 0.1582 - accuracy: 0.8848 - jacard_coef: 0.0705 6/17 [=========>....................] - ETA: 1s - loss: 0.1591 - accuracy: 0.8766 - jacard_coef: 0.0771 7/17 [===========>..................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8580 - jacard_coef: 0.0774 8/17 [=============>................] - ETA: 1s - loss: 0.1609 - accuracy: 0.8470 - jacard_coef: 0.0785 9/17 [==============>...............] - ETA: 1s - loss: 0.1609 - accuracy: 0.8404 - jacard_coef: 0.078410/17 [================>.............] - ETA: 0s - loss: 0.1615 - accuracy: 0.8367 - jacard_coef: 0.077311/17 [==================>...........] - ETA: 0s - loss: 0.1614 - accuracy: 0.8381 - jacard_coef: 0.074512/17 [====================>.........] - ETA: 0s - loss: 0.1613 - accuracy: 0.8423 - jacard_coef: 0.073513/17 [=====================>........] - ETA: 0s - loss: 0.1611 - accuracy: 0.8445 - jacard_coef: 0.074614/17 [=======================>......] - ETA: 0s - loss: 0.1607 - accuracy: 0.8470 - jacard_coef: 0.075315/17 [=========================>....] - ETA: 0s - loss: 0.1603 - accuracy: 0.8496 - jacard_coef: 0.075816/17 [===========================>..] - ETA: 0s - loss: 0.1595 - accuracy: 0.8532 - jacard_coef: 0.075017/17 [==============================] - 2s 137ms/step - loss: 0.1593 - accuracy: 0.8538 - jacard_coef: 0.0742 - val_loss: 0.1218 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1632 - accuracy: 0.8626 - jacard_coef: 0.0720 2/17 [==>...........................] - ETA: 2s - loss: 0.1553 - accuracy: 0.8784 - jacard_coef: 0.0706 3/17 [====>.........................] - ETA: 1s - loss: 0.1515 - accuracy: 0.8944 - jacard_coef: 0.0671 4/17 [======>.......................] - ETA: 1s - loss: 0.1481 - accuracy: 0.8995 - jacard_coef: 0.0697 5/17 [=======>......................] - ETA: 1s - loss: 0.1476 - accuracy: 0.9001 - jacard_coef: 0.0731 6/17 [=========>....................] - ETA: 1s - loss: 0.1489 - accuracy: 0.9049 - jacard_coef: 0.0715 7/17 [===========>..................] - ETA: 1s - loss: 0.1482 - accuracy: 0.9117 - jacard_coef: 0.0675 8/17 [=============>................] - ETA: 1s - loss: 0.1478 - accuracy: 0.9178 - jacard_coef: 0.0636 9/17 [==============>...............] - ETA: 1s - loss: 0.1480 - accuracy: 0.9141 - jacard_coef: 0.066710/17 [================>.............] - ETA: 0s - loss: 0.1483 - accuracy: 0.9143 - jacard_coef: 0.067411/17 [==================>...........] - ETA: 0s - loss: 0.1483 - accuracy: 0.9115 - jacard_coef: 0.070512/17 [====================>.........] - ETA: 0s - loss: 0.1483 - accuracy: 0.9088 - jacard_coef: 0.073313/17 [=====================>........] - ETA: 0s - loss: 0.1479 - accuracy: 0.9092 - jacard_coef: 0.072614/17 [=======================>......] - ETA: 0s - loss: 0.1479 - accuracy: 0.9063 - jacard_coef: 0.073715/17 [=========================>....] - ETA: 0s - loss: 0.1530 - accuracy: 0.8967 - jacard_coef: 0.075316/17 [===========================>..] - ETA: 0s - loss: 0.1524 - accuracy: 0.8976 - jacard_coef: 0.075517/17 [==============================] - 2s 137ms/step - loss: 0.1525 - accuracy: 0.8979 - jacard_coef: 0.0713 - val_loss: 0.1242 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1482 - accuracy: 0.9159 - jacard_coef: 0.0758 2/17 [==>...........................] - ETA: 2s - loss: 0.1497 - accuracy: 0.9110 - jacard_coef: 0.0796 3/17 [====>.........................] - ETA: 1s - loss: 0.1488 - accuracy: 0.9270 - jacard_coef: 0.0654 4/17 [======>.......................] - ETA: 1s - loss: 0.1494 - accuracy: 0.9179 - jacard_coef: 0.0727 5/17 [=======>......................] - ETA: 1s - loss: 0.1496 - accuracy: 0.9156 - jacard_coef: 0.0749 6/17 [=========>....................] - ETA: 1s - loss: 0.1490 - accuracy: 0.9209 - jacard_coef: 0.0704 7/17 [===========>..................] - ETA: 1s - loss: 0.1490 - accuracy: 0.9222 - jacard_coef: 0.0694 8/17 [=============>................] - ETA: 1s - loss: 0.1515 - accuracy: 0.9226 - jacard_coef: 0.0687 9/17 [==============>...............] - ETA: 1s - loss: 0.1525 - accuracy: 0.9211 - jacard_coef: 0.069710/17 [================>.............] - ETA: 0s - loss: 0.1526 - accuracy: 0.9195 - jacard_coef: 0.071011/17 [==================>...........] - ETA: 0s - loss: 0.1536 - accuracy: 0.9147 - jacard_coef: 0.074612/17 [====================>.........] - ETA: 0s - loss: 0.1533 - accuracy: 0.9128 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.1531 - accuracy: 0.9133 - jacard_coef: 0.075514/17 [=======================>......] - ETA: 0s - loss: 0.1529 - accuracy: 0.9133 - jacard_coef: 0.075215/17 [=========================>....] - ETA: 0s - loss: 0.1521 - accuracy: 0.9133 - jacard_coef: 0.075316/17 [===========================>..] - ETA: 0s - loss: 0.1520 - accuracy: 0.9137 - jacard_coef: 0.074917/17 [==============================] - 2s 137ms/step - loss: 0.1520 - accuracy: 0.9138 - jacard_coef: 0.0737 - val_loss: 0.1234 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1466 - accuracy: 0.8973 - jacard_coef: 0.0884 2/17 [==>...........................] - ETA: 2s - loss: 0.1465 - accuracy: 0.9100 - jacard_coef: 0.0783 3/17 [====>.........................] - ETA: 1s - loss: 0.1460 - accuracy: 0.9175 - jacard_coef: 0.0725 4/17 [======>.......................] - ETA: 1s - loss: 0.1456 - accuracy: 0.9174 - jacard_coef: 0.0731 5/17 [=======>......................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9167 - jacard_coef: 0.0740 6/17 [=========>....................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9131 - jacard_coef: 0.0771 7/17 [===========>..................] - ETA: 1s - loss: 0.1453 - accuracy: 0.9092 - jacard_coef: 0.0753 8/17 [=============>................] - ETA: 1s - loss: 0.1453 - accuracy: 0.9081 - jacard_coef: 0.0769 9/17 [==============>...............] - ETA: 1s - loss: 0.1448 - accuracy: 0.9089 - jacard_coef: 0.077010/17 [================>.............] - ETA: 0s - loss: 0.1445 - accuracy: 0.9097 - jacard_coef: 0.076811/17 [==================>...........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9088 - jacard_coef: 0.077812/17 [====================>.........] - ETA: 0s - loss: 0.1441 - accuracy: 0.9110 - jacard_coef: 0.075713/17 [=====================>........] - ETA: 0s - loss: 0.1439 - accuracy: 0.9102 - jacard_coef: 0.075614/17 [=======================>......] - ETA: 0s - loss: 0.1440 - accuracy: 0.9097 - jacard_coef: 0.075715/17 [=========================>....] - ETA: 0s - loss: 0.1438 - accuracy: 0.9096 - jacard_coef: 0.076016/17 [===========================>..] - ETA: 0s - loss: 0.1435 - accuracy: 0.9108 - jacard_coef: 0.075217/17 [==============================] - 2s 137ms/step - loss: 0.1436 - accuracy: 0.9096 - jacard_coef: 0.0741 - val_loss: 0.1163 - val_accuracy: 0.9304 - val_jacard_coef: 0.0620 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1423 - accuracy: 0.9348 - jacard_coef: 0.0599 2/17 [==>...........................] - ETA: 2s - loss: 0.1426 - accuracy: 0.9166 - jacard_coef: 0.0752 3/17 [====>.........................] - ETA: 1s - loss: 0.1424 - accuracy: 0.9183 - jacard_coef: 0.0738 4/17 [======>.......................] - ETA: 1s - loss: 0.1442 - accuracy: 0.9164 - jacard_coef: 0.0753 5/17 [=======>......................] - ETA: 1s - loss: 0.1438 - accuracy: 0.9199 - jacard_coef: 0.0722 6/17 [=========>....................] - ETA: 1s - loss: 0.1443 - accuracy: 0.9195 - jacard_coef: 0.0727 7/17 [===========>..................] - ETA: 1s - loss: 0.1453 - accuracy: 0.9182 - jacard_coef: 0.0737 8/17 [=============>................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9185 - jacard_coef: 0.0735 9/17 [==============>...............] - ETA: 1s - loss: 0.1458 - accuracy: 0.9168 - jacard_coef: 0.074810/17 [================>.............] - ETA: 0s - loss: 0.1455 - accuracy: 0.9153 - jacard_coef: 0.076111/17 [==================>...........] - ETA: 0s - loss: 0.1456 - accuracy: 0.9156 - jacard_coef: 0.075812/17 [====================>.........] - ETA: 0s - loss: 0.1451 - accuracy: 0.9150 - jacard_coef: 0.076413/17 [=====================>........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9162 - jacard_coef: 0.075414/17 [=======================>......] - ETA: 0s - loss: 0.1446 - accuracy: 0.9144 - jacard_coef: 0.076815/17 [=========================>....] - ETA: 0s - loss: 0.1441 - accuracy: 0.9152 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9170 - jacard_coef: 0.074417/17 [==============================] - 2s 137ms/step - loss: 0.1437 - accuracy: 0.9162 - jacard_coef: 0.0785 - val_loss: 0.1182 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1375 - accuracy: 0.9217 - jacard_coef: 0.0670 2/17 [==>...........................] - ETA: 2s - loss: 0.1380 - accuracy: 0.9110 - jacard_coef: 0.0698 3/17 [====>.........................] - ETA: 1s - loss: 0.1381 - accuracy: 0.9048 - jacard_coef: 0.0736 4/17 [======>.......................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9002 - jacard_coef: 0.0796 5/17 [=======>......................] - ETA: 1s - loss: 0.1395 - accuracy: 0.8999 - jacard_coef: 0.0815 6/17 [=========>....................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9057 - jacard_coef: 0.0772 7/17 [===========>..................] - ETA: 1s - loss: 0.1384 - accuracy: 0.9067 - jacard_coef: 0.0728 8/17 [=============>................] - ETA: 1s - loss: 0.1386 - accuracy: 0.9056 - jacard_coef: 0.0746 9/17 [==============>...............] - ETA: 1s - loss: 0.1381 - accuracy: 0.9097 - jacard_coef: 0.072310/17 [================>.............] - ETA: 0s - loss: 0.1380 - accuracy: 0.9102 - jacard_coef: 0.072411/17 [==================>...........] - ETA: 0s - loss: 0.1378 - accuracy: 0.9125 - jacard_coef: 0.071112/17 [====================>.........] - ETA: 0s - loss: 0.1380 - accuracy: 0.9097 - jacard_coef: 0.071113/17 [=====================>........] - ETA: 0s - loss: 0.1381 - accuracy: 0.9090 - jacard_coef: 0.072514/17 [=======================>......] - ETA: 0s - loss: 0.1378 - accuracy: 0.9119 - jacard_coef: 0.070715/17 [=========================>....] - ETA: 0s - loss: 0.1379 - accuracy: 0.9106 - jacard_coef: 0.072316/17 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9091 - jacard_coef: 0.074017/17 [==============================] - 2s 137ms/step - loss: 0.1383 - accuracy: 0.9083 - jacard_coef: 0.0787 - val_loss: 0.1170 - val_accuracy: 0.9304 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0688 (epoch 4)
  Final Val Loss: 0.1170
  Training Time: 0:01:28.369425
  Stability (std): 0.0900

Results saved to: hyperparameter_optimization_20250926_123742/exp_1_UNet_lr1e-4_bs8/UNet_lr0.0001_bs8_results.json

Experiment 1 completed in 134s
Progress: 1/36 completed
Estimated remaining time: 78 minutes

ðŸ”¬ EXPERIMENT 2/36
================================================
Architecture: UNet
Learning Rate: 1e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758861658.601494 3179911 service.cc:145] XLA service 0x1454f5ce7f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758861658.601572 3179911 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758861659.062111 3179911 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 5:59 - loss: 0.3351 - accuracy: 0.4928 - jacard_coef: 0.09712/9 [=====>........................] - ETA: 52s - loss: 0.3002 - accuracy: 0.4386 - jacard_coef: 0.0858 3/9 [=========>....................] - ETA: 33s - loss: 0.2785 - accuracy: 0.3951 - jacard_coef: 0.08604/9 [============>.................] - ETA: 23s - loss: 0.2609 - accuracy: 0.3518 - jacard_coef: 0.08475/9 [===============>..............] - ETA: 14s - loss: 0.2492 - accuracy: 0.3178 - jacard_coef: 0.08116/9 [===================>..........] - ETA: 8s - loss: 0.2403 - accuracy: 0.2955 - jacard_coef: 0.0805 7/9 [======================>.......] - ETA: 5s - loss: 0.2338 - accuracy: 0.2785 - jacard_coef: 0.08038/9 [=========================>....] - ETA: 2s - loss: 0.2288 - accuracy: 0.2610 - jacard_coef: 0.07609/9 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.2608 - jacard_coef: 0.08159/9 [==============================] - 67s 3s/step - loss: 0.2285 - accuracy: 0.2608 - jacard_coef: 0.0815 - val_loss: 0.1855 - val_accuracy: 0.9300 - val_jacard_coef: 0.0361 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1917 - accuracy: 0.1847 - jacard_coef: 0.08222/9 [=====>........................] - ETA: 1s - loss: 0.1907 - accuracy: 0.1812 - jacard_coef: 0.07333/9 [=========>....................] - ETA: 1s - loss: 0.1891 - accuracy: 0.1854 - jacard_coef: 0.07554/9 [============>.................] - ETA: 1s - loss: 0.1886 - accuracy: 0.1879 - jacard_coef: 0.07585/9 [===============>..............] - ETA: 0s - loss: 0.1879 - accuracy: 0.1912 - jacard_coef: 0.07716/9 [===================>..........] - ETA: 0s - loss: 0.1876 - accuracy: 0.1923 - jacard_coef: 0.07877/9 [======================>.......] - ETA: 0s - loss: 0.1874 - accuracy: 0.1897 - jacard_coef: 0.07798/9 [=========================>....] - ETA: 0s - loss: 0.1875 - accuracy: 0.1871 - jacard_coef: 0.07699/9 [==============================] - 2s 234ms/step - loss: 0.1874 - accuracy: 0.1870 - jacard_coef: 0.0757 - val_loss: 0.3439 - val_accuracy: 0.9301 - val_jacard_coef: 0.0150 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1849 - accuracy: 0.1856 - jacard_coef: 0.07532/9 [=====>........................] - ETA: 1s - loss: 0.1841 - accuracy: 0.1989 - jacard_coef: 0.08533/9 [=========>....................] - ETA: 1s - loss: 0.1842 - accuracy: 0.1989 - jacard_coef: 0.08024/9 [============>.................] - ETA: 1s - loss: 0.1844 - accuracy: 0.2049 - jacard_coef: 0.08295/9 [===============>..............] - ETA: 0s - loss: 0.1846 - accuracy: 0.2025 - jacard_coef: 0.07926/9 [===================>..........] - ETA: 0s - loss: 0.1844 - accuracy: 0.2021 - jacard_coef: 0.07667/9 [======================>.......] - ETA: 0s - loss: 0.1840 - accuracy: 0.2029 - jacard_coef: 0.07538/9 [=========================>....] - ETA: 0s - loss: 0.1840 - accuracy: 0.2047 - jacard_coef: 0.07639/9 [==============================] - 2s 241ms/step - loss: 0.1840 - accuracy: 0.2054 - jacard_coef: 0.0820 - val_loss: 0.1776 - val_accuracy: 0.9200 - val_jacard_coef: 0.0376 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1817 - accuracy: 0.1898 - jacard_coef: 0.05312/9 [=====>........................] - ETA: 1s - loss: 0.1815 - accuracy: 0.2140 - jacard_coef: 0.07423/9 [=========>....................] - ETA: 1s - loss: 0.1813 - accuracy: 0.2208 - jacard_coef: 0.07924/9 [============>.................] - ETA: 1s - loss: 0.1807 - accuracy: 0.2172 - jacard_coef: 0.07575/9 [===============>..............] - ETA: 0s - loss: 0.1804 - accuracy: 0.2213 - jacard_coef: 0.07626/9 [===================>..........] - ETA: 0s - loss: 0.1804 - accuracy: 0.2208 - jacard_coef: 0.07507/9 [======================>.......] - ETA: 0s - loss: 0.1803 - accuracy: 0.2266 - jacard_coef: 0.07758/9 [=========================>....] - ETA: 0s - loss: 0.1801 - accuracy: 0.2284 - jacard_coef: 0.07629/9 [==============================] - 2s 242ms/step - loss: 0.1800 - accuracy: 0.2294 - jacard_coef: 0.0813 - val_loss: 0.1167 - val_accuracy: 0.8948 - val_jacard_coef: 0.0502 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1758 - accuracy: 0.2866 - jacard_coef: 0.07512/9 [=====>........................] - ETA: 1s - loss: 0.1756 - accuracy: 0.2909 - jacard_coef: 0.06513/9 [=========>....................] - ETA: 1s - loss: 0.1754 - accuracy: 0.3035 - jacard_coef: 0.06064/9 [============>.................] - ETA: 1s - loss: 0.1752 - accuracy: 0.3240 - jacard_coef: 0.06665/9 [===============>..............] - ETA: 0s - loss: 0.1746 - accuracy: 0.3224 - jacard_coef: 0.06686/9 [===================>..........] - ETA: 0s - loss: 0.1740 - accuracy: 0.3578 - jacard_coef: 0.07277/9 [======================>.......] - ETA: 0s - loss: 0.1738 - accuracy: 0.3822 - jacard_coef: 0.07688/9 [=========================>....] - ETA: 0s - loss: 0.1737 - accuracy: 0.3990 - jacard_coef: 0.07649/9 [==============================] - 2s 240ms/step - loss: 0.1737 - accuracy: 0.4009 - jacard_coef: 0.0736 - val_loss: 0.1966 - val_accuracy: 0.9192 - val_jacard_coef: 0.0599 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1712 - accuracy: 0.6280 - jacard_coef: 0.06352/9 [=====>........................] - ETA: 1s - loss: 0.1689 - accuracy: 0.7021 - jacard_coef: 0.06703/9 [=========>....................] - ETA: 1s - loss: 0.1680 - accuracy: 0.7160 - jacard_coef: 0.06954/9 [============>.................] - ETA: 1s - loss: 0.1676 - accuracy: 0.6999 - jacard_coef: 0.07195/9 [===============>..............] - ETA: 0s - loss: 0.1694 - accuracy: 0.6350 - jacard_coef: 0.07506/9 [===================>..........] - ETA: 0s - loss: 0.1690 - accuracy: 0.6712 - jacard_coef: 0.07477/9 [======================>.......] - ETA: 0s - loss: 0.1691 - accuracy: 0.6929 - jacard_coef: 0.07508/9 [=========================>....] - ETA: 0s - loss: 0.1694 - accuracy: 0.7026 - jacard_coef: 0.07649/9 [==============================] - 2s 239ms/step - loss: 0.1694 - accuracy: 0.7018 - jacard_coef: 0.0711 - val_loss: 2.2134 - val_accuracy: 0.1285 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1693 - accuracy: 0.7258 - jacard_coef: 0.05792/9 [=====>........................] - ETA: 1s - loss: 0.1696 - accuracy: 0.6957 - jacard_coef: 0.07663/9 [=========>....................] - ETA: 1s - loss: 0.1693 - accuracy: 0.6792 - jacard_coef: 0.08214/9 [============>.................] - ETA: 1s - loss: 0.1691 - accuracy: 0.6712 - jacard_coef: 0.07715/9 [===============>..............] - ETA: 0s - loss: 0.1688 - accuracy: 0.6587 - jacard_coef: 0.07866/9 [===================>..........] - ETA: 0s - loss: 0.1686 - accuracy: 0.6435 - jacard_coef: 0.07807/9 [======================>.......] - ETA: 0s - loss: 0.1680 - accuracy: 0.6442 - jacard_coef: 0.07728/9 [=========================>....] - ETA: 0s - loss: 0.1677 - accuracy: 0.6550 - jacard_coef: 0.07579/9 [==============================] - 2s 235ms/step - loss: 0.1677 - accuracy: 0.6554 - jacard_coef: 0.0828 - val_loss: 1.0931 - val_accuracy: 0.9303 - val_jacard_coef: 8.6569e-04 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1637 - accuracy: 0.7007 - jacard_coef: 0.06932/9 [=====>........................] - ETA: 1s - loss: 0.1635 - accuracy: 0.7054 - jacard_coef: 0.07543/9 [=========>....................] - ETA: 1s - loss: 0.1630 - accuracy: 0.7103 - jacard_coef: 0.08084/9 [============>.................] - ETA: 1s - loss: 0.1626 - accuracy: 0.7258 - jacard_coef: 0.07795/9 [===============>..............] - ETA: 0s - loss: 0.1624 - accuracy: 0.7246 - jacard_coef: 0.07386/9 [===================>..........] - ETA: 0s - loss: 0.1626 - accuracy: 0.7442 - jacard_coef: 0.07327/9 [======================>.......] - ETA: 0s - loss: 0.1625 - accuracy: 0.7479 - jacard_coef: 0.07588/9 [=========================>....] - ETA: 0s - loss: 0.1623 - accuracy: 0.7627 - jacard_coef: 0.07569/9 [==============================] - 2s 236ms/step - loss: 0.1624 - accuracy: 0.7607 - jacard_coef: 0.0810 - val_loss: 0.2921 - val_accuracy: 0.9161 - val_jacard_coef: 0.0428 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1604 - accuracy: 0.9050 - jacard_coef: 0.07572/9 [=====>........................] - ETA: 1s - loss: 0.1618 - accuracy: 0.8819 - jacard_coef: 0.07483/9 [=========>....................] - ETA: 1s - loss: 0.1629 - accuracy: 0.8502 - jacard_coef: 0.08034/9 [============>.................] - ETA: 1s - loss: 0.1630 - accuracy: 0.8376 - jacard_coef: 0.07575/9 [===============>..............] - ETA: 0s - loss: 0.1635 - accuracy: 0.8228 - jacard_coef: 0.07626/9 [===================>..........] - ETA: 0s - loss: 0.1635 - accuracy: 0.8146 - jacard_coef: 0.07377/9 [======================>.......] - ETA: 0s - loss: 0.1635 - accuracy: 0.8116 - jacard_coef: 0.07558/9 [=========================>....] - ETA: 0s - loss: 0.1635 - accuracy: 0.8080 - jacard_coef: 0.07649/9 [==============================] - 2s 237ms/step - loss: 0.1635 - accuracy: 0.8083 - jacard_coef: 0.0684 - val_loss: 0.8143 - val_accuracy: 0.9208 - val_jacard_coef: 0.0214 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1639 - accuracy: 0.8471 - jacard_coef: 0.08132/9 [=====>........................] - ETA: 1s - loss: 0.1628 - accuracy: 0.8615 - jacard_coef: 0.07843/9 [=========>....................] - ETA: 1s - loss: 0.1619 - accuracy: 0.8676 - jacard_coef: 0.07414/9 [============>.................] - ETA: 1s - loss: 0.1621 - accuracy: 0.8329 - jacard_coef: 0.07875/9 [===============>..............] - ETA: 0s - loss: 0.1620 - accuracy: 0.8182 - jacard_coef: 0.08416/9 [===================>..........] - ETA: 0s - loss: 0.1616 - accuracy: 0.8274 - jacard_coef: 0.08237/9 [======================>.......] - ETA: 0s - loss: 0.1611 - accuracy: 0.8373 - jacard_coef: 0.07888/9 [=========================>....] - ETA: 0s - loss: 0.1606 - accuracy: 0.8491 - jacard_coef: 0.07529/9 [==============================] - 2s 236ms/step - loss: 0.1607 - accuracy: 0.8486 - jacard_coef: 0.0815 - val_loss: 0.6812 - val_accuracy: 0.9174 - val_jacard_coef: 0.0317 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1598 - accuracy: 0.8520 - jacard_coef: 0.11252/9 [=====>........................] - ETA: 1s - loss: 0.1578 - accuracy: 0.8988 - jacard_coef: 0.07933/9 [=========>....................] - ETA: 1s - loss: 0.1573 - accuracy: 0.9085 - jacard_coef: 0.07434/9 [============>.................] - ETA: 1s - loss: 0.1573 - accuracy: 0.9045 - jacard_coef: 0.07795/9 [===============>..............] - ETA: 0s - loss: 0.1575 - accuracy: 0.8991 - jacard_coef: 0.08196/9 [===================>..........] - ETA: 0s - loss: 0.1573 - accuracy: 0.8975 - jacard_coef: 0.08167/9 [======================>.......] - ETA: 0s - loss: 0.1570 - accuracy: 0.8987 - jacard_coef: 0.07858/9 [=========================>....] - ETA: 0s - loss: 0.1567 - accuracy: 0.9022 - jacard_coef: 0.07509/9 [==============================] - 2s 236ms/step - loss: 0.1567 - accuracy: 0.9013 - jacard_coef: 0.0795 - val_loss: 0.1255 - val_accuracy: 0.8908 - val_jacard_coef: 0.0634 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1556 - accuracy: 0.9052 - jacard_coef: 0.08372/9 [=====>........................] - ETA: 1s - loss: 0.1542 - accuracy: 0.9163 - jacard_coef: 0.07263/9 [=========>....................] - ETA: 1s - loss: 0.1538 - accuracy: 0.9189 - jacard_coef: 0.06944/9 [============>.................] - ETA: 1s - loss: 0.1541 - accuracy: 0.9170 - jacard_coef: 0.06935/9 [===============>..............] - ETA: 1s - loss: 0.1543 - accuracy: 0.9096 - jacard_coef: 0.07366/9 [===================>..........] - ETA: 0s - loss: 0.1543 - accuracy: 0.9077 - jacard_coef: 0.07407/9 [======================>.......] - ETA: 0s - loss: 0.1545 - accuracy: 0.9081 - jacard_coef: 0.07508/9 [=========================>....] - ETA: 0s - loss: 0.1546 - accuracy: 0.9079 - jacard_coef: 0.07619/9 [==============================] - 2s 242ms/step - loss: 0.1546 - accuracy: 0.9082 - jacard_coef: 0.0717 - val_loss: 0.1642 - val_accuracy: 0.7294 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1544 - accuracy: 0.9071 - jacard_coef: 0.08282/9 [=====>........................] - ETA: 1s - loss: 0.1549 - accuracy: 0.8978 - jacard_coef: 0.08623/9 [=========>....................] - ETA: 1s - loss: 0.1548 - accuracy: 0.8954 - jacard_coef: 0.08534/9 [============>.................] - ETA: 1s - loss: 0.1546 - accuracy: 0.8945 - jacard_coef: 0.08525/9 [===============>..............] - ETA: 1s - loss: 0.1542 - accuracy: 0.8986 - jacard_coef: 0.08086/9 [===================>..........] - ETA: 0s - loss: 0.1538 - accuracy: 0.8967 - jacard_coef: 0.07647/9 [======================>.......] - ETA: 0s - loss: 0.1540 - accuracy: 0.8886 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1536 - accuracy: 0.8884 - jacard_coef: 0.07589/9 [==============================] - 2s 242ms/step - loss: 0.1541 - accuracy: 0.8859 - jacard_coef: 0.0753 - val_loss: 0.1550 - val_accuracy: 0.8843 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1531 - accuracy: 0.9228 - jacard_coef: 0.07092/9 [=====>........................] - ETA: 1s - loss: 0.1549 - accuracy: 0.8358 - jacard_coef: 0.07383/9 [=========>....................] - ETA: 1s - loss: 0.1553 - accuracy: 0.7995 - jacard_coef: 0.07184/9 [============>.................] - ETA: 1s - loss: 0.1554 - accuracy: 0.7831 - jacard_coef: 0.07245/9 [===============>..............] - ETA: 1s - loss: 0.1556 - accuracy: 0.7574 - jacard_coef: 0.07186/9 [===================>..........] - ETA: 0s - loss: 0.1574 - accuracy: 0.7250 - jacard_coef: 0.07237/9 [======================>.......] - ETA: 0s - loss: 0.1573 - accuracy: 0.7163 - jacard_coef: 0.07548/9 [=========================>....] - ETA: 0s - loss: 0.1575 - accuracy: 0.7036 - jacard_coef: 0.07619/9 [==============================] - 2s 242ms/step - loss: 0.1574 - accuracy: 0.7052 - jacard_coef: 0.0738 - val_loss: 0.1592 - val_accuracy: 0.9225 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1550 - accuracy: 0.8844 - jacard_coef: 0.09922/9 [=====>........................] - ETA: 1s - loss: 0.1542 - accuracy: 0.9022 - jacard_coef: 0.08233/9 [=========>....................] - ETA: 1s - loss: 0.1528 - accuracy: 0.9150 - jacard_coef: 0.07284/9 [============>.................] - ETA: 1s - loss: 0.1528 - accuracy: 0.9135 - jacard_coef: 0.07385/9 [===============>..............] - ETA: 1s - loss: 0.1528 - accuracy: 0.9157 - jacard_coef: 0.07196/9 [===================>..........] - ETA: 0s - loss: 0.1531 - accuracy: 0.9119 - jacard_coef: 0.07507/9 [======================>.......] - ETA: 0s - loss: 0.1529 - accuracy: 0.9132 - jacard_coef: 0.07398/9 [=========================>....] - ETA: 0s - loss: 0.1529 - accuracy: 0.9112 - jacard_coef: 0.07599/9 [==============================] - 2s 242ms/step - loss: 0.1529 - accuracy: 0.9118 - jacard_coef: 0.0677 - val_loss: 0.1697 - val_accuracy: 0.8639 - val_jacard_coef: 0.0652 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1510 - accuracy: 0.9372 - jacard_coef: 0.05472/9 [=====>........................] - ETA: 1s - loss: 0.1512 - accuracy: 0.9254 - jacard_coef: 0.06543/9 [=========>....................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9268 - jacard_coef: 0.06534/9 [============>.................] - ETA: 1s - loss: 0.1513 - accuracy: 0.9219 - jacard_coef: 0.06995/9 [===============>..............] - ETA: 1s - loss: 0.1511 - accuracy: 0.9222 - jacard_coef: 0.07006/9 [===================>..........] - ETA: 0s - loss: 0.1512 - accuracy: 0.9206 - jacard_coef: 0.07147/9 [======================>.......] - ETA: 0s - loss: 0.1514 - accuracy: 0.9181 - jacard_coef: 0.07378/9 [=========================>....] - ETA: 0s - loss: 0.1514 - accuracy: 0.9157 - jacard_coef: 0.07579/9 [==============================] - 2s 242ms/step - loss: 0.1517 - accuracy: 0.9120 - jacard_coef: 0.0733 - val_loss: 0.1742 - val_accuracy: 0.8868 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0697 (epoch 6)
  Final Val Loss: 0.1742
  Training Time: 0:01:40.567897
  Stability (std): 0.3306

Results saved to: hyperparameter_optimization_20250926_123742/exp_2_UNet_lr1e-4_bs16/UNet_lr0.0001_bs16_results.json

Experiment 2 completed in 135s
Progress: 2/36 completed
Estimated remaining time: 76 minutes

ðŸ”¬ EXPERIMENT 3/36
================================================
Architecture: UNet
Learning Rate: 1e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758861797.175153 3183930 service.cc:145] XLA service 0x153e695f6310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758861797.175193 3183930 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758861797.562642 3183930 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 3:21 - loss: 0.3468 - accuracy: 0.5048 - jacard_coef: 0.07692/5 [===========>..................] - ETA: 38s - loss: 0.3144 - accuracy: 0.4386 - jacard_coef: 0.0816 3/5 [=================>............] - ETA: 14s - loss: 0.2895 - accuracy: 0.3976 - jacard_coef: 0.07784/5 [=======================>......] - ETA: 5s - loss: 0.2746 - accuracy: 0.3462 - jacard_coef: 0.0770 5/5 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.3444 - jacard_coef: 0.06685/5 [==============================] - 74s 6s/step - loss: 0.2742 - accuracy: 0.3444 - jacard_coef: 0.0668 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2037 - accuracy: 0.1630 - jacard_coef: 0.06932/5 [===========>..................] - ETA: 1s - loss: 0.1988 - accuracy: 0.2020 - jacard_coef: 0.07263/5 [=================>............] - ETA: 0s - loss: 0.1949 - accuracy: 0.2231 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.2437 - jacard_coef: 0.07655/5 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.2437 - jacard_coef: 0.07365/5 [==============================] - 3s 420ms/step - loss: 0.1946 - accuracy: 0.2437 - jacard_coef: 0.0736 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.2330 - accuracy: 0.1596 - jacard_coef: 0.06672/5 [===========>..................] - ETA: 1s - loss: 0.2226 - accuracy: 0.1681 - jacard_coef: 0.08073/5 [=================>............] - ETA: 0s - loss: 0.2201 - accuracy: 0.1743 - jacard_coef: 0.07234/5 [=======================>......] - ETA: 0s - loss: 0.2139 - accuracy: 0.1975 - jacard_coef: 0.07545/5 [==============================] - 2s 398ms/step - loss: 0.2138 - accuracy: 0.1968 - jacard_coef: 0.0608 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1930 - accuracy: 0.2035 - jacard_coef: 0.07812/5 [===========>..................] - ETA: 1s - loss: 0.1956 - accuracy: 0.2350 - jacard_coef: 0.07133/5 [=================>............] - ETA: 0s - loss: 0.1925 - accuracy: 0.2614 - jacard_coef: 0.07954/5 [=======================>......] - ETA: 0s - loss: 0.1906 - accuracy: 0.2610 - jacard_coef: 0.07665/5 [==============================] - 2s 398ms/step - loss: 0.1908 - accuracy: 0.2604 - jacard_coef: 0.0685 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1872 - accuracy: 0.2668 - jacard_coef: 0.07122/5 [===========>..................] - ETA: 1s - loss: 0.1865 - accuracy: 0.3001 - jacard_coef: 0.08013/5 [=================>............] - ETA: 0s - loss: 0.1867 - accuracy: 0.3007 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 0s - loss: 0.1877 - accuracy: 0.2896 - jacard_coef: 0.07675/5 [==============================] - 2s 398ms/step - loss: 0.1877 - accuracy: 0.2886 - jacard_coef: 0.0723 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1854 - accuracy: 0.3976 - jacard_coef: 0.07512/5 [===========>..................] - ETA: 1s - loss: 0.1854 - accuracy: 0.3967 - jacard_coef: 0.07733/5 [=================>............] - ETA: 0s - loss: 0.1836 - accuracy: 0.3816 - jacard_coef: 0.07774/5 [=======================>......] - ETA: 0s - loss: 0.1829 - accuracy: 0.3692 - jacard_coef: 0.07675/5 [==============================] - 2s 401ms/step - loss: 0.1832 - accuracy: 0.3679 - jacard_coef: 0.0736 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1811 - accuracy: 0.4271 - jacard_coef: 0.07592/5 [===========>..................] - ETA: 1s - loss: 0.1828 - accuracy: 0.5013 - jacard_coef: 0.07583/5 [=================>............] - ETA: 0s - loss: 0.1814 - accuracy: 0.5034 - jacard_coef: 0.07604/5 [=======================>......] - ETA: 0s - loss: 0.1804 - accuracy: 0.4912 - jacard_coef: 0.07605/5 [==============================] - 2s 407ms/step - loss: 0.1810 - accuracy: 0.4899 - jacard_coef: 0.0819 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1746 - accuracy: 0.4418 - jacard_coef: 0.07472/5 [===========>..................] - ETA: 1s - loss: 0.1766 - accuracy: 0.3797 - jacard_coef: 0.07643/5 [=================>............] - ETA: 0s - loss: 0.1783 - accuracy: 0.3504 - jacard_coef: 0.07864/5 [=======================>......] - ETA: 0s - loss: 0.1793 - accuracy: 0.3514 - jacard_coef: 0.07615/5 [==============================] - 2s 406ms/step - loss: 0.1794 - accuracy: 0.3509 - jacard_coef: 0.0842 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4598e-05 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1841 - accuracy: 0.3991 - jacard_coef: 0.08902/5 [===========>..................] - ETA: 1s - loss: 0.1822 - accuracy: 0.4102 - jacard_coef: 0.07733/5 [=================>............] - ETA: 0s - loss: 0.1818 - accuracy: 0.3858 - jacard_coef: 0.07384/5 [=======================>......] - ETA: 0s - loss: 0.1810 - accuracy: 0.3839 - jacard_coef: 0.07595/5 [==============================] - 2s 442ms/step - loss: 0.1810 - accuracy: 0.3832 - jacard_coef: 0.0859 - val_loss: 1.0870 - val_accuracy: 0.9294 - val_jacard_coef: 0.0089 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1783 - accuracy: 0.3727 - jacard_coef: 0.08922/5 [===========>..................] - ETA: 1s - loss: 0.1798 - accuracy: 0.3926 - jacard_coef: 0.08253/5 [=================>............] - ETA: 0s - loss: 0.1784 - accuracy: 0.4172 - jacard_coef: 0.08044/5 [=======================>......] - ETA: 0s - loss: 0.1789 - accuracy: 0.4130 - jacard_coef: 0.07705/5 [==============================] - 2s 435ms/step - loss: 0.1789 - accuracy: 0.4117 - jacard_coef: 0.0628 - val_loss: 0.8279 - val_accuracy: 0.9156 - val_jacard_coef: 0.0309 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1754 - accuracy: 0.5271 - jacard_coef: 0.07732/5 [===========>..................] - ETA: 1s - loss: 0.1772 - accuracy: 0.5197 - jacard_coef: 0.07533/5 [=================>............] - ETA: 0s - loss: 0.1762 - accuracy: 0.5525 - jacard_coef: 0.07414/5 [=======================>......] - ETA: 0s - loss: 0.1754 - accuracy: 0.5372 - jacard_coef: 0.07685/5 [==============================] - 2s 432ms/step - loss: 0.1755 - accuracy: 0.5367 - jacard_coef: 0.0620 - val_loss: 0.4178 - val_accuracy: 0.8527 - val_jacard_coef: 0.0484 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1724 - accuracy: 0.4666 - jacard_coef: 0.06662/5 [===========>..................] - ETA: 1s - loss: 0.1743 - accuracy: 0.4523 - jacard_coef: 0.07513/5 [=================>............] - ETA: 0s - loss: 0.1731 - accuracy: 0.4823 - jacard_coef: 0.07334/5 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.4922 - jacard_coef: 0.07675/5 [==============================] - 2s 436ms/step - loss: 0.1730 - accuracy: 0.4914 - jacard_coef: 0.0628 - val_loss: 0.2051 - val_accuracy: 0.6317 - val_jacard_coef: 0.0637 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1719 - accuracy: 0.5273 - jacard_coef: 0.06412/5 [===========>..................] - ETA: 1s - loss: 0.1735 - accuracy: 0.5669 - jacard_coef: 0.07823/5 [=================>............] - ETA: 0s - loss: 0.1724 - accuracy: 0.6088 - jacard_coef: 0.07694/5 [=======================>......] - ETA: 0s - loss: 0.1720 - accuracy: 0.6116 - jacard_coef: 0.07635/5 [==============================] - 2s 432ms/step - loss: 0.1721 - accuracy: 0.6103 - jacard_coef: 0.0690 - val_loss: 0.1863 - val_accuracy: 0.6337 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1701 - accuracy: 0.7570 - jacard_coef: 0.07362/5 [===========>..................] - ETA: 1s - loss: 0.1705 - accuracy: 0.6919 - jacard_coef: 0.06653/5 [=================>............] - ETA: 0s - loss: 0.1714 - accuracy: 0.6616 - jacard_coef: 0.07494/5 [=======================>......] - ETA: 0s - loss: 0.1709 - accuracy: 0.6463 - jacard_coef: 0.07575/5 [==============================] - 2s 430ms/step - loss: 0.1710 - accuracy: 0.6448 - jacard_coef: 0.0832 - val_loss: 0.1708 - val_accuracy: 0.6575 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1686 - accuracy: 0.6167 - jacard_coef: 0.08672/5 [===========>..................] - ETA: 1s - loss: 0.1699 - accuracy: 0.6065 - jacard_coef: 0.08243/5 [=================>............] - ETA: 0s - loss: 0.1695 - accuracy: 0.6030 - jacard_coef: 0.07914/5 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.6265 - jacard_coef: 0.07675/5 [==============================] - 2s 429ms/step - loss: 0.1690 - accuracy: 0.6253 - jacard_coef: 0.0627 - val_loss: 0.1838 - val_accuracy: 0.2601 - val_jacard_coef: 0.0649 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1698 - accuracy: 0.7271 - jacard_coef: 0.09022/5 [===========>..................] - ETA: 1s - loss: 0.1676 - accuracy: 0.7493 - jacard_coef: 0.07433/5 [=================>............] - ETA: 0s - loss: 0.1673 - accuracy: 0.7259 - jacard_coef: 0.07624/5 [=======================>......] - ETA: 0s - loss: 0.1671 - accuracy: 0.7150 - jacard_coef: 0.07625/5 [==============================] - 2s 425ms/step - loss: 0.1674 - accuracy: 0.7124 - jacard_coef: 0.0692 - val_loss: 0.1732 - val_accuracy: 0.5833 - val_jacard_coef: 0.0655 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1686 - accuracy: 0.5657 - jacard_coef: 0.08092/5 [===========>..................] - ETA: 1s - loss: 0.1703 - accuracy: 0.5494 - jacard_coef: 0.08723/5 [=================>............] - ETA: 0s - loss: 0.1707 - accuracy: 0.5570 - jacard_coef: 0.08004/5 [=======================>......] - ETA: 0s - loss: 0.1711 - accuracy: 0.5639 - jacard_coef: 0.07665/5 [==============================] - 2s 407ms/step - loss: 0.1712 - accuracy: 0.5639 - jacard_coef: 0.0618 - val_loss: 0.1798 - val_accuracy: 0.4726 - val_jacard_coef: 0.0639 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1674 - accuracy: 0.6012 - jacard_coef: 0.08322/5 [===========>..................] - ETA: 1s - loss: 0.1677 - accuracy: 0.5835 - jacard_coef: 0.07363/5 [=================>............] - ETA: 0s - loss: 0.1675 - accuracy: 0.5921 - jacard_coef: 0.07394/5 [=======================>......] - ETA: 0s - loss: 0.1685 - accuracy: 0.5847 - jacard_coef: 0.07565/5 [==============================] - 2s 407ms/step - loss: 0.1686 - accuracy: 0.5838 - jacard_coef: 0.0923 - val_loss: 0.1503 - val_accuracy: 0.6234 - val_jacard_coef: 0.0558 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1669 - accuracy: 0.5857 - jacard_coef: 0.07942/5 [===========>..................] - ETA: 1s - loss: 0.1671 - accuracy: 0.6032 - jacard_coef: 0.07723/5 [=================>............] - ETA: 0s - loss: 0.1678 - accuracy: 0.6141 - jacard_coef: 0.07784/5 [=======================>......] - ETA: 0s - loss: 0.1684 - accuracy: 0.6358 - jacard_coef: 0.07675/5 [==============================] - 2s 407ms/step - loss: 0.1684 - accuracy: 0.6352 - jacard_coef: 0.0617 - val_loss: 0.1821 - val_accuracy: 0.7433 - val_jacard_coef: 0.0521 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1656 - accuracy: 0.7491 - jacard_coef: 0.07242/5 [===========>..................] - ETA: 1s - loss: 0.1666 - accuracy: 0.7396 - jacard_coef: 0.07123/5 [=================>............] - ETA: 0s - loss: 0.1660 - accuracy: 0.7354 - jacard_coef: 0.07404/5 [=======================>......] - ETA: 0s - loss: 0.1663 - accuracy: 0.7069 - jacard_coef: 0.07645/5 [==============================] - 2s 407ms/step - loss: 0.1664 - accuracy: 0.7044 - jacard_coef: 0.0726 - val_loss: 0.2160 - val_accuracy: 0.7814 - val_jacard_coef: 0.0488 - lr: 5.0000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1729 - accuracy: 0.5148 - jacard_coef: 0.06822/5 [===========>..................] - ETA: 1s - loss: 0.1698 - accuracy: 0.6501 - jacard_coef: 0.07663/5 [=================>............] - ETA: 0s - loss: 0.1678 - accuracy: 0.7168 - jacard_coef: 0.07134/5 [=======================>......] - ETA: 0s - loss: 0.1672 - accuracy: 0.7476 - jacard_coef: 0.07535/5 [==============================] - 2s 407ms/step - loss: 0.1672 - accuracy: 0.7469 - jacard_coef: 0.0905 - val_loss: 0.1408 - val_accuracy: 0.7444 - val_jacard_coef: 0.0575 - lr: 5.0000e-04
Epoch 22/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1639 - accuracy: 0.7656 - jacard_coef: 0.07542/5 [===========>..................] - ETA: 1s - loss: 0.1638 - accuracy: 0.7497 - jacard_coef: 0.07723/5 [=================>............] - ETA: 0s - loss: 0.1648 - accuracy: 0.7223 - jacard_coef: 0.07814/5 [=======================>......] - ETA: 0s - loss: 0.1641 - accuracy: 0.7257 - jacard_coef: 0.07575/5 [==============================] - 2s 407ms/step - loss: 0.1642 - accuracy: 0.7234 - jacard_coef: 0.0866 - val_loss: 0.1746 - val_accuracy: 0.4677 - val_jacard_coef: 0.0654 - lr: 2.5000e-04
Epoch 23/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1657 - accuracy: 0.6895 - jacard_coef: 0.08402/5 [===========>..................] - ETA: 1s - loss: 0.1659 - accuracy: 0.6715 - jacard_coef: 0.08713/5 [=================>............] - ETA: 0s - loss: 0.1654 - accuracy: 0.6925 - jacard_coef: 0.07854/5 [=======================>......] - ETA: 0s - loss: 0.1652 - accuracy: 0.7062 - jacard_coef: 0.07565/5 [==============================] - 2s 407ms/step - loss: 0.1653 - accuracy: 0.7051 - jacard_coef: 0.0856 - val_loss: 0.1873 - val_accuracy: 0.0783 - val_jacard_coef: 0.0654 - lr: 2.5000e-04
Epoch 24/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1677 - accuracy: 0.7518 - jacard_coef: 0.08112/5 [===========>..................] - ETA: 1s - loss: 0.1663 - accuracy: 0.7708 - jacard_coef: 0.07563/5 [=================>............] - ETA: 0s - loss: 0.1660 - accuracy: 0.7728 - jacard_coef: 0.07464/5 [=======================>......] - ETA: 0s - loss: 0.1658 - accuracy: 0.7816 - jacard_coef: 0.07675/5 [==============================] - 2s 407ms/step - loss: 0.1659 - accuracy: 0.7783 - jacard_coef: 0.0619 - val_loss: 0.1812 - val_accuracy: 0.0887 - val_jacard_coef: 0.0653 - lr: 2.5000e-04
Epoch 25/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1649 - accuracy: 0.8427 - jacard_coef: 0.07682/5 [===========>..................] - ETA: 1s - loss: 0.1659 - accuracy: 0.8499 - jacard_coef: 0.07753/5 [=================>............] - ETA: 0s - loss: 0.1659 - accuracy: 0.8493 - jacard_coef: 0.07894/5 [=======================>......] - ETA: 0s - loss: 0.1657 - accuracy: 0.8598 - jacard_coef: 0.07555/5 [==============================] - 2s 407ms/step - loss: 0.1658 - accuracy: 0.8575 - jacard_coef: 0.0907 - val_loss: 0.1787 - val_accuracy: 0.1679 - val_jacard_coef: 0.0652 - lr: 2.5000e-04
Epoch 26/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1670 - accuracy: 0.8957 - jacard_coef: 0.06662/5 [===========>..................] - ETA: 1s - loss: 0.1661 - accuracy: 0.8687 - jacard_coef: 0.06453/5 [=================>............] - ETA: 0s - loss: 0.1657 - accuracy: 0.8598 - jacard_coef: 0.07244/5 [=======================>......] - ETA: 0s - loss: 0.1657 - accuracy: 0.8495 - jacard_coef: 0.07615/5 [==============================] - 2s 407ms/step - loss: 0.1657 - accuracy: 0.8475 - jacard_coef: 0.0722 - val_loss: 0.1781 - val_accuracy: 0.1490 - val_jacard_coef: 0.0651 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0655 (epoch 16)
  Final Val Loss: 0.1781
  Training Time: 0:02:08.827502
  Stability (std): 0.0193

Results saved to: hyperparameter_optimization_20250926_123742/exp_3_UNet_lr1e-4_bs32/UNet_lr0.0001_bs32_results.json

Experiment 3 completed in 163s
Progress: 3/36 completed
Estimated remaining time: 89 minutes

ðŸ”¬ EXPERIMENT 4/36
================================================
Architecture: UNet
Learning Rate: 5e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758861954.287189 3188635 service.cc:145] XLA service 0x148ff1b6b6f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758861954.287230 3188635 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758861954.675954 3188635 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 10:52 - loss: 0.3503 - accuracy: 0.5178 - jacard_coef: 0.0648 2/17 [==>...........................] - ETA: 1:01 - loss: 0.3387 - accuracy: 0.4891 - jacard_coef: 0.0637  3/17 [====>.........................] - ETA: 37s - loss: 0.3083 - accuracy: 0.4283 - jacard_coef: 0.0754  4/17 [======>.......................] - ETA: 26s - loss: 0.2861 - accuracy: 0.3786 - jacard_coef: 0.0773 5/17 [=======>......................] - ETA: 18s - loss: 0.2732 - accuracy: 0.3376 - jacard_coef: 0.0734 6/17 [=========>....................] - ETA: 14s - loss: 0.2643 - accuracy: 0.3154 - jacard_coef: 0.0725 7/17 [===========>..................] - ETA: 11s - loss: 0.2570 - accuracy: 0.3018 - jacard_coef: 0.0744 8/17 [=============>................] - ETA: 8s - loss: 0.2493 - accuracy: 0.2991 - jacard_coef: 0.0722  9/17 [==============>...............] - ETA: 6s - loss: 0.2426 - accuracy: 0.2984 - jacard_coef: 0.072610/17 [================>.............] - ETA: 5s - loss: 0.2381 - accuracy: 0.3035 - jacard_coef: 0.071711/17 [==================>...........] - ETA: 4s - loss: 0.2331 - accuracy: 0.3147 - jacard_coef: 0.072412/17 [====================>.........] - ETA: 3s - loss: 0.2291 - accuracy: 0.3160 - jacard_coef: 0.072213/17 [=====================>........] - ETA: 2s - loss: 0.2256 - accuracy: 0.3171 - jacard_coef: 0.075414/17 [=======================>......] - ETA: 1s - loss: 0.2227 - accuracy: 0.3154 - jacard_coef: 0.074715/17 [=========================>....] - ETA: 1s - loss: 0.2200 - accuracy: 0.3192 - jacard_coef: 0.074416/17 [===========================>..] - ETA: 0s - loss: 0.2176 - accuracy: 0.3141 - jacard_coef: 0.076017/17 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.3134 - jacard_coef: 0.077917/17 [==============================] - 56s 944ms/step - loss: 0.2174 - accuracy: 0.3134 - jacard_coef: 0.0779 - val_loss: 1.1011 - val_accuracy: 0.9303 - val_jacard_coef: 4.9847e-04 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1925 - accuracy: 0.2963 - jacard_coef: 0.0905 2/17 [==>...........................] - ETA: 1s - loss: 0.1859 - accuracy: 0.3210 - jacard_coef: 0.0860 3/17 [====>.........................] - ETA: 1s - loss: 0.1846 - accuracy: 0.3205 - jacard_coef: 0.0814 4/17 [======>.......................] - ETA: 1s - loss: 0.1831 - accuracy: 0.3250 - jacard_coef: 0.0764 5/17 [=======>......................] - ETA: 1s - loss: 0.1859 - accuracy: 0.3190 - jacard_coef: 0.0728 6/17 [=========>....................] - ETA: 1s - loss: 0.1862 - accuracy: 0.3080 - jacard_coef: 0.0717 7/17 [===========>..................] - ETA: 1s - loss: 0.1850 - accuracy: 0.3162 - jacard_coef: 0.0772 8/17 [=============>................] - ETA: 1s - loss: 0.1839 - accuracy: 0.3260 - jacard_coef: 0.0763 9/17 [==============>...............] - ETA: 1s - loss: 0.1833 - accuracy: 0.3296 - jacard_coef: 0.076010/17 [================>.............] - ETA: 0s - loss: 0.1828 - accuracy: 0.3378 - jacard_coef: 0.075111/17 [==================>...........] - ETA: 0s - loss: 0.1821 - accuracy: 0.3580 - jacard_coef: 0.074712/17 [====================>.........] - ETA: 0s - loss: 0.1815 - accuracy: 0.3688 - jacard_coef: 0.074413/17 [=====================>........] - ETA: 0s - loss: 0.1809 - accuracy: 0.3732 - jacard_coef: 0.074014/17 [=======================>......] - ETA: 0s - loss: 0.1806 - accuracy: 0.3773 - jacard_coef: 0.075015/17 [=========================>....] - ETA: 0s - loss: 0.1802 - accuracy: 0.3849 - jacard_coef: 0.074016/17 [===========================>..] - ETA: 0s - loss: 0.1798 - accuracy: 0.3975 - jacard_coef: 0.075317/17 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.3980 - jacard_coef: 0.080217/17 [==============================] - 2s 139ms/step - loss: 0.1798 - accuracy: 0.3980 - jacard_coef: 0.0802 - val_loss: 0.6407 - val_accuracy: 0.9301 - val_jacard_coef: 0.0103 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1783 - accuracy: 0.6001 - jacard_coef: 0.0656 2/17 [==>...........................] - ETA: 1s - loss: 0.1746 - accuracy: 0.5608 - jacard_coef: 0.0605 3/17 [====>.........................] - ETA: 1s - loss: 0.1737 - accuracy: 0.5479 - jacard_coef: 0.0670 4/17 [======>.......................] - ETA: 1s - loss: 0.1735 - accuracy: 0.5638 - jacard_coef: 0.0798 5/17 [=======>......................] - ETA: 1s - loss: 0.1732 - accuracy: 0.5814 - jacard_coef: 0.0748 6/17 [=========>....................] - ETA: 1s - loss: 0.1733 - accuracy: 0.5695 - jacard_coef: 0.0745 7/17 [===========>..................] - ETA: 1s - loss: 0.1735 - accuracy: 0.5585 - jacard_coef: 0.0806 8/17 [=============>................] - ETA: 1s - loss: 0.1735 - accuracy: 0.5559 - jacard_coef: 0.0788 9/17 [==============>...............] - ETA: 1s - loss: 0.1741 - accuracy: 0.5600 - jacard_coef: 0.080410/17 [================>.............] - ETA: 0s - loss: 0.1738 - accuracy: 0.5546 - jacard_coef: 0.079711/17 [==================>...........] - ETA: 0s - loss: 0.1733 - accuracy: 0.5580 - jacard_coef: 0.076112/17 [====================>.........] - ETA: 0s - loss: 0.1727 - accuracy: 0.5623 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.1723 - accuracy: 0.5697 - jacard_coef: 0.075914/17 [=======================>......] - ETA: 0s - loss: 0.1720 - accuracy: 0.5740 - jacard_coef: 0.077515/17 [=========================>....] - ETA: 0s - loss: 0.1716 - accuracy: 0.5831 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1713 - accuracy: 0.5942 - jacard_coef: 0.075917/17 [==============================] - 2s 136ms/step - loss: 0.1717 - accuracy: 0.5924 - jacard_coef: 0.0749 - val_loss: 0.3219 - val_accuracy: 0.9279 - val_jacard_coef: 0.0416 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1702 - accuracy: 0.5506 - jacard_coef: 0.0754 2/17 [==>...........................] - ETA: 1s - loss: 0.1745 - accuracy: 0.5293 - jacard_coef: 0.0975 3/17 [====>.........................] - ETA: 1s - loss: 0.1818 - accuracy: 0.5128 - jacard_coef: 0.0939 4/17 [======>.......................] - ETA: 1s - loss: 0.1797 - accuracy: 0.5169 - jacard_coef: 0.0886 5/17 [=======>......................] - ETA: 1s - loss: 0.1789 - accuracy: 0.5260 - jacard_coef: 0.0833 6/17 [=========>....................] - ETA: 1s - loss: 0.1786 - accuracy: 0.5199 - jacard_coef: 0.0807 7/17 [===========>..................] - ETA: 1s - loss: 0.1775 - accuracy: 0.5325 - jacard_coef: 0.0748 8/17 [=============>................] - ETA: 1s - loss: 0.1764 - accuracy: 0.5462 - jacard_coef: 0.0766 9/17 [==============>...............] - ETA: 1s - loss: 0.1766 - accuracy: 0.5485 - jacard_coef: 0.077010/17 [================>.............] - ETA: 0s - loss: 0.1761 - accuracy: 0.5553 - jacard_coef: 0.074411/17 [==================>...........] - ETA: 0s - loss: 0.1754 - accuracy: 0.5659 - jacard_coef: 0.076912/17 [====================>.........] - ETA: 0s - loss: 0.1750 - accuracy: 0.5556 - jacard_coef: 0.078013/17 [=====================>........] - ETA: 0s - loss: 0.1744 - accuracy: 0.5717 - jacard_coef: 0.077514/17 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.5872 - jacard_coef: 0.077815/17 [=========================>....] - ETA: 0s - loss: 0.1735 - accuracy: 0.6028 - jacard_coef: 0.077516/17 [===========================>..] - ETA: 0s - loss: 0.1732 - accuracy: 0.6119 - jacard_coef: 0.076217/17 [==============================] - 2s 133ms/step - loss: 0.1732 - accuracy: 0.6099 - jacard_coef: 0.0749 - val_loss: 1.0927 - val_accuracy: 0.9294 - val_jacard_coef: 0.0018 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1672 - accuracy: 0.6218 - jacard_coef: 0.0623 2/17 [==>...........................] - ETA: 1s - loss: 0.1684 - accuracy: 0.5833 - jacard_coef: 0.0601 3/17 [====>.........................] - ETA: 1s - loss: 0.1693 - accuracy: 0.5550 - jacard_coef: 0.0763 4/17 [======>.......................] - ETA: 1s - loss: 0.1686 - accuracy: 0.5860 - jacard_coef: 0.0784 5/17 [=======>......................] - ETA: 1s - loss: 0.1704 - accuracy: 0.6040 - jacard_coef: 0.0813 6/17 [=========>....................] - ETA: 1s - loss: 0.1696 - accuracy: 0.6183 - jacard_coef: 0.0766 7/17 [===========>..................] - ETA: 1s - loss: 0.1689 - accuracy: 0.6351 - jacard_coef: 0.0731 8/17 [=============>................] - ETA: 1s - loss: 0.1702 - accuracy: 0.6277 - jacard_coef: 0.0716 9/17 [==============>...............] - ETA: 1s - loss: 0.1699 - accuracy: 0.6404 - jacard_coef: 0.072410/17 [================>.............] - ETA: 0s - loss: 0.1698 - accuracy: 0.6549 - jacard_coef: 0.071711/17 [==================>...........] - ETA: 0s - loss: 0.1696 - accuracy: 0.6369 - jacard_coef: 0.071312/17 [====================>.........] - ETA: 0s - loss: 0.1698 - accuracy: 0.6427 - jacard_coef: 0.075313/17 [=====================>........] - ETA: 0s - loss: 0.1708 - accuracy: 0.6396 - jacard_coef: 0.076314/17 [=======================>......] - ETA: 0s - loss: 0.1709 - accuracy: 0.6376 - jacard_coef: 0.076615/17 [=========================>....] - ETA: 0s - loss: 0.1721 - accuracy: 0.6340 - jacard_coef: 0.076516/17 [===========================>..] - ETA: 0s - loss: 0.1720 - accuracy: 0.6329 - jacard_coef: 0.076017/17 [==============================] - 2s 134ms/step - loss: 0.1720 - accuracy: 0.6326 - jacard_coef: 0.0728 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4102e-05 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1673 - accuracy: 0.4780 - jacard_coef: 0.0713 2/17 [==>...........................] - ETA: 1s - loss: 0.1684 - accuracy: 0.5481 - jacard_coef: 0.0757 3/17 [====>.........................] - ETA: 1s - loss: 0.1670 - accuracy: 0.5804 - jacard_coef: 0.0793 4/17 [======>.......................] - ETA: 1s - loss: 0.1677 - accuracy: 0.6197 - jacard_coef: 0.0778 5/17 [=======>......................] - ETA: 1s - loss: 0.1670 - accuracy: 0.6163 - jacard_coef: 0.0766 6/17 [=========>....................] - ETA: 1s - loss: 0.1659 - accuracy: 0.6503 - jacard_coef: 0.0780 7/17 [===========>..................] - ETA: 1s - loss: 0.1652 - accuracy: 0.6776 - jacard_coef: 0.0791 8/17 [=============>................] - ETA: 1s - loss: 0.1649 - accuracy: 0.6959 - jacard_coef: 0.0807 9/17 [==============>...............] - ETA: 1s - loss: 0.1648 - accuracy: 0.7104 - jacard_coef: 0.084010/17 [================>.............] - ETA: 0s - loss: 0.1649 - accuracy: 0.7153 - jacard_coef: 0.081211/17 [==================>...........] - ETA: 0s - loss: 0.1650 - accuracy: 0.7113 - jacard_coef: 0.078112/17 [====================>.........] - ETA: 0s - loss: 0.1648 - accuracy: 0.7193 - jacard_coef: 0.077413/17 [=====================>........] - ETA: 0s - loss: 0.1643 - accuracy: 0.7317 - jacard_coef: 0.076114/17 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.7380 - jacard_coef: 0.078315/17 [=========================>....] - ETA: 0s - loss: 0.1637 - accuracy: 0.7454 - jacard_coef: 0.077016/17 [===========================>..] - ETA: 0s - loss: 0.1634 - accuracy: 0.7512 - jacard_coef: 0.076217/17 [==============================] - 2s 136ms/step - loss: 0.1634 - accuracy: 0.7493 - jacard_coef: 0.0718 - val_loss: 2.2546 - val_accuracy: 0.8526 - val_jacard_coef: 0.0408 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8504 - jacard_coef: 0.0559 2/17 [==>...........................] - ETA: 2s - loss: 0.1616 - accuracy: 0.8127 - jacard_coef: 0.0786 3/17 [====>.........................] - ETA: 1s - loss: 0.1618 - accuracy: 0.8118 - jacard_coef: 0.0724 4/17 [======>.......................] - ETA: 1s - loss: 0.1619 - accuracy: 0.8126 - jacard_coef: 0.0814 5/17 [=======>......................] - ETA: 1s - loss: 0.1615 - accuracy: 0.8242 - jacard_coef: 0.0818 6/17 [=========>....................] - ETA: 1s - loss: 0.1610 - accuracy: 0.8367 - jacard_coef: 0.0798 7/17 [===========>..................] - ETA: 1s - loss: 0.1622 - accuracy: 0.8437 - jacard_coef: 0.0804 8/17 [=============>................] - ETA: 1s - loss: 0.1614 - accuracy: 0.8541 - jacard_coef: 0.0759 9/17 [==============>...............] - ETA: 1s - loss: 0.1625 - accuracy: 0.8589 - jacard_coef: 0.076210/17 [================>.............] - ETA: 0s - loss: 0.1622 - accuracy: 0.8605 - jacard_coef: 0.076811/17 [==================>...........] - ETA: 0s - loss: 0.1621 - accuracy: 0.8631 - jacard_coef: 0.076612/17 [====================>.........] - ETA: 0s - loss: 0.1615 - accuracy: 0.8659 - jacard_coef: 0.073913/17 [=====================>........] - ETA: 0s - loss: 0.1611 - accuracy: 0.8710 - jacard_coef: 0.071514/17 [=======================>......] - ETA: 0s - loss: 0.1609 - accuracy: 0.8722 - jacard_coef: 0.072115/17 [=========================>....] - ETA: 0s - loss: 0.1609 - accuracy: 0.8727 - jacard_coef: 0.073616/17 [===========================>..] - ETA: 0s - loss: 0.1609 - accuracy: 0.8717 - jacard_coef: 0.075717/17 [==============================] - 2s 143ms/step - loss: 0.1608 - accuracy: 0.8723 - jacard_coef: 0.0714 - val_loss: 10.7675 - val_accuracy: 0.1597 - val_jacard_coef: 0.0665 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1635 - accuracy: 0.9055 - jacard_coef: 0.0671 2/17 [==>...........................] - ETA: 2s - loss: 0.1581 - accuracy: 0.8990 - jacard_coef: 0.0641 3/17 [====>.........................] - ETA: 1s - loss: 0.1599 - accuracy: 0.7980 - jacard_coef: 0.0689 4/17 [======>.......................] - ETA: 1s - loss: 0.1575 - accuracy: 0.8380 - jacard_coef: 0.0612 5/17 [=======>......................] - ETA: 1s - loss: 0.1572 - accuracy: 0.8551 - jacard_coef: 0.0615 6/17 [=========>....................] - ETA: 1s - loss: 0.1573 - accuracy: 0.8571 - jacard_coef: 0.0643 7/17 [===========>..................] - ETA: 1s - loss: 0.1573 - accuracy: 0.8564 - jacard_coef: 0.0644 8/17 [=============>................] - ETA: 1s - loss: 0.1574 - accuracy: 0.8526 - jacard_coef: 0.0650 9/17 [==============>...............] - ETA: 1s - loss: 0.1576 - accuracy: 0.8491 - jacard_coef: 0.068710/17 [================>.............] - ETA: 0s - loss: 0.1590 - accuracy: 0.8490 - jacard_coef: 0.071311/17 [==================>...........] - ETA: 0s - loss: 0.1595 - accuracy: 0.8522 - jacard_coef: 0.073512/17 [====================>.........] - ETA: 0s - loss: 0.1590 - accuracy: 0.8595 - jacard_coef: 0.071813/17 [=====================>........] - ETA: 0s - loss: 0.1588 - accuracy: 0.8617 - jacard_coef: 0.073814/17 [=======================>......] - ETA: 0s - loss: 0.1586 - accuracy: 0.8656 - jacard_coef: 0.073615/17 [=========================>....] - ETA: 0s - loss: 0.1583 - accuracy: 0.8685 - jacard_coef: 0.073916/17 [===========================>..] - ETA: 0s - loss: 0.1581 - accuracy: 0.8683 - jacard_coef: 0.075817/17 [==============================] - 2s 143ms/step - loss: 0.1580 - accuracy: 0.8683 - jacard_coef: 0.0731 - val_loss: 12.8182 - val_accuracy: 0.1864 - val_jacard_coef: 0.0699 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1535 - accuracy: 0.8315 - jacard_coef: 0.1070 2/17 [==>...........................] - ETA: 2s - loss: 0.1545 - accuracy: 0.8773 - jacard_coef: 0.0829 3/17 [====>.........................] - ETA: 1s - loss: 0.1574 - accuracy: 0.8715 - jacard_coef: 0.0911 4/17 [======>.......................] - ETA: 1s - loss: 0.1555 - accuracy: 0.8773 - jacard_coef: 0.0897 5/17 [=======>......................] - ETA: 1s - loss: 0.1541 - accuracy: 0.8842 - jacard_coef: 0.0846 6/17 [=========>....................] - ETA: 1s - loss: 0.1531 - accuracy: 0.8889 - jacard_coef: 0.0791 7/17 [===========>..................] - ETA: 1s - loss: 0.1539 - accuracy: 0.8808 - jacard_coef: 0.0797 8/17 [=============>................] - ETA: 1s - loss: 0.1533 - accuracy: 0.8821 - jacard_coef: 0.0811 9/17 [==============>...............] - ETA: 1s - loss: 0.1526 - accuracy: 0.8876 - jacard_coef: 0.078410/17 [================>.............] - ETA: 0s - loss: 0.1523 - accuracy: 0.8885 - jacard_coef: 0.079011/17 [==================>...........] - ETA: 0s - loss: 0.1519 - accuracy: 0.8902 - jacard_coef: 0.078912/17 [====================>.........] - ETA: 0s - loss: 0.1514 - accuracy: 0.8921 - jacard_coef: 0.078513/17 [=====================>........] - ETA: 0s - loss: 0.1510 - accuracy: 0.8937 - jacard_coef: 0.078214/17 [=======================>......] - ETA: 0s - loss: 0.1507 - accuracy: 0.8955 - jacard_coef: 0.077715/17 [=========================>....] - ETA: 0s - loss: 0.1507 - accuracy: 0.8992 - jacard_coef: 0.075516/17 [===========================>..] - ETA: 0s - loss: 0.1504 - accuracy: 0.9002 - jacard_coef: 0.075617/17 [==============================] - 2s 137ms/step - loss: 0.1504 - accuracy: 0.9009 - jacard_coef: 0.0716 - val_loss: 13.2116 - val_accuracy: 0.1006 - val_jacard_coef: 0.0694 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1437 - accuracy: 0.9609 - jacard_coef: 0.0342 2/17 [==>...........................] - ETA: 2s - loss: 0.1447 - accuracy: 0.9474 - jacard_coef: 0.0441 3/17 [====>.........................] - ETA: 1s - loss: 0.1456 - accuracy: 0.9376 - jacard_coef: 0.0532 4/17 [======>.......................] - ETA: 1s - loss: 0.1455 - accuracy: 0.9349 - jacard_coef: 0.0558 5/17 [=======>......................] - ETA: 1s - loss: 0.1458 - accuracy: 0.9297 - jacard_coef: 0.0602 6/17 [=========>....................] - ETA: 1s - loss: 0.1460 - accuracy: 0.9288 - jacard_coef: 0.0613 7/17 [===========>..................] - ETA: 1s - loss: 0.1471 - accuracy: 0.9228 - jacard_coef: 0.0668 8/17 [=============>................] - ETA: 1s - loss: 0.1469 - accuracy: 0.9208 - jacard_coef: 0.0691 9/17 [==============>...............] - ETA: 1s - loss: 0.1466 - accuracy: 0.9197 - jacard_coef: 0.070410/17 [================>.............] - ETA: 0s - loss: 0.1463 - accuracy: 0.9190 - jacard_coef: 0.071311/17 [==================>...........] - ETA: 0s - loss: 0.1461 - accuracy: 0.9165 - jacard_coef: 0.073512/17 [====================>.........] - ETA: 0s - loss: 0.1459 - accuracy: 0.9178 - jacard_coef: 0.071413/17 [=====================>........] - ETA: 0s - loss: 0.1456 - accuracy: 0.9190 - jacard_coef: 0.070714/17 [=======================>......] - ETA: 0s - loss: 0.1454 - accuracy: 0.9187 - jacard_coef: 0.071215/17 [=========================>....] - ETA: 0s - loss: 0.1454 - accuracy: 0.9156 - jacard_coef: 0.073916/17 [===========================>..] - ETA: 0s - loss: 0.1458 - accuracy: 0.9150 - jacard_coef: 0.074617/17 [==============================] - 2s 137ms/step - loss: 0.1460 - accuracy: 0.9138 - jacard_coef: 0.0776 - val_loss: 12.8898 - val_accuracy: 0.1564 - val_jacard_coef: 0.0651 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1461 - accuracy: 0.8985 - jacard_coef: 0.0813 2/17 [==>...........................] - ETA: 2s - loss: 0.1533 - accuracy: 0.9019 - jacard_coef: 0.0745 3/17 [====>.........................] - ETA: 1s - loss: 0.1527 - accuracy: 0.8956 - jacard_coef: 0.0780 4/17 [======>.......................] - ETA: 1s - loss: 0.1535 - accuracy: 0.8909 - jacard_coef: 0.0813 5/17 [=======>......................] - ETA: 1s - loss: 0.1527 - accuracy: 0.8925 - jacard_coef: 0.0745 6/17 [=========>....................] - ETA: 1s - loss: 0.1523 - accuracy: 0.8786 - jacard_coef: 0.0724 7/17 [===========>..................] - ETA: 1s - loss: 0.1521 - accuracy: 0.8712 - jacard_coef: 0.0758 8/17 [=============>................] - ETA: 1s - loss: 0.1519 - accuracy: 0.8612 - jacard_coef: 0.0784 9/17 [==============>...............] - ETA: 1s - loss: 0.1511 - accuracy: 0.8685 - jacard_coef: 0.075010/17 [================>.............] - ETA: 0s - loss: 0.1507 - accuracy: 0.8706 - jacard_coef: 0.075111/17 [==================>...........] - ETA: 0s - loss: 0.1503 - accuracy: 0.8718 - jacard_coef: 0.076112/17 [====================>.........] - ETA: 0s - loss: 0.1508 - accuracy: 0.8706 - jacard_coef: 0.078213/17 [=====================>........] - ETA: 0s - loss: 0.1501 - accuracy: 0.8733 - jacard_coef: 0.078014/17 [=======================>......] - ETA: 0s - loss: 0.1496 - accuracy: 0.8755 - jacard_coef: 0.077915/17 [=========================>....] - ETA: 0s - loss: 0.1490 - accuracy: 0.8796 - jacard_coef: 0.075816/17 [===========================>..] - ETA: 0s - loss: 0.1486 - accuracy: 0.8812 - jacard_coef: 0.075117/17 [==============================] - 2s 137ms/step - loss: 0.1489 - accuracy: 0.8790 - jacard_coef: 0.0737 - val_loss: 1.0298 - val_accuracy: 0.8898 - val_jacard_coef: 0.0334 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1519 - accuracy: 0.8603 - jacard_coef: 0.1048 2/17 [==>...........................] - ETA: 2s - loss: 0.1550 - accuracy: 0.8577 - jacard_coef: 0.1069 3/17 [====>.........................] - ETA: 1s - loss: 0.1532 - accuracy: 0.8845 - jacard_coef: 0.0844 4/17 [======>.......................] - ETA: 1s - loss: 0.1531 - accuracy: 0.8898 - jacard_coef: 0.0806 5/17 [=======>......................] - ETA: 1s - loss: 0.1548 - accuracy: 0.8860 - jacard_coef: 0.0830 6/17 [=========>....................] - ETA: 1s - loss: 0.1556 - accuracy: 0.8857 - jacard_coef: 0.0825 7/17 [===========>..................] - ETA: 1s - loss: 0.1557 - accuracy: 0.8867 - jacard_coef: 0.0809 8/17 [=============>................] - ETA: 1s - loss: 0.1572 - accuracy: 0.8867 - jacard_coef: 0.0814 9/17 [==============>...............] - ETA: 1s - loss: 0.1581 - accuracy: 0.8932 - jacard_coef: 0.076510/17 [================>.............] - ETA: 0s - loss: 0.1573 - accuracy: 0.8915 - jacard_coef: 0.078111/17 [==================>...........] - ETA: 0s - loss: 0.1563 - accuracy: 0.8939 - jacard_coef: 0.076712/17 [====================>.........] - ETA: 0s - loss: 0.1553 - accuracy: 0.8961 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 0s - loss: 0.1546 - accuracy: 0.8954 - jacard_coef: 0.072914/17 [=======================>......] - ETA: 0s - loss: 0.1538 - accuracy: 0.8972 - jacard_coef: 0.072015/17 [=========================>....] - ETA: 0s - loss: 0.1539 - accuracy: 0.8988 - jacard_coef: 0.071616/17 [===========================>..] - ETA: 0s - loss: 0.1540 - accuracy: 0.8969 - jacard_coef: 0.074317/17 [==============================] - 2s 137ms/step - loss: 0.1540 - accuracy: 0.8960 - jacard_coef: 0.0772 - val_loss: 0.1516 - val_accuracy: 0.9303 - val_jacard_coef: 0.0631 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1531 - accuracy: 0.9051 - jacard_coef: 0.0841 2/17 [==>...........................] - ETA: 2s - loss: 0.1454 - accuracy: 0.9210 - jacard_coef: 0.0711 3/17 [====>.........................] - ETA: 1s - loss: 0.1476 - accuracy: 0.9047 - jacard_coef: 0.0840 4/17 [======>.......................] - ETA: 1s - loss: 0.1462 - accuracy: 0.8834 - jacard_coef: 0.0772 5/17 [=======>......................] - ETA: 1s - loss: 0.1453 - accuracy: 0.8839 - jacard_coef: 0.0810 6/17 [=========>....................] - ETA: 1s - loss: 0.1445 - accuracy: 0.8894 - jacard_coef: 0.0789 7/17 [===========>..................] - ETA: 1s - loss: 0.1439 - accuracy: 0.8919 - jacard_coef: 0.0789 8/17 [=============>................] - ETA: 1s - loss: 0.1434 - accuracy: 0.8971 - jacard_coef: 0.0758 9/17 [==============>...............] - ETA: 1s - loss: 0.1422 - accuracy: 0.9043 - jacard_coef: 0.070910/17 [================>.............] - ETA: 0s - loss: 0.1420 - accuracy: 0.9043 - jacard_coef: 0.071911/17 [==================>...........] - ETA: 0s - loss: 0.1416 - accuracy: 0.9040 - jacard_coef: 0.072712/17 [====================>.........] - ETA: 0s - loss: 0.1413 - accuracy: 0.9057 - jacard_coef: 0.071813/17 [=====================>........] - ETA: 0s - loss: 0.1408 - accuracy: 0.9064 - jacard_coef: 0.070814/17 [=======================>......] - ETA: 0s - loss: 0.1404 - accuracy: 0.9083 - jacard_coef: 0.069915/17 [=========================>....] - ETA: 0s - loss: 0.1404 - accuracy: 0.9054 - jacard_coef: 0.072716/17 [===========================>..] - ETA: 0s - loss: 0.1407 - accuracy: 0.9045 - jacard_coef: 0.074117/17 [==============================] - 2s 137ms/step - loss: 0.1408 - accuracy: 0.9041 - jacard_coef: 0.0771 - val_loss: 0.1708 - val_accuracy: 0.9304 - val_jacard_coef: 0.0606 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1350 - accuracy: 0.9379 - jacard_coef: 0.0554 2/17 [==>...........................] - ETA: 2s - loss: 0.1369 - accuracy: 0.9243 - jacard_coef: 0.0667 3/17 [====>.........................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9281 - jacard_coef: 0.0640 4/17 [======>.......................] - ETA: 1s - loss: 0.1355 - accuracy: 0.9242 - jacard_coef: 0.0676 5/17 [=======>......................] - ETA: 1s - loss: 0.1358 - accuracy: 0.9201 - jacard_coef: 0.0714 6/17 [=========>....................] - ETA: 1s - loss: 0.1371 - accuracy: 0.9165 - jacard_coef: 0.0743 7/17 [===========>..................] - ETA: 1s - loss: 0.1372 - accuracy: 0.9156 - jacard_coef: 0.0737 8/17 [=============>................] - ETA: 1s - loss: 0.1370 - accuracy: 0.9160 - jacard_coef: 0.0736 9/17 [==============>...............] - ETA: 1s - loss: 0.1365 - accuracy: 0.9183 - jacard_coef: 0.071810/17 [================>.............] - ETA: 0s - loss: 0.1367 - accuracy: 0.9156 - jacard_coef: 0.074111/17 [==================>...........] - ETA: 0s - loss: 0.1365 - accuracy: 0.9151 - jacard_coef: 0.074612/17 [====================>.........] - ETA: 0s - loss: 0.1362 - accuracy: 0.9162 - jacard_coef: 0.073713/17 [=====================>........] - ETA: 0s - loss: 0.1364 - accuracy: 0.9146 - jacard_coef: 0.075014/17 [=======================>......] - ETA: 0s - loss: 0.1360 - accuracy: 0.9185 - jacard_coef: 0.071615/17 [=========================>....] - ETA: 0s - loss: 0.1360 - accuracy: 0.9163 - jacard_coef: 0.073316/17 [===========================>..] - ETA: 0s - loss: 0.1360 - accuracy: 0.9151 - jacard_coef: 0.074217/17 [==============================] - 2s 137ms/step - loss: 0.1361 - accuracy: 0.9141 - jacard_coef: 0.0780 - val_loss: 0.1429 - val_accuracy: 0.9304 - val_jacard_coef: 0.0616 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1377 - accuracy: 0.9248 - jacard_coef: 0.0626 2/17 [==>...........................] - ETA: 2s - loss: 0.1411 - accuracy: 0.9009 - jacard_coef: 0.0823 3/17 [====>.........................] - ETA: 1s - loss: 0.1421 - accuracy: 0.8954 - jacard_coef: 0.0857 4/17 [======>.......................] - ETA: 1s - loss: 0.1428 - accuracy: 0.8965 - jacard_coef: 0.0857 5/17 [=======>......................] - ETA: 1s - loss: 0.1416 - accuracy: 0.9005 - jacard_coef: 0.0824 6/17 [=========>....................] - ETA: 1s - loss: 0.1408 - accuracy: 0.9046 - jacard_coef: 0.0792 7/17 [===========>..................] - ETA: 1s - loss: 0.1411 - accuracy: 0.8994 - jacard_coef: 0.0834 8/17 [=============>................] - ETA: 1s - loss: 0.1407 - accuracy: 0.9034 - jacard_coef: 0.0802 9/17 [==============>...............] - ETA: 1s - loss: 0.1403 - accuracy: 0.9042 - jacard_coef: 0.079710/17 [================>.............] - ETA: 0s - loss: 0.1396 - accuracy: 0.9073 - jacard_coef: 0.077511/17 [==================>...........] - ETA: 0s - loss: 0.1403 - accuracy: 0.9066 - jacard_coef: 0.078412/17 [====================>.........] - ETA: 0s - loss: 0.1399 - accuracy: 0.9081 - jacard_coef: 0.077513/17 [=====================>........] - ETA: 0s - loss: 0.1396 - accuracy: 0.9101 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1394 - accuracy: 0.9138 - jacard_coef: 0.073215/17 [=========================>....] - ETA: 0s - loss: 0.1392 - accuracy: 0.9130 - jacard_coef: 0.074216/17 [===========================>..] - ETA: 0s - loss: 0.1391 - accuracy: 0.9125 - jacard_coef: 0.074817/17 [==============================] - 2s 137ms/step - loss: 0.1391 - accuracy: 0.9126 - jacard_coef: 0.0740 - val_loss: 0.1373 - val_accuracy: 0.9304 - val_jacard_coef: 0.0624 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1351 - accuracy: 0.9023 - jacard_coef: 0.0862 2/17 [==>...........................] - ETA: 2s - loss: 0.1387 - accuracy: 0.8940 - jacard_coef: 0.0928 3/17 [====>.........................] - ETA: 1s - loss: 0.1370 - accuracy: 0.9014 - jacard_coef: 0.0866 4/17 [======>.......................] - ETA: 1s - loss: 0.1358 - accuracy: 0.9080 - jacard_coef: 0.0812 5/17 [=======>......................] - ETA: 1s - loss: 0.1353 - accuracy: 0.9102 - jacard_coef: 0.0795 6/17 [=========>....................] - ETA: 1s - loss: 0.1352 - accuracy: 0.9100 - jacard_coef: 0.0798 7/17 [===========>..................] - ETA: 1s - loss: 0.1350 - accuracy: 0.9117 - jacard_coef: 0.0785 8/17 [=============>................] - ETA: 1s - loss: 0.1348 - accuracy: 0.9104 - jacard_coef: 0.0796 9/17 [==============>...............] - ETA: 1s - loss: 0.1347 - accuracy: 0.9103 - jacard_coef: 0.079810/17 [================>.............] - ETA: 0s - loss: 0.1356 - accuracy: 0.9122 - jacard_coef: 0.078211/17 [==================>...........] - ETA: 0s - loss: 0.1349 - accuracy: 0.9160 - jacard_coef: 0.075012/17 [====================>.........] - ETA: 0s - loss: 0.1349 - accuracy: 0.9139 - jacard_coef: 0.076713/17 [=====================>........] - ETA: 0s - loss: 0.1352 - accuracy: 0.9126 - jacard_coef: 0.077614/17 [=======================>......] - ETA: 0s - loss: 0.1345 - accuracy: 0.9163 - jacard_coef: 0.074415/17 [=========================>....] - ETA: 0s - loss: 0.1345 - accuracy: 0.9157 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1343 - accuracy: 0.9158 - jacard_coef: 0.075017/17 [==============================] - 2s 138ms/step - loss: 0.1344 - accuracy: 0.9161 - jacard_coef: 0.0718 - val_loss: 0.1300 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1319 - accuracy: 0.9145 - jacard_coef: 0.0761 2/17 [==>...........................] - ETA: 2s - loss: 0.1305 - accuracy: 0.9291 - jacard_coef: 0.0638 3/17 [====>.........................] - ETA: 1s - loss: 0.1315 - accuracy: 0.9182 - jacard_coef: 0.0723 4/17 [======>.......................] - ETA: 1s - loss: 0.1324 - accuracy: 0.9125 - jacard_coef: 0.0773 5/17 [=======>......................] - ETA: 1s - loss: 0.1316 - accuracy: 0.9165 - jacard_coef: 0.0741 6/17 [=========>....................] - ETA: 1s - loss: 0.1327 - accuracy: 0.9068 - jacard_coef: 0.0817 7/17 [===========>..................] - ETA: 1s - loss: 0.1323 - accuracy: 0.9106 - jacard_coef: 0.0787 8/17 [=============>................] - ETA: 1s - loss: 0.1334 - accuracy: 0.9104 - jacard_coef: 0.0789 9/17 [==============>...............] - ETA: 1s - loss: 0.1329 - accuracy: 0.9148 - jacard_coef: 0.075210/17 [================>.............] - ETA: 0s - loss: 0.1322 - accuracy: 0.9188 - jacard_coef: 0.071811/17 [==================>...........] - ETA: 0s - loss: 0.1330 - accuracy: 0.9160 - jacard_coef: 0.074112/17 [====================>.........] - ETA: 0s - loss: 0.1331 - accuracy: 0.9144 - jacard_coef: 0.075513/17 [=====================>........] - ETA: 0s - loss: 0.1330 - accuracy: 0.9132 - jacard_coef: 0.076514/17 [=======================>......] - ETA: 0s - loss: 0.1327 - accuracy: 0.9155 - jacard_coef: 0.074615/17 [=========================>....] - ETA: 0s - loss: 0.1329 - accuracy: 0.9151 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9166 - jacard_coef: 0.073817/17 [==============================] - 2s 137ms/step - loss: 0.1326 - accuracy: 0.9159 - jacard_coef: 0.0780 - val_loss: 0.1277 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1284 - accuracy: 0.9333 - jacard_coef: 0.0609 2/17 [==>...........................] - ETA: 2s - loss: 0.1311 - accuracy: 0.9176 - jacard_coef: 0.0741 3/17 [====>.........................] - ETA: 1s - loss: 0.1323 - accuracy: 0.9138 - jacard_coef: 0.0771 4/17 [======>.......................] - ETA: 1s - loss: 0.1334 - accuracy: 0.9021 - jacard_coef: 0.0865 5/17 [=======>......................] - ETA: 1s - loss: 0.1328 - accuracy: 0.9094 - jacard_coef: 0.0806 6/17 [=========>....................] - ETA: 1s - loss: 0.1321 - accuracy: 0.9115 - jacard_coef: 0.0787 7/17 [===========>..................] - ETA: 1s - loss: 0.1321 - accuracy: 0.9101 - jacard_coef: 0.0800 8/17 [=============>................] - ETA: 1s - loss: 0.1320 - accuracy: 0.9088 - jacard_coef: 0.0809 9/17 [==============>...............] - ETA: 1s - loss: 0.1315 - accuracy: 0.9110 - jacard_coef: 0.079210/17 [================>.............] - ETA: 0s - loss: 0.1312 - accuracy: 0.9132 - jacard_coef: 0.077311/17 [==================>...........] - ETA: 0s - loss: 0.1312 - accuracy: 0.9127 - jacard_coef: 0.077712/17 [====================>.........] - ETA: 0s - loss: 0.1310 - accuracy: 0.9135 - jacard_coef: 0.077113/17 [=====================>........] - ETA: 0s - loss: 0.1310 - accuracy: 0.9159 - jacard_coef: 0.075014/17 [=======================>......] - ETA: 0s - loss: 0.1312 - accuracy: 0.9133 - jacard_coef: 0.077015/17 [=========================>....] - ETA: 0s - loss: 0.1311 - accuracy: 0.9125 - jacard_coef: 0.077616/17 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.9157 - jacard_coef: 0.074817/17 [==============================] - 2s 137ms/step - loss: 0.1306 - accuracy: 0.9161 - jacard_coef: 0.0719 - val_loss: 0.1268 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0699 (epoch 8)
  Final Val Loss: 0.1268
  Training Time: 0:01:36.457689
  Stability (std): 5.1267

Results saved to: hyperparameter_optimization_20250926_123742/exp_4_UNet_lr5e-4_bs8/UNet_lr0.0005_bs8_results.json

Experiment 4 completed in 131s
Progress: 4/36 completed
Estimated remaining time: 69 minutes

ðŸ”¬ EXPERIMENT 5/36
================================================
Architecture: UNet
Learning Rate: 5e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758862085.867091 3192690 service.cc:145] XLA service 0x14bee5848c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758862085.867166 3192690 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758862086.327500 3192690 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 6:00 - loss: 0.3481 - accuracy: 0.5051 - jacard_coef: 0.05862/9 [=====>........................] - ETA: 49s - loss: 0.3278 - accuracy: 0.4576 - jacard_coef: 0.0596 3/9 [=========>....................] - ETA: 29s - loss: 0.3005 - accuracy: 0.3845 - jacard_coef: 0.06264/9 [============>.................] - ETA: 20s - loss: 0.2797 - accuracy: 0.3599 - jacard_coef: 0.06825/9 [===============>..............] - ETA: 12s - loss: 0.2654 - accuracy: 0.3339 - jacard_coef: 0.06916/9 [===================>..........] - ETA: 7s - loss: 0.2545 - accuracy: 0.3177 - jacard_coef: 0.0761 7/9 [======================>.......] - ETA: 4s - loss: 0.2460 - accuracy: 0.3083 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 1s - loss: 0.2387 - accuracy: 0.3111 - jacard_coef: 0.07579/9 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.3110 - jacard_coef: 0.08249/9 [==============================] - 65s 3s/step - loss: 0.2384 - accuracy: 0.3110 - jacard_coef: 0.0824 - val_loss: 1.1102 - val_accuracy: 0.9304 - val_jacard_coef: 3.4119e-04 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1842 - accuracy: 0.4353 - jacard_coef: 0.07232/9 [=====>........................] - ETA: 1s - loss: 0.1845 - accuracy: 0.4021 - jacard_coef: 0.07033/9 [=========>....................] - ETA: 1s - loss: 0.1826 - accuracy: 0.4229 - jacard_coef: 0.07414/9 [============>.................] - ETA: 1s - loss: 0.1815 - accuracy: 0.4245 - jacard_coef: 0.07635/9 [===============>..............] - ETA: 0s - loss: 0.1807 - accuracy: 0.4242 - jacard_coef: 0.07736/9 [===================>..........] - ETA: 0s - loss: 0.1800 - accuracy: 0.4231 - jacard_coef: 0.07747/9 [======================>.......] - ETA: 0s - loss: 0.1793 - accuracy: 0.4289 - jacard_coef: 0.07818/9 [=========================>....] - ETA: 0s - loss: 0.1787 - accuracy: 0.4317 - jacard_coef: 0.07569/9 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.4318 - jacard_coef: 0.08219/9 [==============================] - 2s 244ms/step - loss: 0.1788 - accuracy: 0.4318 - jacard_coef: 0.0821 - val_loss: 1.1220 - val_accuracy: 0.9304 - val_jacard_coef: 1.4611e-05 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1750 - accuracy: 0.5187 - jacard_coef: 0.07172/9 [=====>........................] - ETA: 1s - loss: 0.1746 - accuracy: 0.5649 - jacard_coef: 0.08183/9 [=========>....................] - ETA: 1s - loss: 0.1742 - accuracy: 0.5962 - jacard_coef: 0.07994/9 [============>.................] - ETA: 1s - loss: 0.1739 - accuracy: 0.6189 - jacard_coef: 0.07525/9 [===============>..............] - ETA: 0s - loss: 0.1736 - accuracy: 0.6214 - jacard_coef: 0.07426/9 [===================>..........] - ETA: 0s - loss: 0.1732 - accuracy: 0.6137 - jacard_coef: 0.07757/9 [======================>.......] - ETA: 0s - loss: 0.1731 - accuracy: 0.5980 - jacard_coef: 0.07778/9 [=========================>....] - ETA: 0s - loss: 0.1728 - accuracy: 0.5880 - jacard_coef: 0.07669/9 [==============================] - 2s 234ms/step - loss: 0.1732 - accuracy: 0.5852 - jacard_coef: 0.0687 - val_loss: 1.1209 - val_accuracy: 0.9304 - val_jacard_coef: 1.9471e-05 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.2003 - accuracy: 0.1640 - jacard_coef: 0.09662/9 [=====>........................] - ETA: 1s - loss: 0.1923 - accuracy: 0.1868 - jacard_coef: 0.08533/9 [=========>....................] - ETA: 1s - loss: 0.1873 - accuracy: 0.2692 - jacard_coef: 0.08054/9 [============>.................] - ETA: 1s - loss: 0.1850 - accuracy: 0.3128 - jacard_coef: 0.07345/9 [===============>..............] - ETA: 0s - loss: 0.1827 - accuracy: 0.3465 - jacard_coef: 0.07526/9 [===================>..........] - ETA: 0s - loss: 0.1812 - accuracy: 0.3740 - jacard_coef: 0.07407/9 [======================>.......] - ETA: 0s - loss: 0.1799 - accuracy: 0.4076 - jacard_coef: 0.07498/9 [=========================>....] - ETA: 0s - loss: 0.1792 - accuracy: 0.4379 - jacard_coef: 0.07659/9 [==============================] - 2s 241ms/step - loss: 0.1795 - accuracy: 0.4387 - jacard_coef: 0.0683 - val_loss: 1.0319 - val_accuracy: 0.9180 - val_jacard_coef: 0.0124 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1702 - accuracy: 0.7340 - jacard_coef: 0.09782/9 [=====>........................] - ETA: 1s - loss: 0.1706 - accuracy: 0.7134 - jacard_coef: 0.08293/9 [=========>....................] - ETA: 1s - loss: 0.1708 - accuracy: 0.6844 - jacard_coef: 0.07624/9 [============>.................] - ETA: 1s - loss: 0.1708 - accuracy: 0.6709 - jacard_coef: 0.07185/9 [===============>..............] - ETA: 0s - loss: 0.1704 - accuracy: 0.6635 - jacard_coef: 0.07106/9 [===================>..........] - ETA: 0s - loss: 0.1705 - accuracy: 0.6514 - jacard_coef: 0.07397/9 [======================>.......] - ETA: 0s - loss: 0.1708 - accuracy: 0.6258 - jacard_coef: 0.07728/9 [=========================>....] - ETA: 0s - loss: 0.1707 - accuracy: 0.6285 - jacard_coef: 0.07549/9 [==============================] - 2s 235ms/step - loss: 0.1707 - accuracy: 0.6277 - jacard_coef: 0.0813 - val_loss: 1.0435 - val_accuracy: 0.9214 - val_jacard_coef: 0.0113 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1694 - accuracy: 0.6962 - jacard_coef: 0.07832/9 [=====>........................] - ETA: 1s - loss: 0.1688 - accuracy: 0.7122 - jacard_coef: 0.08533/9 [=========>....................] - ETA: 1s - loss: 0.1681 - accuracy: 0.7338 - jacard_coef: 0.07764/9 [============>.................] - ETA: 1s - loss: 0.1683 - accuracy: 0.7284 - jacard_coef: 0.07765/9 [===============>..............] - ETA: 0s - loss: 0.1679 - accuracy: 0.7504 - jacard_coef: 0.07816/9 [===================>..........] - ETA: 0s - loss: 0.1758 - accuracy: 0.6632 - jacard_coef: 0.07787/9 [======================>.......] - ETA: 0s - loss: 0.1744 - accuracy: 0.6893 - jacard_coef: 0.07908/9 [=========================>....] - ETA: 0s - loss: 0.1733 - accuracy: 0.7118 - jacard_coef: 0.07659/9 [==============================] - 2s 243ms/step - loss: 0.1733 - accuracy: 0.7106 - jacard_coef: 0.0687 - val_loss: 0.7515 - val_accuracy: 0.9209 - val_jacard_coef: 0.0235 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1671 - accuracy: 0.8471 - jacard_coef: 0.07292/9 [=====>........................] - ETA: 1s - loss: 0.1676 - accuracy: 0.8411 - jacard_coef: 0.07753/9 [=========>....................] - ETA: 1s - loss: 0.1678 - accuracy: 0.8331 - jacard_coef: 0.08664/9 [============>.................] - ETA: 1s - loss: 0.1681 - accuracy: 0.8373 - jacard_coef: 0.08465/9 [===============>..............] - ETA: 0s - loss: 0.1676 - accuracy: 0.8446 - jacard_coef: 0.08316/9 [===================>..........] - ETA: 0s - loss: 0.1673 - accuracy: 0.8493 - jacard_coef: 0.08007/9 [======================>.......] - ETA: 0s - loss: 0.1672 - accuracy: 0.8509 - jacard_coef: 0.07648/9 [=========================>....] - ETA: 0s - loss: 0.1675 - accuracy: 0.8395 - jacard_coef: 0.07609/9 [==============================] - 2s 237ms/step - loss: 0.1675 - accuracy: 0.8396 - jacard_coef: 0.0723 - val_loss: 0.9511 - val_accuracy: 0.9261 - val_jacard_coef: 0.0091 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1658 - accuracy: 0.8731 - jacard_coef: 0.07752/9 [=====>........................] - ETA: 1s - loss: 0.1652 - accuracy: 0.8783 - jacard_coef: 0.07843/9 [=========>....................] - ETA: 1s - loss: 0.1644 - accuracy: 0.8882 - jacard_coef: 0.07374/9 [============>.................] - ETA: 1s - loss: 0.1645 - accuracy: 0.8835 - jacard_coef: 0.08085/9 [===============>..............] - ETA: 0s - loss: 0.1646 - accuracy: 0.8816 - jacard_coef: 0.08466/9 [===================>..........] - ETA: 0s - loss: 0.1643 - accuracy: 0.8859 - jacard_coef: 0.08257/9 [======================>.......] - ETA: 0s - loss: 0.1641 - accuracy: 0.8906 - jacard_coef: 0.07978/9 [=========================>....] - ETA: 0s - loss: 0.1636 - accuracy: 0.8956 - jacard_coef: 0.07629/9 [==============================] - 2s 235ms/step - loss: 0.1636 - accuracy: 0.8958 - jacard_coef: 0.0680 - val_loss: 0.7256 - val_accuracy: 0.9227 - val_jacard_coef: 0.0143 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1624 - accuracy: 0.9146 - jacard_coef: 0.07022/9 [=====>........................] - ETA: 1s - loss: 0.1617 - accuracy: 0.9095 - jacard_coef: 0.07493/9 [=========>....................] - ETA: 1s - loss: 0.1619 - accuracy: 0.8937 - jacard_coef: 0.08764/9 [============>.................] - ETA: 1s - loss: 0.1618 - accuracy: 0.9002 - jacard_coef: 0.07985/9 [===============>..............] - ETA: 0s - loss: 0.1613 - accuracy: 0.9022 - jacard_coef: 0.07866/9 [===================>..........] - ETA: 0s - loss: 0.1612 - accuracy: 0.9043 - jacard_coef: 0.07787/9 [======================>.......] - ETA: 0s - loss: 0.1610 - accuracy: 0.9074 - jacard_coef: 0.07608/9 [=========================>....] - ETA: 0s - loss: 0.1607 - accuracy: 0.9087 - jacard_coef: 0.07529/9 [==============================] - 2s 235ms/step - loss: 0.1607 - accuracy: 0.9082 - jacard_coef: 0.0807 - val_loss: 0.5379 - val_accuracy: 0.9267 - val_jacard_coef: 0.0174 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1591 - accuracy: 0.9040 - jacard_coef: 0.07852/9 [=====>........................] - ETA: 1s - loss: 0.1597 - accuracy: 0.9007 - jacard_coef: 0.08393/9 [=========>....................] - ETA: 1s - loss: 0.1590 - accuracy: 0.9006 - jacard_coef: 0.08474/9 [============>.................] - ETA: 1s - loss: 0.1588 - accuracy: 0.9012 - jacard_coef: 0.08485/9 [===============>..............] - ETA: 0s - loss: 0.1584 - accuracy: 0.9068 - jacard_coef: 0.08036/9 [===================>..........] - ETA: 0s - loss: 0.1583 - accuracy: 0.9099 - jacard_coef: 0.07797/9 [======================>.......] - ETA: 0s - loss: 0.1582 - accuracy: 0.9100 - jacard_coef: 0.07818/9 [=========================>....] - ETA: 0s - loss: 0.1580 - accuracy: 0.9128 - jacard_coef: 0.07599/9 [==============================] - 2s 235ms/step - loss: 0.1580 - accuracy: 0.9130 - jacard_coef: 0.0734 - val_loss: 0.4860 - val_accuracy: 0.9269 - val_jacard_coef: 0.0182 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1569 - accuracy: 0.9214 - jacard_coef: 0.07052/9 [=====>........................] - ETA: 1s - loss: 0.1563 - accuracy: 0.9165 - jacard_coef: 0.07533/9 [=========>....................] - ETA: 1s - loss: 0.1569 - accuracy: 0.9078 - jacard_coef: 0.08264/9 [============>.................] - ETA: 1s - loss: 0.1564 - accuracy: 0.9120 - jacard_coef: 0.07895/9 [===============>..............] - ETA: 0s - loss: 0.1559 - accuracy: 0.9151 - jacard_coef: 0.07646/9 [===================>..........] - ETA: 0s - loss: 0.1560 - accuracy: 0.9131 - jacard_coef: 0.07807/9 [======================>.......] - ETA: 0s - loss: 0.1558 - accuracy: 0.9134 - jacard_coef: 0.07798/9 [=========================>....] - ETA: 0s - loss: 0.1556 - accuracy: 0.9154 - jacard_coef: 0.07629/9 [==============================] - 2s 235ms/step - loss: 0.1560 - accuracy: 0.9108 - jacard_coef: 0.0683 - val_loss: 0.5114 - val_accuracy: 0.9271 - val_jacard_coef: 0.0188 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1559 - accuracy: 0.9087 - jacard_coef: 0.07632/9 [=====>........................] - ETA: 1s - loss: 0.1553 - accuracy: 0.9127 - jacard_coef: 0.07473/9 [=========>....................] - ETA: 1s - loss: 0.1556 - accuracy: 0.8723 - jacard_coef: 0.07584/9 [============>.................] - ETA: 1s - loss: 0.1556 - accuracy: 0.8596 - jacard_coef: 0.07385/9 [===============>..............] - ETA: 0s - loss: 0.1554 - accuracy: 0.8615 - jacard_coef: 0.07316/9 [===================>..........] - ETA: 0s - loss: 0.1553 - accuracy: 0.8640 - jacard_coef: 0.07407/9 [======================>.......] - ETA: 0s - loss: 0.1555 - accuracy: 0.8535 - jacard_coef: 0.07378/9 [=========================>....] - ETA: 0s - loss: 0.1555 - accuracy: 0.8595 - jacard_coef: 0.07539/9 [==============================] - 2s 241ms/step - loss: 0.1556 - accuracy: 0.8578 - jacard_coef: 0.0847 - val_loss: 0.0921 - val_accuracy: 0.9041 - val_jacard_coef: 0.0634 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1559 - accuracy: 0.8992 - jacard_coef: 0.08442/9 [=====>........................] - ETA: 1s - loss: 0.1558 - accuracy: 0.8896 - jacard_coef: 0.08973/9 [=========>....................] - ETA: 1s - loss: 0.1552 - accuracy: 0.8939 - jacard_coef: 0.08594/9 [============>.................] - ETA: 1s - loss: 0.1549 - accuracy: 0.8970 - jacard_coef: 0.08215/9 [===============>..............] - ETA: 0s - loss: 0.1547 - accuracy: 0.8980 - jacard_coef: 0.08076/9 [===================>..........] - ETA: 0s - loss: 0.1546 - accuracy: 0.8991 - jacard_coef: 0.07937/9 [======================>.......] - ETA: 0s - loss: 0.1543 - accuracy: 0.9024 - jacard_coef: 0.07658/9 [=========================>....] - ETA: 0s - loss: 0.1540 - accuracy: 0.9038 - jacard_coef: 0.07589/9 [==============================] - 2s 240ms/step - loss: 0.1540 - accuracy: 0.9031 - jacard_coef: 0.0730 - val_loss: 0.1246 - val_accuracy: 0.9292 - val_jacard_coef: 0.0659 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1509 - accuracy: 0.9374 - jacard_coef: 0.05642/9 [=====>........................] - ETA: 1s - loss: 0.1520 - accuracy: 0.9203 - jacard_coef: 0.07153/9 [=========>....................] - ETA: 1s - loss: 0.1524 - accuracy: 0.9182 - jacard_coef: 0.07354/9 [============>.................] - ETA: 1s - loss: 0.1526 - accuracy: 0.9144 - jacard_coef: 0.07695/9 [===============>..............] - ETA: 0s - loss: 0.1525 - accuracy: 0.9163 - jacard_coef: 0.07536/9 [===================>..........] - ETA: 0s - loss: 0.1525 - accuracy: 0.9172 - jacard_coef: 0.07467/9 [======================>.......] - ETA: 0s - loss: 0.1528 - accuracy: 0.9156 - jacard_coef: 0.07618/9 [=========================>....] - ETA: 0s - loss: 0.1526 - accuracy: 0.9168 - jacard_coef: 0.07519/9 [==============================] - 2s 235ms/step - loss: 0.1528 - accuracy: 0.9162 - jacard_coef: 0.0817 - val_loss: 0.1405 - val_accuracy: 0.9301 - val_jacard_coef: 0.0652 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9334 - jacard_coef: 0.06152/9 [=====>........................] - ETA: 1s - loss: 0.1513 - accuracy: 0.9260 - jacard_coef: 0.06773/9 [=========>....................] - ETA: 1s - loss: 0.1512 - accuracy: 0.9296 - jacard_coef: 0.06424/9 [============>.................] - ETA: 1s - loss: 0.1517 - accuracy: 0.9195 - jacard_coef: 0.07235/9 [===============>..............] - ETA: 0s - loss: 0.1519 - accuracy: 0.9167 - jacard_coef: 0.07436/9 [===================>..........] - ETA: 0s - loss: 0.1522 - accuracy: 0.9127 - jacard_coef: 0.07727/9 [======================>.......] - ETA: 0s - loss: 0.1522 - accuracy: 0.9112 - jacard_coef: 0.07858/9 [=========================>....] - ETA: 0s - loss: 0.1517 - accuracy: 0.9152 - jacard_coef: 0.07519/9 [==============================] - 2s 235ms/step - loss: 0.1518 - accuracy: 0.9136 - jacard_coef: 0.0783 - val_loss: 0.1487 - val_accuracy: 0.9285 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1502 - accuracy: 0.9219 - jacard_coef: 0.07122/9 [=====>........................] - ETA: 1s - loss: 0.1507 - accuracy: 0.9255 - jacard_coef: 0.06763/9 [=========>....................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9210 - jacard_coef: 0.07164/9 [============>.................] - ETA: 1s - loss: 0.1504 - accuracy: 0.9242 - jacard_coef: 0.06905/9 [===============>..............] - ETA: 0s - loss: 0.1508 - accuracy: 0.9187 - jacard_coef: 0.07366/9 [===================>..........] - ETA: 0s - loss: 0.1510 - accuracy: 0.9156 - jacard_coef: 0.07637/9 [======================>.......] - ETA: 0s - loss: 0.1508 - accuracy: 0.9183 - jacard_coef: 0.07418/9 [=========================>....] - ETA: 0s - loss: 0.1507 - accuracy: 0.9175 - jacard_coef: 0.07499/9 [==============================] - 2s 235ms/step - loss: 0.1509 - accuracy: 0.9154 - jacard_coef: 0.0832 - val_loss: 0.1466 - val_accuracy: 0.9297 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1516 - accuracy: 0.8981 - jacard_coef: 0.09132/9 [=====>........................] - ETA: 1s - loss: 0.1511 - accuracy: 0.9107 - jacard_coef: 0.08073/9 [=========>....................] - ETA: 1s - loss: 0.1498 - accuracy: 0.9258 - jacard_coef: 0.06754/9 [============>.................] - ETA: 1s - loss: 0.1503 - accuracy: 0.9177 - jacard_coef: 0.07425/9 [===============>..............] - ETA: 0s - loss: 0.1506 - accuracy: 0.9127 - jacard_coef: 0.07846/9 [===================>..........] - ETA: 0s - loss: 0.1500 - accuracy: 0.9192 - jacard_coef: 0.07277/9 [======================>.......] - ETA: 0s - loss: 0.1502 - accuracy: 0.9157 - jacard_coef: 0.07568/9 [=========================>....] - ETA: 0s - loss: 0.1500 - accuracy: 0.9158 - jacard_coef: 0.07569/9 [==============================] - 2s 235ms/step - loss: 0.1501 - accuracy: 0.9158 - jacard_coef: 0.0677 - val_loss: 0.1444 - val_accuracy: 0.9284 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1487 - accuracy: 0.9201 - jacard_coef: 0.07282/9 [=====>........................] - ETA: 1s - loss: 0.1485 - accuracy: 0.9278 - jacard_coef: 0.06573/9 [=========>....................] - ETA: 1s - loss: 0.1490 - accuracy: 0.9182 - jacard_coef: 0.07384/9 [============>.................] - ETA: 1s - loss: 0.1491 - accuracy: 0.9178 - jacard_coef: 0.07425/9 [===============>..............] - ETA: 0s - loss: 0.1491 - accuracy: 0.9171 - jacard_coef: 0.07486/9 [===================>..........] - ETA: 0s - loss: 0.1491 - accuracy: 0.9155 - jacard_coef: 0.07617/9 [======================>.......] - ETA: 0s - loss: 0.1490 - accuracy: 0.9163 - jacard_coef: 0.07558/9 [=========================>....] - ETA: 0s - loss: 0.1489 - accuracy: 0.9169 - jacard_coef: 0.07519/9 [==============================] - 2s 235ms/step - loss: 0.1491 - accuracy: 0.9164 - jacard_coef: 0.0801 - val_loss: 0.1419 - val_accuracy: 0.9290 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1487 - accuracy: 0.9277 - jacard_coef: 0.06552/9 [=====>........................] - ETA: 1s - loss: 0.1482 - accuracy: 0.9294 - jacard_coef: 0.06463/9 [=========>....................] - ETA: 1s - loss: 0.1485 - accuracy: 0.9192 - jacard_coef: 0.07324/9 [============>.................] - ETA: 1s - loss: 0.1486 - accuracy: 0.9177 - jacard_coef: 0.07455/9 [===============>..............] - ETA: 1s - loss: 0.1486 - accuracy: 0.9139 - jacard_coef: 0.07786/9 [===================>..........] - ETA: 0s - loss: 0.1495 - accuracy: 0.9103 - jacard_coef: 0.08027/9 [======================>.......] - ETA: 0s - loss: 0.1492 - accuracy: 0.9131 - jacard_coef: 0.07798/9 [=========================>....] - ETA: 0s - loss: 0.1488 - accuracy: 0.9168 - jacard_coef: 0.07489/9 [==============================] - 2s 238ms/step - loss: 0.1490 - accuracy: 0.9149 - jacard_coef: 0.0820 - val_loss: 0.1438 - val_accuracy: 0.9285 - val_jacard_coef: 0.0650 - lr: 2.5000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1468 - accuracy: 0.9287 - jacard_coef: 0.06522/9 [=====>........................] - ETA: 1s - loss: 0.1496 - accuracy: 0.9265 - jacard_coef: 0.06523/9 [=========>....................] - ETA: 1s - loss: 0.1487 - accuracy: 0.9303 - jacard_coef: 0.06264/9 [============>.................] - ETA: 1s - loss: 0.1485 - accuracy: 0.9262 - jacard_coef: 0.06645/9 [===============>..............] - ETA: 0s - loss: 0.1486 - accuracy: 0.9207 - jacard_coef: 0.07116/9 [===================>..........] - ETA: 0s - loss: 0.1487 - accuracy: 0.9167 - jacard_coef: 0.07467/9 [======================>.......] - ETA: 0s - loss: 0.1486 - accuracy: 0.9161 - jacard_coef: 0.07528/9 [=========================>....] - ETA: 0s - loss: 0.1486 - accuracy: 0.9154 - jacard_coef: 0.07599/9 [==============================] - 2s 235ms/step - loss: 0.1486 - accuracy: 0.9159 - jacard_coef: 0.0678 - val_loss: 0.1465 - val_accuracy: 0.9272 - val_jacard_coef: 0.0649 - lr: 2.5000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1466 - accuracy: 0.9221 - jacard_coef: 0.07082/9 [=====>........................] - ETA: 1s - loss: 0.1472 - accuracy: 0.9176 - jacard_coef: 0.07493/9 [=========>....................] - ETA: 1s - loss: 0.1475 - accuracy: 0.9160 - jacard_coef: 0.07634/9 [============>.................] - ETA: 1s - loss: 0.1474 - accuracy: 0.9173 - jacard_coef: 0.07505/9 [===============>..............] - ETA: 0s - loss: 0.1474 - accuracy: 0.9176 - jacard_coef: 0.07476/9 [===================>..........] - ETA: 0s - loss: 0.1474 - accuracy: 0.9178 - jacard_coef: 0.07447/9 [======================>.......] - ETA: 0s - loss: 0.1474 - accuracy: 0.9184 - jacard_coef: 0.07408/9 [=========================>....] - ETA: 0s - loss: 0.1475 - accuracy: 0.9162 - jacard_coef: 0.07599/9 [==============================] - 2s 235ms/step - loss: 0.1476 - accuracy: 0.9166 - jacard_coef: 0.0704 - val_loss: 0.1477 - val_accuracy: 0.9296 - val_jacard_coef: 0.0647 - lr: 2.5000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9247 - jacard_coef: 0.06932/9 [=====>........................] - ETA: 1s - loss: 0.1457 - accuracy: 0.9289 - jacard_coef: 0.06573/9 [=========>....................] - ETA: 1s - loss: 0.1462 - accuracy: 0.9263 - jacard_coef: 0.06784/9 [============>.................] - ETA: 1s - loss: 0.1466 - accuracy: 0.9202 - jacard_coef: 0.07305/9 [===============>..............] - ETA: 0s - loss: 0.1466 - accuracy: 0.9187 - jacard_coef: 0.07436/9 [===================>..........] - ETA: 0s - loss: 0.1468 - accuracy: 0.9181 - jacard_coef: 0.07477/9 [======================>.......] - ETA: 0s - loss: 0.1467 - accuracy: 0.9183 - jacard_coef: 0.07468/9 [=========================>....] - ETA: 0s - loss: 0.1467 - accuracy: 0.9182 - jacard_coef: 0.07479/9 [==============================] - 2s 236ms/step - loss: 0.1468 - accuracy: 0.9172 - jacard_coef: 0.0850 - val_loss: 0.1471 - val_accuracy: 0.9300 - val_jacard_coef: 0.0646 - lr: 2.5000e-04
Epoch 23/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1468 - accuracy: 0.9188 - jacard_coef: 0.07392/9 [=====>........................] - ETA: 1s - loss: 0.1474 - accuracy: 0.9068 - jacard_coef: 0.08413/9 [=========>....................] - ETA: 1s - loss: 0.1470 - accuracy: 0.9132 - jacard_coef: 0.07874/9 [============>.................] - ETA: 1s - loss: 0.1465 - accuracy: 0.9175 - jacard_coef: 0.07515/9 [===============>..............] - ETA: 0s - loss: 0.1465 - accuracy: 0.9189 - jacard_coef: 0.07406/9 [===================>..........] - ETA: 0s - loss: 0.1463 - accuracy: 0.9206 - jacard_coef: 0.07257/9 [======================>.......] - ETA: 0s - loss: 0.1464 - accuracy: 0.9170 - jacard_coef: 0.07558/9 [=========================>....] - ETA: 0s - loss: 0.1464 - accuracy: 0.9168 - jacard_coef: 0.07579/9 [==============================] - 2s 236ms/step - loss: 0.1464 - accuracy: 0.9172 - jacard_coef: 0.0704 - val_loss: 0.1463 - val_accuracy: 0.9302 - val_jacard_coef: 0.0646 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0659 (epoch 13)
  Final Val Loss: 0.1463
  Training Time: 0:01:53.434345
  Stability (std): 0.0025

Results saved to: hyperparameter_optimization_20250926_123742/exp_5_UNet_lr5e-4_bs16/UNet_lr0.0005_bs16_results.json

Experiment 5 completed in 145s
Progress: 5/36 completed
Estimated remaining time: 74 minutes

ðŸ”¬ EXPERIMENT 6/36
================================================
Architecture: UNet
Learning Rate: 5e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758862237.447445 3196951 service.cc:145] XLA service 0x1507604c3d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758862237.447487 3196951 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758862237.837862 3196951 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 3:29 - loss: 0.3512 - accuracy: 0.5139 - jacard_coef: 0.07592/5 [===========>..................] - ETA: 41s - loss: 0.3214 - accuracy: 0.4557 - jacard_coef: 0.0777 3/5 [=================>............] - ETA: 15s - loss: 0.2901 - accuracy: 0.3931 - jacard_coef: 0.07844/5 [=======================>......] - ETA: 5s - loss: 0.2778 - accuracy: 0.3519 - jacard_coef: 0.0762 5/5 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.3509 - jacard_coef: 0.08825/5 [==============================] - 76s 6s/step - loss: 0.2773 - accuracy: 0.3509 - jacard_coef: 0.0882 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 1s - loss: 0.2096 - accuracy: 0.1952 - jacard_coef: 0.07382/5 [===========>..................] - ETA: 1s - loss: 0.2094 - accuracy: 0.1810 - jacard_coef: 0.07353/5 [=================>............] - ETA: 0s - loss: 0.2047 - accuracy: 0.1895 - jacard_coef: 0.07484/5 [=======================>......] - ETA: 0s - loss: 0.2047 - accuracy: 0.1849 - jacard_coef: 0.07615/5 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.1848 - jacard_coef: 0.07115/5 [==============================] - 2s 422ms/step - loss: 0.2046 - accuracy: 0.1848 - jacard_coef: 0.0711 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1901 - accuracy: 0.4205 - jacard_coef: 0.08032/5 [===========>..................] - ETA: 1s - loss: 0.1914 - accuracy: 0.4041 - jacard_coef: 0.08193/5 [=================>............] - ETA: 0s - loss: 0.1886 - accuracy: 0.3889 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 0s - loss: 0.1869 - accuracy: 0.3920 - jacard_coef: 0.07605/5 [==============================] - 2s 397ms/step - loss: 0.1870 - accuracy: 0.3914 - jacard_coef: 0.0847 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1887 - accuracy: 0.2588 - jacard_coef: 0.07742/5 [===========>..................] - ETA: 1s - loss: 0.1875 - accuracy: 0.2480 - jacard_coef: 0.08343/5 [=================>............] - ETA: 0s - loss: 0.1866 - accuracy: 0.2437 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1895 - accuracy: 0.2491 - jacard_coef: 0.07645/5 [==============================] - 2s 398ms/step - loss: 0.1896 - accuracy: 0.2487 - jacard_coef: 0.0675 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4578e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1846 - accuracy: 0.3687 - jacard_coef: 0.07452/5 [===========>..................] - ETA: 1s - loss: 0.1836 - accuracy: 0.3819 - jacard_coef: 0.07783/5 [=================>............] - ETA: 0s - loss: 0.1820 - accuracy: 0.4074 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 0s - loss: 0.1814 - accuracy: 0.4192 - jacard_coef: 0.07585/5 [==============================] - 2s 415ms/step - loss: 0.1814 - accuracy: 0.4186 - jacard_coef: 0.0908 - val_loss: 1.1013 - val_accuracy: 0.9304 - val_jacard_coef: 9.2168e-04 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1760 - accuracy: 0.5410 - jacard_coef: 0.05392/5 [===========>..................] - ETA: 1s - loss: 0.1864 - accuracy: 0.4988 - jacard_coef: 0.07003/5 [=================>............] - ETA: 0s - loss: 0.1841 - accuracy: 0.5010 - jacard_coef: 0.07574/5 [=======================>......] - ETA: 0s - loss: 0.1824 - accuracy: 0.4855 - jacard_coef: 0.07555/5 [==============================] - 2s 414ms/step - loss: 0.1825 - accuracy: 0.4836 - jacard_coef: 0.0868 - val_loss: 1.0557 - val_accuracy: 0.9303 - val_jacard_coef: 0.0030 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1735 - accuracy: 0.4980 - jacard_coef: 0.08812/5 [===========>..................] - ETA: 1s - loss: 0.1760 - accuracy: 0.4538 - jacard_coef: 0.07953/5 [=================>............] - ETA: 0s - loss: 0.1749 - accuracy: 0.4619 - jacard_coef: 0.07454/5 [=======================>......] - ETA: 0s - loss: 0.1741 - accuracy: 0.4813 - jacard_coef: 0.07545/5 [==============================] - 2s 401ms/step - loss: 0.1743 - accuracy: 0.4809 - jacard_coef: 0.0933 - val_loss: 1.0949 - val_accuracy: 0.9304 - val_jacard_coef: 0.0011 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1730 - accuracy: 0.5771 - jacard_coef: 0.08632/5 [===========>..................] - ETA: 1s - loss: 0.1744 - accuracy: 0.5277 - jacard_coef: 0.07933/5 [=================>............] - ETA: 0s - loss: 0.1750 - accuracy: 0.5083 - jacard_coef: 0.08054/5 [=======================>......] - ETA: 0s - loss: 0.1754 - accuracy: 0.5098 - jacard_coef: 0.07575/5 [==============================] - 2s 408ms/step - loss: 0.1754 - accuracy: 0.5076 - jacard_coef: 0.0867 - val_loss: 1.1218 - val_accuracy: 0.9302 - val_jacard_coef: 1.4510e-05 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1733 - accuracy: 0.5369 - jacard_coef: 0.07212/5 [===========>..................] - ETA: 1s - loss: 0.1726 - accuracy: 0.5496 - jacard_coef: 0.08723/5 [=================>............] - ETA: 0s - loss: 0.1724 - accuracy: 0.5318 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 0s - loss: 0.1722 - accuracy: 0.5485 - jacard_coef: 0.07615/5 [==============================] - 2s 436ms/step - loss: 0.1723 - accuracy: 0.5458 - jacard_coef: 0.0733 - val_loss: 1.0918 - val_accuracy: 0.9251 - val_jacard_coef: 0.0043 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1684 - accuracy: 0.7010 - jacard_coef: 0.07242/5 [===========>..................] - ETA: 1s - loss: 0.1681 - accuracy: 0.6700 - jacard_coef: 0.07553/5 [=================>............] - ETA: 0s - loss: 0.1692 - accuracy: 0.6394 - jacard_coef: 0.07614/5 [=======================>......] - ETA: 0s - loss: 0.1693 - accuracy: 0.6380 - jacard_coef: 0.07535/5 [==============================] - 2s 428ms/step - loss: 0.1693 - accuracy: 0.6365 - jacard_coef: 0.0932 - val_loss: 0.5136 - val_accuracy: 0.7905 - val_jacard_coef: 0.0663 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1683 - accuracy: 0.6525 - jacard_coef: 0.07902/5 [===========>..................] - ETA: 1s - loss: 0.1681 - accuracy: 0.6239 - jacard_coef: 0.08073/5 [=================>............] - ETA: 0s - loss: 0.1684 - accuracy: 0.5984 - jacard_coef: 0.07764/5 [=======================>......] - ETA: 0s - loss: 0.1686 - accuracy: 0.5836 - jacard_coef: 0.07565/5 [==============================] - 2s 407ms/step - loss: 0.1687 - accuracy: 0.5820 - jacard_coef: 0.0919 - val_loss: 1.0209 - val_accuracy: 0.9117 - val_jacard_coef: 0.0168 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1674 - accuracy: 0.6731 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 1s - loss: 0.1673 - accuracy: 0.6793 - jacard_coef: 0.08023/5 [=================>............] - ETA: 0s - loss: 0.1685 - accuracy: 0.6292 - jacard_coef: 0.07864/5 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.6059 - jacard_coef: 0.07605/5 [==============================] - 2s 408ms/step - loss: 0.1690 - accuracy: 0.6051 - jacard_coef: 0.0686 - val_loss: 1.1115 - val_accuracy: 0.9299 - val_jacard_coef: 1.3228e-04 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1718 - accuracy: 0.5730 - jacard_coef: 0.08102/5 [===========>..................] - ETA: 1s - loss: 0.1697 - accuracy: 0.6453 - jacard_coef: 0.08243/5 [=================>............] - ETA: 0s - loss: 0.1685 - accuracy: 0.7191 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1679 - accuracy: 0.7324 - jacard_coef: 0.07625/5 [==============================] - 2s 406ms/step - loss: 0.1679 - accuracy: 0.7302 - jacard_coef: 0.0720 - val_loss: 1.0854 - val_accuracy: 0.9261 - val_jacard_coef: 0.0023 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1635 - accuracy: 0.6398 - jacard_coef: 0.08652/5 [===========>..................] - ETA: 1s - loss: 0.1647 - accuracy: 0.6175 - jacard_coef: 0.07443/5 [=================>............] - ETA: 0s - loss: 0.1654 - accuracy: 0.5967 - jacard_coef: 0.07794/5 [=======================>......] - ETA: 0s - loss: 0.1650 - accuracy: 0.5960 - jacard_coef: 0.07625/5 [==============================] - 2s 407ms/step - loss: 0.1652 - accuracy: 0.5945 - jacard_coef: 0.0727 - val_loss: 0.1217 - val_accuracy: 0.9151 - val_jacard_coef: 0.0629 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1620 - accuracy: 0.8101 - jacard_coef: 0.07472/5 [===========>..................] - ETA: 1s - loss: 0.1625 - accuracy: 0.8051 - jacard_coef: 0.08103/5 [=================>............] - ETA: 0s - loss: 0.1621 - accuracy: 0.7720 - jacard_coef: 0.07684/5 [=======================>......] - ETA: 0s - loss: 0.1624 - accuracy: 0.7369 - jacard_coef: 0.07605/5 [==============================] - 2s 407ms/step - loss: 0.1624 - accuracy: 0.7353 - jacard_coef: 0.0716 - val_loss: 0.4714 - val_accuracy: 0.4886 - val_jacard_coef: 0.0642 - lr: 0.0010
Epoch 16/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1635 - accuracy: 0.8786 - jacard_coef: 0.08242/5 [===========>..................] - ETA: 1s - loss: 0.1628 - accuracy: 0.8892 - jacard_coef: 0.07833/5 [=================>............] - ETA: 0s - loss: 0.1634 - accuracy: 0.8893 - jacard_coef: 0.07664/5 [=======================>......] - ETA: 0s - loss: 0.1631 - accuracy: 0.8880 - jacard_coef: 0.07615/5 [==============================] - 2s 407ms/step - loss: 0.1632 - accuracy: 0.8855 - jacard_coef: 0.0729 - val_loss: 0.1374 - val_accuracy: 0.7704 - val_jacard_coef: 0.0605 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1606 - accuracy: 0.8535 - jacard_coef: 0.07672/5 [===========>..................] - ETA: 1s - loss: 0.1598 - accuracy: 0.8484 - jacard_coef: 0.07053/5 [=================>............] - ETA: 0s - loss: 0.1597 - accuracy: 0.8546 - jacard_coef: 0.07184/5 [=======================>......] - ETA: 0s - loss: 0.1600 - accuracy: 0.8512 - jacard_coef: 0.07535/5 [==============================] - 2s 407ms/step - loss: 0.1605 - accuracy: 0.8475 - jacard_coef: 0.0878 - val_loss: 0.1150 - val_accuracy: 0.9281 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1678 - accuracy: 0.7200 - jacard_coef: 0.07642/5 [===========>..................] - ETA: 1s - loss: 0.1785 - accuracy: 0.4351 - jacard_coef: 0.07293/5 [=================>............] - ETA: 0s - loss: 0.1834 - accuracy: 0.3435 - jacard_coef: 0.07564/5 [=======================>......] - ETA: 0s - loss: 0.1807 - accuracy: 0.4076 - jacard_coef: 0.07575/5 [==============================] - 2s 406ms/step - loss: 0.1807 - accuracy: 0.4091 - jacard_coef: 0.0876 - val_loss: 0.1353 - val_accuracy: 0.9262 - val_jacard_coef: 0.0609 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1744 - accuracy: 0.6337 - jacard_coef: 0.08692/5 [===========>..................] - ETA: 1s - loss: 0.1757 - accuracy: 0.6259 - jacard_coef: 0.08243/5 [=================>............] - ETA: 0s - loss: 0.1751 - accuracy: 0.6298 - jacard_coef: 0.07554/5 [=======================>......] - ETA: 0s - loss: 0.1735 - accuracy: 0.6359 - jacard_coef: 0.07625/5 [==============================] - 2s 408ms/step - loss: 0.1735 - accuracy: 0.6350 - jacard_coef: 0.0661 - val_loss: 1.0025 - val_accuracy: 0.9237 - val_jacard_coef: 0.0236 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1791 - accuracy: 0.6256 - jacard_coef: 0.08012/5 [===========>..................] - ETA: 1s - loss: 0.1760 - accuracy: 0.6252 - jacard_coef: 0.06973/5 [=================>............] - ETA: 0s - loss: 0.1772 - accuracy: 0.5967 - jacard_coef: 0.07504/5 [=======================>......] - ETA: 0s - loss: 0.1760 - accuracy: 0.6066 - jacard_coef: 0.07615/5 [==============================] - 2s 408ms/step - loss: 0.1760 - accuracy: 0.6049 - jacard_coef: 0.0681 - val_loss: 1.0595 - val_accuracy: 0.9284 - val_jacard_coef: 0.0136 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0663 (epoch 10)
  Final Val Loss: 1.0595
  Training Time: 0:01:57.233512
  Stability (std): 0.4417

Results saved to: hyperparameter_optimization_20250926_123742/exp_6_UNet_lr5e-4_bs32/UNet_lr0.0005_bs32_results.json

Experiment 6 completed in 153s
Progress: 6/36 completed
Estimated remaining time: 76 minutes

ðŸ”¬ EXPERIMENT 7/36
================================================
Architecture: UNet
Learning Rate: 1e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758862379.774863 3201636 service.cc:145] XLA service 0x1477964c3770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758862379.774902 3201636 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758862380.213833 3201636 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 10:31 - loss: 0.3415 - accuracy: 0.5010 - jacard_coef: 0.0447 2/17 [==>...........................] - ETA: 56s - loss: 0.3056 - accuracy: 0.4450 - jacard_coef: 0.0574   3/17 [====>.........................] - ETA: 36s - loss: 0.2765 - accuracy: 0.4141 - jacard_coef: 0.0632 4/17 [======>.......................] - ETA: 28s - loss: 0.2719 - accuracy: 0.3809 - jacard_coef: 0.0733 5/17 [=======>......................] - ETA: 20s - loss: 0.2632 - accuracy: 0.3487 - jacard_coef: 0.0717 6/17 [=========>....................] - ETA: 15s - loss: 0.2521 - accuracy: 0.3256 - jacard_coef: 0.0777 7/17 [===========>..................] - ETA: 11s - loss: 0.2443 - accuracy: 0.3036 - jacard_coef: 0.0800 8/17 [=============>................] - ETA: 9s - loss: 0.2379 - accuracy: 0.2899 - jacard_coef: 0.0785  9/17 [==============>...............] - ETA: 7s - loss: 0.2355 - accuracy: 0.2864 - jacard_coef: 0.079410/17 [================>.............] - ETA: 5s - loss: 0.2310 - accuracy: 0.2869 - jacard_coef: 0.076511/17 [==================>...........] - ETA: 4s - loss: 0.2269 - accuracy: 0.2875 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 3s - loss: 0.2234 - accuracy: 0.2883 - jacard_coef: 0.075813/17 [=====================>........] - ETA: 2s - loss: 0.2218 - accuracy: 0.2892 - jacard_coef: 0.074814/17 [=======================>......] - ETA: 1s - loss: 0.2191 - accuracy: 0.2902 - jacard_coef: 0.076215/17 [=========================>....] - ETA: 1s - loss: 0.2169 - accuracy: 0.2922 - jacard_coef: 0.076616/17 [===========================>..] - ETA: 0s - loss: 0.2149 - accuracy: 0.2896 - jacard_coef: 0.076417/17 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.2896 - jacard_coef: 0.077717/17 [==============================] - 55s 967ms/step - loss: 0.2147 - accuracy: 0.2896 - jacard_coef: 0.0777 - val_loss: 1.1116 - val_accuracy: 0.9304 - val_jacard_coef: 6.8279e-04 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1817 - accuracy: 0.4040 - jacard_coef: 0.0958 2/17 [==>...........................] - ETA: 1s - loss: 0.1823 - accuracy: 0.3365 - jacard_coef: 0.1038 3/17 [====>.........................] - ETA: 1s - loss: 0.1838 - accuracy: 0.2973 - jacard_coef: 0.0940 4/17 [======>.......................] - ETA: 1s - loss: 0.1836 - accuracy: 0.2937 - jacard_coef: 0.0910 5/17 [=======>......................] - ETA: 1s - loss: 0.1834 - accuracy: 0.2951 - jacard_coef: 0.0888 6/17 [=========>....................] - ETA: 1s - loss: 0.1849 - accuracy: 0.2861 - jacard_coef: 0.0814 7/17 [===========>..................] - ETA: 1s - loss: 0.1839 - accuracy: 0.3003 - jacard_coef: 0.0789 8/17 [=============>................] - ETA: 1s - loss: 0.1834 - accuracy: 0.3089 - jacard_coef: 0.0762 9/17 [==============>...............] - ETA: 1s - loss: 0.1830 - accuracy: 0.3186 - jacard_coef: 0.073710/17 [================>.............] - ETA: 0s - loss: 0.1823 - accuracy: 0.3398 - jacard_coef: 0.074511/17 [==================>...........] - ETA: 0s - loss: 0.1818 - accuracy: 0.3616 - jacard_coef: 0.076012/17 [====================>.........] - ETA: 0s - loss: 0.1817 - accuracy: 0.3699 - jacard_coef: 0.074113/17 [=====================>........] - ETA: 0s - loss: 0.1816 - accuracy: 0.3667 - jacard_coef: 0.074014/17 [=======================>......] - ETA: 0s - loss: 0.1815 - accuracy: 0.3665 - jacard_coef: 0.075915/17 [=========================>....] - ETA: 0s - loss: 0.1813 - accuracy: 0.3665 - jacard_coef: 0.075916/17 [===========================>..] - ETA: 0s - loss: 0.1818 - accuracy: 0.3707 - jacard_coef: 0.076317/17 [==============================] - ETA: 0s - loss: 0.1818 - accuracy: 0.3715 - jacard_coef: 0.075717/17 [==============================] - 2s 140ms/step - loss: 0.1818 - accuracy: 0.3715 - jacard_coef: 0.0757 - val_loss: 11.3994 - val_accuracy: 0.1745 - val_jacard_coef: 0.0670 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1794 - accuracy: 0.4315 - jacard_coef: 0.0659 2/17 [==>...........................] - ETA: 1s - loss: 0.1791 - accuracy: 0.4258 - jacard_coef: 0.0893 3/17 [====>.........................] - ETA: 1s - loss: 0.1783 - accuracy: 0.4348 - jacard_coef: 0.0837 4/17 [======>.......................] - ETA: 1s - loss: 0.1778 - accuracy: 0.4207 - jacard_coef: 0.0787 5/17 [=======>......................] - ETA: 1s - loss: 0.1771 - accuracy: 0.4206 - jacard_coef: 0.0774 6/17 [=========>....................] - ETA: 1s - loss: 0.1769 - accuracy: 0.4091 - jacard_coef: 0.0771 7/17 [===========>..................] - ETA: 1s - loss: 0.1766 - accuracy: 0.4118 - jacard_coef: 0.0753 8/17 [=============>................] - ETA: 1s - loss: 0.1761 - accuracy: 0.4235 - jacard_coef: 0.0785 9/17 [==============>...............] - ETA: 1s - loss: 0.1775 - accuracy: 0.4204 - jacard_coef: 0.075410/17 [================>.............] - ETA: 0s - loss: 0.1768 - accuracy: 0.4289 - jacard_coef: 0.073311/17 [==================>...........] - ETA: 0s - loss: 0.1775 - accuracy: 0.4248 - jacard_coef: 0.075012/17 [====================>.........] - ETA: 0s - loss: 0.1779 - accuracy: 0.4281 - jacard_coef: 0.072813/17 [=====================>........] - ETA: 0s - loss: 0.1781 - accuracy: 0.4346 - jacard_coef: 0.073314/17 [=======================>......] - ETA: 0s - loss: 0.1780 - accuracy: 0.4423 - jacard_coef: 0.075715/17 [=========================>....] - ETA: 0s - loss: 0.1781 - accuracy: 0.4465 - jacard_coef: 0.077016/17 [===========================>..] - ETA: 0s - loss: 0.1780 - accuracy: 0.4413 - jacard_coef: 0.076517/17 [==============================] - 2s 133ms/step - loss: 0.1781 - accuracy: 0.4390 - jacard_coef: 0.0721 - val_loss: 1.0018 - val_accuracy: 0.9304 - val_jacard_coef: 0.0050 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1828 - accuracy: 0.3484 - jacard_coef: 0.0799 2/17 [==>...........................] - ETA: 1s - loss: 0.1814 - accuracy: 0.3545 - jacard_coef: 0.0834 3/17 [====>.........................] - ETA: 1s - loss: 0.1799 - accuracy: 0.3416 - jacard_coef: 0.0779 4/17 [======>.......................] - ETA: 1s - loss: 0.1793 - accuracy: 0.3119 - jacard_coef: 0.0754 5/17 [=======>......................] - ETA: 1s - loss: 0.1779 - accuracy: 0.3505 - jacard_coef: 0.0730 6/17 [=========>....................] - ETA: 1s - loss: 0.1773 - accuracy: 0.3731 - jacard_coef: 0.0714 7/17 [===========>..................] - ETA: 1s - loss: 0.1765 - accuracy: 0.3878 - jacard_coef: 0.0703 8/17 [=============>................] - ETA: 1s - loss: 0.1780 - accuracy: 0.4030 - jacard_coef: 0.0720 9/17 [==============>...............] - ETA: 1s - loss: 0.1782 - accuracy: 0.4144 - jacard_coef: 0.072010/17 [================>.............] - ETA: 0s - loss: 0.1807 - accuracy: 0.3930 - jacard_coef: 0.072611/17 [==================>...........] - ETA: 0s - loss: 0.1837 - accuracy: 0.3837 - jacard_coef: 0.075212/17 [====================>.........] - ETA: 0s - loss: 0.1848 - accuracy: 0.3716 - jacard_coef: 0.079213/17 [=====================>........] - ETA: 0s - loss: 0.1840 - accuracy: 0.3780 - jacard_coef: 0.076514/17 [=======================>......] - ETA: 0s - loss: 0.1838 - accuracy: 0.3876 - jacard_coef: 0.074215/17 [=========================>....] - ETA: 0s - loss: 0.1837 - accuracy: 0.3972 - jacard_coef: 0.074516/17 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.4104 - jacard_coef: 0.074917/17 [==============================] - 2s 133ms/step - loss: 0.1837 - accuracy: 0.4107 - jacard_coef: 0.0792 - val_loss: 1.1198 - val_accuracy: 0.9304 - val_jacard_coef: 3.8348e-05 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1746 - accuracy: 0.4395 - jacard_coef: 0.0867 2/17 [==>...........................] - ETA: 1s - loss: 0.1825 - accuracy: 0.4264 - jacard_coef: 0.0898 3/17 [====>.........................] - ETA: 1s - loss: 0.1799 - accuracy: 0.4317 - jacard_coef: 0.0838 4/17 [======>.......................] - ETA: 1s - loss: 0.1793 - accuracy: 0.4299 - jacard_coef: 0.0795 5/17 [=======>......................] - ETA: 1s - loss: 0.1805 - accuracy: 0.4280 - jacard_coef: 0.0773 6/17 [=========>....................] - ETA: 1s - loss: 0.1787 - accuracy: 0.4670 - jacard_coef: 0.0808 7/17 [===========>..................] - ETA: 1s - loss: 0.1780 - accuracy: 0.4964 - jacard_coef: 0.0807 8/17 [=============>................] - ETA: 1s - loss: 0.1770 - accuracy: 0.5167 - jacard_coef: 0.0824 9/17 [==============>...............] - ETA: 1s - loss: 0.1779 - accuracy: 0.5323 - jacard_coef: 0.079010/17 [================>.............] - ETA: 0s - loss: 0.1769 - accuracy: 0.5529 - jacard_coef: 0.077811/17 [==================>...........] - ETA: 0s - loss: 0.1769 - accuracy: 0.5324 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 0s - loss: 0.1763 - accuracy: 0.5445 - jacard_coef: 0.077313/17 [=====================>........] - ETA: 0s - loss: 0.1758 - accuracy: 0.5411 - jacard_coef: 0.078614/17 [=======================>......] - ETA: 0s - loss: 0.1752 - accuracy: 0.5524 - jacard_coef: 0.077515/17 [=========================>....] - ETA: 0s - loss: 0.1745 - accuracy: 0.5609 - jacard_coef: 0.076516/17 [===========================>..] - ETA: 0s - loss: 0.1740 - accuracy: 0.5685 - jacard_coef: 0.076217/17 [==============================] - 2s 133ms/step - loss: 0.1740 - accuracy: 0.5671 - jacard_coef: 0.0750 - val_loss: 1.1195 - val_accuracy: 0.9302 - val_jacard_coef: 2.0291e-04 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1642 - accuracy: 0.7261 - jacard_coef: 0.0981 2/17 [==>...........................] - ETA: 1s - loss: 0.1651 - accuracy: 0.6751 - jacard_coef: 0.0885 3/17 [====>.........................] - ETA: 1s - loss: 0.1656 - accuracy: 0.6435 - jacard_coef: 0.0865 4/17 [======>.......................] - ETA: 1s - loss: 0.1662 - accuracy: 0.6299 - jacard_coef: 0.0844 5/17 [=======>......................] - ETA: 1s - loss: 0.1655 - accuracy: 0.6532 - jacard_coef: 0.0801 6/17 [=========>....................] - ETA: 1s - loss: 0.1674 - accuracy: 0.6572 - jacard_coef: 0.0750 7/17 [===========>..................] - ETA: 1s - loss: 0.1667 - accuracy: 0.6754 - jacard_coef: 0.0748 8/17 [=============>................] - ETA: 1s - loss: 0.1659 - accuracy: 0.6863 - jacard_coef: 0.0742 9/17 [==============>...............] - ETA: 1s - loss: 0.1653 - accuracy: 0.6852 - jacard_coef: 0.077910/17 [================>.............] - ETA: 0s - loss: 0.1652 - accuracy: 0.6724 - jacard_coef: 0.075611/17 [==================>...........] - ETA: 0s - loss: 0.1648 - accuracy: 0.6750 - jacard_coef: 0.079412/17 [====================>.........] - ETA: 0s - loss: 0.1646 - accuracy: 0.6699 - jacard_coef: 0.077413/17 [=====================>........] - ETA: 0s - loss: 0.1643 - accuracy: 0.6806 - jacard_coef: 0.078114/17 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.6886 - jacard_coef: 0.076915/17 [=========================>....] - ETA: 0s - loss: 0.1637 - accuracy: 0.6973 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1637 - accuracy: 0.7048 - jacard_coef: 0.075317/17 [==============================] - 2s 133ms/step - loss: 0.1637 - accuracy: 0.7038 - jacard_coef: 0.0797 - val_loss: 0.6319 - val_accuracy: 0.9060 - val_jacard_coef: 0.0243 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8088 - jacard_coef: 0.0865 2/17 [==>...........................] - ETA: 1s - loss: 0.1928 - accuracy: 0.5892 - jacard_coef: 0.0723 3/17 [====>.........................] - ETA: 1s - loss: 0.1814 - accuracy: 0.6576 - jacard_coef: 0.0786 4/17 [======>.......................] - ETA: 1s - loss: 0.1765 - accuracy: 0.6661 - jacard_coef: 0.0813 5/17 [=======>......................] - ETA: 1s - loss: 0.1730 - accuracy: 0.6708 - jacard_coef: 0.0788 6/17 [=========>....................] - ETA: 1s - loss: 0.1729 - accuracy: 0.6557 - jacard_coef: 0.0750 7/17 [===========>..................] - ETA: 1s - loss: 0.1718 - accuracy: 0.6573 - jacard_coef: 0.0798 8/17 [=============>................] - ETA: 1s - loss: 0.1716 - accuracy: 0.6553 - jacard_coef: 0.0787 9/17 [==============>...............] - ETA: 1s - loss: 0.1700 - accuracy: 0.6771 - jacard_coef: 0.075610/17 [================>.............] - ETA: 0s - loss: 0.1705 - accuracy: 0.6882 - jacard_coef: 0.076911/17 [==================>...........] - ETA: 0s - loss: 0.1693 - accuracy: 0.6989 - jacard_coef: 0.077612/17 [====================>.........] - ETA: 0s - loss: 0.1685 - accuracy: 0.7066 - jacard_coef: 0.075313/17 [=====================>........] - ETA: 0s - loss: 0.1675 - accuracy: 0.7128 - jacard_coef: 0.071714/17 [=======================>......] - ETA: 0s - loss: 0.1669 - accuracy: 0.7156 - jacard_coef: 0.073715/17 [=========================>....] - ETA: 0s - loss: 0.1668 - accuracy: 0.7199 - jacard_coef: 0.074716/17 [===========================>..] - ETA: 0s - loss: 0.1664 - accuracy: 0.7201 - jacard_coef: 0.074717/17 [==============================] - 2s 133ms/step - loss: 0.1664 - accuracy: 0.7191 - jacard_coef: 0.0792 - val_loss: 1.0448 - val_accuracy: 0.9095 - val_jacard_coef: 0.0152 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8695 - jacard_coef: 0.0600 2/17 [==>...........................] - ETA: 1s - loss: 0.1574 - accuracy: 0.8781 - jacard_coef: 0.0639 3/17 [====>.........................] - ETA: 1s - loss: 0.1647 - accuracy: 0.8415 - jacard_coef: 0.0843 4/17 [======>.......................] - ETA: 1s - loss: 0.1628 - accuracy: 0.8476 - jacard_coef: 0.0783 5/17 [=======>......................] - ETA: 1s - loss: 0.1620 - accuracy: 0.8463 - jacard_coef: 0.0775 6/17 [=========>....................] - ETA: 1s - loss: 0.1606 - accuracy: 0.8533 - jacard_coef: 0.0771 7/17 [===========>..................] - ETA: 1s - loss: 0.1600 - accuracy: 0.8542 - jacard_coef: 0.0790 8/17 [=============>................] - ETA: 1s - loss: 0.1592 - accuracy: 0.8559 - jacard_coef: 0.0777 9/17 [==============>...............] - ETA: 1s - loss: 0.1586 - accuracy: 0.8618 - jacard_coef: 0.075410/17 [================>.............] - ETA: 0s - loss: 0.1584 - accuracy: 0.8637 - jacard_coef: 0.076211/17 [==================>...........] - ETA: 0s - loss: 0.1582 - accuracy: 0.8650 - jacard_coef: 0.078512/17 [====================>.........] - ETA: 0s - loss: 0.1579 - accuracy: 0.8695 - jacard_coef: 0.077613/17 [=====================>........] - ETA: 0s - loss: 0.1575 - accuracy: 0.8734 - jacard_coef: 0.076914/17 [=======================>......] - ETA: 0s - loss: 0.1582 - accuracy: 0.8769 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 0s - loss: 0.1578 - accuracy: 0.8769 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1578 - accuracy: 0.8658 - jacard_coef: 0.075617/17 [==============================] - 2s 133ms/step - loss: 0.1581 - accuracy: 0.8658 - jacard_coef: 0.0731 - val_loss: 1.0561 - val_accuracy: 0.8543 - val_jacard_coef: 0.0158 - lr: 5.0000e-04
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1571 - accuracy: 0.8456 - jacard_coef: 0.1216 2/17 [==>...........................] - ETA: 1s - loss: 0.1549 - accuracy: 0.8873 - jacard_coef: 0.0872 3/17 [====>.........................] - ETA: 1s - loss: 0.1546 - accuracy: 0.8905 - jacard_coef: 0.0860 4/17 [======>.......................] - ETA: 1s - loss: 0.1534 - accuracy: 0.8980 - jacard_coef: 0.0715 5/17 [=======>......................] - ETA: 1s - loss: 0.1540 - accuracy: 0.8380 - jacard_coef: 0.0673 6/17 [=========>....................] - ETA: 1s - loss: 0.1532 - accuracy: 0.8518 - jacard_coef: 0.0642 7/17 [===========>..................] - ETA: 1s - loss: 0.1528 - accuracy: 0.8596 - jacard_coef: 0.0655 8/17 [=============>................] - ETA: 1s - loss: 0.1528 - accuracy: 0.8644 - jacard_coef: 0.0679 9/17 [==============>...............] - ETA: 1s - loss: 0.1526 - accuracy: 0.8695 - jacard_coef: 0.068610/17 [================>.............] - ETA: 0s - loss: 0.1536 - accuracy: 0.8754 - jacard_coef: 0.067811/17 [==================>...........] - ETA: 0s - loss: 0.1535 - accuracy: 0.8765 - jacard_coef: 0.070112/17 [====================>.........] - ETA: 0s - loss: 0.1534 - accuracy: 0.8785 - jacard_coef: 0.071113/17 [=====================>........] - ETA: 0s - loss: 0.1533 - accuracy: 0.8792 - jacard_coef: 0.072914/17 [=======================>......] - ETA: 0s - loss: 0.1531 - accuracy: 0.8798 - jacard_coef: 0.073515/17 [=========================>....] - ETA: 0s - loss: 0.1530 - accuracy: 0.8809 - jacard_coef: 0.074416/17 [===========================>..] - ETA: 0s - loss: 0.1530 - accuracy: 0.8822 - jacard_coef: 0.074817/17 [==============================] - 2s 133ms/step - loss: 0.1531 - accuracy: 0.8810 - jacard_coef: 0.0777 - val_loss: 0.2994 - val_accuracy: 0.8206 - val_jacard_coef: 0.0585 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1501 - accuracy: 0.9084 - jacard_coef: 0.0811 2/17 [==>...........................] - ETA: 1s - loss: 0.1504 - accuracy: 0.9091 - jacard_coef: 0.0813 3/17 [====>.........................] - ETA: 1s - loss: 0.1504 - accuracy: 0.9040 - jacard_coef: 0.0855 4/17 [======>.......................] - ETA: 1s - loss: 0.1509 - accuracy: 0.9007 - jacard_coef: 0.0875 5/17 [=======>......................] - ETA: 1s - loss: 0.1502 - accuracy: 0.9137 - jacard_coef: 0.0753 6/17 [=========>....................] - ETA: 1s - loss: 0.1512 - accuracy: 0.9174 - jacard_coef: 0.0720 7/17 [===========>..................] - ETA: 1s - loss: 0.1509 - accuracy: 0.9204 - jacard_coef: 0.0693 8/17 [=============>................] - ETA: 1s - loss: 0.1505 - accuracy: 0.9208 - jacard_coef: 0.0690 9/17 [==============>...............] - ETA: 1s - loss: 0.1503 - accuracy: 0.9224 - jacard_coef: 0.067510/17 [================>.............] - ETA: 0s - loss: 0.1502 - accuracy: 0.9218 - jacard_coef: 0.068311/17 [==================>...........] - ETA: 0s - loss: 0.1501 - accuracy: 0.9183 - jacard_coef: 0.071312/17 [====================>.........] - ETA: 0s - loss: 0.1500 - accuracy: 0.9185 - jacard_coef: 0.071413/17 [=====================>........] - ETA: 0s - loss: 0.1499 - accuracy: 0.9187 - jacard_coef: 0.071514/17 [=======================>......] - ETA: 0s - loss: 0.1498 - accuracy: 0.9192 - jacard_coef: 0.071315/17 [=========================>....] - ETA: 0s - loss: 0.1503 - accuracy: 0.9174 - jacard_coef: 0.072916/17 [===========================>..] - ETA: 0s - loss: 0.1504 - accuracy: 0.9154 - jacard_coef: 0.074717/17 [==============================] - 2s 133ms/step - loss: 0.1508 - accuracy: 0.9121 - jacard_coef: 0.0778 - val_loss: 0.4222 - val_accuracy: 0.4656 - val_jacard_coef: 0.0627 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1461 - accuracy: 0.9370 - jacard_coef: 0.0576 2/17 [==>...........................] - ETA: 1s - loss: 0.1471 - accuracy: 0.9312 - jacard_coef: 0.0602 3/17 [====>.........................] - ETA: 1s - loss: 0.1498 - accuracy: 0.9158 - jacard_coef: 0.0675 4/17 [======>.......................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9027 - jacard_coef: 0.0785 5/17 [=======>......................] - ETA: 1s - loss: 0.1512 - accuracy: 0.8992 - jacard_coef: 0.0810 6/17 [=========>....................] - ETA: 1s - loss: 0.1514 - accuracy: 0.9027 - jacard_coef: 0.0777 7/17 [===========>..................] - ETA: 1s - loss: 0.1516 - accuracy: 0.9040 - jacard_coef: 0.0768 8/17 [=============>................] - ETA: 1s - loss: 0.1528 - accuracy: 0.8991 - jacard_coef: 0.0773 9/17 [==============>...............] - ETA: 1s - loss: 0.1534 - accuracy: 0.9008 - jacard_coef: 0.075710/17 [================>.............] - ETA: 0s - loss: 0.1535 - accuracy: 0.8996 - jacard_coef: 0.075811/17 [==================>...........] - ETA: 0s - loss: 0.1546 - accuracy: 0.8975 - jacard_coef: 0.076112/17 [====================>.........] - ETA: 0s - loss: 0.1558 - accuracy: 0.8836 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.1557 - accuracy: 0.8815 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1555 - accuracy: 0.8817 - jacard_coef: 0.075015/17 [=========================>....] - ETA: 0s - loss: 0.1553 - accuracy: 0.8829 - jacard_coef: 0.074516/17 [===========================>..] - ETA: 0s - loss: 0.1553 - accuracy: 0.8821 - jacard_coef: 0.075117/17 [==============================] - 2s 133ms/step - loss: 0.1553 - accuracy: 0.8809 - jacard_coef: 0.0785 - val_loss: 0.1794 - val_accuracy: 0.3131 - val_jacard_coef: 0.0635 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1497 - accuracy: 0.9100 - jacard_coef: 0.0727 2/17 [==>...........................] - ETA: 1s - loss: 0.1528 - accuracy: 0.8961 - jacard_coef: 0.0835 3/17 [====>.........................] - ETA: 1s - loss: 0.1522 - accuracy: 0.8967 - jacard_coef: 0.0811 4/17 [======>.......................] - ETA: 1s - loss: 0.1528 - accuracy: 0.8933 - jacard_coef: 0.0778 5/17 [=======>......................] - ETA: 1s - loss: 0.1527 - accuracy: 0.8909 - jacard_coef: 0.0809 6/17 [=========>....................] - ETA: 1s - loss: 0.1545 - accuracy: 0.8900 - jacard_coef: 0.0824 7/17 [===========>..................] - ETA: 1s - loss: 0.1543 - accuracy: 0.8926 - jacard_coef: 0.0800 8/17 [=============>................] - ETA: 1s - loss: 0.1536 - accuracy: 0.8962 - jacard_coef: 0.0756 9/17 [==============>...............] - ETA: 1s - loss: 0.1533 - accuracy: 0.8938 - jacard_coef: 0.074610/17 [================>.............] - ETA: 0s - loss: 0.1531 - accuracy: 0.8889 - jacard_coef: 0.077811/17 [==================>...........] - ETA: 0s - loss: 0.1526 - accuracy: 0.8879 - jacard_coef: 0.077212/17 [====================>.........] - ETA: 0s - loss: 0.1521 - accuracy: 0.8893 - jacard_coef: 0.075613/17 [=====================>........] - ETA: 0s - loss: 0.1517 - accuracy: 0.8902 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1515 - accuracy: 0.8887 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 0s - loss: 0.1515 - accuracy: 0.8847 - jacard_coef: 0.076316/17 [===========================>..] - ETA: 0s - loss: 0.1512 - accuracy: 0.8863 - jacard_coef: 0.075817/17 [==============================] - 2s 133ms/step - loss: 0.1513 - accuracy: 0.8849 - jacard_coef: 0.0729 - val_loss: 0.1519 - val_accuracy: 0.9223 - val_jacard_coef: 0.0630 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0670 (epoch 2)
  Final Val Loss: 0.1519
  Training Time: 0:01:20.781428
  Stability (std): 0.3876

Results saved to: hyperparameter_optimization_20250926_123742/exp_7_UNet_lr1e-3_bs8/UNet_lr0.001_bs8_results.json

Experiment 7 completed in 112s
Progress: 7/36 completed
Estimated remaining time: 54 minutes

ðŸ”¬ EXPERIMENT 8/36
================================================
Architecture: UNet
Learning Rate: 1e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758862496.074298 3205425 service.cc:145] XLA service 0x1516320b73b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758862496.074373 3205425 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758862496.534660 3205425 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 5:58 - loss: 0.3380 - accuracy: 0.4854 - jacard_coef: 0.05392/9 [=====>........................] - ETA: 48s - loss: 0.2979 - accuracy: 0.3872 - jacard_coef: 0.0664 3/9 [=========>....................] - ETA: 28s - loss: 0.2695 - accuracy: 0.3333 - jacard_coef: 0.07584/9 [============>.................] - ETA: 20s - loss: 0.2527 - accuracy: 0.3025 - jacard_coef: 0.07765/9 [===============>..............] - ETA: 12s - loss: 0.2418 - accuracy: 0.2886 - jacard_coef: 0.07836/9 [===================>..........] - ETA: 7s - loss: 0.2319 - accuracy: 0.2956 - jacard_coef: 0.0780 7/9 [======================>.......] - ETA: 4s - loss: 0.2243 - accuracy: 0.3066 - jacard_coef: 0.07588/9 [=========================>....] - ETA: 1s - loss: 0.2213 - accuracy: 0.3034 - jacard_coef: 0.07759/9 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.3015 - jacard_coef: 0.06929/9 [==============================] - 65s 3s/step - loss: 0.2211 - accuracy: 0.3015 - jacard_coef: 0.0692 - val_loss: 0.7190 - val_accuracy: 0.9304 - val_jacard_coef: 0.0023 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.2012 - accuracy: 0.1906 - jacard_coef: 0.07392/9 [=====>........................] - ETA: 1s - loss: 0.1934 - accuracy: 0.2430 - jacard_coef: 0.08463/9 [=========>....................] - ETA: 1s - loss: 0.1899 - accuracy: 0.2842 - jacard_coef: 0.09034/9 [============>.................] - ETA: 1s - loss: 0.1878 - accuracy: 0.3093 - jacard_coef: 0.08335/9 [===============>..............] - ETA: 0s - loss: 0.1862 - accuracy: 0.3325 - jacard_coef: 0.08156/9 [===================>..........] - ETA: 0s - loss: 0.1847 - accuracy: 0.3511 - jacard_coef: 0.07947/9 [======================>.......] - ETA: 0s - loss: 0.1834 - accuracy: 0.3583 - jacard_coef: 0.07628/9 [=========================>....] - ETA: 0s - loss: 0.1829 - accuracy: 0.3554 - jacard_coef: 0.07629/9 [==============================] - 2s 241ms/step - loss: 0.1829 - accuracy: 0.3545 - jacard_coef: 0.0741 - val_loss: 0.9744 - val_accuracy: 0.9304 - val_jacard_coef: 0.0072 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1806 - accuracy: 0.3649 - jacard_coef: 0.07952/9 [=====>........................] - ETA: 1s - loss: 0.1822 - accuracy: 0.3575 - jacard_coef: 0.07853/9 [=========>....................] - ETA: 1s - loss: 0.1830 - accuracy: 0.3470 - jacard_coef: 0.07494/9 [============>.................] - ETA: 1s - loss: 0.1837 - accuracy: 0.3510 - jacard_coef: 0.07595/9 [===============>..............] - ETA: 0s - loss: 0.1831 - accuracy: 0.3506 - jacard_coef: 0.07476/9 [===================>..........] - ETA: 0s - loss: 0.1830 - accuracy: 0.3535 - jacard_coef: 0.07407/9 [======================>.......] - ETA: 0s - loss: 0.1833 - accuracy: 0.3618 - jacard_coef: 0.07358/9 [=========================>....] - ETA: 0s - loss: 0.1835 - accuracy: 0.3811 - jacard_coef: 0.07679/9 [==============================] - 2s 234ms/step - loss: 0.1835 - accuracy: 0.3818 - jacard_coef: 0.0686 - val_loss: 0.9378 - val_accuracy: 0.9266 - val_jacard_coef: 0.0065 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1797 - accuracy: 0.4347 - jacard_coef: 0.06052/9 [=====>........................] - ETA: 1s - loss: 0.1787 - accuracy: 0.3966 - jacard_coef: 0.07233/9 [=========>....................] - ETA: 1s - loss: 0.1785 - accuracy: 0.4100 - jacard_coef: 0.07974/9 [============>.................] - ETA: 1s - loss: 0.1782 - accuracy: 0.4144 - jacard_coef: 0.07955/9 [===============>..............] - ETA: 0s - loss: 0.1773 - accuracy: 0.4130 - jacard_coef: 0.07136/9 [===================>..........] - ETA: 0s - loss: 0.1769 - accuracy: 0.4153 - jacard_coef: 0.07597/9 [======================>.......] - ETA: 0s - loss: 0.1763 - accuracy: 0.4248 - jacard_coef: 0.07578/9 [=========================>....] - ETA: 0s - loss: 0.1760 - accuracy: 0.4289 - jacard_coef: 0.07599/9 [==============================] - 2s 242ms/step - loss: 0.1759 - accuracy: 0.4295 - jacard_coef: 0.0745 - val_loss: 1.1656 - val_accuracy: 0.1110 - val_jacard_coef: 0.0684 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1725 - accuracy: 0.5073 - jacard_coef: 0.09512/9 [=====>........................] - ETA: 1s - loss: 0.1721 - accuracy: 0.5554 - jacard_coef: 0.07963/9 [=========>....................] - ETA: 1s - loss: 0.1712 - accuracy: 0.5813 - jacard_coef: 0.07244/9 [============>.................] - ETA: 1s - loss: 0.1705 - accuracy: 0.5760 - jacard_coef: 0.06985/9 [===============>..............] - ETA: 0s - loss: 0.1701 - accuracy: 0.5819 - jacard_coef: 0.07446/9 [===================>..........] - ETA: 0s - loss: 0.1705 - accuracy: 0.5820 - jacard_coef: 0.07597/9 [======================>.......] - ETA: 0s - loss: 0.1704 - accuracy: 0.5824 - jacard_coef: 0.07478/9 [=========================>....] - ETA: 0s - loss: 0.1702 - accuracy: 0.5897 - jacard_coef: 0.07669/9 [==============================] - 2s 236ms/step - loss: 0.1702 - accuracy: 0.5893 - jacard_coef: 0.0684 - val_loss: 0.1651 - val_accuracy: 0.7982 - val_jacard_coef: 0.0652 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1674 - accuracy: 0.6809 - jacard_coef: 0.09512/9 [=====>........................] - ETA: 1s - loss: 0.1672 - accuracy: 0.7049 - jacard_coef: 0.07943/9 [=========>....................] - ETA: 1s - loss: 0.1679 - accuracy: 0.7342 - jacard_coef: 0.08054/9 [============>.................] - ETA: 1s - loss: 0.1698 - accuracy: 0.6818 - jacard_coef: 0.07725/9 [===============>..............] - ETA: 0s - loss: 0.1697 - accuracy: 0.6903 - jacard_coef: 0.07806/9 [===================>..........] - ETA: 0s - loss: 0.1700 - accuracy: 0.6886 - jacard_coef: 0.07497/9 [======================>.......] - ETA: 0s - loss: 0.1705 - accuracy: 0.6848 - jacard_coef: 0.07668/9 [=========================>....] - ETA: 0s - loss: 0.1705 - accuracy: 0.6795 - jacard_coef: 0.07579/9 [==============================] - 2s 236ms/step - loss: 0.1706 - accuracy: 0.6780 - jacard_coef: 0.0807 - val_loss: 1.0685 - val_accuracy: 0.9205 - val_jacard_coef: 0.0149 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1718 - accuracy: 0.6344 - jacard_coef: 0.07312/9 [=====>........................] - ETA: 1s - loss: 0.1706 - accuracy: 0.6362 - jacard_coef: 0.07663/9 [=========>....................] - ETA: 1s - loss: 0.1718 - accuracy: 0.6227 - jacard_coef: 0.07674/9 [============>.................] - ETA: 1s - loss: 0.1723 - accuracy: 0.6260 - jacard_coef: 0.07455/9 [===============>..............] - ETA: 0s - loss: 0.1719 - accuracy: 0.6337 - jacard_coef: 0.07696/9 [===================>..........] - ETA: 0s - loss: 0.1711 - accuracy: 0.6519 - jacard_coef: 0.07437/9 [======================>.......] - ETA: 0s - loss: 0.1711 - accuracy: 0.6574 - jacard_coef: 0.07538/9 [=========================>....] - ETA: 0s - loss: 0.1709 - accuracy: 0.6569 - jacard_coef: 0.07589/9 [==============================] - 2s 234ms/step - loss: 0.1710 - accuracy: 0.6560 - jacard_coef: 0.0785 - val_loss: 1.1362 - val_accuracy: 0.9158 - val_jacard_coef: 0.0170 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1698 - accuracy: 0.5310 - jacard_coef: 0.07992/9 [=====>........................] - ETA: 1s - loss: 0.1696 - accuracy: 0.5081 - jacard_coef: 0.07393/9 [=========>....................] - ETA: 1s - loss: 0.1682 - accuracy: 0.5493 - jacard_coef: 0.07794/9 [============>.................] - ETA: 1s - loss: 0.1680 - accuracy: 0.6106 - jacard_coef: 0.07945/9 [===============>..............] - ETA: 0s - loss: 0.1677 - accuracy: 0.6530 - jacard_coef: 0.07546/9 [===================>..........] - ETA: 0s - loss: 0.1675 - accuracy: 0.6767 - jacard_coef: 0.07537/9 [======================>.......] - ETA: 0s - loss: 0.1672 - accuracy: 0.6934 - jacard_coef: 0.07798/9 [=========================>....] - ETA: 0s - loss: 0.1670 - accuracy: 0.7153 - jacard_coef: 0.07609/9 [==============================] - 2s 234ms/step - loss: 0.1670 - accuracy: 0.7147 - jacard_coef: 0.0717 - val_loss: 0.5106 - val_accuracy: 0.8352 - val_jacard_coef: 0.0498 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1638 - accuracy: 0.8619 - jacard_coef: 0.08512/9 [=====>........................] - ETA: 1s - loss: 0.1642 - accuracy: 0.8563 - jacard_coef: 0.07693/9 [=========>....................] - ETA: 1s - loss: 0.1636 - accuracy: 0.8536 - jacard_coef: 0.07724/9 [============>.................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8538 - jacard_coef: 0.08015/9 [===============>..............] - ETA: 0s - loss: 0.1631 - accuracy: 0.8541 - jacard_coef: 0.08006/9 [===================>..........] - ETA: 0s - loss: 0.1627 - accuracy: 0.8541 - jacard_coef: 0.07707/9 [======================>.......] - ETA: 0s - loss: 0.1625 - accuracy: 0.8541 - jacard_coef: 0.07668/9 [=========================>....] - ETA: 0s - loss: 0.1621 - accuracy: 0.8539 - jacard_coef: 0.07629/9 [==============================] - 2s 234ms/step - loss: 0.1622 - accuracy: 0.8527 - jacard_coef: 0.0702 - val_loss: 0.4596 - val_accuracy: 0.1552 - val_jacard_coef: 0.0659 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1606 - accuracy: 0.8597 - jacard_coef: 0.08432/9 [=====>........................] - ETA: 1s - loss: 0.1615 - accuracy: 0.8374 - jacard_coef: 0.07693/9 [=========>....................] - ETA: 1s - loss: 0.1609 - accuracy: 0.8478 - jacard_coef: 0.07664/9 [============>.................] - ETA: 1s - loss: 0.1603 - accuracy: 0.8578 - jacard_coef: 0.07565/9 [===============>..............] - ETA: 0s - loss: 0.1602 - accuracy: 0.8591 - jacard_coef: 0.07586/9 [===================>..........] - ETA: 0s - loss: 0.1599 - accuracy: 0.8627 - jacard_coef: 0.07577/9 [======================>.......] - ETA: 0s - loss: 0.1596 - accuracy: 0.8637 - jacard_coef: 0.07648/9 [=========================>....] - ETA: 0s - loss: 0.1595 - accuracy: 0.8617 - jacard_coef: 0.07539/9 [==============================] - 2s 234ms/step - loss: 0.1596 - accuracy: 0.8588 - jacard_coef: 0.0837 - val_loss: 0.1945 - val_accuracy: 0.3912 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1607 - accuracy: 0.6729 - jacard_coef: 0.07702/9 [=====>........................] - ETA: 1s - loss: 0.1611 - accuracy: 0.6534 - jacard_coef: 0.07643/9 [=========>....................] - ETA: 1s - loss: 0.1616 - accuracy: 0.6440 - jacard_coef: 0.07334/9 [============>.................] - ETA: 1s - loss: 0.1614 - accuracy: 0.6399 - jacard_coef: 0.07195/9 [===============>..............] - ETA: 0s - loss: 0.1621 - accuracy: 0.6314 - jacard_coef: 0.07746/9 [===================>..........] - ETA: 0s - loss: 0.1626 - accuracy: 0.6269 - jacard_coef: 0.07947/9 [======================>.......] - ETA: 0s - loss: 0.1626 - accuracy: 0.6288 - jacard_coef: 0.07798/9 [=========================>....] - ETA: 0s - loss: 0.1624 - accuracy: 0.6515 - jacard_coef: 0.07509/9 [==============================] - 2s 234ms/step - loss: 0.1624 - accuracy: 0.6521 - jacard_coef: 0.0803 - val_loss: 0.1880 - val_accuracy: 0.5453 - val_jacard_coef: 0.0635 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1606 - accuracy: 0.9062 - jacard_coef: 0.07552/9 [=====>........................] - ETA: 1s - loss: 0.1612 - accuracy: 0.9044 - jacard_coef: 0.07523/9 [=========>....................] - ETA: 1s - loss: 0.1608 - accuracy: 0.9107 - jacard_coef: 0.07214/9 [============>.................] - ETA: 1s - loss: 0.1604 - accuracy: 0.9070 - jacard_coef: 0.07225/9 [===============>..............] - ETA: 0s - loss: 0.1602 - accuracy: 0.9005 - jacard_coef: 0.07266/9 [===================>..........] - ETA: 0s - loss: 0.1601 - accuracy: 0.8926 - jacard_coef: 0.07517/9 [======================>.......] - ETA: 0s - loss: 0.1600 - accuracy: 0.8882 - jacard_coef: 0.07488/9 [=========================>....] - ETA: 0s - loss: 0.1599 - accuracy: 0.8812 - jacard_coef: 0.07609/9 [==============================] - 2s 235ms/step - loss: 0.1599 - accuracy: 0.8803 - jacard_coef: 0.0734 - val_loss: 0.1739 - val_accuracy: 0.6764 - val_jacard_coef: 0.0634 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1588 - accuracy: 0.8270 - jacard_coef: 0.09212/9 [=====>........................] - ETA: 1s - loss: 0.1577 - accuracy: 0.8578 - jacard_coef: 0.07613/9 [=========>....................] - ETA: 1s - loss: 0.1575 - accuracy: 0.8744 - jacard_coef: 0.07254/9 [============>.................] - ETA: 1s - loss: 0.1580 - accuracy: 0.8691 - jacard_coef: 0.07545/9 [===============>..............] - ETA: 0s - loss: 0.1577 - accuracy: 0.8793 - jacard_coef: 0.07336/9 [===================>..........] - ETA: 0s - loss: 0.1577 - accuracy: 0.8845 - jacard_coef: 0.07407/9 [======================>.......] - ETA: 0s - loss: 0.1580 - accuracy: 0.8874 - jacard_coef: 0.07528/9 [=========================>....] - ETA: 0s - loss: 0.1578 - accuracy: 0.8905 - jacard_coef: 0.07539/9 [==============================] - 2s 234ms/step - loss: 0.1578 - accuracy: 0.8901 - jacard_coef: 0.0812 - val_loss: 0.1670 - val_accuracy: 0.7635 - val_jacard_coef: 0.0637 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1563 - accuracy: 0.9005 - jacard_coef: 0.08772/9 [=====>........................] - ETA: 1s - loss: 0.1561 - accuracy: 0.8969 - jacard_coef: 0.09073/9 [=========>....................] - ETA: 1s - loss: 0.1563 - accuracy: 0.9037 - jacard_coef: 0.08424/9 [============>.................] - ETA: 1s - loss: 0.1560 - accuracy: 0.9129 - jacard_coef: 0.07545/9 [===============>..............] - ETA: 0s - loss: 0.1560 - accuracy: 0.9136 - jacard_coef: 0.07416/9 [===================>..........] - ETA: 0s - loss: 0.1561 - accuracy: 0.9099 - jacard_coef: 0.07777/9 [======================>.......] - ETA: 0s - loss: 0.1560 - accuracy: 0.9082 - jacard_coef: 0.07898/9 [=========================>....] - ETA: 0s - loss: 0.1559 - accuracy: 0.9110 - jacard_coef: 0.07609/9 [==============================] - 2s 235ms/step - loss: 0.1558 - accuracy: 0.9112 - jacard_coef: 0.0678 - val_loss: 0.1630 - val_accuracy: 0.9074 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0684 (epoch 4)
  Final Val Loss: 0.1630
  Training Time: 0:01:33.957024
  Stability (std): 0.3610

Results saved to: hyperparameter_optimization_20250926_123742/exp_8_UNet_lr1e-3_bs16/UNet_lr0.001_bs16_results.json

Experiment 8 completed in 126s
Progress: 8/36 completed
Estimated remaining time: 58 minutes

ðŸ”¬ EXPERIMENT 9/36
================================================
Architecture: UNet
Learning Rate: 1e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758862629.174457 3209327 service.cc:145] XLA service 0x151ee2560260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758862629.174496 3209327 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758862629.561641 3209327 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 3:29 - loss: 0.3499 - accuracy: 0.5219 - jacard_coef: 0.06392/5 [===========>..................] - ETA: 37s - loss: 0.3380 - accuracy: 0.4884 - jacard_coef: 0.0685 3/5 [=================>............] - ETA: 15s - loss: 0.3058 - accuracy: 0.4450 - jacard_coef: 0.07284/5 [=======================>......] - ETA: 5s - loss: 0.2860 - accuracy: 0.4246 - jacard_coef: 0.0757 5/5 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.4224 - jacard_coef: 0.06425/5 [==============================] - 77s 6s/step - loss: 0.2858 - accuracy: 0.4224 - jacard_coef: 0.0642 - val_loss: 0.4826 - val_accuracy: 0.9304 - val_jacard_coef: 0.0227 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 1s - loss: 0.2142 - accuracy: 0.2111 - jacard_coef: 0.06992/5 [===========>..................] - ETA: 1s - loss: 0.2022 - accuracy: 0.2743 - jacard_coef: 0.07223/5 [=================>............] - ETA: 0s - loss: 0.2010 - accuracy: 0.2824 - jacard_coef: 0.07294/5 [=======================>......] - ETA: 0s - loss: 0.1981 - accuracy: 0.2953 - jacard_coef: 0.07675/5 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.2943 - jacard_coef: 0.06975/5 [==============================] - 2s 434ms/step - loss: 0.1982 - accuracy: 0.2943 - jacard_coef: 0.0697 - val_loss: 1.1217 - val_accuracy: 0.9304 - val_jacard_coef: 1.4652e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.2077 - accuracy: 0.3211 - jacard_coef: 0.07282/5 [===========>..................] - ETA: 1s - loss: 0.2023 - accuracy: 0.3238 - jacard_coef: 0.08083/5 [=================>............] - ETA: 0s - loss: 0.2007 - accuracy: 0.2855 - jacard_coef: 0.07404/5 [=======================>......] - ETA: 0s - loss: 0.1981 - accuracy: 0.2652 - jacard_coef: 0.07685/5 [==============================] - 2s 396ms/step - loss: 0.1981 - accuracy: 0.2641 - jacard_coef: 0.0737 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1922 - accuracy: 0.2058 - jacard_coef: 0.06972/5 [===========>..................] - ETA: 1s - loss: 0.1899 - accuracy: 0.2484 - jacard_coef: 0.07623/5 [=================>............] - ETA: 0s - loss: 0.1923 - accuracy: 0.2505 - jacard_coef: 0.07494/5 [=======================>......] - ETA: 0s - loss: 0.1912 - accuracy: 0.2622 - jacard_coef: 0.07685/5 [==============================] - 2s 397ms/step - loss: 0.1912 - accuracy: 0.2613 - jacard_coef: 0.0724 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1840 - accuracy: 0.3392 - jacard_coef: 0.09232/5 [===========>..................] - ETA: 1s - loss: 0.1833 - accuracy: 0.3336 - jacard_coef: 0.08313/5 [=================>............] - ETA: 0s - loss: 0.1853 - accuracy: 0.3358 - jacard_coef: 0.08014/5 [=======================>......] - ETA: 0s - loss: 0.1838 - accuracy: 0.3444 - jacard_coef: 0.07675/5 [==============================] - 2s 400ms/step - loss: 0.1841 - accuracy: 0.3443 - jacard_coef: 0.0666 - val_loss: 1.1111 - val_accuracy: 0.9304 - val_jacard_coef: 7.6163e-05 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1804 - accuracy: 0.3989 - jacard_coef: 0.07182/5 [===========>..................] - ETA: 1s - loss: 0.1853 - accuracy: 0.3952 - jacard_coef: 0.07643/5 [=================>............] - ETA: 0s - loss: 0.1845 - accuracy: 0.3863 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1847 - accuracy: 0.3640 - jacard_coef: 0.07695/5 [==============================] - 2s 396ms/step - loss: 0.1848 - accuracy: 0.3618 - jacard_coef: 0.0634 - val_loss: 0.9801 - val_accuracy: 0.9304 - val_jacard_coef: 0.0038 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1832 - accuracy: 0.3048 - jacard_coef: 0.08082/5 [===========>..................] - ETA: 1s - loss: 0.1853 - accuracy: 0.3030 - jacard_coef: 0.07643/5 [=================>............] - ETA: 0s - loss: 0.1839 - accuracy: 0.3089 - jacard_coef: 0.08004/5 [=======================>......] - ETA: 0s - loss: 0.1854 - accuracy: 0.3060 - jacard_coef: 0.07675/5 [==============================] - 2s 396ms/step - loss: 0.1855 - accuracy: 0.3045 - jacard_coef: 0.0730 - val_loss: 0.3799 - val_accuracy: 0.9304 - val_jacard_coef: 0.0213 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1813 - accuracy: 0.3136 - jacard_coef: 0.07312/5 [===========>..................] - ETA: 1s - loss: 0.1845 - accuracy: 0.3234 - jacard_coef: 0.07723/5 [=================>............] - ETA: 0s - loss: 0.1833 - accuracy: 0.3191 - jacard_coef: 0.07614/5 [=======================>......] - ETA: 0s - loss: 0.1827 - accuracy: 0.3221 - jacard_coef: 0.07675/5 [==============================] - 2s 415ms/step - loss: 0.1827 - accuracy: 0.3210 - jacard_coef: 0.0721 - val_loss: 0.2034 - val_accuracy: 0.9303 - val_jacard_coef: 0.0392 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1787 - accuracy: 0.3316 - jacard_coef: 0.07072/5 [===========>..................] - ETA: 1s - loss: 0.1780 - accuracy: 0.3453 - jacard_coef: 0.07423/5 [=================>............] - ETA: 0s - loss: 0.1801 - accuracy: 0.3529 - jacard_coef: 0.07514/5 [=======================>......] - ETA: 0s - loss: 0.1794 - accuracy: 0.3546 - jacard_coef: 0.07605/5 [==============================] - 2s 413ms/step - loss: 0.1795 - accuracy: 0.3536 - jacard_coef: 0.0880 - val_loss: 0.1152 - val_accuracy: 0.9270 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1759 - accuracy: 0.4201 - jacard_coef: 0.07772/5 [===========>..................] - ETA: 1s - loss: 0.1754 - accuracy: 0.4437 - jacard_coef: 0.07513/5 [=================>............] - ETA: 0s - loss: 0.1752 - accuracy: 0.4527 - jacard_coef: 0.07864/5 [=======================>......] - ETA: 0s - loss: 0.1766 - accuracy: 0.4487 - jacard_coef: 0.07675/5 [==============================] - 2s 409ms/step - loss: 0.1767 - accuracy: 0.4467 - jacard_coef: 0.0672 - val_loss: 0.1001 - val_accuracy: 0.8346 - val_jacard_coef: 0.0667 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1754 - accuracy: 0.5100 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 1s - loss: 0.1744 - accuracy: 0.5459 - jacard_coef: 0.07233/5 [=================>............] - ETA: 0s - loss: 0.1756 - accuracy: 0.5489 - jacard_coef: 0.07394/5 [=======================>......] - ETA: 0s - loss: 0.1748 - accuracy: 0.5362 - jacard_coef: 0.07675/5 [==============================] - 2s 396ms/step - loss: 0.1749 - accuracy: 0.5342 - jacard_coef: 0.0618 - val_loss: 0.1424 - val_accuracy: 0.6584 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1717 - accuracy: 0.5419 - jacard_coef: 0.08822/5 [===========>..................] - ETA: 1s - loss: 0.1719 - accuracy: 0.5771 - jacard_coef: 0.08193/5 [=================>............] - ETA: 0s - loss: 0.1734 - accuracy: 0.5999 - jacard_coef: 0.07864/5 [=======================>......] - ETA: 0s - loss: 0.1726 - accuracy: 0.6163 - jacard_coef: 0.07665/5 [==============================] - 2s 398ms/step - loss: 0.1727 - accuracy: 0.6137 - jacard_coef: 0.0684 - val_loss: 0.1615 - val_accuracy: 0.6271 - val_jacard_coef: 0.0652 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1698 - accuracy: 0.6506 - jacard_coef: 0.07782/5 [===========>..................] - ETA: 1s - loss: 0.1717 - accuracy: 0.6193 - jacard_coef: 0.07783/5 [=================>............] - ETA: 0s - loss: 0.1705 - accuracy: 0.6067 - jacard_coef: 0.07934/5 [=======================>......] - ETA: 0s - loss: 0.1701 - accuracy: 0.5965 - jacard_coef: 0.07555/5 [==============================] - 2s 397ms/step - loss: 0.1702 - accuracy: 0.5947 - jacard_coef: 0.0932 - val_loss: 0.1660 - val_accuracy: 0.6445 - val_jacard_coef: 0.0654 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1706 - accuracy: 0.7205 - jacard_coef: 0.07192/5 [===========>..................] - ETA: 1s - loss: 0.1697 - accuracy: 0.6910 - jacard_coef: 0.07033/5 [=================>............] - ETA: 0s - loss: 0.1697 - accuracy: 0.6766 - jacard_coef: 0.07304/5 [=======================>......] - ETA: 0s - loss: 0.1696 - accuracy: 0.6659 - jacard_coef: 0.07665/5 [==============================] - 2s 399ms/step - loss: 0.1697 - accuracy: 0.6649 - jacard_coef: 0.0625 - val_loss: 0.1618 - val_accuracy: 0.7204 - val_jacard_coef: 0.0653 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1673 - accuracy: 0.6303 - jacard_coef: 0.08572/5 [===========>..................] - ETA: 1s - loss: 0.1663 - accuracy: 0.6571 - jacard_coef: 0.07723/5 [=================>............] - ETA: 0s - loss: 0.1669 - accuracy: 0.6496 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1677 - accuracy: 0.6469 - jacard_coef: 0.07555/5 [==============================] - 2s 396ms/step - loss: 0.1677 - accuracy: 0.6453 - jacard_coef: 0.0899 - val_loss: 0.1503 - val_accuracy: 0.8691 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1681 - accuracy: 0.7098 - jacard_coef: 0.08022/5 [===========>..................] - ETA: 1s - loss: 0.1672 - accuracy: 0.7214 - jacard_coef: 0.07433/5 [=================>............] - ETA: 0s - loss: 0.1665 - accuracy: 0.7350 - jacard_coef: 0.07564/5 [=======================>......] - ETA: 0s - loss: 0.1661 - accuracy: 0.7426 - jacard_coef: 0.07575/5 [==============================] - 2s 397ms/step - loss: 0.1665 - accuracy: 0.7400 - jacard_coef: 0.0879 - val_loss: 0.1574 - val_accuracy: 0.8135 - val_jacard_coef: 0.0644 - lr: 2.5000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1650 - accuracy: 0.7498 - jacard_coef: 0.08322/5 [===========>..................] - ETA: 1s - loss: 0.1656 - accuracy: 0.7011 - jacard_coef: 0.07543/5 [=================>............] - ETA: 0s - loss: 0.1660 - accuracy: 0.6614 - jacard_coef: 0.07744/5 [=======================>......] - ETA: 0s - loss: 0.1667 - accuracy: 0.6426 - jacard_coef: 0.07625/5 [==============================] - 2s 400ms/step - loss: 0.1667 - accuracy: 0.6415 - jacard_coef: 0.0736 - val_loss: 0.0863 - val_accuracy: 0.9148 - val_jacard_coef: 0.0636 - lr: 2.5000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1702 - accuracy: 0.5818 - jacard_coef: 0.09482/5 [===========>..................] - ETA: 1s - loss: 0.1712 - accuracy: 0.5728 - jacard_coef: 0.08293/5 [=================>............] - ETA: 0s - loss: 0.1697 - accuracy: 0.5917 - jacard_coef: 0.07814/5 [=======================>......] - ETA: 0s - loss: 0.1691 - accuracy: 0.6048 - jacard_coef: 0.07665/5 [==============================] - 2s 397ms/step - loss: 0.1691 - accuracy: 0.6034 - jacard_coef: 0.0618 - val_loss: 0.0780 - val_accuracy: 0.9215 - val_jacard_coef: 0.0601 - lr: 2.5000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1668 - accuracy: 0.6856 - jacard_coef: 0.07682/5 [===========>..................] - ETA: 1s - loss: 0.1673 - accuracy: 0.7132 - jacard_coef: 0.06993/5 [=================>............] - ETA: 0s - loss: 0.1672 - accuracy: 0.7106 - jacard_coef: 0.07134/5 [=======================>......] - ETA: 0s - loss: 0.1676 - accuracy: 0.6952 - jacard_coef: 0.07665/5 [==============================] - 2s 399ms/step - loss: 0.1676 - accuracy: 0.6932 - jacard_coef: 0.0625 - val_loss: 0.0778 - val_accuracy: 0.9199 - val_jacard_coef: 0.0608 - lr: 2.5000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1672 - accuracy: 0.7960 - jacard_coef: 0.07122/5 [===========>..................] - ETA: 1s - loss: 0.1659 - accuracy: 0.8065 - jacard_coef: 0.07723/5 [=================>............] - ETA: 0s - loss: 0.1655 - accuracy: 0.8109 - jacard_coef: 0.07574/5 [=======================>......] - ETA: 0s - loss: 0.1654 - accuracy: 0.8081 - jacard_coef: 0.07645/5 [==============================] - 2s 399ms/step - loss: 0.1655 - accuracy: 0.8065 - jacard_coef: 0.0671 - val_loss: 0.0851 - val_accuracy: 0.9173 - val_jacard_coef: 0.0630 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0667 (epoch 10)
  Final Val Loss: 0.0851
  Training Time: 0:01:57.431445
  Stability (std): 0.0372

Results saved to: hyperparameter_optimization_20250926_123742/exp_9_UNet_lr1e-3_bs32/UNet_lr0.001_bs32_results.json

Experiment 9 completed in 153s
Progress: 9/36 completed
Estimated remaining time: 68 minutes

ðŸ”¬ EXPERIMENT 10/36
================================================
Architecture: UNet
Learning Rate: 5e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758862773.691421 3213475 service.cc:145] XLA service 0x1544865f4290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758862773.691504 3213475 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758862774.153839 3213475 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 10:46 - loss: 0.3452 - accuracy: 0.4878 - jacard_coef: 0.0818 2/17 [==>...........................] - ETA: 56s - loss: 0.3120 - accuracy: 0.4512 - jacard_coef: 0.0985   3/17 [====>.........................] - ETA: 34s - loss: 0.2866 - accuracy: 0.3638 - jacard_coef: 0.0846 4/17 [======>.......................] - ETA: 25s - loss: 0.2655 - accuracy: 0.3216 - jacard_coef: 0.0764 5/17 [=======>......................] - ETA: 18s - loss: 0.2517 - accuracy: 0.2950 - jacard_coef: 0.0766 6/17 [=========>....................] - ETA: 13s - loss: 0.2490 - accuracy: 0.2670 - jacard_coef: 0.0719 7/17 [===========>..................] - ETA: 10s - loss: 0.2433 - accuracy: 0.2480 - jacard_coef: 0.0721 8/17 [=============>................] - ETA: 8s - loss: 0.2396 - accuracy: 0.2288 - jacard_coef: 0.0674  9/17 [==============>...............] - ETA: 6s - loss: 0.2385 - accuracy: 0.2224 - jacard_coef: 0.066510/17 [================>.............] - ETA: 5s - loss: 0.2346 - accuracy: 0.2261 - jacard_coef: 0.068311/17 [==================>...........] - ETA: 4s - loss: 0.2306 - accuracy: 0.2288 - jacard_coef: 0.071312/17 [====================>.........] - ETA: 3s - loss: 0.2296 - accuracy: 0.2284 - jacard_coef: 0.072313/17 [=====================>........] - ETA: 2s - loss: 0.2277 - accuracy: 0.2285 - jacard_coef: 0.074114/17 [=======================>......] - ETA: 1s - loss: 0.2249 - accuracy: 0.2298 - jacard_coef: 0.074215/17 [=========================>....] - ETA: 1s - loss: 0.2251 - accuracy: 0.2300 - jacard_coef: 0.077316/17 [===========================>..] - ETA: 0s - loss: 0.2240 - accuracy: 0.2341 - jacard_coef: 0.076617/17 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.2339 - jacard_coef: 0.072417/17 [==============================] - 55s 927ms/step - loss: 0.2239 - accuracy: 0.2339 - jacard_coef: 0.0724 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4109e-05 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1946 - accuracy: 0.2586 - jacard_coef: 0.0628 2/17 [==>...........................] - ETA: 1s - loss: 0.1936 - accuracy: 0.2345 - jacard_coef: 0.0704 3/17 [====>.........................] - ETA: 1s - loss: 0.2010 - accuracy: 0.2196 - jacard_coef: 0.0819 4/17 [======>.......................] - ETA: 1s - loss: 0.1998 - accuracy: 0.2025 - jacard_coef: 0.0709 5/17 [=======>......................] - ETA: 1s - loss: 0.1970 - accuracy: 0.2248 - jacard_coef: 0.0727 6/17 [=========>....................] - ETA: 1s - loss: 0.1977 - accuracy: 0.2346 - jacard_coef: 0.0798 7/17 [===========>..................] - ETA: 1s - loss: 0.1989 - accuracy: 0.2357 - jacard_coef: 0.0791 8/17 [=============>................] - ETA: 1s - loss: 0.2001 - accuracy: 0.2379 - jacard_coef: 0.0817 9/17 [==============>...............] - ETA: 1s - loss: 0.1986 - accuracy: 0.2402 - jacard_coef: 0.079610/17 [================>.............] - ETA: 0s - loss: 0.1969 - accuracy: 0.2507 - jacard_coef: 0.077611/17 [==================>...........] - ETA: 0s - loss: 0.1956 - accuracy: 0.2607 - jacard_coef: 0.077812/17 [====================>.........] - ETA: 0s - loss: 0.1944 - accuracy: 0.2692 - jacard_coef: 0.075213/17 [=====================>........] - ETA: 0s - loss: 0.1932 - accuracy: 0.2786 - jacard_coef: 0.076414/17 [=======================>......] - ETA: 0s - loss: 0.1928 - accuracy: 0.2887 - jacard_coef: 0.075815/17 [=========================>....] - ETA: 0s - loss: 0.1919 - accuracy: 0.2952 - jacard_coef: 0.077016/17 [===========================>..] - ETA: 0s - loss: 0.1910 - accuracy: 0.2949 - jacard_coef: 0.076517/17 [==============================] - ETA: 0s - loss: 0.1910 - accuracy: 0.2937 - jacard_coef: 0.072417/17 [==============================] - 2s 142ms/step - loss: 0.1910 - accuracy: 0.2937 - jacard_coef: 0.0724 - val_loss: 1.0260 - val_accuracy: 0.9293 - val_jacard_coef: 0.0130 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1787 - accuracy: 0.3196 - jacard_coef: 0.0671 2/17 [==>...........................] - ETA: 1s - loss: 0.1796 - accuracy: 0.3202 - jacard_coef: 0.0747 3/17 [====>.........................] - ETA: 1s - loss: 0.1785 - accuracy: 0.3329 - jacard_coef: 0.0798 4/17 [======>.......................] - ETA: 1s - loss: 0.1772 - accuracy: 0.3481 - jacard_coef: 0.0821 5/17 [=======>......................] - ETA: 1s - loss: 0.1763 - accuracy: 0.3671 - jacard_coef: 0.0792 6/17 [=========>....................] - ETA: 1s - loss: 0.1756 - accuracy: 0.4048 - jacard_coef: 0.0796 7/17 [===========>..................] - ETA: 1s - loss: 0.1757 - accuracy: 0.4131 - jacard_coef: 0.0812 8/17 [=============>................] - ETA: 1s - loss: 0.1755 - accuracy: 0.4308 - jacard_coef: 0.0773 9/17 [==============>...............] - ETA: 1s - loss: 0.1753 - accuracy: 0.4486 - jacard_coef: 0.077110/17 [================>.............] - ETA: 0s - loss: 0.1751 - accuracy: 0.4604 - jacard_coef: 0.075711/17 [==================>...........] - ETA: 0s - loss: 0.1746 - accuracy: 0.4710 - jacard_coef: 0.075812/17 [====================>.........] - ETA: 0s - loss: 0.1747 - accuracy: 0.4646 - jacard_coef: 0.076513/17 [=====================>........] - ETA: 0s - loss: 0.1754 - accuracy: 0.4775 - jacard_coef: 0.078914/17 [=======================>......] - ETA: 0s - loss: 0.1752 - accuracy: 0.4877 - jacard_coef: 0.077415/17 [=========================>....] - ETA: 0s - loss: 0.1750 - accuracy: 0.4963 - jacard_coef: 0.075416/17 [===========================>..] - ETA: 0s - loss: 0.1748 - accuracy: 0.5000 - jacard_coef: 0.076517/17 [==============================] - 2s 137ms/step - loss: 0.1748 - accuracy: 0.4992 - jacard_coef: 0.0748 - val_loss: 1.0685 - val_accuracy: 0.9260 - val_jacard_coef: 0.0133 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1709 - accuracy: 0.4732 - jacard_coef: 0.0456 2/17 [==>...........................] - ETA: 1s - loss: 0.1719 - accuracy: 0.4939 - jacard_coef: 0.0740 3/17 [====>.........................] - ETA: 1s - loss: 0.1756 - accuracy: 0.5086 - jacard_coef: 0.0776 4/17 [======>.......................] - ETA: 1s - loss: 0.1741 - accuracy: 0.5221 - jacard_coef: 0.0707 5/17 [=======>......................] - ETA: 1s - loss: 0.1732 - accuracy: 0.5296 - jacard_coef: 0.0698 6/17 [=========>....................] - ETA: 1s - loss: 0.1729 - accuracy: 0.5324 - jacard_coef: 0.0731 7/17 [===========>..................] - ETA: 1s - loss: 0.1723 - accuracy: 0.5405 - jacard_coef: 0.0736 8/17 [=============>................] - ETA: 1s - loss: 0.1718 - accuracy: 0.5515 - jacard_coef: 0.0725 9/17 [==============>...............] - ETA: 1s - loss: 0.1714 - accuracy: 0.5630 - jacard_coef: 0.072810/17 [================>.............] - ETA: 0s - loss: 0.1749 - accuracy: 0.5391 - jacard_coef: 0.071511/17 [==================>...........] - ETA: 0s - loss: 0.1743 - accuracy: 0.5595 - jacard_coef: 0.073212/17 [====================>.........] - ETA: 0s - loss: 0.1750 - accuracy: 0.5410 - jacard_coef: 0.073113/17 [=====================>........] - ETA: 0s - loss: 0.1788 - accuracy: 0.5237 - jacard_coef: 0.074514/17 [=======================>......] - ETA: 0s - loss: 0.1792 - accuracy: 0.4991 - jacard_coef: 0.073915/17 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.4820 - jacard_coef: 0.075616/17 [===========================>..] - ETA: 0s - loss: 0.1816 - accuracy: 0.4661 - jacard_coef: 0.075117/17 [==============================] - 2s 141ms/step - loss: 0.1816 - accuracy: 0.4652 - jacard_coef: 0.0786 - val_loss: 0.5482 - val_accuracy: 0.9187 - val_jacard_coef: 0.0443 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1830 - accuracy: 0.2155 - jacard_coef: 0.0436 2/17 [==>...........................] - ETA: 2s - loss: 0.1854 - accuracy: 0.2224 - jacard_coef: 0.0621 3/17 [====>.........................] - ETA: 1s - loss: 0.1843 - accuracy: 0.2594 - jacard_coef: 0.0765 4/17 [======>.......................] - ETA: 1s - loss: 0.1819 - accuracy: 0.2754 - jacard_coef: 0.0775 5/17 [=======>......................] - ETA: 1s - loss: 0.1811 - accuracy: 0.2868 - jacard_coef: 0.0739 6/17 [=========>....................] - ETA: 1s - loss: 0.1790 - accuracy: 0.3356 - jacard_coef: 0.0758 7/17 [===========>..................] - ETA: 1s - loss: 0.1776 - accuracy: 0.3731 - jacard_coef: 0.0791 8/17 [=============>................] - ETA: 1s - loss: 0.1792 - accuracy: 0.3886 - jacard_coef: 0.0812 9/17 [==============>...............] - ETA: 1s - loss: 0.1790 - accuracy: 0.4287 - jacard_coef: 0.081910/17 [================>.............] - ETA: 0s - loss: 0.1796 - accuracy: 0.4655 - jacard_coef: 0.081211/17 [==================>...........] - ETA: 0s - loss: 0.1787 - accuracy: 0.4893 - jacard_coef: 0.080412/17 [====================>.........] - ETA: 0s - loss: 0.1777 - accuracy: 0.5110 - jacard_coef: 0.077913/17 [=====================>........] - ETA: 0s - loss: 0.1767 - accuracy: 0.5346 - jacard_coef: 0.078114/17 [=======================>......] - ETA: 0s - loss: 0.1760 - accuracy: 0.5529 - jacard_coef: 0.077815/17 [=========================>....] - ETA: 0s - loss: 0.1751 - accuracy: 0.5694 - jacard_coef: 0.075616/17 [===========================>..] - ETA: 0s - loss: 0.1745 - accuracy: 0.5793 - jacard_coef: 0.075617/17 [==============================] - 2s 137ms/step - loss: 0.1744 - accuracy: 0.5792 - jacard_coef: 0.0790 - val_loss: 1.0553 - val_accuracy: 0.9135 - val_jacard_coef: 0.0143 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1636 - accuracy: 0.8271 - jacard_coef: 0.0353 2/17 [==>...........................] - ETA: 2s - loss: 0.1631 - accuracy: 0.8258 - jacard_coef: 0.0699 3/17 [====>.........................] - ETA: 1s - loss: 0.1635 - accuracy: 0.8331 - jacard_coef: 0.0629 4/17 [======>.......................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8351 - jacard_coef: 0.0621 5/17 [=======>......................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8322 - jacard_coef: 0.0619 6/17 [=========>....................] - ETA: 1s - loss: 0.1629 - accuracy: 0.8333 - jacard_coef: 0.0612 7/17 [===========>..................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8212 - jacard_coef: 0.0670 8/17 [=============>................] - ETA: 1s - loss: 0.1630 - accuracy: 0.8316 - jacard_coef: 0.0663 9/17 [==============>...............] - ETA: 1s - loss: 0.1626 - accuracy: 0.8383 - jacard_coef: 0.067910/17 [================>.............] - ETA: 0s - loss: 0.1624 - accuracy: 0.8408 - jacard_coef: 0.070911/17 [==================>...........] - ETA: 0s - loss: 0.1622 - accuracy: 0.8446 - jacard_coef: 0.072412/17 [====================>.........] - ETA: 0s - loss: 0.1620 - accuracy: 0.8445 - jacard_coef: 0.075113/17 [=====================>........] - ETA: 0s - loss: 0.1617 - accuracy: 0.8412 - jacard_coef: 0.074314/17 [=======================>......] - ETA: 0s - loss: 0.1613 - accuracy: 0.8426 - jacard_coef: 0.074215/17 [=========================>....] - ETA: 0s - loss: 0.1610 - accuracy: 0.8434 - jacard_coef: 0.074416/17 [===========================>..] - ETA: 0s - loss: 0.1609 - accuracy: 0.8419 - jacard_coef: 0.075117/17 [==============================] - 2s 137ms/step - loss: 0.1610 - accuracy: 0.8392 - jacard_coef: 0.0791 - val_loss: 1.0130 - val_accuracy: 0.9143 - val_jacard_coef: 0.0226 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1590 - accuracy: 0.9067 - jacard_coef: 0.0686 2/17 [==>...........................] - ETA: 2s - loss: 0.1599 - accuracy: 0.8558 - jacard_coef: 0.0676 3/17 [====>.........................] - ETA: 1s - loss: 0.1623 - accuracy: 0.7483 - jacard_coef: 0.0752 4/17 [======>.......................] - ETA: 1s - loss: 0.1616 - accuracy: 0.7509 - jacard_coef: 0.0712 5/17 [=======>......................] - ETA: 1s - loss: 0.1647 - accuracy: 0.6810 - jacard_coef: 0.0708 6/17 [=========>....................] - ETA: 1s - loss: 0.1639 - accuracy: 0.7154 - jacard_coef: 0.0689 7/17 [===========>..................] - ETA: 1s - loss: 0.1648 - accuracy: 0.7271 - jacard_coef: 0.0693 8/17 [=============>................] - ETA: 1s - loss: 0.1645 - accuracy: 0.7161 - jacard_coef: 0.0687 9/17 [==============>...............] - ETA: 1s - loss: 0.1640 - accuracy: 0.7170 - jacard_coef: 0.069810/17 [================>.............] - ETA: 0s - loss: 0.1635 - accuracy: 0.7122 - jacard_coef: 0.067611/17 [==================>...........] - ETA: 0s - loss: 0.1635 - accuracy: 0.7024 - jacard_coef: 0.068712/17 [====================>.........] - ETA: 0s - loss: 0.1635 - accuracy: 0.7014 - jacard_coef: 0.068513/17 [=====================>........] - ETA: 0s - loss: 0.1632 - accuracy: 0.7038 - jacard_coef: 0.069514/17 [=======================>......] - ETA: 0s - loss: 0.1630 - accuracy: 0.7043 - jacard_coef: 0.071815/17 [=========================>....] - ETA: 0s - loss: 0.1627 - accuracy: 0.7152 - jacard_coef: 0.074016/17 [===========================>..] - ETA: 0s - loss: 0.1623 - accuracy: 0.7248 - jacard_coef: 0.075017/17 [==============================] - 2s 137ms/step - loss: 0.1623 - accuracy: 0.7243 - jacard_coef: 0.0775 - val_loss: 0.5329 - val_accuracy: 0.9243 - val_jacard_coef: 0.0152 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1543 - accuracy: 0.9112 - jacard_coef: 0.0728 2/17 [==>...........................] - ETA: 2s - loss: 0.1544 - accuracy: 0.9293 - jacard_coef: 0.0501 3/17 [====>.........................] - ETA: 1s - loss: 0.1544 - accuracy: 0.9153 - jacard_coef: 0.0635 4/17 [======>.......................] - ETA: 1s - loss: 0.1545 - accuracy: 0.9025 - jacard_coef: 0.0621 5/17 [=======>......................] - ETA: 1s - loss: 0.1560 - accuracy: 0.8806 - jacard_coef: 0.0678 6/17 [=========>....................] - ETA: 1s - loss: 0.1559 - accuracy: 0.8814 - jacard_coef: 0.0712 7/17 [===========>..................] - ETA: 1s - loss: 0.1560 - accuracy: 0.8783 - jacard_coef: 0.0782 8/17 [=============>................] - ETA: 1s - loss: 0.1556 - accuracy: 0.8817 - jacard_coef: 0.0788 9/17 [==============>...............] - ETA: 1s - loss: 0.1552 - accuracy: 0.8870 - jacard_coef: 0.077010/17 [================>.............] - ETA: 0s - loss: 0.1548 - accuracy: 0.8906 - jacard_coef: 0.076111/17 [==================>...........] - ETA: 0s - loss: 0.1546 - accuracy: 0.8929 - jacard_coef: 0.076112/17 [====================>.........] - ETA: 0s - loss: 0.1542 - accuracy: 0.8977 - jacard_coef: 0.073413/17 [=====================>........] - ETA: 0s - loss: 0.1541 - accuracy: 0.8954 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1540 - accuracy: 0.8953 - jacard_coef: 0.072915/17 [=========================>....] - ETA: 0s - loss: 0.1539 - accuracy: 0.8911 - jacard_coef: 0.072516/17 [===========================>..] - ETA: 0s - loss: 0.1539 - accuracy: 0.8895 - jacard_coef: 0.074417/17 [==============================] - 2s 142ms/step - loss: 0.1540 - accuracy: 0.8871 - jacard_coef: 0.0783 - val_loss: 0.1602 - val_accuracy: 0.9176 - val_jacard_coef: 0.0631 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1532 - accuracy: 0.8893 - jacard_coef: 0.0916 2/17 [==>...........................] - ETA: 2s - loss: 0.1522 - accuracy: 0.8975 - jacard_coef: 0.0878 3/17 [====>.........................] - ETA: 1s - loss: 0.1510 - accuracy: 0.9110 - jacard_coef: 0.0771 4/17 [======>.......................] - ETA: 1s - loss: 0.1518 - accuracy: 0.9073 - jacard_coef: 0.0798 5/17 [=======>......................] - ETA: 1s - loss: 0.1527 - accuracy: 0.9059 - jacard_coef: 0.0810 6/17 [=========>....................] - ETA: 1s - loss: 0.1520 - accuracy: 0.9119 - jacard_coef: 0.0765 7/17 [===========>..................] - ETA: 1s - loss: 0.1513 - accuracy: 0.9165 - jacard_coef: 0.0731 8/17 [=============>................] - ETA: 1s - loss: 0.1510 - accuracy: 0.9146 - jacard_coef: 0.0751 9/17 [==============>...............] - ETA: 1s - loss: 0.1514 - accuracy: 0.9166 - jacard_coef: 0.073610/17 [================>.............] - ETA: 0s - loss: 0.1511 - accuracy: 0.9154 - jacard_coef: 0.074911/17 [==================>...........] - ETA: 0s - loss: 0.1511 - accuracy: 0.9138 - jacard_coef: 0.076212/17 [====================>.........] - ETA: 0s - loss: 0.1508 - accuracy: 0.9136 - jacard_coef: 0.076213/17 [=====================>........] - ETA: 0s - loss: 0.1506 - accuracy: 0.9141 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1506 - accuracy: 0.9117 - jacard_coef: 0.076615/17 [=========================>....] - ETA: 0s - loss: 0.1502 - accuracy: 0.9136 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1501 - accuracy: 0.9112 - jacard_coef: 0.075917/17 [==============================] - 2s 143ms/step - loss: 0.1501 - accuracy: 0.9093 - jacard_coef: 0.0716 - val_loss: 0.4136 - val_accuracy: 0.6973 - val_jacard_coef: 0.0655 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1514 - accuracy: 0.8790 - jacard_coef: 0.0973 2/17 [==>...........................] - ETA: 2s - loss: 0.1520 - accuracy: 0.8789 - jacard_coef: 0.0948 3/17 [====>.........................] - ETA: 1s - loss: 0.1522 - accuracy: 0.8837 - jacard_coef: 0.0878 4/17 [======>.......................] - ETA: 1s - loss: 0.1554 - accuracy: 0.8847 - jacard_coef: 0.0848 5/17 [=======>......................] - ETA: 1s - loss: 0.1552 - accuracy: 0.8785 - jacard_coef: 0.0871 6/17 [=========>....................] - ETA: 1s - loss: 0.1541 - accuracy: 0.8814 - jacard_coef: 0.0836 7/17 [===========>..................] - ETA: 1s - loss: 0.1553 - accuracy: 0.8783 - jacard_coef: 0.0833 8/17 [=============>................] - ETA: 1s - loss: 0.1562 - accuracy: 0.8764 - jacard_coef: 0.0815 9/17 [==============>...............] - ETA: 1s - loss: 0.1554 - accuracy: 0.8830 - jacard_coef: 0.076710/17 [================>.............] - ETA: 0s - loss: 0.1547 - accuracy: 0.8877 - jacard_coef: 0.075411/17 [==================>...........] - ETA: 0s - loss: 0.1553 - accuracy: 0.8899 - jacard_coef: 0.075712/17 [====================>.........] - ETA: 0s - loss: 0.1543 - accuracy: 0.8955 - jacard_coef: 0.072713/17 [=====================>........] - ETA: 0s - loss: 0.1551 - accuracy: 0.8938 - jacard_coef: 0.075314/17 [=======================>......] - ETA: 0s - loss: 0.1553 - accuracy: 0.8933 - jacard_coef: 0.076415/17 [=========================>....] - ETA: 0s - loss: 0.1550 - accuracy: 0.8930 - jacard_coef: 0.076616/17 [===========================>..] - ETA: 0s - loss: 0.1544 - accuracy: 0.8947 - jacard_coef: 0.075517/17 [==============================] - 2s 137ms/step - loss: 0.1545 - accuracy: 0.8926 - jacard_coef: 0.0713 - val_loss: 1.0308 - val_accuracy: 0.9113 - val_jacard_coef: 0.0267 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1479 - accuracy: 0.9163 - jacard_coef: 0.0565 2/17 [==>...........................] - ETA: 2s - loss: 0.1502 - accuracy: 0.9147 - jacard_coef: 0.0617 3/17 [====>.........................] - ETA: 1s - loss: 0.1494 - accuracy: 0.9164 - jacard_coef: 0.0625 4/17 [======>.......................] - ETA: 1s - loss: 0.1499 - accuracy: 0.8935 - jacard_coef: 0.0679 5/17 [=======>......................] - ETA: 1s - loss: 0.1499 - accuracy: 0.8852 - jacard_coef: 0.0728 6/17 [=========>....................] - ETA: 1s - loss: 0.1499 - accuracy: 0.8783 - jacard_coef: 0.0744 7/17 [===========>..................] - ETA: 1s - loss: 0.1492 - accuracy: 0.8810 - jacard_coef: 0.0762 8/17 [=============>................] - ETA: 1s - loss: 0.1488 - accuracy: 0.8859 - jacard_coef: 0.0752 9/17 [==============>...............] - ETA: 1s - loss: 0.1483 - accuracy: 0.8889 - jacard_coef: 0.075310/17 [================>.............] - ETA: 0s - loss: 0.1478 - accuracy: 0.8937 - jacard_coef: 0.072311/17 [==================>...........] - ETA: 0s - loss: 0.1479 - accuracy: 0.8903 - jacard_coef: 0.076312/17 [====================>.........] - ETA: 0s - loss: 0.1477 - accuracy: 0.8896 - jacard_coef: 0.077613/17 [=====================>........] - ETA: 0s - loss: 0.1472 - accuracy: 0.8913 - jacard_coef: 0.076914/17 [=======================>......] - ETA: 0s - loss: 0.1469 - accuracy: 0.8938 - jacard_coef: 0.075515/17 [=========================>....] - ETA: 0s - loss: 0.1466 - accuracy: 0.8957 - jacard_coef: 0.075316/17 [===========================>..] - ETA: 0s - loss: 0.1463 - accuracy: 0.8974 - jacard_coef: 0.074717/17 [==============================] - 2s 137ms/step - loss: 0.1463 - accuracy: 0.8967 - jacard_coef: 0.0781 - val_loss: 0.1921 - val_accuracy: 0.2713 - val_jacard_coef: 0.0632 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1446 - accuracy: 0.8945 - jacard_coef: 0.0758 2/17 [==>...........................] - ETA: 2s - loss: 0.1437 - accuracy: 0.8964 - jacard_coef: 0.0689 3/17 [====>.........................] - ETA: 1s - loss: 0.1443 - accuracy: 0.8926 - jacard_coef: 0.0730 4/17 [======>.......................] - ETA: 1s - loss: 0.1439 - accuracy: 0.8886 - jacard_coef: 0.0779 5/17 [=======>......................] - ETA: 1s - loss: 0.1432 - accuracy: 0.8955 - jacard_coef: 0.0747 6/17 [=========>....................] - ETA: 1s - loss: 0.1425 - accuracy: 0.9013 - jacard_coef: 0.0723 7/17 [===========>..................] - ETA: 1s - loss: 0.1423 - accuracy: 0.9037 - jacard_coef: 0.0725 8/17 [=============>................] - ETA: 1s - loss: 0.1418 - accuracy: 0.9085 - jacard_coef: 0.0701 9/17 [==============>...............] - ETA: 1s - loss: 0.1417 - accuracy: 0.9060 - jacard_coef: 0.073510/17 [================>.............] - ETA: 0s - loss: 0.1413 - accuracy: 0.9088 - jacard_coef: 0.072311/17 [==================>...........] - ETA: 0s - loss: 0.1411 - accuracy: 0.9093 - jacard_coef: 0.072812/17 [====================>.........] - ETA: 0s - loss: 0.1410 - accuracy: 0.9088 - jacard_coef: 0.074013/17 [=====================>........] - ETA: 0s - loss: 0.1412 - accuracy: 0.9061 - jacard_coef: 0.076714/17 [=======================>......] - ETA: 0s - loss: 0.1413 - accuracy: 0.9070 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 0s - loss: 0.1410 - accuracy: 0.9089 - jacard_coef: 0.074916/17 [===========================>..] - ETA: 0s - loss: 0.1409 - accuracy: 0.9084 - jacard_coef: 0.075517/17 [==============================] - 2s 137ms/step - loss: 0.1409 - accuracy: 0.9091 - jacard_coef: 0.0712 - val_loss: 0.1574 - val_accuracy: 0.8593 - val_jacard_coef: 0.0632 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1368 - accuracy: 0.9305 - jacard_coef: 0.0617 2/17 [==>...........................] - ETA: 2s - loss: 0.1380 - accuracy: 0.9170 - jacard_coef: 0.0734 3/17 [====>.........................] - ETA: 1s - loss: 0.1383 - accuracy: 0.9123 - jacard_coef: 0.0775 4/17 [======>.......................] - ETA: 1s - loss: 0.1391 - accuracy: 0.9093 - jacard_coef: 0.0787 5/17 [=======>......................] - ETA: 1s - loss: 0.1389 - accuracy: 0.9114 - jacard_coef: 0.0771 6/17 [=========>....................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9108 - jacard_coef: 0.0782 7/17 [===========>..................] - ETA: 1s - loss: 0.1379 - accuracy: 0.9174 - jacard_coef: 0.0728 8/17 [=============>................] - ETA: 1s - loss: 0.1379 - accuracy: 0.9170 - jacard_coef: 0.0736 9/17 [==============>...............] - ETA: 1s - loss: 0.1378 - accuracy: 0.9159 - jacard_coef: 0.074710/17 [================>.............] - ETA: 0s - loss: 0.1377 - accuracy: 0.9177 - jacard_coef: 0.073311/17 [==================>...........] - ETA: 0s - loss: 0.1376 - accuracy: 0.9187 - jacard_coef: 0.072712/17 [====================>.........] - ETA: 0s - loss: 0.1376 - accuracy: 0.9172 - jacard_coef: 0.074013/17 [=====================>........] - ETA: 0s - loss: 0.1375 - accuracy: 0.9175 - jacard_coef: 0.073914/17 [=======================>......] - ETA: 0s - loss: 0.1374 - accuracy: 0.9170 - jacard_coef: 0.074415/17 [=========================>....] - ETA: 0s - loss: 0.1374 - accuracy: 0.9158 - jacard_coef: 0.075516/17 [===========================>..] - ETA: 0s - loss: 0.1372 - accuracy: 0.9159 - jacard_coef: 0.075417/17 [==============================] - 2s 137ms/step - loss: 0.1372 - accuracy: 0.9162 - jacard_coef: 0.0733 - val_loss: 0.1542 - val_accuracy: 0.8162 - val_jacard_coef: 0.0632 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1341 - accuracy: 0.9421 - jacard_coef: 0.0520 2/17 [==>...........................] - ETA: 2s - loss: 0.1359 - accuracy: 0.9166 - jacard_coef: 0.0735 3/17 [====>.........................] - ETA: 1s - loss: 0.1363 - accuracy: 0.9094 - jacard_coef: 0.0786 4/17 [======>.......................] - ETA: 1s - loss: 0.1361 - accuracy: 0.9080 - jacard_coef: 0.0796 5/17 [=======>......................] - ETA: 1s - loss: 0.1356 - accuracy: 0.9128 - jacard_coef: 0.0760 6/17 [=========>....................] - ETA: 1s - loss: 0.1355 - accuracy: 0.9153 - jacard_coef: 0.0737 7/17 [===========>..................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9131 - jacard_coef: 0.0755 8/17 [=============>................] - ETA: 1s - loss: 0.1356 - accuracy: 0.9140 - jacard_coef: 0.0750 9/17 [==============>...............] - ETA: 1s - loss: 0.1355 - accuracy: 0.9130 - jacard_coef: 0.075810/17 [================>.............] - ETA: 0s - loss: 0.1350 - accuracy: 0.9163 - jacard_coef: 0.073211/17 [==================>...........] - ETA: 0s - loss: 0.1347 - accuracy: 0.9176 - jacard_coef: 0.072212/17 [====================>.........] - ETA: 0s - loss: 0.1347 - accuracy: 0.9161 - jacard_coef: 0.073713/17 [=====================>........] - ETA: 0s - loss: 0.1348 - accuracy: 0.9145 - jacard_coef: 0.075214/17 [=======================>......] - ETA: 0s - loss: 0.1346 - accuracy: 0.9151 - jacard_coef: 0.074815/17 [=========================>....] - ETA: 0s - loss: 0.1343 - accuracy: 0.9169 - jacard_coef: 0.073516/17 [===========================>..] - ETA: 0s - loss: 0.1345 - accuracy: 0.9150 - jacard_coef: 0.075117/17 [==============================] - 2s 137ms/step - loss: 0.1348 - accuracy: 0.9115 - jacard_coef: 0.0729 - val_loss: 0.1371 - val_accuracy: 0.9304 - val_jacard_coef: 0.0626 - lr: 0.0010
Epoch 15/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1357 - accuracy: 0.8936 - jacard_coef: 0.0940 2/17 [==>...........................] - ETA: 2s - loss: 0.1363 - accuracy: 0.8923 - jacard_coef: 0.0944 3/17 [====>.........................] - ETA: 1s - loss: 0.1367 - accuracy: 0.8915 - jacard_coef: 0.0938 4/17 [======>.......................] - ETA: 1s - loss: 0.1356 - accuracy: 0.9002 - jacard_coef: 0.0865 5/17 [=======>......................] - ETA: 1s - loss: 0.1355 - accuracy: 0.9053 - jacard_coef: 0.0812 6/17 [=========>....................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9003 - jacard_coef: 0.0850 7/17 [===========>..................] - ETA: 1s - loss: 0.1350 - accuracy: 0.9081 - jacard_coef: 0.0788 8/17 [=============>................] - ETA: 1s - loss: 0.1351 - accuracy: 0.9060 - jacard_coef: 0.0810 9/17 [==============>...............] - ETA: 1s - loss: 0.1343 - accuracy: 0.9119 - jacard_coef: 0.076310/17 [================>.............] - ETA: 0s - loss: 0.1341 - accuracy: 0.9144 - jacard_coef: 0.074511/17 [==================>...........] - ETA: 0s - loss: 0.1341 - accuracy: 0.9139 - jacard_coef: 0.075212/17 [====================>.........] - ETA: 0s - loss: 0.1345 - accuracy: 0.9148 - jacard_coef: 0.074613/17 [=====================>........] - ETA: 0s - loss: 0.1344 - accuracy: 0.9152 - jacard_coef: 0.074614/17 [=======================>......] - ETA: 0s - loss: 0.1340 - accuracy: 0.9173 - jacard_coef: 0.072915/17 [=========================>....] - ETA: 0s - loss: 0.1342 - accuracy: 0.9154 - jacard_coef: 0.074616/17 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.9161 - jacard_coef: 0.074117/17 [==============================] - 2s 137ms/step - loss: 0.1341 - accuracy: 0.9155 - jacard_coef: 0.0775 - val_loss: 0.1278 - val_accuracy: 0.9304 - val_jacard_coef: 0.0624 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1343 - accuracy: 0.9044 - jacard_coef: 0.0835 2/17 [==>...........................] - ETA: 2s - loss: 0.1299 - accuracy: 0.9368 - jacard_coef: 0.0559 3/17 [====>.........................] - ETA: 1s - loss: 0.1287 - accuracy: 0.9469 - jacard_coef: 0.0474 4/17 [======>.......................] - ETA: 1s - loss: 0.1302 - accuracy: 0.9344 - jacard_coef: 0.0581 5/17 [=======>......................] - ETA: 1s - loss: 0.1314 - accuracy: 0.9256 - jacard_coef: 0.0652 6/17 [=========>....................] - ETA: 1s - loss: 0.1314 - accuracy: 0.9261 - jacard_coef: 0.0645 7/17 [===========>..................] - ETA: 1s - loss: 0.1314 - accuracy: 0.9226 - jacard_coef: 0.0675 8/17 [=============>................] - ETA: 1s - loss: 0.1320 - accuracy: 0.9165 - jacard_coef: 0.0723 9/17 [==============>...............] - ETA: 1s - loss: 0.1320 - accuracy: 0.9168 - jacard_coef: 0.072110/17 [================>.............] - ETA: 0s - loss: 0.1328 - accuracy: 0.9144 - jacard_coef: 0.074111/17 [==================>...........] - ETA: 0s - loss: 0.1328 - accuracy: 0.9128 - jacard_coef: 0.075612/17 [====================>.........] - ETA: 0s - loss: 0.1326 - accuracy: 0.9130 - jacard_coef: 0.075713/17 [=====================>........] - ETA: 0s - loss: 0.1325 - accuracy: 0.9133 - jacard_coef: 0.075614/17 [=======================>......] - ETA: 0s - loss: 0.1329 - accuracy: 0.9099 - jacard_coef: 0.078315/17 [=========================>....] - ETA: 0s - loss: 0.1325 - accuracy: 0.9123 - jacard_coef: 0.076516/17 [===========================>..] - ETA: 0s - loss: 0.1321 - accuracy: 0.9154 - jacard_coef: 0.073917/17 [==============================] - 2s 138ms/step - loss: 0.1322 - accuracy: 0.9147 - jacard_coef: 0.0770 - val_loss: 0.1321 - val_accuracy: 0.9304 - val_jacard_coef: 0.0626 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1319 - accuracy: 0.9133 - jacard_coef: 0.0706 2/17 [==>...........................] - ETA: 2s - loss: 0.1336 - accuracy: 0.9122 - jacard_coef: 0.0718 3/17 [====>.........................] - ETA: 1s - loss: 0.1341 - accuracy: 0.9119 - jacard_coef: 0.0729 4/17 [======>.......................] - ETA: 1s - loss: 0.1349 - accuracy: 0.9079 - jacard_coef: 0.0771 5/17 [=======>......................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9161 - jacard_coef: 0.0704 6/17 [=========>....................] - ETA: 1s - loss: 0.1342 - accuracy: 0.9101 - jacard_coef: 0.0754 7/17 [===========>..................] - ETA: 1s - loss: 0.1338 - accuracy: 0.9121 - jacard_coef: 0.0740 8/17 [=============>................] - ETA: 1s - loss: 0.1340 - accuracy: 0.9089 - jacard_coef: 0.0768 9/17 [==============>...............] - ETA: 1s - loss: 0.1341 - accuracy: 0.9080 - jacard_coef: 0.078110/17 [================>.............] - ETA: 0s - loss: 0.1336 - accuracy: 0.9109 - jacard_coef: 0.075911/17 [==================>...........] - ETA: 0s - loss: 0.1334 - accuracy: 0.9119 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 0s - loss: 0.1332 - accuracy: 0.9119 - jacard_coef: 0.075813/17 [=====================>........] - ETA: 0s - loss: 0.1329 - accuracy: 0.9133 - jacard_coef: 0.074814/17 [=======================>......] - ETA: 0s - loss: 0.1327 - accuracy: 0.9144 - jacard_coef: 0.074115/17 [=========================>....] - ETA: 0s - loss: 0.1325 - accuracy: 0.9153 - jacard_coef: 0.073616/17 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9150 - jacard_coef: 0.074117/17 [==============================] - 2s 137ms/step - loss: 0.1327 - accuracy: 0.9142 - jacard_coef: 0.0792 - val_loss: 0.1269 - val_accuracy: 0.9272 - val_jacard_coef: 0.0626 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1280 - accuracy: 0.9301 - jacard_coef: 0.0648 2/17 [==>...........................] - ETA: 2s - loss: 0.1303 - accuracy: 0.9137 - jacard_coef: 0.0781 3/17 [====>.........................] - ETA: 1s - loss: 0.1301 - accuracy: 0.9171 - jacard_coef: 0.0754 4/17 [======>.......................] - ETA: 1s - loss: 0.1292 - accuracy: 0.9221 - jacard_coef: 0.0713 5/17 [=======>......................] - ETA: 1s - loss: 0.1295 - accuracy: 0.9211 - jacard_coef: 0.0721 6/17 [=========>....................] - ETA: 1s - loss: 0.1296 - accuracy: 0.9210 - jacard_coef: 0.0722 7/17 [===========>..................] - ETA: 1s - loss: 0.1300 - accuracy: 0.9186 - jacard_coef: 0.0742 8/17 [=============>................] - ETA: 1s - loss: 0.1298 - accuracy: 0.9187 - jacard_coef: 0.0741 9/17 [==============>...............] - ETA: 1s - loss: 0.1298 - accuracy: 0.9186 - jacard_coef: 0.074210/17 [================>.............] - ETA: 0s - loss: 0.1296 - accuracy: 0.9183 - jacard_coef: 0.074511/17 [==================>...........] - ETA: 0s - loss: 0.1301 - accuracy: 0.9147 - jacard_coef: 0.077512/17 [====================>.........] - ETA: 0s - loss: 0.1295 - accuracy: 0.9182 - jacard_coef: 0.074513/17 [=====================>........] - ETA: 0s - loss: 0.1296 - accuracy: 0.9168 - jacard_coef: 0.075614/17 [=======================>......] - ETA: 0s - loss: 0.1295 - accuracy: 0.9167 - jacard_coef: 0.075715/17 [=========================>....] - ETA: 0s - loss: 0.1294 - accuracy: 0.9163 - jacard_coef: 0.076016/17 [===========================>..] - ETA: 0s - loss: 0.1292 - accuracy: 0.9170 - jacard_coef: 0.075217/17 [==============================] - 2s 137ms/step - loss: 0.1292 - accuracy: 0.9173 - jacard_coef: 0.0729 - val_loss: 0.1320 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 5.0000e-04
Epoch 19/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1277 - accuracy: 0.9114 - jacard_coef: 0.0780 2/17 [==>...........................] - ETA: 2s - loss: 0.1289 - accuracy: 0.9077 - jacard_coef: 0.0806 3/17 [====>.........................] - ETA: 1s - loss: 0.1287 - accuracy: 0.9077 - jacard_coef: 0.0805 4/17 [======>.......................] - ETA: 1s - loss: 0.1280 - accuracy: 0.9135 - jacard_coef: 0.0760 5/17 [=======>......................] - ETA: 1s - loss: 0.1280 - accuracy: 0.9128 - jacard_coef: 0.0765 6/17 [=========>....................] - ETA: 1s - loss: 0.1278 - accuracy: 0.9141 - jacard_coef: 0.0753 7/17 [===========>..................] - ETA: 1s - loss: 0.1276 - accuracy: 0.9152 - jacard_coef: 0.0744 8/17 [=============>................] - ETA: 1s - loss: 0.1274 - accuracy: 0.9157 - jacard_coef: 0.0741 9/17 [==============>...............] - ETA: 1s - loss: 0.1271 - accuracy: 0.9186 - jacard_coef: 0.071810/17 [================>.............] - ETA: 0s - loss: 0.1273 - accuracy: 0.9167 - jacard_coef: 0.073511/17 [==================>...........] - ETA: 0s - loss: 0.1272 - accuracy: 0.9174 - jacard_coef: 0.073112/17 [====================>.........] - ETA: 0s - loss: 0.1273 - accuracy: 0.9165 - jacard_coef: 0.073913/17 [=====================>........] - ETA: 0s - loss: 0.1279 - accuracy: 0.9158 - jacard_coef: 0.074614/17 [=======================>......] - ETA: 0s - loss: 0.1280 - accuracy: 0.9147 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1280 - accuracy: 0.9142 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1276 - accuracy: 0.9164 - jacard_coef: 0.074217/17 [==============================] - 2s 138ms/step - loss: 0.1277 - accuracy: 0.9158 - jacard_coef: 0.0776 - val_loss: 0.1281 - val_accuracy: 0.9304 - val_jacard_coef: 0.0624 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0655 (epoch 9)
  Final Val Loss: 0.1281
  Training Time: 0:01:38.518290
  Stability (std): 0.2670

Results saved to: hyperparameter_optimization_20250926_123742/exp_10_UNet_lr5e-3_bs8/UNet_lr0.005_bs8_results.json

Experiment 10 completed in 133s
Progress: 10/36 completed
Estimated remaining time: 57 minutes

ðŸ”¬ EXPERIMENT 11/36
================================================
Architecture: UNet
Learning Rate: 5e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758862909.510380 3217662 service.cc:145] XLA service 0x153e32667150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758862909.510455 3217662 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758862909.976163 3217662 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 5:59 - loss: 0.3368 - accuracy: 0.4668 - jacard_coef: 0.06632/9 [=====>........................] - ETA: 51s - loss: 0.2890 - accuracy: 0.3544 - jacard_coef: 0.0688 3/9 [=========>....................] - ETA: 27s - loss: 0.2617 - accuracy: 0.3259 - jacard_coef: 0.07474/9 [============>.................] - ETA: 17s - loss: 0.2461 - accuracy: 0.3242 - jacard_coef: 0.07465/9 [===============>..............] - ETA: 10s - loss: 0.2354 - accuracy: 0.3056 - jacard_coef: 0.07466/9 [===================>..........] - ETA: 6s - loss: 0.2285 - accuracy: 0.2824 - jacard_coef: 0.0712 7/9 [======================>.......] - ETA: 3s - loss: 0.2228 - accuracy: 0.2709 - jacard_coef: 0.07548/9 [=========================>....] - ETA: 1s - loss: 0.2188 - accuracy: 0.2578 - jacard_coef: 0.07669/9 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.2565 - jacard_coef: 0.07019/9 [==============================] - 64s 2s/step - loss: 0.2186 - accuracy: 0.2565 - jacard_coef: 0.0701 - val_loss: 0.2286 - val_accuracy: 0.9253 - val_jacard_coef: 0.0417 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1917 - accuracy: 0.1269 - jacard_coef: 0.06022/9 [=====>........................] - ETA: 1s - loss: 0.1918 - accuracy: 0.1299 - jacard_coef: 0.06753/9 [=========>....................] - ETA: 1s - loss: 0.1907 - accuracy: 0.1290 - jacard_coef: 0.07154/9 [============>.................] - ETA: 1s - loss: 0.1904 - accuracy: 0.1304 - jacard_coef: 0.07555/9 [===============>..............] - ETA: 0s - loss: 0.1898 - accuracy: 0.1249 - jacard_coef: 0.07246/9 [===================>..........] - ETA: 0s - loss: 0.1895 - accuracy: 0.1237 - jacard_coef: 0.07347/9 [======================>.......] - ETA: 0s - loss: 0.1891 - accuracy: 0.1251 - jacard_coef: 0.07608/9 [=========================>....] - ETA: 0s - loss: 0.1887 - accuracy: 0.1259 - jacard_coef: 0.07649/9 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.1261 - jacard_coef: 0.07559/9 [==============================] - 2s 241ms/step - loss: 0.1886 - accuracy: 0.1261 - jacard_coef: 0.0755 - val_loss: 1.1040 - val_accuracy: 0.9304 - val_jacard_coef: 0.0010 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1826 - accuracy: 0.1590 - jacard_coef: 0.08022/9 [=====>........................] - ETA: 1s - loss: 0.1827 - accuracy: 0.1561 - jacard_coef: 0.07843/9 [=========>....................] - ETA: 1s - loss: 0.1834 - accuracy: 0.1480 - jacard_coef: 0.06834/9 [============>.................] - ETA: 1s - loss: 0.1826 - accuracy: 0.1678 - jacard_coef: 0.06875/9 [===============>..............] - ETA: 0s - loss: 0.1822 - accuracy: 0.1768 - jacard_coef: 0.07176/9 [===================>..........] - ETA: 0s - loss: 0.1816 - accuracy: 0.2172 - jacard_coef: 0.07677/9 [======================>.......] - ETA: 0s - loss: 0.1818 - accuracy: 0.2316 - jacard_coef: 0.07338/9 [=========================>....] - ETA: 0s - loss: 0.1814 - accuracy: 0.2369 - jacard_coef: 0.07529/9 [==============================] - 2s 234ms/step - loss: 0.1813 - accuracy: 0.2370 - jacard_coef: 0.0853 - val_loss: 0.3054 - val_accuracy: 0.9298 - val_jacard_coef: 0.0287 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1782 - accuracy: 0.2185 - jacard_coef: 0.07912/9 [=====>........................] - ETA: 1s - loss: 0.1780 - accuracy: 0.2514 - jacard_coef: 0.09113/9 [=========>....................] - ETA: 1s - loss: 0.1780 - accuracy: 0.2654 - jacard_coef: 0.08484/9 [============>.................] - ETA: 1s - loss: 0.1778 - accuracy: 0.2812 - jacard_coef: 0.08135/9 [===============>..............] - ETA: 0s - loss: 0.1774 - accuracy: 0.2984 - jacard_coef: 0.08006/9 [===================>..........] - ETA: 0s - loss: 0.1770 - accuracy: 0.3218 - jacard_coef: 0.07567/9 [======================>.......] - ETA: 0s - loss: 0.1764 - accuracy: 0.3485 - jacard_coef: 0.07758/9 [=========================>....] - ETA: 0s - loss: 0.1756 - accuracy: 0.3786 - jacard_coef: 0.07669/9 [==============================] - 2s 242ms/step - loss: 0.1760 - accuracy: 0.3779 - jacard_coef: 0.0690 - val_loss: 0.1660 - val_accuracy: 0.8410 - val_jacard_coef: 0.0635 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1780 - accuracy: 0.4613 - jacard_coef: 0.08402/9 [=====>........................] - ETA: 1s - loss: 0.1794 - accuracy: 0.3843 - jacard_coef: 0.07433/9 [=========>....................] - ETA: 1s - loss: 0.1803 - accuracy: 0.3212 - jacard_coef: 0.07064/9 [============>.................] - ETA: 1s - loss: 0.1805 - accuracy: 0.2869 - jacard_coef: 0.07165/9 [===============>..............] - ETA: 0s - loss: 0.1798 - accuracy: 0.2719 - jacard_coef: 0.07616/9 [===================>..........] - ETA: 0s - loss: 0.1797 - accuracy: 0.2644 - jacard_coef: 0.07627/9 [======================>.......] - ETA: 0s - loss: 0.1790 - accuracy: 0.2825 - jacard_coef: 0.07778/9 [=========================>....] - ETA: 0s - loss: 0.1783 - accuracy: 0.3163 - jacard_coef: 0.07569/9 [==============================] - 2s 234ms/step - loss: 0.1783 - accuracy: 0.3177 - jacard_coef: 0.0814 - val_loss: 1.1383 - val_accuracy: 0.9267 - val_jacard_coef: 0.0012 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1703 - accuracy: 0.6534 - jacard_coef: 0.07672/9 [=====>........................] - ETA: 1s - loss: 0.1708 - accuracy: 0.7155 - jacard_coef: 0.06893/9 [=========>....................] - ETA: 1s - loss: 0.1700 - accuracy: 0.7467 - jacard_coef: 0.07404/9 [============>.................] - ETA: 1s - loss: 0.1698 - accuracy: 0.7616 - jacard_coef: 0.07595/9 [===============>..............] - ETA: 0s - loss: 0.1697 - accuracy: 0.7631 - jacard_coef: 0.08096/9 [===================>..........] - ETA: 0s - loss: 0.1691 - accuracy: 0.7663 - jacard_coef: 0.07917/9 [======================>.......] - ETA: 0s - loss: 0.1689 - accuracy: 0.7780 - jacard_coef: 0.07688/9 [=========================>....] - ETA: 0s - loss: 0.1685 - accuracy: 0.7868 - jacard_coef: 0.07519/9 [==============================] - 2s 234ms/step - loss: 0.1691 - accuracy: 0.7842 - jacard_coef: 0.0848 - val_loss: 1.1436 - val_accuracy: 0.9267 - val_jacard_coef: 0.0011 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1668 - accuracy: 0.6402 - jacard_coef: 0.06322/9 [=====>........................] - ETA: 1s - loss: 0.1675 - accuracy: 0.6069 - jacard_coef: 0.06933/9 [=========>....................] - ETA: 1s - loss: 0.1692 - accuracy: 0.5451 - jacard_coef: 0.06584/9 [============>.................] - ETA: 1s - loss: 0.1717 - accuracy: 0.5119 - jacard_coef: 0.07475/9 [===============>..............] - ETA: 0s - loss: 0.1728 - accuracy: 0.4923 - jacard_coef: 0.07536/9 [===================>..........] - ETA: 0s - loss: 0.1720 - accuracy: 0.5076 - jacard_coef: 0.07787/9 [======================>.......] - ETA: 0s - loss: 0.1716 - accuracy: 0.5403 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1710 - accuracy: 0.5772 - jacard_coef: 0.07629/9 [==============================] - 2s 234ms/step - loss: 0.1709 - accuracy: 0.5794 - jacard_coef: 0.0679 - val_loss: 1.1699 - val_accuracy: 0.9272 - val_jacard_coef: 0.0047 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1678 - accuracy: 0.7622 - jacard_coef: 0.09242/9 [=====>........................] - ETA: 1s - loss: 0.1671 - accuracy: 0.7547 - jacard_coef: 0.09023/9 [=========>....................] - ETA: 1s - loss: 0.1670 - accuracy: 0.7548 - jacard_coef: 0.08414/9 [============>.................] - ETA: 1s - loss: 0.1663 - accuracy: 0.7597 - jacard_coef: 0.08055/9 [===============>..............] - ETA: 0s - loss: 0.1659 - accuracy: 0.7690 - jacard_coef: 0.07776/9 [===================>..........] - ETA: 0s - loss: 0.1657 - accuracy: 0.7799 - jacard_coef: 0.07757/9 [======================>.......] - ETA: 0s - loss: 0.1653 - accuracy: 0.7927 - jacard_coef: 0.07488/9 [=========================>....] - ETA: 0s - loss: 0.1654 - accuracy: 0.7991 - jacard_coef: 0.07529/9 [==============================] - 2s 234ms/step - loss: 0.1654 - accuracy: 0.7986 - jacard_coef: 0.0836 - val_loss: 1.2124 - val_accuracy: 0.9141 - val_jacard_coef: 0.0152 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1633 - accuracy: 0.8708 - jacard_coef: 0.07732/9 [=====>........................] - ETA: 1s - loss: 0.1630 - accuracy: 0.8737 - jacard_coef: 0.07143/9 [=========>....................] - ETA: 1s - loss: 0.1626 - accuracy: 0.8637 - jacard_coef: 0.07344/9 [============>.................] - ETA: 1s - loss: 0.1626 - accuracy: 0.8537 - jacard_coef: 0.07415/9 [===============>..............] - ETA: 0s - loss: 0.1625 - accuracy: 0.8516 - jacard_coef: 0.07616/9 [===================>..........] - ETA: 0s - loss: 0.1623 - accuracy: 0.8555 - jacard_coef: 0.07567/9 [======================>.......] - ETA: 0s - loss: 0.1620 - accuracy: 0.8572 - jacard_coef: 0.07738/9 [=========================>....] - ETA: 0s - loss: 0.1617 - accuracy: 0.8619 - jacard_coef: 0.07549/9 [==============================] - 2s 235ms/step - loss: 0.1617 - accuracy: 0.8604 - jacard_coef: 0.0818 - val_loss: 0.5823 - val_accuracy: 0.8943 - val_jacard_coef: 0.0503 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1594 - accuracy: 0.8062 - jacard_coef: 0.07972/9 [=====>........................] - ETA: 1s - loss: 0.1593 - accuracy: 0.8476 - jacard_coef: 0.07873/9 [=========>....................] - ETA: 1s - loss: 0.1590 - accuracy: 0.8609 - jacard_coef: 0.07514/9 [============>.................] - ETA: 1s - loss: 0.1591 - accuracy: 0.8708 - jacard_coef: 0.07575/9 [===============>..............] - ETA: 0s - loss: 0.1589 - accuracy: 0.8775 - jacard_coef: 0.07546/9 [===================>..........] - ETA: 0s - loss: 0.1590 - accuracy: 0.8836 - jacard_coef: 0.07407/9 [======================>.......] - ETA: 0s - loss: 0.1590 - accuracy: 0.8840 - jacard_coef: 0.07628/9 [=========================>....] - ETA: 0s - loss: 0.1589 - accuracy: 0.8850 - jacard_coef: 0.07619/9 [==============================] - 2s 234ms/step - loss: 0.1589 - accuracy: 0.8852 - jacard_coef: 0.0735 - val_loss: 0.2560 - val_accuracy: 0.9046 - val_jacard_coef: 0.0513 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1583 - accuracy: 0.8858 - jacard_coef: 0.07632/9 [=====>........................] - ETA: 1s - loss: 0.1577 - accuracy: 0.8674 - jacard_coef: 0.07803/9 [=========>....................] - ETA: 1s - loss: 0.1575 - accuracy: 0.8471 - jacard_coef: 0.07474/9 [============>.................] - ETA: 1s - loss: 0.1576 - accuracy: 0.8554 - jacard_coef: 0.07595/9 [===============>..............] - ETA: 0s - loss: 0.1574 - accuracy: 0.8636 - jacard_coef: 0.07476/9 [===================>..........] - ETA: 0s - loss: 0.1573 - accuracy: 0.8688 - jacard_coef: 0.07467/9 [======================>.......] - ETA: 0s - loss: 0.1572 - accuracy: 0.8712 - jacard_coef: 0.07468/9 [=========================>....] - ETA: 0s - loss: 0.1573 - accuracy: 0.8727 - jacard_coef: 0.07589/9 [==============================] - 2s 235ms/step - loss: 0.1578 - accuracy: 0.8692 - jacard_coef: 0.0791 - val_loss: 0.1636 - val_accuracy: 0.8559 - val_jacard_coef: 0.0541 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1567 - accuracy: 0.8718 - jacard_coef: 0.06962/9 [=====>........................] - ETA: 1s - loss: 0.1586 - accuracy: 0.8229 - jacard_coef: 0.07753/9 [=========>....................] - ETA: 1s - loss: 0.1585 - accuracy: 0.8093 - jacard_coef: 0.07864/9 [============>.................] - ETA: 1s - loss: 0.1586 - accuracy: 0.7988 - jacard_coef: 0.07445/9 [===============>..............] - ETA: 0s - loss: 0.1623 - accuracy: 0.7537 - jacard_coef: 0.07226/9 [===================>..........] - ETA: 0s - loss: 0.1617 - accuracy: 0.7585 - jacard_coef: 0.07297/9 [======================>.......] - ETA: 0s - loss: 0.1612 - accuracy: 0.7639 - jacard_coef: 0.07488/9 [=========================>....] - ETA: 0s - loss: 0.1610 - accuracy: 0.7718 - jacard_coef: 0.07539/9 [==============================] - 2s 241ms/step - loss: 0.1610 - accuracy: 0.7719 - jacard_coef: 0.0812 - val_loss: 0.1742 - val_accuracy: 0.5444 - val_jacard_coef: 0.0643 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1580 - accuracy: 0.8750 - jacard_coef: 0.08012/9 [=====>........................] - ETA: 1s - loss: 0.1577 - accuracy: 0.8895 - jacard_coef: 0.07253/9 [=========>....................] - ETA: 1s - loss: 0.1580 - accuracy: 0.8867 - jacard_coef: 0.07584/9 [============>.................] - ETA: 1s - loss: 0.1576 - accuracy: 0.8899 - jacard_coef: 0.07185/9 [===============>..............] - ETA: 0s - loss: 0.1578 - accuracy: 0.8844 - jacard_coef: 0.07256/9 [===================>..........] - ETA: 0s - loss: 0.1578 - accuracy: 0.8787 - jacard_coef: 0.07517/9 [======================>.......] - ETA: 0s - loss: 0.1578 - accuracy: 0.8729 - jacard_coef: 0.07698/9 [=========================>....] - ETA: 0s - loss: 0.1577 - accuracy: 0.8712 - jacard_coef: 0.07639/9 [==============================] - 2s 234ms/step - loss: 0.1579 - accuracy: 0.8687 - jacard_coef: 0.0685 - val_loss: 0.1589 - val_accuracy: 0.9241 - val_jacard_coef: 0.0640 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1557 - accuracy: 0.8569 - jacard_coef: 0.05232/9 [=====>........................] - ETA: 1s - loss: 0.1563 - accuracy: 0.8550 - jacard_coef: 0.06763/9 [=========>....................] - ETA: 1s - loss: 0.1567 - accuracy: 0.8516 - jacard_coef: 0.06854/9 [============>.................] - ETA: 1s - loss: 0.1572 - accuracy: 0.8498 - jacard_coef: 0.07055/9 [===============>..............] - ETA: 0s - loss: 0.1573 - accuracy: 0.8478 - jacard_coef: 0.07446/9 [===================>..........] - ETA: 0s - loss: 0.1574 - accuracy: 0.8452 - jacard_coef: 0.07377/9 [======================>.......] - ETA: 0s - loss: 0.1574 - accuracy: 0.8452 - jacard_coef: 0.07608/9 [=========================>....] - ETA: 0s - loss: 0.1573 - accuracy: 0.8490 - jacard_coef: 0.07609/9 [==============================] - 2s 234ms/step - loss: 0.1573 - accuracy: 0.8490 - jacard_coef: 0.0744 - val_loss: 0.1611 - val_accuracy: 0.9134 - val_jacard_coef: 0.0642 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1557 - accuracy: 0.9148 - jacard_coef: 0.05512/9 [=====>........................] - ETA: 1s - loss: 0.1562 - accuracy: 0.8921 - jacard_coef: 0.07953/9 [=========>....................] - ETA: 1s - loss: 0.1568 - accuracy: 0.8868 - jacard_coef: 0.08454/9 [============>.................] - ETA: 1s - loss: 0.1565 - accuracy: 0.8922 - jacard_coef: 0.07995/9 [===============>..............] - ETA: 0s - loss: 0.1562 - accuracy: 0.8938 - jacard_coef: 0.07836/9 [===================>..........] - ETA: 0s - loss: 0.1559 - accuracy: 0.8961 - jacard_coef: 0.07637/9 [======================>.......] - ETA: 0s - loss: 0.1559 - accuracy: 0.8959 - jacard_coef: 0.07628/9 [=========================>....] - ETA: 0s - loss: 0.1556 - accuracy: 0.8971 - jacard_coef: 0.07609/9 [==============================] - 2s 242ms/step - loss: 0.1557 - accuracy: 0.8970 - jacard_coef: 0.0679 - val_loss: 0.1618 - val_accuracy: 0.9259 - val_jacard_coef: 0.0643 - lr: 5.0000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1547 - accuracy: 0.9211 - jacard_coef: 0.06542/9 [=====>........................] - ETA: 1s - loss: 0.1539 - accuracy: 0.9261 - jacard_coef: 0.06333/9 [=========>....................] - ETA: 1s - loss: 0.1539 - accuracy: 0.9201 - jacard_coef: 0.06944/9 [============>.................] - ETA: 1s - loss: 0.1538 - accuracy: 0.9196 - jacard_coef: 0.07045/9 [===============>..............] - ETA: 0s - loss: 0.1540 - accuracy: 0.9155 - jacard_coef: 0.07396/9 [===================>..........] - ETA: 0s - loss: 0.1539 - accuracy: 0.9144 - jacard_coef: 0.07527/9 [======================>.......] - ETA: 0s - loss: 0.1543 - accuracy: 0.9097 - jacard_coef: 0.07938/9 [=========================>....] - ETA: 0s - loss: 0.1542 - accuracy: 0.9147 - jacard_coef: 0.07529/9 [==============================] - 2s 240ms/step - loss: 0.1543 - accuracy: 0.9118 - jacard_coef: 0.0784 - val_loss: 0.1624 - val_accuracy: 0.9263 - val_jacard_coef: 0.0645 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1518 - accuracy: 0.9336 - jacard_coef: 0.06122/9 [=====>........................] - ETA: 1s - loss: 0.1526 - accuracy: 0.9264 - jacard_coef: 0.06633/9 [=========>....................] - ETA: 1s - loss: 0.1526 - accuracy: 0.9177 - jacard_coef: 0.07374/9 [============>.................] - ETA: 1s - loss: 0.1526 - accuracy: 0.9195 - jacard_coef: 0.07215/9 [===============>..............] - ETA: 0s - loss: 0.1528 - accuracy: 0.9182 - jacard_coef: 0.07256/9 [===================>..........] - ETA: 0s - loss: 0.1528 - accuracy: 0.9177 - jacard_coef: 0.07257/9 [======================>.......] - ETA: 0s - loss: 0.1529 - accuracy: 0.9161 - jacard_coef: 0.07388/9 [=========================>....] - ETA: 0s - loss: 0.1531 - accuracy: 0.9140 - jacard_coef: 0.07559/9 [==============================] - 2s 234ms/step - loss: 0.1531 - accuracy: 0.9134 - jacard_coef: 0.0785 - val_loss: 0.1600 - val_accuracy: 0.9261 - val_jacard_coef: 0.0643 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1533 - accuracy: 0.9249 - jacard_coef: 0.06562/9 [=====>........................] - ETA: 1s - loss: 0.1529 - accuracy: 0.9154 - jacard_coef: 0.07503/9 [=========>....................] - ETA: 1s - loss: 0.1523 - accuracy: 0.9135 - jacard_coef: 0.07724/9 [============>.................] - ETA: 1s - loss: 0.1523 - accuracy: 0.9147 - jacard_coef: 0.07655/9 [===============>..............] - ETA: 0s - loss: 0.1522 - accuracy: 0.9144 - jacard_coef: 0.07686/9 [===================>..........] - ETA: 0s - loss: 0.1521 - accuracy: 0.9150 - jacard_coef: 0.07617/9 [======================>.......] - ETA: 0s - loss: 0.1520 - accuracy: 0.9156 - jacard_coef: 0.07578/9 [=========================>....] - ETA: 0s - loss: 0.1520 - accuracy: 0.9150 - jacard_coef: 0.07529/9 [==============================] - 2s 234ms/step - loss: 0.1521 - accuracy: 0.9143 - jacard_coef: 0.0824 - val_loss: 0.1523 - val_accuracy: 0.9271 - val_jacard_coef: 0.0640 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1520 - accuracy: 0.9014 - jacard_coef: 0.08742/9 [=====>........................] - ETA: 1s - loss: 0.1520 - accuracy: 0.9113 - jacard_coef: 0.07963/9 [=========>....................] - ETA: 1s - loss: 0.1517 - accuracy: 0.9114 - jacard_coef: 0.07994/9 [============>.................] - ETA: 1s - loss: 0.1511 - accuracy: 0.9125 - jacard_coef: 0.07915/9 [===============>..............] - ETA: 0s - loss: 0.1510 - accuracy: 0.9129 - jacard_coef: 0.07876/9 [===================>..........] - ETA: 0s - loss: 0.1509 - accuracy: 0.9127 - jacard_coef: 0.07887/9 [======================>.......] - ETA: 0s - loss: 0.1506 - accuracy: 0.9156 - jacard_coef: 0.07618/9 [=========================>....] - ETA: 0s - loss: 0.1504 - accuracy: 0.9161 - jacard_coef: 0.07559/9 [==============================] - 2s 235ms/step - loss: 0.1506 - accuracy: 0.9132 - jacard_coef: 0.0775 - val_loss: 0.1566 - val_accuracy: 0.9201 - val_jacard_coef: 0.0645 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1501 - accuracy: 0.9058 - jacard_coef: 0.08212/9 [=====>........................] - ETA: 1s - loss: 0.1492 - accuracy: 0.9192 - jacard_coef: 0.06963/9 [=========>....................] - ETA: 1s - loss: 0.1498 - accuracy: 0.9130 - jacard_coef: 0.07354/9 [============>.................] - ETA: 1s - loss: 0.1494 - accuracy: 0.9143 - jacard_coef: 0.07095/9 [===============>..............] - ETA: 0s - loss: 0.1497 - accuracy: 0.9109 - jacard_coef: 0.07266/9 [===================>..........] - ETA: 0s - loss: 0.1497 - accuracy: 0.9078 - jacard_coef: 0.07567/9 [======================>.......] - ETA: 0s - loss: 0.1495 - accuracy: 0.9107 - jacard_coef: 0.07328/9 [=========================>....] - ETA: 0s - loss: 0.1497 - accuracy: 0.9086 - jacard_coef: 0.07509/9 [==============================] - 2s 235ms/step - loss: 0.1498 - accuracy: 0.9072 - jacard_coef: 0.0796 - val_loss: 0.1569 - val_accuracy: 0.9150 - val_jacard_coef: 0.0642 - lr: 5.0000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1534 - accuracy: 0.8775 - jacard_coef: 0.10302/9 [=====>........................] - ETA: 1s - loss: 0.1505 - accuracy: 0.9021 - jacard_coef: 0.08463/9 [=========>....................] - ETA: 1s - loss: 0.1513 - accuracy: 0.9027 - jacard_coef: 0.08394/9 [============>.................] - ETA: 1s - loss: 0.1508 - accuracy: 0.9065 - jacard_coef: 0.08135/9 [===============>..............] - ETA: 0s - loss: 0.1500 - accuracy: 0.9137 - jacard_coef: 0.07566/9 [===================>..........] - ETA: 0s - loss: 0.1497 - accuracy: 0.9157 - jacard_coef: 0.07427/9 [======================>.......] - ETA: 0s - loss: 0.1500 - accuracy: 0.9106 - jacard_coef: 0.07858/9 [=========================>....] - ETA: 0s - loss: 0.1497 - accuracy: 0.9145 - jacard_coef: 0.07539/9 [==============================] - 2s 235ms/step - loss: 0.1497 - accuracy: 0.9146 - jacard_coef: 0.0737 - val_loss: 0.1566 - val_accuracy: 0.9216 - val_jacard_coef: 0.0645 - lr: 5.0000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1497 - accuracy: 0.9001 - jacard_coef: 0.08892/9 [=====>........................] - ETA: 1s - loss: 0.1484 - accuracy: 0.9215 - jacard_coef: 0.07093/9 [=========>....................] - ETA: 1s - loss: 0.1488 - accuracy: 0.9112 - jacard_coef: 0.07974/9 [============>.................] - ETA: 1s - loss: 0.1483 - accuracy: 0.9148 - jacard_coef: 0.07685/9 [===============>..............] - ETA: 0s - loss: 0.1482 - accuracy: 0.9195 - jacard_coef: 0.07296/9 [===================>..........] - ETA: 0s - loss: 0.1484 - accuracy: 0.9152 - jacard_coef: 0.07617/9 [======================>.......] - ETA: 0s - loss: 0.1484 - accuracy: 0.9146 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1483 - accuracy: 0.9159 - jacard_coef: 0.07589/9 [==============================] - 2s 235ms/step - loss: 0.1483 - accuracy: 0.9163 - jacard_coef: 0.0705 - val_loss: 0.1526 - val_accuracy: 0.9277 - val_jacard_coef: 0.0644 - lr: 2.5000e-04
Epoch 23/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1475 - accuracy: 0.9164 - jacard_coef: 0.07632/9 [=====>........................] - ETA: 1s - loss: 0.1482 - accuracy: 0.8979 - jacard_coef: 0.09133/9 [=========>....................] - ETA: 1s - loss: 0.1474 - accuracy: 0.9113 - jacard_coef: 0.08024/9 [============>.................] - ETA: 1s - loss: 0.1474 - accuracy: 0.9172 - jacard_coef: 0.07545/9 [===============>..............] - ETA: 0s - loss: 0.1469 - accuracy: 0.9214 - jacard_coef: 0.07196/9 [===================>..........] - ETA: 0s - loss: 0.1468 - accuracy: 0.9210 - jacard_coef: 0.07237/9 [======================>.......] - ETA: 0s - loss: 0.1468 - accuracy: 0.9206 - jacard_coef: 0.07278/9 [=========================>....] - ETA: 0s - loss: 0.1472 - accuracy: 0.9171 - jacard_coef: 0.07559/9 [==============================] - 2s 235ms/step - loss: 0.1472 - accuracy: 0.9173 - jacard_coef: 0.0730 - val_loss: 0.1509 - val_accuracy: 0.9274 - val_jacard_coef: 0.0644 - lr: 2.5000e-04
Epoch 24/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1470 - accuracy: 0.9227 - jacard_coef: 0.07132/9 [=====>........................] - ETA: 1s - loss: 0.1466 - accuracy: 0.9237 - jacard_coef: 0.06973/9 [=========>....................] - ETA: 1s - loss: 0.1466 - accuracy: 0.9239 - jacard_coef: 0.06964/9 [============>.................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9241 - jacard_coef: 0.06965/9 [===============>..............] - ETA: 0s - loss: 0.1465 - accuracy: 0.9205 - jacard_coef: 0.07266/9 [===================>..........] - ETA: 0s - loss: 0.1466 - accuracy: 0.9192 - jacard_coef: 0.07377/9 [======================>.......] - ETA: 0s - loss: 0.1467 - accuracy: 0.9165 - jacard_coef: 0.07598/9 [=========================>....] - ETA: 0s - loss: 0.1468 - accuracy: 0.9169 - jacard_coef: 0.07519/9 [==============================] - 2s 235ms/step - loss: 0.1468 - accuracy: 0.9162 - jacard_coef: 0.0818 - val_loss: 0.1503 - val_accuracy: 0.9246 - val_jacard_coef: 0.0644 - lr: 2.5000e-04
Epoch 25/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1466 - accuracy: 0.9122 - jacard_coef: 0.08002/9 [=====>........................] - ETA: 1s - loss: 0.1466 - accuracy: 0.9137 - jacard_coef: 0.07853/9 [=========>....................] - ETA: 1s - loss: 0.1468 - accuracy: 0.9061 - jacard_coef: 0.08464/9 [============>.................] - ETA: 1s - loss: 0.1460 - accuracy: 0.9156 - jacard_coef: 0.07655/9 [===============>..............] - ETA: 0s - loss: 0.1461 - accuracy: 0.9160 - jacard_coef: 0.07636/9 [===================>..........] - ETA: 0s - loss: 0.1461 - accuracy: 0.9158 - jacard_coef: 0.07657/9 [======================>.......] - ETA: 0s - loss: 0.1459 - accuracy: 0.9180 - jacard_coef: 0.07478/9 [=========================>....] - ETA: 0s - loss: 0.1460 - accuracy: 0.9163 - jacard_coef: 0.07539/9 [==============================] - 2s 234ms/step - loss: 0.1461 - accuracy: 0.9162 - jacard_coef: 0.0772 - val_loss: 0.1494 - val_accuracy: 0.9248 - val_jacard_coef: 0.0644 - lr: 2.5000e-04
Epoch 26/30
1/9 [==>...........................] - ETA: 1s - loss: 0.1449 - accuracy: 0.9286 - jacard_coef: 0.06532/9 [=====>........................] - ETA: 1s - loss: 0.1451 - accuracy: 0.9254 - jacard_coef: 0.06613/9 [=========>....................] - ETA: 1s - loss: 0.1455 - accuracy: 0.9154 - jacard_coef: 0.07414/9 [============>.................] - ETA: 1s - loss: 0.1454 - accuracy: 0.9181 - jacard_coef: 0.07195/9 [===============>..............] - ETA: 0s - loss: 0.1452 - accuracy: 0.9174 - jacard_coef: 0.07256/9 [===================>..........] - ETA: 0s - loss: 0.1454 - accuracy: 0.9146 - jacard_coef: 0.07477/9 [======================>.......] - ETA: 0s - loss: 0.1456 - accuracy: 0.9135 - jacard_coef: 0.07538/9 [=========================>....] - ETA: 0s - loss: 0.1456 - accuracy: 0.9136 - jacard_coef: 0.07549/9 [==============================] - 2s 235ms/step - loss: 0.1458 - accuracy: 0.9106 - jacard_coef: 0.0779 - val_loss: 0.1479 - val_accuracy: 0.9201 - val_jacard_coef: 0.0645 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0645 (epoch 16)
  Final Val Loss: 0.1479
  Training Time: 0:01:57.666264
  Stability (std): 0.0037

Results saved to: hyperparameter_optimization_20250926_123742/exp_11_UNet_lr5e-3_bs16/UNet_lr0.005_bs16_results.json

Experiment 11 completed in 151s
Progress: 11/36 completed
Estimated remaining time: 62 minutes

ðŸ”¬ EXPERIMENT 12/36
================================================
Architecture: UNet
Learning Rate: 5e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: UNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
âœ“ focal_loss imported successfully
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['activation_20[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_22[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_24[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_26[0][0]']       
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['activation_28[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'activation_26[0][0]']       
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['activation_30[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'activation_24[0][0]']       
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['activation_32[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'activation_22[0][0]']       
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['activation_34[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'activation_20[0][0]']       
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['activation_36[0][0]']       
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758863063.709671 3222062 service.cc:145] XLA service 0x14da59cf8ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758863063.709709 3222062 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758863064.098163 3222062 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 3:30 - loss: 0.3446 - accuracy: 0.5034 - jacard_coef: 0.07472/5 [===========>..................] - ETA: 36s - loss: 0.3205 - accuracy: 0.4767 - jacard_coef: 0.0735 3/5 [=================>............] - ETA: 13s - loss: 0.2937 - accuracy: 0.3914 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 5s - loss: 0.2731 - accuracy: 0.3349 - jacard_coef: 0.0788 5/5 [==============================] - ETA: 0s - loss: 0.2728 - accuracy: 0.3335 - jacard_coef: 0.06935/5 [==============================] - 74s 5s/step - loss: 0.2728 - accuracy: 0.3335 - jacard_coef: 0.0693 - val_loss: 0.2068 - val_accuracy: 0.9304 - val_jacard_coef: 0.0316 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.2087 - accuracy: 0.1560 - jacard_coef: 0.08562/5 [===========>..................] - ETA: 1s - loss: 0.2071 - accuracy: 0.1531 - jacard_coef: 0.08883/5 [=================>............] - ETA: 0s - loss: 0.2060 - accuracy: 0.1452 - jacard_coef: 0.08354/5 [=======================>......] - ETA: 0s - loss: 0.2050 - accuracy: 0.1386 - jacard_coef: 0.07725/5 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.1389 - jacard_coef: 0.08145/5 [==============================] - 2s 433ms/step - loss: 0.2049 - accuracy: 0.1389 - jacard_coef: 0.0814 - val_loss: 0.8806 - val_accuracy: 0.9304 - val_jacard_coef: 0.0038 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1922 - accuracy: 0.2252 - jacard_coef: 0.06852/5 [===========>..................] - ETA: 1s - loss: 0.1928 - accuracy: 0.2616 - jacard_coef: 0.06693/5 [=================>............] - ETA: 0s - loss: 0.1908 - accuracy: 0.2811 - jacard_coef: 0.07414/5 [=======================>......] - ETA: 0s - loss: 0.1898 - accuracy: 0.2866 - jacard_coef: 0.07615/5 [==============================] - 2s 396ms/step - loss: 0.1900 - accuracy: 0.2869 - jacard_coef: 0.0895 - val_loss: 1.0855 - val_accuracy: 0.9304 - val_jacard_coef: 6.3384e-04 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1866 - accuracy: 0.3268 - jacard_coef: 0.06762/5 [===========>..................] - ETA: 1s - loss: 0.1855 - accuracy: 0.3640 - jacard_coef: 0.07663/5 [=================>............] - ETA: 0s - loss: 0.1847 - accuracy: 0.3717 - jacard_coef: 0.07804/5 [=======================>......] - ETA: 0s - loss: 0.1832 - accuracy: 0.3767 - jacard_coef: 0.07585/5 [==============================] - 2s 396ms/step - loss: 0.1832 - accuracy: 0.3767 - jacard_coef: 0.0868 - val_loss: 1.1043 - val_accuracy: 0.9304 - val_jacard_coef: 1.2575e-04 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1777 - accuracy: 0.3486 - jacard_coef: 0.06962/5 [===========>..................] - ETA: 1s - loss: 0.1780 - accuracy: 0.3569 - jacard_coef: 0.07403/5 [=================>............] - ETA: 0s - loss: 0.1771 - accuracy: 0.3734 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1772 - accuracy: 0.3628 - jacard_coef: 0.07575/5 [==============================] - 2s 395ms/step - loss: 0.1782 - accuracy: 0.3619 - jacard_coef: 0.0872 - val_loss: 1.1214 - val_accuracy: 0.9304 - val_jacard_coef: 1.4773e-05 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1822 - accuracy: 0.3075 - jacard_coef: 0.07262/5 [===========>..................] - ETA: 1s - loss: 0.1826 - accuracy: 0.3495 - jacard_coef: 0.07403/5 [=================>............] - ETA: 0s - loss: 0.1829 - accuracy: 0.3556 - jacard_coef: 0.07544/5 [=======================>......] - ETA: 0s - loss: 0.1825 - accuracy: 0.3586 - jacard_coef: 0.07625/5 [==============================] - 2s 397ms/step - loss: 0.1826 - accuracy: 0.3583 - jacard_coef: 0.0860 - val_loss: 1.1185 - val_accuracy: 0.9303 - val_jacard_coef: 2.8589e-04 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1848 - accuracy: 0.3226 - jacard_coef: 0.07392/5 [===========>..................] - ETA: 1s - loss: 0.1834 - accuracy: 0.3302 - jacard_coef: 0.07773/5 [=================>............] - ETA: 0s - loss: 0.1834 - accuracy: 0.3262 - jacard_coef: 0.07644/5 [=======================>......] - ETA: 0s - loss: 0.1837 - accuracy: 0.3229 - jacard_coef: 0.07685/5 [==============================] - 2s 396ms/step - loss: 0.1837 - accuracy: 0.3221 - jacard_coef: 0.0689 - val_loss: 1.1171 - val_accuracy: 0.9303 - val_jacard_coef: 2.3646e-04 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1848 - accuracy: 0.3551 - jacard_coef: 0.07982/5 [===========>..................] - ETA: 1s - loss: 0.1833 - accuracy: 0.3577 - jacard_coef: 0.07603/5 [=================>............] - ETA: 0s - loss: 0.1832 - accuracy: 0.3593 - jacard_coef: 0.07674/5 [=======================>......] - ETA: 0s - loss: 0.1832 - accuracy: 0.3493 - jacard_coef: 0.07705/5 [==============================] - 2s 399ms/step - loss: 0.1833 - accuracy: 0.3480 - jacard_coef: 0.0673 - val_loss: 1.1057 - val_accuracy: 0.9303 - val_jacard_coef: 0.0010 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1830 - accuracy: 0.2593 - jacard_coef: 0.06292/5 [===========>..................] - ETA: 1s - loss: 0.1819 - accuracy: 0.2663 - jacard_coef: 0.06763/5 [=================>............] - ETA: 0s - loss: 0.1823 - accuracy: 0.2742 - jacard_coef: 0.07474/5 [=======================>......] - ETA: 0s - loss: 0.1818 - accuracy: 0.2799 - jacard_coef: 0.07665/5 [==============================] - 2s 399ms/step - loss: 0.1818 - accuracy: 0.2795 - jacard_coef: 0.0722 - val_loss: 1.0765 - val_accuracy: 0.9300 - val_jacard_coef: 0.0039 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1798 - accuracy: 0.3073 - jacard_coef: 0.08202/5 [===========>..................] - ETA: 1s - loss: 0.1803 - accuracy: 0.3016 - jacard_coef: 0.07433/5 [=================>............] - ETA: 0s - loss: 0.1801 - accuracy: 0.3057 - jacard_coef: 0.08094/5 [=======================>......] - ETA: 0s - loss: 0.1800 - accuracy: 0.3069 - jacard_coef: 0.07685/5 [==============================] - 2s 399ms/step - loss: 0.1802 - accuracy: 0.3060 - jacard_coef: 0.0621 - val_loss: 0.9924 - val_accuracy: 0.9293 - val_jacard_coef: 0.0095 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 1s - loss: 0.1781 - accuracy: 0.3306 - jacard_coef: 0.07172/5 [===========>..................] - ETA: 1s - loss: 0.1790 - accuracy: 0.3309 - jacard_coef: 0.07683/5 [=================>............] - ETA: 0s - loss: 0.1792 - accuracy: 0.3323 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 0s - loss: 0.1787 - accuracy: 0.3386 - jacard_coef: 0.07705/5 [==============================] - 2s 397ms/step - loss: 0.1787 - accuracy: 0.3375 - jacard_coef: 0.0629 - val_loss: 0.7688 - val_accuracy: 0.9268 - val_jacard_coef: 0.0194 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0316 (epoch 1)
  Final Val Loss: 0.7688
  Training Time: 0:01:36.370751
  Stability (std): 0.1148

Results saved to: hyperparameter_optimization_20250926_123742/exp_12_UNet_lr5e-3_bs32/UNet_lr0.005_bs32_results.json

Experiment 12 completed in 128s
Progress: 12/36 completed
Estimated remaining time: 51 minutes

ðŸ”¬ EXPERIMENT 13/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758863194.361724 3225831 service.cc:145] XLA service 0x1470a55775f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758863194.361802 3225831 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758863194.823508 3225831 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 14:54 - loss: 0.3470 - accuracy: 0.5151 - jacard_coef: 0.0653 2/17 [==>...........................] - ETA: 1:10 - loss: 0.3252 - accuracy: 0.4446 - jacard_coef: 0.0696  3/17 [====>.........................] - ETA: 34s - loss: 0.3026 - accuracy: 0.4035 - jacard_coef: 0.0749  4/17 [======>.......................] - ETA: 22s - loss: 0.2862 - accuracy: 0.3563 - jacard_coef: 0.0796 5/17 [=======>......................] - ETA: 15s - loss: 0.2733 - accuracy: 0.3254 - jacard_coef: 0.0831 6/17 [=========>....................] - ETA: 12s - loss: 0.2639 - accuracy: 0.3022 - jacard_coef: 0.0841 7/17 [===========>..................] - ETA: 9s - loss: 0.2544 - accuracy: 0.2939 - jacard_coef: 0.0786  8/17 [=============>................] - ETA: 7s - loss: 0.2471 - accuracy: 0.2735 - jacard_coef: 0.0762 9/17 [==============>...............] - ETA: 6s - loss: 0.2412 - accuracy: 0.2591 - jacard_coef: 0.077710/17 [================>.............] - ETA: 4s - loss: 0.2357 - accuracy: 0.2508 - jacard_coef: 0.081211/17 [==================>...........] - ETA: 3s - loss: 0.2313 - accuracy: 0.2469 - jacard_coef: 0.082612/17 [====================>.........] - ETA: 2s - loss: 0.2270 - accuracy: 0.2589 - jacard_coef: 0.082413/17 [=====================>........] - ETA: 2s - loss: 0.2258 - accuracy: 0.2576 - jacard_coef: 0.081914/17 [=======================>......] - ETA: 1s - loss: 0.2233 - accuracy: 0.2603 - jacard_coef: 0.081115/17 [=========================>....] - ETA: 1s - loss: 0.2224 - accuracy: 0.2557 - jacard_coef: 0.080216/17 [===========================>..] - ETA: 0s - loss: 0.2205 - accuracy: 0.2484 - jacard_coef: 0.076917/17 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.2475 - jacard_coef: 0.074617/17 [==============================] - 71s 934ms/step - loss: 0.2202 - accuracy: 0.2475 - jacard_coef: 0.0746 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4109e-05 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1865 - accuracy: 0.1618 - jacard_coef: 0.0830 2/17 [==>...........................] - ETA: 2s - loss: 0.1833 - accuracy: 0.3137 - jacard_coef: 0.0843 3/17 [====>.........................] - ETA: 2s - loss: 0.1822 - accuracy: 0.2880 - jacard_coef: 0.0790 4/17 [======>.......................] - ETA: 2s - loss: 0.1821 - accuracy: 0.3327 - jacard_coef: 0.0745 5/17 [=======>......................] - ETA: 2s - loss: 0.1822 - accuracy: 0.3064 - jacard_coef: 0.0746 6/17 [=========>....................] - ETA: 2s - loss: 0.1821 - accuracy: 0.2779 - jacard_coef: 0.0765 7/17 [===========>..................] - ETA: 1s - loss: 0.1824 - accuracy: 0.2618 - jacard_coef: 0.0745 8/17 [=============>................] - ETA: 1s - loss: 0.1831 - accuracy: 0.2579 - jacard_coef: 0.0763 9/17 [==============>...............] - ETA: 1s - loss: 0.1838 - accuracy: 0.2521 - jacard_coef: 0.073810/17 [================>.............] - ETA: 1s - loss: 0.1831 - accuracy: 0.2559 - jacard_coef: 0.073911/17 [==================>...........] - ETA: 1s - loss: 0.1824 - accuracy: 0.2659 - jacard_coef: 0.074612/17 [====================>.........] - ETA: 0s - loss: 0.1817 - accuracy: 0.2768 - jacard_coef: 0.075813/17 [=====================>........] - ETA: 0s - loss: 0.1812 - accuracy: 0.2922 - jacard_coef: 0.076614/17 [=======================>......] - ETA: 0s - loss: 0.1805 - accuracy: 0.3017 - jacard_coef: 0.074915/17 [=========================>....] - ETA: 0s - loss: 0.1799 - accuracy: 0.3284 - jacard_coef: 0.077216/17 [===========================>..] - ETA: 0s - loss: 0.1792 - accuracy: 0.3484 - jacard_coef: 0.076217/17 [==============================] - 3s 188ms/step - loss: 0.1795 - accuracy: 0.3493 - jacard_coef: 0.0751 - val_loss: 0.9571 - val_accuracy: 0.9243 - val_jacard_coef: 0.0134 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1722 - accuracy: 0.7860 - jacard_coef: 0.0671 2/17 [==>...........................] - ETA: 2s - loss: 0.1733 - accuracy: 0.7378 - jacard_coef: 0.0585 3/17 [====>.........................] - ETA: 2s - loss: 0.1742 - accuracy: 0.6618 - jacard_coef: 0.0739 4/17 [======>.......................] - ETA: 2s - loss: 0.1752 - accuracy: 0.6032 - jacard_coef: 0.0768 5/17 [=======>......................] - ETA: 2s - loss: 0.1746 - accuracy: 0.5700 - jacard_coef: 0.0753 6/17 [=========>....................] - ETA: 2s - loss: 0.1742 - accuracy: 0.5454 - jacard_coef: 0.0753 7/17 [===========>..................] - ETA: 1s - loss: 0.1745 - accuracy: 0.5268 - jacard_coef: 0.0724 8/17 [=============>................] - ETA: 1s - loss: 0.1740 - accuracy: 0.5202 - jacard_coef: 0.0726 9/17 [==============>...............] - ETA: 1s - loss: 0.1734 - accuracy: 0.5290 - jacard_coef: 0.070710/17 [================>.............] - ETA: 1s - loss: 0.1731 - accuracy: 0.5329 - jacard_coef: 0.071411/17 [==================>...........] - ETA: 1s - loss: 0.1730 - accuracy: 0.5394 - jacard_coef: 0.070712/17 [====================>.........] - ETA: 0s - loss: 0.1725 - accuracy: 0.5552 - jacard_coef: 0.067913/17 [=====================>........] - ETA: 0s - loss: 0.1722 - accuracy: 0.5669 - jacard_coef: 0.070314/17 [=======================>......] - ETA: 0s - loss: 0.1725 - accuracy: 0.5671 - jacard_coef: 0.072515/17 [=========================>....] - ETA: 0s - loss: 0.1722 - accuracy: 0.5780 - jacard_coef: 0.074516/17 [===========================>..] - ETA: 0s - loss: 0.1718 - accuracy: 0.5922 - jacard_coef: 0.075217/17 [==============================] - 3s 187ms/step - loss: 0.1726 - accuracy: 0.5907 - jacard_coef: 0.0782 - val_loss: 0.7319 - val_accuracy: 0.9302 - val_jacard_coef: 0.0182 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.2028 - accuracy: 0.3647 - jacard_coef: 0.0637 2/17 [==>...........................] - ETA: 2s - loss: 0.1998 - accuracy: 0.4322 - jacard_coef: 0.0746 3/17 [====>.........................] - ETA: 2s - loss: 0.1915 - accuracy: 0.4764 - jacard_coef: 0.0743 4/17 [======>.......................] - ETA: 2s - loss: 0.1870 - accuracy: 0.5022 - jacard_coef: 0.0756 5/17 [=======>......................] - ETA: 2s - loss: 0.1843 - accuracy: 0.4946 - jacard_coef: 0.0807 6/17 [=========>....................] - ETA: 2s - loss: 0.1819 - accuracy: 0.5114 - jacard_coef: 0.0813 7/17 [===========>..................] - ETA: 1s - loss: 0.1802 - accuracy: 0.5267 - jacard_coef: 0.0775 8/17 [=============>................] - ETA: 1s - loss: 0.1786 - accuracy: 0.5441 - jacard_coef: 0.0775 9/17 [==============>...............] - ETA: 1s - loss: 0.1776 - accuracy: 0.5546 - jacard_coef: 0.076210/17 [================>.............] - ETA: 1s - loss: 0.1767 - accuracy: 0.5691 - jacard_coef: 0.074511/17 [==================>...........] - ETA: 1s - loss: 0.1761 - accuracy: 0.5809 - jacard_coef: 0.074212/17 [====================>.........] - ETA: 0s - loss: 0.1753 - accuracy: 0.5912 - jacard_coef: 0.073513/17 [=====================>........] - ETA: 0s - loss: 0.1747 - accuracy: 0.6003 - jacard_coef: 0.075514/17 [=======================>......] - ETA: 0s - loss: 0.1741 - accuracy: 0.6130 - jacard_coef: 0.075215/17 [=========================>....] - ETA: 0s - loss: 0.1762 - accuracy: 0.5818 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.5911 - jacard_coef: 0.075617/17 [==============================] - 3s 183ms/step - loss: 0.1756 - accuracy: 0.5880 - jacard_coef: 0.0791 - val_loss: 1.1149 - val_accuracy: 0.9300 - val_jacard_coef: 5.6740e-04 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1675 - accuracy: 0.7640 - jacard_coef: 0.0825 2/17 [==>...........................] - ETA: 2s - loss: 0.1658 - accuracy: 0.7927 - jacard_coef: 0.0845 3/17 [====>.........................] - ETA: 2s - loss: 0.1666 - accuracy: 0.8104 - jacard_coef: 0.0708 4/17 [======>.......................] - ETA: 2s - loss: 0.1659 - accuracy: 0.8190 - jacard_coef: 0.0681 5/17 [=======>......................] - ETA: 2s - loss: 0.1654 - accuracy: 0.8213 - jacard_coef: 0.0669 6/17 [=========>....................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8251 - jacard_coef: 0.0686 7/17 [===========>..................] - ETA: 1s - loss: 0.1647 - accuracy: 0.8278 - jacard_coef: 0.0729 8/17 [=============>................] - ETA: 1s - loss: 0.1646 - accuracy: 0.8318 - jacard_coef: 0.0740 9/17 [==============>...............] - ETA: 1s - loss: 0.1641 - accuracy: 0.8381 - jacard_coef: 0.073810/17 [================>.............] - ETA: 1s - loss: 0.1677 - accuracy: 0.7846 - jacard_coef: 0.077811/17 [==================>...........] - ETA: 1s - loss: 0.1694 - accuracy: 0.7394 - jacard_coef: 0.074212/17 [====================>.........] - ETA: 0s - loss: 0.1710 - accuracy: 0.7034 - jacard_coef: 0.074713/17 [=====================>........] - ETA: 0s - loss: 0.1722 - accuracy: 0.6663 - jacard_coef: 0.074314/17 [=======================>......] - ETA: 0s - loss: 0.1746 - accuracy: 0.6285 - jacard_coef: 0.074815/17 [=========================>....] - ETA: 0s - loss: 0.1777 - accuracy: 0.5958 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.5657 - jacard_coef: 0.075717/17 [==============================] - 3s 185ms/step - loss: 0.1784 - accuracy: 0.5632 - jacard_coef: 0.0788 - val_loss: 1.1211 - val_accuracy: 0.9304 - val_jacard_coef: 3.4068e-05 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1794 - accuracy: 0.2399 - jacard_coef: 0.0678 2/17 [==>...........................] - ETA: 2s - loss: 0.1781 - accuracy: 0.2493 - jacard_coef: 0.0863 3/17 [====>.........................] - ETA: 2s - loss: 0.1807 - accuracy: 0.2414 - jacard_coef: 0.0871 4/17 [======>.......................] - ETA: 2s - loss: 0.1793 - accuracy: 0.2461 - jacard_coef: 0.0748 5/17 [=======>......................] - ETA: 2s - loss: 0.1833 - accuracy: 0.3061 - jacard_coef: 0.0734 6/17 [=========>....................] - ETA: 2s - loss: 0.1846 - accuracy: 0.3444 - jacard_coef: 0.0704 7/17 [===========>..................] - ETA: 1s - loss: 0.1830 - accuracy: 0.3487 - jacard_coef: 0.0746 8/17 [=============>................] - ETA: 1s - loss: 0.1823 - accuracy: 0.3955 - jacard_coef: 0.0794 9/17 [==============>...............] - ETA: 1s - loss: 0.1804 - accuracy: 0.4463 - jacard_coef: 0.078810/17 [================>.............] - ETA: 1s - loss: 0.1802 - accuracy: 0.4402 - jacard_coef: 0.080611/17 [==================>...........] - ETA: 1s - loss: 0.1792 - accuracy: 0.4623 - jacard_coef: 0.084012/17 [====================>.........] - ETA: 0s - loss: 0.1784 - accuracy: 0.4763 - jacard_coef: 0.080413/17 [=====================>........] - ETA: 0s - loss: 0.1775 - accuracy: 0.4874 - jacard_coef: 0.080714/17 [=======================>......] - ETA: 0s - loss: 0.1766 - accuracy: 0.4931 - jacard_coef: 0.079215/17 [=========================>....] - ETA: 0s - loss: 0.1758 - accuracy: 0.5135 - jacard_coef: 0.077816/17 [===========================>..] - ETA: 0s - loss: 0.1750 - accuracy: 0.5248 - jacard_coef: 0.075517/17 [==============================] - 3s 185ms/step - loss: 0.1750 - accuracy: 0.5241 - jacard_coef: 0.0763 - val_loss: 1.1059 - val_accuracy: 0.9302 - val_jacard_coef: 7.7718e-04 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1643 - accuracy: 0.7961 - jacard_coef: 0.0835 2/17 [==>...........................] - ETA: 2s - loss: 0.1645 - accuracy: 0.7002 - jacard_coef: 0.0960 3/17 [====>.........................] - ETA: 2s - loss: 0.1664 - accuracy: 0.6191 - jacard_coef: 0.0974 4/17 [======>.......................] - ETA: 2s - loss: 0.1658 - accuracy: 0.6511 - jacard_coef: 0.0949 5/17 [=======>......................] - ETA: 2s - loss: 0.1651 - accuracy: 0.6766 - jacard_coef: 0.0899 6/17 [=========>....................] - ETA: 2s - loss: 0.1643 - accuracy: 0.7036 - jacard_coef: 0.0918 7/17 [===========>..................] - ETA: 1s - loss: 0.1644 - accuracy: 0.6820 - jacard_coef: 0.0883 8/17 [=============>................] - ETA: 1s - loss: 0.1639 - accuracy: 0.6982 - jacard_coef: 0.0840 9/17 [==============>...............] - ETA: 1s - loss: 0.1634 - accuracy: 0.7063 - jacard_coef: 0.083710/17 [================>.............] - ETA: 1s - loss: 0.1630 - accuracy: 0.7114 - jacard_coef: 0.083311/17 [==================>...........] - ETA: 1s - loss: 0.1626 - accuracy: 0.7290 - jacard_coef: 0.080612/17 [====================>.........] - ETA: 0s - loss: 0.1622 - accuracy: 0.7300 - jacard_coef: 0.081113/17 [=====================>........] - ETA: 0s - loss: 0.1623 - accuracy: 0.7232 - jacard_coef: 0.080214/17 [=======================>......] - ETA: 0s - loss: 0.1630 - accuracy: 0.7325 - jacard_coef: 0.078215/17 [=========================>....] - ETA: 0s - loss: 0.1625 - accuracy: 0.7376 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1621 - accuracy: 0.7486 - jacard_coef: 0.076017/17 [==============================] - 3s 185ms/step - loss: 0.1625 - accuracy: 0.7456 - jacard_coef: 0.0733 - val_loss: 1.0695 - val_accuracy: 0.9200 - val_jacard_coef: 0.0133 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1739 - accuracy: 0.9105 - jacard_coef: 0.0733 2/17 [==>...........................] - ETA: 2s - loss: 0.1643 - accuracy: 0.9115 - jacard_coef: 0.0596 3/17 [====>.........................] - ETA: 2s - loss: 0.1631 - accuracy: 0.8554 - jacard_coef: 0.0773 4/17 [======>.......................] - ETA: 2s - loss: 0.1620 - accuracy: 0.8469 - jacard_coef: 0.0823 5/17 [=======>......................] - ETA: 2s - loss: 0.1634 - accuracy: 0.7845 - jacard_coef: 0.0742 6/17 [=========>....................] - ETA: 2s - loss: 0.1643 - accuracy: 0.8002 - jacard_coef: 0.0733 7/17 [===========>..................] - ETA: 1s - loss: 0.1635 - accuracy: 0.8133 - jacard_coef: 0.0729 8/17 [=============>................] - ETA: 1s - loss: 0.1625 - accuracy: 0.8174 - jacard_coef: 0.0736 9/17 [==============>...............] - ETA: 1s - loss: 0.1619 - accuracy: 0.8150 - jacard_coef: 0.073110/17 [================>.............] - ETA: 1s - loss: 0.1611 - accuracy: 0.8108 - jacard_coef: 0.071711/17 [==================>...........] - ETA: 1s - loss: 0.1609 - accuracy: 0.8148 - jacard_coef: 0.071112/17 [====================>.........] - ETA: 0s - loss: 0.1612 - accuracy: 0.8165 - jacard_coef: 0.072613/17 [=====================>........] - ETA: 0s - loss: 0.1606 - accuracy: 0.8215 - jacard_coef: 0.072414/17 [=======================>......] - ETA: 0s - loss: 0.1600 - accuracy: 0.8270 - jacard_coef: 0.072615/17 [=========================>....] - ETA: 0s - loss: 0.1596 - accuracy: 0.8318 - jacard_coef: 0.073616/17 [===========================>..] - ETA: 0s - loss: 0.1594 - accuracy: 0.8343 - jacard_coef: 0.075517/17 [==============================] - 3s 197ms/step - loss: 0.1595 - accuracy: 0.8312 - jacard_coef: 0.0743 - val_loss: 0.2847 - val_accuracy: 0.2727 - val_jacard_coef: 0.0663 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1521 - accuracy: 0.8872 - jacard_coef: 0.0547 2/17 [==>...........................] - ETA: 2s - loss: 0.1530 - accuracy: 0.8653 - jacard_coef: 0.0652 3/17 [====>.........................] - ETA: 2s - loss: 0.1532 - accuracy: 0.8619 - jacard_coef: 0.0740 4/17 [======>.......................] - ETA: 2s - loss: 0.1522 - accuracy: 0.8708 - jacard_coef: 0.0748 5/17 [=======>......................] - ETA: 2s - loss: 0.1516 - accuracy: 0.8782 - jacard_coef: 0.0686 6/17 [=========>....................] - ETA: 2s - loss: 0.1523 - accuracy: 0.8587 - jacard_coef: 0.0676 7/17 [===========>..................] - ETA: 1s - loss: 0.1531 - accuracy: 0.8579 - jacard_coef: 0.0727 8/17 [=============>................] - ETA: 1s - loss: 0.1529 - accuracy: 0.8634 - jacard_coef: 0.0747 9/17 [==============>...............] - ETA: 1s - loss: 0.1526 - accuracy: 0.8705 - jacard_coef: 0.073710/17 [================>.............] - ETA: 1s - loss: 0.1522 - accuracy: 0.8750 - jacard_coef: 0.073911/17 [==================>...........] - ETA: 1s - loss: 0.1520 - accuracy: 0.8769 - jacard_coef: 0.074612/17 [====================>.........] - ETA: 0s - loss: 0.1519 - accuracy: 0.8798 - jacard_coef: 0.074913/17 [=====================>........] - ETA: 0s - loss: 0.1528 - accuracy: 0.8826 - jacard_coef: 0.074414/17 [=======================>......] - ETA: 0s - loss: 0.1524 - accuracy: 0.8864 - jacard_coef: 0.073315/17 [=========================>....] - ETA: 0s - loss: 0.1521 - accuracy: 0.8893 - jacard_coef: 0.072616/17 [===========================>..] - ETA: 0s - loss: 0.1521 - accuracy: 0.8793 - jacard_coef: 0.074717/17 [==============================] - 3s 190ms/step - loss: 0.1522 - accuracy: 0.8764 - jacard_coef: 0.0791 - val_loss: 0.1829 - val_accuracy: 0.3322 - val_jacard_coef: 0.0635 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1510 - accuracy: 0.7672 - jacard_coef: 0.0601 2/17 [==>...........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.7241 - jacard_coef: 0.0639 3/17 [====>.........................] - ETA: 2s - loss: 0.1576 - accuracy: 0.7807 - jacard_coef: 0.0678 4/17 [======>.......................] - ETA: 2s - loss: 0.1574 - accuracy: 0.8032 - jacard_coef: 0.0751 5/17 [=======>......................] - ETA: 2s - loss: 0.1562 - accuracy: 0.8213 - jacard_coef: 0.0769 6/17 [=========>....................] - ETA: 2s - loss: 0.1551 - accuracy: 0.8364 - jacard_coef: 0.0768 7/17 [===========>..................] - ETA: 1s - loss: 0.1541 - accuracy: 0.8493 - jacard_coef: 0.0746 8/17 [=============>................] - ETA: 1s - loss: 0.1536 - accuracy: 0.8586 - jacard_coef: 0.0729 9/17 [==============>...............] - ETA: 1s - loss: 0.1531 - accuracy: 0.8619 - jacard_coef: 0.073510/17 [================>.............] - ETA: 1s - loss: 0.1542 - accuracy: 0.8628 - jacard_coef: 0.077011/17 [==================>...........] - ETA: 1s - loss: 0.1535 - accuracy: 0.8691 - jacard_coef: 0.075312/17 [====================>.........] - ETA: 0s - loss: 0.1529 - accuracy: 0.8716 - jacard_coef: 0.076213/17 [=====================>........] - ETA: 0s - loss: 0.1527 - accuracy: 0.8731 - jacard_coef: 0.077314/17 [=======================>......] - ETA: 0s - loss: 0.1527 - accuracy: 0.8750 - jacard_coef: 0.077915/17 [=========================>....] - ETA: 0s - loss: 0.1523 - accuracy: 0.8743 - jacard_coef: 0.076916/17 [===========================>..] - ETA: 0s - loss: 0.1519 - accuracy: 0.8777 - jacard_coef: 0.075617/17 [==============================] - 3s 190ms/step - loss: 0.1519 - accuracy: 0.8766 - jacard_coef: 0.0713 - val_loss: 0.1787 - val_accuracy: 0.5348 - val_jacard_coef: 0.0634 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1498 - accuracy: 0.8878 - jacard_coef: 0.0824 2/17 [==>...........................] - ETA: 2s - loss: 0.1517 - accuracy: 0.8726 - jacard_coef: 0.0931 3/17 [====>.........................] - ETA: 2s - loss: 0.1496 - accuracy: 0.8893 - jacard_coef: 0.0825 4/17 [======>.......................] - ETA: 2s - loss: 0.1480 - accuracy: 0.8966 - jacard_coef: 0.0785 5/17 [=======>......................] - ETA: 2s - loss: 0.1474 - accuracy: 0.9052 - jacard_coef: 0.0741 6/17 [=========>....................] - ETA: 2s - loss: 0.1478 - accuracy: 0.9074 - jacard_coef: 0.0743 7/17 [===========>..................] - ETA: 1s - loss: 0.1478 - accuracy: 0.9067 - jacard_coef: 0.0761 8/17 [=============>................] - ETA: 1s - loss: 0.1472 - accuracy: 0.9105 - jacard_coef: 0.0739 9/17 [==============>...............] - ETA: 1s - loss: 0.1467 - accuracy: 0.9140 - jacard_coef: 0.071610/17 [================>.............] - ETA: 1s - loss: 0.1478 - accuracy: 0.9105 - jacard_coef: 0.074711/17 [==================>...........] - ETA: 1s - loss: 0.1473 - accuracy: 0.9091 - jacard_coef: 0.075012/17 [====================>.........] - ETA: 0s - loss: 0.1472 - accuracy: 0.9084 - jacard_coef: 0.073513/17 [=====================>........] - ETA: 0s - loss: 0.1468 - accuracy: 0.9093 - jacard_coef: 0.071714/17 [=======================>......] - ETA: 0s - loss: 0.1475 - accuracy: 0.9071 - jacard_coef: 0.071915/17 [=========================>....] - ETA: 0s - loss: 0.1475 - accuracy: 0.9039 - jacard_coef: 0.073616/17 [===========================>..] - ETA: 0s - loss: 0.1473 - accuracy: 0.9021 - jacard_coef: 0.074417/17 [==============================] - 3s 190ms/step - loss: 0.1476 - accuracy: 0.8991 - jacard_coef: 0.0795 - val_loss: 0.1815 - val_accuracy: 0.0946 - val_jacard_coef: 0.0636 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1408 - accuracy: 0.9108 - jacard_coef: 0.0750 2/17 [==>...........................] - ETA: 2s - loss: 0.1426 - accuracy: 0.9163 - jacard_coef: 0.0665 3/17 [====>.........................] - ETA: 2s - loss: 0.1433 - accuracy: 0.9004 - jacard_coef: 0.0688 4/17 [======>.......................] - ETA: 2s - loss: 0.1456 - accuracy: 0.8649 - jacard_coef: 0.0690 5/17 [=======>......................] - ETA: 2s - loss: 0.1452 - accuracy: 0.8729 - jacard_coef: 0.0720 6/17 [=========>....................] - ETA: 2s - loss: 0.1446 - accuracy: 0.8798 - jacard_coef: 0.0709 7/17 [===========>..................] - ETA: 1s - loss: 0.1447 - accuracy: 0.8822 - jacard_coef: 0.0735 8/17 [=============>................] - ETA: 1s - loss: 0.1448 - accuracy: 0.8863 - jacard_coef: 0.0732 9/17 [==============>...............] - ETA: 1s - loss: 0.1457 - accuracy: 0.8871 - jacard_coef: 0.074510/17 [================>.............] - ETA: 1s - loss: 0.1451 - accuracy: 0.8926 - jacard_coef: 0.072011/17 [==================>...........] - ETA: 1s - loss: 0.1457 - accuracy: 0.8934 - jacard_coef: 0.072812/17 [====================>.........] - ETA: 0s - loss: 0.1471 - accuracy: 0.8957 - jacard_coef: 0.071713/17 [=====================>........] - ETA: 0s - loss: 0.1465 - accuracy: 0.8970 - jacard_coef: 0.071614/17 [=======================>......] - ETA: 0s - loss: 0.1464 - accuracy: 0.8962 - jacard_coef: 0.072315/17 [=========================>....] - ETA: 0s - loss: 0.1463 - accuracy: 0.8952 - jacard_coef: 0.073916/17 [===========================>..] - ETA: 0s - loss: 0.1461 - accuracy: 0.8944 - jacard_coef: 0.075217/17 [==============================] - 3s 190ms/step - loss: 0.1462 - accuracy: 0.8926 - jacard_coef: 0.0765 - val_loss: 0.1827 - val_accuracy: 0.2594 - val_jacard_coef: 0.0637 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1359 - accuracy: 0.9412 - jacard_coef: 0.0553 2/17 [==>...........................] - ETA: 2s - loss: 0.1399 - accuracy: 0.9053 - jacard_coef: 0.0842 3/17 [====>.........................] - ETA: 2s - loss: 0.1426 - accuracy: 0.9034 - jacard_coef: 0.0830 4/17 [======>.......................] - ETA: 2s - loss: 0.1430 - accuracy: 0.9009 - jacard_coef: 0.0779 5/17 [=======>......................] - ETA: 2s - loss: 0.1422 - accuracy: 0.9046 - jacard_coef: 0.0740 6/17 [=========>....................] - ETA: 2s - loss: 0.1415 - accuracy: 0.9060 - jacard_coef: 0.0733 7/17 [===========>..................] - ETA: 1s - loss: 0.1424 - accuracy: 0.9010 - jacard_coef: 0.0751 8/17 [=============>................] - ETA: 1s - loss: 0.1416 - accuracy: 0.9024 - jacard_coef: 0.0749 9/17 [==============>...............] - ETA: 1s - loss: 0.1415 - accuracy: 0.8992 - jacard_coef: 0.075810/17 [================>.............] - ETA: 1s - loss: 0.1412 - accuracy: 0.8965 - jacard_coef: 0.076111/17 [==================>...........] - ETA: 1s - loss: 0.1410 - accuracy: 0.8982 - jacard_coef: 0.076112/17 [====================>.........] - ETA: 0s - loss: 0.1405 - accuracy: 0.9026 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 0s - loss: 0.1405 - accuracy: 0.9022 - jacard_coef: 0.075014/17 [=======================>......] - ETA: 0s - loss: 0.1405 - accuracy: 0.9014 - jacard_coef: 0.076615/17 [=========================>....] - ETA: 0s - loss: 0.1400 - accuracy: 0.9039 - jacard_coef: 0.075316/17 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.9045 - jacard_coef: 0.075417/17 [==============================] - 3s 190ms/step - loss: 0.1398 - accuracy: 0.9051 - jacard_coef: 0.0724 - val_loss: 0.1488 - val_accuracy: 0.7602 - val_jacard_coef: 0.0630 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1421 - accuracy: 0.8681 - jacard_coef: 0.1116 2/17 [==>...........................] - ETA: 2s - loss: 0.1421 - accuracy: 0.8646 - jacard_coef: 0.1133 3/17 [====>.........................] - ETA: 2s - loss: 0.1406 - accuracy: 0.8767 - jacard_coef: 0.1031 4/17 [======>.......................] - ETA: 2s - loss: 0.1403 - accuracy: 0.8838 - jacard_coef: 0.0961 5/17 [=======>......................] - ETA: 2s - loss: 0.1395 - accuracy: 0.8973 - jacard_coef: 0.0852 6/17 [=========>....................] - ETA: 2s - loss: 0.1401 - accuracy: 0.9012 - jacard_coef: 0.0825 7/17 [===========>..................] - ETA: 1s - loss: 0.1394 - accuracy: 0.9045 - jacard_coef: 0.0806 8/17 [=============>................] - ETA: 1s - loss: 0.1389 - accuracy: 0.9072 - jacard_coef: 0.0791 9/17 [==============>...............] - ETA: 1s - loss: 0.1388 - accuracy: 0.9092 - jacard_coef: 0.077910/17 [================>.............] - ETA: 1s - loss: 0.1381 - accuracy: 0.9132 - jacard_coef: 0.075011/17 [==================>...........] - ETA: 1s - loss: 0.1381 - accuracy: 0.9116 - jacard_coef: 0.076612/17 [====================>.........] - ETA: 0s - loss: 0.1378 - accuracy: 0.9132 - jacard_coef: 0.075513/17 [=====================>........] - ETA: 0s - loss: 0.1375 - accuracy: 0.9141 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1372 - accuracy: 0.9143 - jacard_coef: 0.075115/17 [=========================>....] - ETA: 0s - loss: 0.1371 - accuracy: 0.9144 - jacard_coef: 0.075216/17 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9146 - jacard_coef: 0.075117/17 [==============================] - 3s 190ms/step - loss: 0.1368 - accuracy: 0.9149 - jacard_coef: 0.0730 - val_loss: 0.1457 - val_accuracy: 0.8096 - val_jacard_coef: 0.0629 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1319 - accuracy: 0.9298 - jacard_coef: 0.0540 2/17 [==>...........................] - ETA: 2s - loss: 0.1393 - accuracy: 0.8962 - jacard_coef: 0.0857 3/17 [====>.........................] - ETA: 2s - loss: 0.1375 - accuracy: 0.8989 - jacard_coef: 0.0792 4/17 [======>.......................] - ETA: 2s - loss: 0.1371 - accuracy: 0.8961 - jacard_coef: 0.0813 5/17 [=======>......................] - ETA: 2s - loss: 0.1363 - accuracy: 0.8999 - jacard_coef: 0.0803 6/17 [=========>....................] - ETA: 2s - loss: 0.1352 - accuracy: 0.9086 - jacard_coef: 0.0745 7/17 [===========>..................] - ETA: 1s - loss: 0.1352 - accuracy: 0.9087 - jacard_coef: 0.0755 8/17 [=============>................] - ETA: 1s - loss: 0.1352 - accuracy: 0.9073 - jacard_coef: 0.0774 9/17 [==============>...............] - ETA: 1s - loss: 0.1350 - accuracy: 0.9071 - jacard_coef: 0.078210/17 [================>.............] - ETA: 1s - loss: 0.1345 - accuracy: 0.9107 - jacard_coef: 0.075711/17 [==================>...........] - ETA: 1s - loss: 0.1346 - accuracy: 0.9090 - jacard_coef: 0.077512/17 [====================>.........] - ETA: 0s - loss: 0.1345 - accuracy: 0.9098 - jacard_coef: 0.077213/17 [=====================>........] - ETA: 0s - loss: 0.1343 - accuracy: 0.9117 - jacard_coef: 0.075914/17 [=======================>......] - ETA: 0s - loss: 0.1342 - accuracy: 0.9114 - jacard_coef: 0.076415/17 [=========================>....] - ETA: 0s - loss: 0.1343 - accuracy: 0.9110 - jacard_coef: 0.077016/17 [===========================>..] - ETA: 0s - loss: 0.1339 - accuracy: 0.9133 - jacard_coef: 0.075217/17 [==============================] - 3s 190ms/step - loss: 0.1339 - accuracy: 0.9139 - jacard_coef: 0.0709 - val_loss: 0.1739 - val_accuracy: 0.2470 - val_jacard_coef: 0.0635 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1310 - accuracy: 0.9302 - jacard_coef: 0.0626 2/17 [==>...........................] - ETA: 2s - loss: 0.1322 - accuracy: 0.9165 - jacard_coef: 0.0740 3/17 [====>.........................] - ETA: 2s - loss: 0.1334 - accuracy: 0.9093 - jacard_coef: 0.0805 4/17 [======>.......................] - ETA: 2s - loss: 0.1326 - accuracy: 0.9140 - jacard_coef: 0.0769 5/17 [=======>......................] - ETA: 2s - loss: 0.1330 - accuracy: 0.9098 - jacard_coef: 0.0804 6/17 [=========>....................] - ETA: 2s - loss: 0.1322 - accuracy: 0.9173 - jacard_coef: 0.0742 7/17 [===========>..................] - ETA: 1s - loss: 0.1322 - accuracy: 0.9166 - jacard_coef: 0.0749 8/17 [=============>................] - ETA: 1s - loss: 0.1312 - accuracy: 0.9237 - jacard_coef: 0.0686 9/17 [==============>...............] - ETA: 1s - loss: 0.1316 - accuracy: 0.9210 - jacard_coef: 0.070910/17 [================>.............] - ETA: 1s - loss: 0.1316 - accuracy: 0.9210 - jacard_coef: 0.071011/17 [==================>...........] - ETA: 1s - loss: 0.1320 - accuracy: 0.9161 - jacard_coef: 0.073912/17 [====================>.........] - ETA: 0s - loss: 0.1320 - accuracy: 0.9155 - jacard_coef: 0.074513/17 [=====================>........] - ETA: 0s - loss: 0.1320 - accuracy: 0.9147 - jacard_coef: 0.075314/17 [=======================>......] - ETA: 0s - loss: 0.1322 - accuracy: 0.9134 - jacard_coef: 0.076515/17 [=========================>....] - ETA: 0s - loss: 0.1318 - accuracy: 0.9156 - jacard_coef: 0.074816/17 [===========================>..] - ETA: 0s - loss: 0.1317 - accuracy: 0.9154 - jacard_coef: 0.075017/17 [==============================] - 3s 190ms/step - loss: 0.1319 - accuracy: 0.9140 - jacard_coef: 0.0708 - val_loss: 0.1731 - val_accuracy: 0.5189 - val_jacard_coef: 0.0635 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1307 - accuracy: 0.9386 - jacard_coef: 0.0569 2/17 [==>...........................] - ETA: 2s - loss: 0.1294 - accuracy: 0.9372 - jacard_coef: 0.0538 3/17 [====>.........................] - ETA: 2s - loss: 0.1293 - accuracy: 0.9313 - jacard_coef: 0.0540 4/17 [======>.......................] - ETA: 2s - loss: 0.1301 - accuracy: 0.9218 - jacard_coef: 0.0634 5/17 [=======>......................] - ETA: 2s - loss: 0.1310 - accuracy: 0.9147 - jacard_coef: 0.0703 6/17 [=========>....................] - ETA: 2s - loss: 0.1316 - accuracy: 0.9071 - jacard_coef: 0.0764 7/17 [===========>..................] - ETA: 1s - loss: 0.1313 - accuracy: 0.9093 - jacard_coef: 0.0755 8/17 [=============>................] - ETA: 1s - loss: 0.1312 - accuracy: 0.9114 - jacard_coef: 0.0744 9/17 [==============>...............] - ETA: 1s - loss: 0.1319 - accuracy: 0.9081 - jacard_coef: 0.076310/17 [================>.............] - ETA: 1s - loss: 0.1313 - accuracy: 0.9128 - jacard_coef: 0.073011/17 [==================>...........] - ETA: 1s - loss: 0.1311 - accuracy: 0.9135 - jacard_coef: 0.072912/17 [====================>.........] - ETA: 0s - loss: 0.1309 - accuracy: 0.9144 - jacard_coef: 0.072613/17 [=====================>........] - ETA: 0s - loss: 0.1310 - accuracy: 0.9135 - jacard_coef: 0.073714/17 [=======================>......] - ETA: 0s - loss: 0.1315 - accuracy: 0.9114 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1312 - accuracy: 0.9128 - jacard_coef: 0.074616/17 [===========================>..] - ETA: 0s - loss: 0.1310 - accuracy: 0.9141 - jacard_coef: 0.073817/17 [==============================] - 3s 190ms/step - loss: 0.1312 - accuracy: 0.9116 - jacard_coef: 0.0781 - val_loss: 0.1760 - val_accuracy: 0.1983 - val_jacard_coef: 0.0635 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1293 - accuracy: 0.9102 - jacard_coef: 0.0657 2/17 [==>...........................] - ETA: 2s - loss: 0.1307 - accuracy: 0.9020 - jacard_coef: 0.0747 3/17 [====>.........................] - ETA: 2s - loss: 0.1320 - accuracy: 0.9008 - jacard_coef: 0.0781 4/17 [======>.......................] - ETA: 2s - loss: 0.1344 - accuracy: 0.8969 - jacard_coef: 0.0810 5/17 [=======>......................] - ETA: 2s - loss: 0.1348 - accuracy: 0.8908 - jacard_coef: 0.0858 6/17 [=========>....................] - ETA: 2s - loss: 0.1355 - accuracy: 0.8926 - jacard_coef: 0.0857 7/17 [===========>..................] - ETA: 1s - loss: 0.1383 - accuracy: 0.8957 - jacard_coef: 0.0838 8/17 [=============>................] - ETA: 1s - loss: 0.1381 - accuracy: 0.9011 - jacard_coef: 0.0793 9/17 [==============>...............] - ETA: 1s - loss: 0.1380 - accuracy: 0.9048 - jacard_coef: 0.076910/17 [================>.............] - ETA: 1s - loss: 0.1372 - accuracy: 0.9075 - jacard_coef: 0.075611/17 [==================>...........] - ETA: 1s - loss: 0.1368 - accuracy: 0.9059 - jacard_coef: 0.077512/17 [====================>.........] - ETA: 0s - loss: 0.1362 - accuracy: 0.9075 - jacard_coef: 0.076513/17 [=====================>........] - ETA: 0s - loss: 0.1364 - accuracy: 0.9076 - jacard_coef: 0.076814/17 [=======================>......] - ETA: 0s - loss: 0.1365 - accuracy: 0.9082 - jacard_coef: 0.076515/17 [=========================>....] - ETA: 0s - loss: 0.1362 - accuracy: 0.9082 - jacard_coef: 0.076516/17 [===========================>..] - ETA: 0s - loss: 0.1356 - accuracy: 0.9093 - jacard_coef: 0.075017/17 [==============================] - 3s 190ms/step - loss: 0.1356 - accuracy: 0.9093 - jacard_coef: 0.0739 - val_loss: 0.1459 - val_accuracy: 0.9304 - val_jacard_coef: 0.0629 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0663 (epoch 8)
  Final Val Loss: 0.1459
  Training Time: 0:02:06.452884
  Stability (std): 0.0149

Results saved to: hyperparameter_optimization_20250926_123742/exp_13_Attention_UNet_lr1e-4_bs8/Attention_UNet_lr0.0001_bs8_results.json

Experiment 13 completed in 161s
Progress: 13/36 completed
Estimated remaining time: 61 minutes

ðŸ”¬ EXPERIMENT 14/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758863358.214600 3232297 service.cc:145] XLA service 0x1479a99ab130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758863358.214635 3232297 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758863358.670508 3232297 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 7:55 - loss: 0.3434 - accuracy: 0.5138 - jacard_coef: 0.08582/9 [=====>........................] - ETA: 59s - loss: 0.3163 - accuracy: 0.5416 - jacard_coef: 0.0772 3/9 [=========>....................] - ETA: 26s - loss: 0.2913 - accuracy: 0.5084 - jacard_coef: 0.07944/9 [============>.................] - ETA: 15s - loss: 0.2737 - accuracy: 0.5005 - jacard_coef: 0.07745/9 [===============>..............] - ETA: 9s - loss: 0.2650 - accuracy: 0.5251 - jacard_coef: 0.0799 6/9 [===================>..........] - ETA: 5s - loss: 0.2567 - accuracy: 0.5471 - jacard_coef: 0.07817/9 [======================>.......] - ETA: 3s - loss: 0.2484 - accuracy: 0.5729 - jacard_coef: 0.07728/9 [=========================>....] - ETA: 1s - loss: 0.2411 - accuracy: 0.5877 - jacard_coef: 0.07689/9 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.5858 - jacard_coef: 0.06869/9 [==============================] - 79s 2s/step - loss: 0.2407 - accuracy: 0.5858 - jacard_coef: 0.0686 - val_loss: 1.2954 - val_accuracy: 0.9195 - val_jacard_coef: 0.0050 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1864 - accuracy: 0.4436 - jacard_coef: 0.06642/9 [=====>........................] - ETA: 2s - loss: 0.1831 - accuracy: 0.4547 - jacard_coef: 0.07713/9 [=========>....................] - ETA: 2s - loss: 0.1796 - accuracy: 0.5358 - jacard_coef: 0.06974/9 [============>.................] - ETA: 1s - loss: 0.1781 - accuracy: 0.5970 - jacard_coef: 0.07415/9 [===============>..............] - ETA: 1s - loss: 0.1770 - accuracy: 0.6432 - jacard_coef: 0.07206/9 [===================>..........] - ETA: 1s - loss: 0.1761 - accuracy: 0.6693 - jacard_coef: 0.07377/9 [======================>.......] - ETA: 0s - loss: 0.1753 - accuracy: 0.6901 - jacard_coef: 0.07448/9 [=========================>....] - ETA: 0s - loss: 0.1748 - accuracy: 0.7072 - jacard_coef: 0.07559/9 [==============================] - 3s 337ms/step - loss: 0.1759 - accuracy: 0.7072 - jacard_coef: 0.0776 - val_loss: 1.7897 - val_accuracy: 0.8869 - val_jacard_coef: 0.0111 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1700 - accuracy: 0.4508 - jacard_coef: 0.07742/9 [=====>........................] - ETA: 2s - loss: 0.1707 - accuracy: 0.4130 - jacard_coef: 0.07543/9 [=========>....................] - ETA: 2s - loss: 0.1707 - accuracy: 0.4169 - jacard_coef: 0.07244/9 [============>.................] - ETA: 1s - loss: 0.1711 - accuracy: 0.4172 - jacard_coef: 0.07335/9 [===============>..............] - ETA: 1s - loss: 0.1711 - accuracy: 0.4236 - jacard_coef: 0.07696/9 [===================>..........] - ETA: 1s - loss: 0.1709 - accuracy: 0.4460 - jacard_coef: 0.07267/9 [======================>.......] - ETA: 0s - loss: 0.1706 - accuracy: 0.4794 - jacard_coef: 0.07388/9 [=========================>....] - ETA: 0s - loss: 0.1705 - accuracy: 0.5186 - jacard_coef: 0.07559/9 [==============================] - 3s 339ms/step - loss: 0.1714 - accuracy: 0.5187 - jacard_coef: 0.0846 - val_loss: 14.0770 - val_accuracy: 0.0770 - val_jacard_coef: 0.0699 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1684 - accuracy: 0.7680 - jacard_coef: 0.08112/9 [=====>........................] - ETA: 2s - loss: 0.1728 - accuracy: 0.7542 - jacard_coef: 0.07333/9 [=========>....................] - ETA: 2s - loss: 0.1736 - accuracy: 0.7402 - jacard_coef: 0.07614/9 [============>.................] - ETA: 1s - loss: 0.1735 - accuracy: 0.7343 - jacard_coef: 0.07485/9 [===============>..............] - ETA: 1s - loss: 0.1731 - accuracy: 0.7318 - jacard_coef: 0.07256/9 [===================>..........] - ETA: 1s - loss: 0.1725 - accuracy: 0.7288 - jacard_coef: 0.07517/9 [======================>.......] - ETA: 0s - loss: 0.1720 - accuracy: 0.7309 - jacard_coef: 0.07488/9 [=========================>....] - ETA: 0s - loss: 0.1716 - accuracy: 0.7303 - jacard_coef: 0.07559/9 [==============================] - 3s 331ms/step - loss: 0.1716 - accuracy: 0.7300 - jacard_coef: 0.0824 - val_loss: 0.7695 - val_accuracy: 0.9199 - val_jacard_coef: 0.0161 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1679 - accuracy: 0.7243 - jacard_coef: 0.06242/9 [=====>........................] - ETA: 2s - loss: 0.1688 - accuracy: 0.7163 - jacard_coef: 0.06953/9 [=========>....................] - ETA: 2s - loss: 0.1689 - accuracy: 0.7109 - jacard_coef: 0.08054/9 [============>.................] - ETA: 1s - loss: 0.1688 - accuracy: 0.7152 - jacard_coef: 0.08075/9 [===============>..............] - ETA: 1s - loss: 0.1688 - accuracy: 0.7375 - jacard_coef: 0.08066/9 [===================>..........] - ETA: 1s - loss: 0.1686 - accuracy: 0.7605 - jacard_coef: 0.07887/9 [======================>.......] - ETA: 0s - loss: 0.1682 - accuracy: 0.7806 - jacard_coef: 0.07708/9 [=========================>....] - ETA: 0s - loss: 0.1677 - accuracy: 0.7961 - jacard_coef: 0.07639/9 [==============================] - 3s 331ms/step - loss: 0.1676 - accuracy: 0.7973 - jacard_coef: 0.0719 - val_loss: 14.8569 - val_accuracy: 0.0730 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1636 - accuracy: 0.8672 - jacard_coef: 0.08862/9 [=====>........................] - ETA: 2s - loss: 0.1636 - accuracy: 0.8470 - jacard_coef: 0.08533/9 [=========>....................] - ETA: 2s - loss: 0.1633 - accuracy: 0.8454 - jacard_coef: 0.07684/9 [============>.................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8431 - jacard_coef: 0.07355/9 [===============>..............] - ETA: 1s - loss: 0.1630 - accuracy: 0.8431 - jacard_coef: 0.07236/9 [===================>..........] - ETA: 1s - loss: 0.1630 - accuracy: 0.8399 - jacard_coef: 0.07447/9 [======================>.......] - ETA: 0s - loss: 0.1629 - accuracy: 0.8400 - jacard_coef: 0.07638/9 [=========================>....] - ETA: 0s - loss: 0.1626 - accuracy: 0.8494 - jacard_coef: 0.07479/9 [==============================] - 3s 338ms/step - loss: 0.1626 - accuracy: 0.8490 - jacard_coef: 0.0851 - val_loss: 13.6747 - val_accuracy: 0.0829 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.9015 - jacard_coef: 0.08862/9 [=====>........................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9129 - jacard_coef: 0.07923/9 [=========>....................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9126 - jacard_coef: 0.07964/9 [============>.................] - ETA: 1s - loss: 0.1607 - accuracy: 0.9092 - jacard_coef: 0.08245/9 [===============>..............] - ETA: 1s - loss: 0.1604 - accuracy: 0.9132 - jacard_coef: 0.07906/9 [===================>..........] - ETA: 1s - loss: 0.1603 - accuracy: 0.9135 - jacard_coef: 0.07887/9 [======================>.......] - ETA: 0s - loss: 0.1601 - accuracy: 0.9156 - jacard_coef: 0.07708/9 [=========================>....] - ETA: 0s - loss: 0.1599 - accuracy: 0.9178 - jacard_coef: 0.07529/9 [==============================] - 3s 338ms/step - loss: 0.1599 - accuracy: 0.9172 - jacard_coef: 0.0828 - val_loss: 0.4150 - val_accuracy: 0.2895 - val_jacard_coef: 0.0681 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1584 - accuracy: 0.9200 - jacard_coef: 0.07372/9 [=====>........................] - ETA: 2s - loss: 0.1586 - accuracy: 0.9154 - jacard_coef: 0.07743/9 [=========>....................] - ETA: 2s - loss: 0.1585 - accuracy: 0.9145 - jacard_coef: 0.07794/9 [============>.................] - ETA: 1s - loss: 0.1584 - accuracy: 0.9122 - jacard_coef: 0.07725/9 [===============>..............] - ETA: 1s - loss: 0.1581 - accuracy: 0.9103 - jacard_coef: 0.07476/9 [===================>..........] - ETA: 1s - loss: 0.1581 - accuracy: 0.9055 - jacard_coef: 0.07687/9 [======================>.......] - ETA: 0s - loss: 0.1581 - accuracy: 0.9008 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.9016 - jacard_coef: 0.07509/9 [==============================] - 3s 338ms/step - loss: 0.1579 - accuracy: 0.9011 - jacard_coef: 0.0835 - val_loss: 0.6146 - val_accuracy: 0.2810 - val_jacard_coef: 0.0679 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1578 - accuracy: 0.8652 - jacard_coef: 0.09712/9 [=====>........................] - ETA: 2s - loss: 0.1574 - accuracy: 0.8639 - jacard_coef: 0.08793/9 [=========>....................] - ETA: 2s - loss: 0.1568 - accuracy: 0.8747 - jacard_coef: 0.07814/9 [============>.................] - ETA: 1s - loss: 0.1567 - accuracy: 0.8751 - jacard_coef: 0.07705/9 [===============>..............] - ETA: 1s - loss: 0.1565 - accuracy: 0.8767 - jacard_coef: 0.07626/9 [===================>..........] - ETA: 1s - loss: 0.1565 - accuracy: 0.8751 - jacard_coef: 0.07557/9 [======================>.......] - ETA: 0s - loss: 0.1565 - accuracy: 0.8723 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1564 - accuracy: 0.8725 - jacard_coef: 0.07599/9 [==============================] - 3s 338ms/step - loss: 0.1564 - accuracy: 0.8729 - jacard_coef: 0.0709 - val_loss: 0.1633 - val_accuracy: 0.8204 - val_jacard_coef: 0.0652 - lr: 5.0000e-04
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1553 - accuracy: 0.8846 - jacard_coef: 0.07442/9 [=====>........................] - ETA: 2s - loss: 0.1554 - accuracy: 0.8889 - jacard_coef: 0.07133/9 [=========>....................] - ETA: 2s - loss: 0.1555 - accuracy: 0.8919 - jacard_coef: 0.07174/9 [============>.................] - ETA: 1s - loss: 0.1553 - accuracy: 0.8990 - jacard_coef: 0.06945/9 [===============>..............] - ETA: 1s - loss: 0.1554 - accuracy: 0.8989 - jacard_coef: 0.07266/9 [===================>..........] - ETA: 1s - loss: 0.1553 - accuracy: 0.9010 - jacard_coef: 0.07297/9 [======================>.......] - ETA: 0s - loss: 0.1553 - accuracy: 0.9014 - jacard_coef: 0.07428/9 [=========================>....] - ETA: 0s - loss: 0.1553 - accuracy: 0.9023 - jacard_coef: 0.07529/9 [==============================] - 3s 338ms/step - loss: 0.1553 - accuracy: 0.9018 - jacard_coef: 0.0817 - val_loss: 0.1511 - val_accuracy: 0.9304 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1545 - accuracy: 0.9191 - jacard_coef: 0.07442/9 [=====>........................] - ETA: 2s - loss: 0.1544 - accuracy: 0.9220 - jacard_coef: 0.07193/9 [=========>....................] - ETA: 2s - loss: 0.1546 - accuracy: 0.9171 - jacard_coef: 0.07614/9 [============>.................] - ETA: 1s - loss: 0.1544 - accuracy: 0.9212 - jacard_coef: 0.07265/9 [===============>..............] - ETA: 1s - loss: 0.1544 - accuracy: 0.9214 - jacard_coef: 0.07246/9 [===================>..........] - ETA: 1s - loss: 0.1543 - accuracy: 0.9229 - jacard_coef: 0.07117/9 [======================>.......] - ETA: 0s - loss: 0.1543 - accuracy: 0.9217 - jacard_coef: 0.07218/9 [=========================>....] - ETA: 0s - loss: 0.1545 - accuracy: 0.9170 - jacard_coef: 0.07609/9 [==============================] - 3s 339ms/step - loss: 0.1545 - accuracy: 0.9174 - jacard_coef: 0.0709 - val_loss: 0.1540 - val_accuracy: 0.9304 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1542 - accuracy: 0.9131 - jacard_coef: 0.07942/9 [=====>........................] - ETA: 2s - loss: 0.1530 - accuracy: 0.9332 - jacard_coef: 0.06183/9 [=========>....................] - ETA: 2s - loss: 0.1529 - accuracy: 0.9339 - jacard_coef: 0.06144/9 [============>.................] - ETA: 1s - loss: 0.1530 - accuracy: 0.9295 - jacard_coef: 0.06525/9 [===============>..............] - ETA: 1s - loss: 0.1531 - accuracy: 0.9273 - jacard_coef: 0.06716/9 [===================>..........] - ETA: 1s - loss: 0.1531 - accuracy: 0.9253 - jacard_coef: 0.06877/9 [======================>.......] - ETA: 0s - loss: 0.1533 - accuracy: 0.9206 - jacard_coef: 0.07248/9 [=========================>....] - ETA: 0s - loss: 0.1534 - accuracy: 0.9176 - jacard_coef: 0.07489/9 [==============================] - 3s 339ms/step - loss: 0.1534 - accuracy: 0.9169 - jacard_coef: 0.0831 - val_loss: 0.1526 - val_accuracy: 0.9304 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1524 - accuracy: 0.9223 - jacard_coef: 0.06852/9 [=====>........................] - ETA: 2s - loss: 0.1522 - accuracy: 0.9238 - jacard_coef: 0.06653/9 [=========>....................] - ETA: 2s - loss: 0.1521 - accuracy: 0.9214 - jacard_coef: 0.06634/9 [============>.................] - ETA: 1s - loss: 0.1524 - accuracy: 0.9147 - jacard_coef: 0.07105/9 [===============>..............] - ETA: 1s - loss: 0.1525 - accuracy: 0.9126 - jacard_coef: 0.07226/9 [===================>..........] - ETA: 1s - loss: 0.1527 - accuracy: 0.9090 - jacard_coef: 0.07597/9 [======================>.......] - ETA: 0s - loss: 0.1526 - accuracy: 0.9098 - jacard_coef: 0.07598/9 [=========================>....] - ETA: 0s - loss: 0.1524 - accuracy: 0.9115 - jacard_coef: 0.07489/9 [==============================] - 3s 339ms/step - loss: 0.1525 - accuracy: 0.9106 - jacard_coef: 0.0845 - val_loss: 0.1521 - val_accuracy: 0.9304 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0699 (epoch 3)
  Final Val Loss: 0.1521
  Training Time: 0:01:56.266436
  Stability (std): 5.5879

Results saved to: hyperparameter_optimization_20250926_123742/exp_14_Attention_UNet_lr1e-4_bs16/Attention_UNet_lr0.0001_bs16_results.json

Experiment 14 completed in 153s
Progress: 14/36 completed
Estimated remaining time: 56 minutes

ðŸ”¬ EXPERIMENT 15/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758863514.335187 3238564 service.cc:145] XLA service 0x14bcc98f0510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758863514.335226 3238564 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758863514.727322 3238564 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:36 - loss: 0.3423 - accuracy: 0.5014 - jacard_coef: 0.08172/5 [===========>..................] - ETA: 46s - loss: 0.3116 - accuracy: 0.4190 - jacard_coef: 0.0788 3/5 [=================>............] - ETA: 16s - loss: 0.2822 - accuracy: 0.3351 - jacard_coef: 0.07574/5 [=======================>......] - ETA: 5s - loss: 0.2698 - accuracy: 0.2953 - jacard_coef: 0.0777 5/5 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.2940 - jacard_coef: 0.07405/5 [==============================] - 95s 6s/step - loss: 0.2693 - accuracy: 0.2940 - jacard_coef: 0.0740 - val_loss: 0.6129 - val_accuracy: 0.9303 - val_jacard_coef: 0.0116 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1952 - accuracy: 0.2002 - jacard_coef: 0.08842/5 [===========>..................] - ETA: 2s - loss: 0.1958 - accuracy: 0.1696 - jacard_coef: 0.07473/5 [=================>............] - ETA: 1s - loss: 0.1953 - accuracy: 0.1594 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.1564 - jacard_coef: 0.07655/5 [==============================] - 3s 566ms/step - loss: 0.1941 - accuracy: 0.1569 - jacard_coef: 0.0875 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1866 - accuracy: 0.1669 - jacard_coef: 0.07512/5 [===========>..................] - ETA: 2s - loss: 0.1841 - accuracy: 0.1703 - jacard_coef: 0.07213/5 [=================>............] - ETA: 1s - loss: 0.1825 - accuracy: 0.1872 - jacard_coef: 0.07484/5 [=======================>......] - ETA: 0s - loss: 0.1814 - accuracy: 0.2152 - jacard_coef: 0.07635/5 [==============================] - 3s 567ms/step - loss: 0.1813 - accuracy: 0.2164 - jacard_coef: 0.0807 - val_loss: 1.1221 - val_accuracy: 0.9304 - val_jacard_coef: 1.4612e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1741 - accuracy: 0.4125 - jacard_coef: 0.05922/5 [===========>..................] - ETA: 2s - loss: 0.1736 - accuracy: 0.4047 - jacard_coef: 0.06693/5 [=================>............] - ETA: 1s - loss: 0.1730 - accuracy: 0.4261 - jacard_coef: 0.07284/5 [=======================>......] - ETA: 0s - loss: 0.1727 - accuracy: 0.4478 - jacard_coef: 0.07575/5 [==============================] - 3s 566ms/step - loss: 0.1727 - accuracy: 0.4473 - jacard_coef: 0.0850 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1705 - accuracy: 0.6090 - jacard_coef: 0.06622/5 [===========>..................] - ETA: 2s - loss: 0.1704 - accuracy: 0.5732 - jacard_coef: 0.07913/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.5400 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 0s - loss: 0.1700 - accuracy: 0.5473 - jacard_coef: 0.07675/5 [==============================] - 3s 568ms/step - loss: 0.1704 - accuracy: 0.5457 - jacard_coef: 0.0619 - val_loss: 1.1275 - val_accuracy: 0.9295 - val_jacard_coef: 9.5950e-04 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1917 - accuracy: 0.1241 - jacard_coef: 0.08052/5 [===========>..................] - ETA: 2s - loss: 0.1914 - accuracy: 0.1562 - jacard_coef: 0.07903/5 [=================>............] - ETA: 1s - loss: 0.1905 - accuracy: 0.2162 - jacard_coef: 0.07884/5 [=======================>......] - ETA: 0s - loss: 0.1890 - accuracy: 0.2468 - jacard_coef: 0.07735/5 [==============================] - 3s 583ms/step - loss: 0.1891 - accuracy: 0.2473 - jacard_coef: 0.0684 - val_loss: 1.3476 - val_accuracy: 0.9146 - val_jacard_coef: 0.0192 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1891 - accuracy: 0.0993 - jacard_coef: 0.07382/5 [===========>..................] - ETA: 2s - loss: 0.1898 - accuracy: 0.1051 - jacard_coef: 0.07613/5 [=================>............] - ETA: 1s - loss: 0.1930 - accuracy: 0.1064 - jacard_coef: 0.07734/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.1060 - jacard_coef: 0.07675/5 [==============================] - 3s 566ms/step - loss: 0.1941 - accuracy: 0.1062 - jacard_coef: 0.0721 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1786 - accuracy: 0.3280 - jacard_coef: 0.08272/5 [===========>..................] - ETA: 2s - loss: 0.1823 - accuracy: 0.5423 - jacard_coef: 0.07873/5 [=================>............] - ETA: 1s - loss: 0.1819 - accuracy: 0.6129 - jacard_coef: 0.07424/5 [=======================>......] - ETA: 0s - loss: 0.1799 - accuracy: 0.5983 - jacard_coef: 0.07555/5 [==============================] - 3s 567ms/step - loss: 0.1800 - accuracy: 0.5956 - jacard_coef: 0.0875 - val_loss: 1.1144 - val_accuracy: 0.9304 - val_jacard_coef: 1.4497e-04 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1872 - accuracy: 0.2044 - jacard_coef: 0.08312/5 [===========>..................] - ETA: 2s - loss: 0.1857 - accuracy: 0.2075 - jacard_coef: 0.07923/5 [=================>............] - ETA: 1s - loss: 0.1839 - accuracy: 0.2086 - jacard_coef: 0.07724/5 [=======================>......] - ETA: 0s - loss: 0.1829 - accuracy: 0.2135 - jacard_coef: 0.07795/5 [==============================] - 3s 587ms/step - loss: 0.1828 - accuracy: 0.2129 - jacard_coef: 0.0632 - val_loss: 8.1040 - val_accuracy: 0.2113 - val_jacard_coef: 0.0702 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1762 - accuracy: 0.3260 - jacard_coef: 0.07872/5 [===========>..................] - ETA: 2s - loss: 0.1756 - accuracy: 0.3498 - jacard_coef: 0.07143/5 [=================>............] - ETA: 1s - loss: 0.1749 - accuracy: 0.3914 - jacard_coef: 0.08094/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.4361 - jacard_coef: 0.07585/5 [==============================] - 3s 569ms/step - loss: 0.1740 - accuracy: 0.4357 - jacard_coef: 0.0841 - val_loss: 0.2484 - val_accuracy: 0.5930 - val_jacard_coef: 0.0644 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1676 - accuracy: 0.7183 - jacard_coef: 0.07132/5 [===========>..................] - ETA: 2s - loss: 0.1684 - accuracy: 0.6872 - jacard_coef: 0.07393/5 [=================>............] - ETA: 1s - loss: 0.1684 - accuracy: 0.6585 - jacard_coef: 0.07534/5 [=======================>......] - ETA: 0s - loss: 0.1684 - accuracy: 0.6516 - jacard_coef: 0.07575/5 [==============================] - 3s 569ms/step - loss: 0.1685 - accuracy: 0.6490 - jacard_coef: 0.0875 - val_loss: 0.7962 - val_accuracy: 0.9252 - val_jacard_coef: 0.0223 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1704 - accuracy: 0.5352 - jacard_coef: 0.09192/5 [===========>..................] - ETA: 2s - loss: 0.1712 - accuracy: 0.4401 - jacard_coef: 0.07493/5 [=================>............] - ETA: 1s - loss: 0.1750 - accuracy: 0.3669 - jacard_coef: 0.07434/5 [=======================>......] - ETA: 0s - loss: 0.1751 - accuracy: 0.3422 - jacard_coef: 0.07575/5 [==============================] - 3s 571ms/step - loss: 0.1751 - accuracy: 0.3424 - jacard_coef: 0.0917 - val_loss: 0.8995 - val_accuracy: 0.9303 - val_jacard_coef: 0.0117 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1722 - accuracy: 0.3672 - jacard_coef: 0.07232/5 [===========>..................] - ETA: 2s - loss: 0.1708 - accuracy: 0.4019 - jacard_coef: 0.07453/5 [=================>............] - ETA: 1s - loss: 0.1703 - accuracy: 0.4400 - jacard_coef: 0.07714/5 [=======================>......] - ETA: 0s - loss: 0.1695 - accuracy: 0.4849 - jacard_coef: 0.07695/5 [==============================] - 3s 581ms/step - loss: 0.1695 - accuracy: 0.4840 - jacard_coef: 0.0633 - val_loss: 0.6061 - val_accuracy: 0.9237 - val_jacard_coef: 0.0137 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1660 - accuracy: 0.6564 - jacard_coef: 0.06772/5 [===========>..................] - ETA: 2s - loss: 0.1660 - accuracy: 0.6697 - jacard_coef: 0.07043/5 [=================>............] - ETA: 1s - loss: 0.1656 - accuracy: 0.6783 - jacard_coef: 0.07764/5 [=======================>......] - ETA: 0s - loss: 0.1653 - accuracy: 0.6802 - jacard_coef: 0.07525/5 [==============================] - 3s 581ms/step - loss: 0.1656 - accuracy: 0.6787 - jacard_coef: 0.0909 - val_loss: 1.0418 - val_accuracy: 0.9302 - val_jacard_coef: 0.0028 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.5857 - jacard_coef: 0.06882/5 [===========>..................] - ETA: 2s - loss: 0.1685 - accuracy: 0.5848 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1686 - accuracy: 0.5819 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 0s - loss: 0.1687 - accuracy: 0.5713 - jacard_coef: 0.07555/5 [==============================] - 3s 581ms/step - loss: 0.1687 - accuracy: 0.5699 - jacard_coef: 0.0912 - val_loss: 0.9450 - val_accuracy: 0.9263 - val_jacard_coef: 0.0159 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.6671 - jacard_coef: 0.08252/5 [===========>..................] - ETA: 2s - loss: 0.1687 - accuracy: 0.6815 - jacard_coef: 0.07873/5 [=================>............] - ETA: 1s - loss: 0.1688 - accuracy: 0.6296 - jacard_coef: 0.07574/5 [=======================>......] - ETA: 0s - loss: 0.1689 - accuracy: 0.6749 - jacard_coef: 0.07665/5 [==============================] - 3s 579ms/step - loss: 0.1689 - accuracy: 0.6731 - jacard_coef: 0.0718 - val_loss: 0.6218 - val_accuracy: 0.9085 - val_jacard_coef: 0.0373 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1684 - accuracy: 0.7954 - jacard_coef: 0.09302/5 [===========>..................] - ETA: 2s - loss: 0.1674 - accuracy: 0.8071 - jacard_coef: 0.08083/5 [=================>............] - ETA: 1s - loss: 0.1681 - accuracy: 0.7821 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1678 - accuracy: 0.7797 - jacard_coef: 0.07685/5 [==============================] - 3s 580ms/step - loss: 0.1678 - accuracy: 0.7792 - jacard_coef: 0.0623 - val_loss: 0.5456 - val_accuracy: 0.9169 - val_jacard_coef: 0.0345 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.7756 - jacard_coef: 0.08122/5 [===========>..................] - ETA: 2s - loss: 0.1658 - accuracy: 0.8075 - jacard_coef: 0.07223/5 [=================>............] - ETA: 1s - loss: 0.1661 - accuracy: 0.8077 - jacard_coef: 0.07254/5 [=======================>......] - ETA: 0s - loss: 0.1659 - accuracy: 0.8111 - jacard_coef: 0.07595/5 [==============================] - 3s 580ms/step - loss: 0.1659 - accuracy: 0.8093 - jacard_coef: 0.0868 - val_loss: 0.1676 - val_accuracy: 0.9047 - val_jacard_coef: 0.0518 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8523 - jacard_coef: 0.08062/5 [===========>..................] - ETA: 2s - loss: 0.1651 - accuracy: 0.8624 - jacard_coef: 0.07783/5 [=================>............] - ETA: 1s - loss: 0.1645 - accuracy: 0.8761 - jacard_coef: 0.07464/5 [=======================>......] - ETA: 0s - loss: 0.1643 - accuracy: 0.8807 - jacard_coef: 0.07595/5 [==============================] - 3s 580ms/step - loss: 0.1644 - accuracy: 0.8780 - jacard_coef: 0.0857 - val_loss: 0.1169 - val_accuracy: 0.8736 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0702 (epoch 9)
  Final Val Loss: 0.1169
  Training Time: 0:02:29.685945
  Stability (std): 0.3146

Results saved to: hyperparameter_optimization_20250926_123742/exp_15_Attention_UNet_lr1e-4_bs32/Attention_UNet_lr0.0001_bs32_results.json

Experiment 15 completed in 183s
Progress: 15/36 completed
Estimated remaining time: 64 minutes

ðŸ”¬ EXPERIMENT 16/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758863692.113027 3245091 service.cc:145] XLA service 0x153e3cd44f10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758863692.113103 3245091 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758863692.578746 3245091 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 14:41 - loss: 0.3385 - accuracy: 0.4956 - jacard_coef: 0.1094 2/17 [==>...........................] - ETA: 1:08 - loss: 0.3105 - accuracy: 0.4105 - jacard_coef: 0.0932  3/17 [====>.........................] - ETA: 33s - loss: 0.2882 - accuracy: 0.3658 - jacard_coef: 0.0919  4/17 [======>.......................] - ETA: 21s - loss: 0.2728 - accuracy: 0.3314 - jacard_coef: 0.0934 5/17 [=======>......................] - ETA: 15s - loss: 0.2603 - accuracy: 0.2967 - jacard_coef: 0.0910 6/17 [=========>....................] - ETA: 11s - loss: 0.2500 - accuracy: 0.2726 - jacard_coef: 0.0903 7/17 [===========>..................] - ETA: 9s - loss: 0.2426 - accuracy: 0.2517 - jacard_coef: 0.0858  8/17 [=============>................] - ETA: 7s - loss: 0.2361 - accuracy: 0.2384 - jacard_coef: 0.0841 9/17 [==============>...............] - ETA: 5s - loss: 0.2310 - accuracy: 0.2275 - jacard_coef: 0.083210/17 [================>.............] - ETA: 4s - loss: 0.2266 - accuracy: 0.2191 - jacard_coef: 0.083011/17 [==================>...........] - ETA: 3s - loss: 0.2230 - accuracy: 0.2098 - jacard_coef: 0.081112/17 [====================>.........] - ETA: 2s - loss: 0.2198 - accuracy: 0.2024 - jacard_coef: 0.078513/17 [=====================>........] - ETA: 2s - loss: 0.2164 - accuracy: 0.2203 - jacard_coef: 0.077214/17 [=======================>......] - ETA: 1s - loss: 0.2185 - accuracy: 0.2270 - jacard_coef: 0.076815/17 [=========================>....] - ETA: 0s - loss: 0.2177 - accuracy: 0.2317 - jacard_coef: 0.076416/17 [===========================>..] - ETA: 0s - loss: 0.2186 - accuracy: 0.2285 - jacard_coef: 0.077217/17 [==============================] - ETA: 0s - loss: 0.2187 - accuracy: 0.2276 - jacard_coef: 0.074817/17 [==============================] - 69s 892ms/step - loss: 0.2187 - accuracy: 0.2276 - jacard_coef: 0.0748 - val_loss: 0.1925 - val_accuracy: 0.3764 - val_jacard_coef: 0.0574 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.2043 - accuracy: 0.1391 - jacard_coef: 0.0552 2/17 [==>...........................] - ETA: 2s - loss: 0.1973 - accuracy: 0.2252 - jacard_coef: 0.0797 3/17 [====>.........................] - ETA: 2s - loss: 0.2156 - accuracy: 0.2395 - jacard_coef: 0.0750 4/17 [======>.......................] - ETA: 2s - loss: 0.2192 - accuracy: 0.2159 - jacard_coef: 0.0661 5/17 [=======>......................] - ETA: 2s - loss: 0.2180 - accuracy: 0.2037 - jacard_coef: 0.0693 6/17 [=========>....................] - ETA: 2s - loss: 0.2140 - accuracy: 0.1896 - jacard_coef: 0.0689 7/17 [===========>..................] - ETA: 1s - loss: 0.2156 - accuracy: 0.1844 - jacard_coef: 0.0729 8/17 [=============>................] - ETA: 1s - loss: 0.2152 - accuracy: 0.1832 - jacard_coef: 0.0748 9/17 [==============>...............] - ETA: 1s - loss: 0.2172 - accuracy: 0.1830 - jacard_coef: 0.072110/17 [================>.............] - ETA: 1s - loss: 0.2148 - accuracy: 0.1884 - jacard_coef: 0.075411/17 [==================>...........] - ETA: 1s - loss: 0.2132 - accuracy: 0.1887 - jacard_coef: 0.074012/17 [====================>.........] - ETA: 0s - loss: 0.2114 - accuracy: 0.1863 - jacard_coef: 0.073813/17 [=====================>........] - ETA: 0s - loss: 0.2094 - accuracy: 0.1905 - jacard_coef: 0.074614/17 [=======================>......] - ETA: 0s - loss: 0.2079 - accuracy: 0.1913 - jacard_coef: 0.074915/17 [=========================>....] - ETA: 0s - loss: 0.2060 - accuracy: 0.1982 - jacard_coef: 0.076616/17 [===========================>..] - ETA: 0s - loss: 0.2051 - accuracy: 0.2068 - jacard_coef: 0.076617/17 [==============================] - 3s 184ms/step - loss: 0.2050 - accuracy: 0.2063 - jacard_coef: 0.0724 - val_loss: 1.1816 - val_accuracy: 0.9266 - val_jacard_coef: 3.1889e-05 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1864 - accuracy: 0.2404 - jacard_coef: 0.0689 2/17 [==>...........................] - ETA: 2s - loss: 0.1822 - accuracy: 0.2958 - jacard_coef: 0.0744 3/17 [====>.........................] - ETA: 2s - loss: 0.1798 - accuracy: 0.3459 - jacard_coef: 0.0690 4/17 [======>.......................] - ETA: 2s - loss: 0.1802 - accuracy: 0.3771 - jacard_coef: 0.0721 5/17 [=======>......................] - ETA: 2s - loss: 0.1795 - accuracy: 0.3781 - jacard_coef: 0.0680 6/17 [=========>....................] - ETA: 2s - loss: 0.1796 - accuracy: 0.3999 - jacard_coef: 0.0705 7/17 [===========>..................] - ETA: 1s - loss: 0.1818 - accuracy: 0.3835 - jacard_coef: 0.0740 8/17 [=============>................] - ETA: 1s - loss: 0.1819 - accuracy: 0.3596 - jacard_coef: 0.0721 9/17 [==============>...............] - ETA: 1s - loss: 0.1819 - accuracy: 0.3401 - jacard_coef: 0.072310/17 [================>.............] - ETA: 1s - loss: 0.1816 - accuracy: 0.3282 - jacard_coef: 0.071511/17 [==================>...........] - ETA: 1s - loss: 0.1819 - accuracy: 0.3216 - jacard_coef: 0.071912/17 [====================>.........] - ETA: 0s - loss: 0.1820 - accuracy: 0.3159 - jacard_coef: 0.072913/17 [=====================>........] - ETA: 0s - loss: 0.1817 - accuracy: 0.3168 - jacard_coef: 0.072214/17 [=======================>......] - ETA: 0s - loss: 0.1822 - accuracy: 0.3237 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1819 - accuracy: 0.3326 - jacard_coef: 0.076616/17 [===========================>..] - ETA: 0s - loss: 0.1818 - accuracy: 0.3411 - jacard_coef: 0.075517/17 [==============================] - 3s 189ms/step - loss: 0.1818 - accuracy: 0.3424 - jacard_coef: 0.0789 - val_loss: 14.5075 - val_accuracy: 0.0710 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1705 - accuracy: 0.5801 - jacard_coef: 0.0387 2/17 [==>...........................] - ETA: 2s - loss: 0.1737 - accuracy: 0.4834 - jacard_coef: 0.0395 3/17 [====>.........................] - ETA: 2s - loss: 0.1724 - accuracy: 0.5912 - jacard_coef: 0.0452 4/17 [======>.......................] - ETA: 2s - loss: 0.1723 - accuracy: 0.5782 - jacard_coef: 0.0510 5/17 [=======>......................] - ETA: 2s - loss: 0.1722 - accuracy: 0.5910 - jacard_coef: 0.0555 6/17 [=========>....................] - ETA: 2s - loss: 0.1724 - accuracy: 0.5954 - jacard_coef: 0.0621 7/17 [===========>..................] - ETA: 1s - loss: 0.1725 - accuracy: 0.5677 - jacard_coef: 0.0661 8/17 [=============>................] - ETA: 1s - loss: 0.1729 - accuracy: 0.5642 - jacard_coef: 0.0684 9/17 [==============>...............] - ETA: 1s - loss: 0.1737 - accuracy: 0.5797 - jacard_coef: 0.071410/17 [================>.............] - ETA: 1s - loss: 0.1735 - accuracy: 0.5961 - jacard_coef: 0.071811/17 [==================>...........] - ETA: 1s - loss: 0.1737 - accuracy: 0.5837 - jacard_coef: 0.073612/17 [====================>.........] - ETA: 0s - loss: 0.1733 - accuracy: 0.6006 - jacard_coef: 0.076313/17 [=====================>........] - ETA: 0s - loss: 0.1729 - accuracy: 0.6162 - jacard_coef: 0.076014/17 [=======================>......] - ETA: 0s - loss: 0.1723 - accuracy: 0.6328 - jacard_coef: 0.074615/17 [=========================>....] - ETA: 0s - loss: 0.1718 - accuracy: 0.6454 - jacard_coef: 0.075216/17 [===========================>..] - ETA: 0s - loss: 0.1716 - accuracy: 0.6529 - jacard_coef: 0.075917/17 [==============================] - 3s 185ms/step - loss: 0.1716 - accuracy: 0.6528 - jacard_coef: 0.0731 - val_loss: 1.1206 - val_accuracy: 0.9257 - val_jacard_coef: 2.8626e-04 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1633 - accuracy: 0.9047 - jacard_coef: 0.0563 2/17 [==>...........................] - ETA: 2s - loss: 0.1645 - accuracy: 0.8375 - jacard_coef: 0.0691 3/17 [====>.........................] - ETA: 2s - loss: 0.1642 - accuracy: 0.8328 - jacard_coef: 0.0741 4/17 [======>.......................] - ETA: 2s - loss: 0.1639 - accuracy: 0.7963 - jacard_coef: 0.0728 5/17 [=======>......................] - ETA: 2s - loss: 0.1649 - accuracy: 0.7484 - jacard_coef: 0.0733 6/17 [=========>....................] - ETA: 2s - loss: 0.1659 - accuracy: 0.7617 - jacard_coef: 0.0734 7/17 [===========>..................] - ETA: 1s - loss: 0.1657 - accuracy: 0.7755 - jacard_coef: 0.0741 8/17 [=============>................] - ETA: 1s - loss: 0.1651 - accuracy: 0.7914 - jacard_coef: 0.0720 9/17 [==============>...............] - ETA: 1s - loss: 0.1649 - accuracy: 0.7719 - jacard_coef: 0.070810/17 [================>.............] - ETA: 1s - loss: 0.1646 - accuracy: 0.7821 - jacard_coef: 0.071211/17 [==================>...........] - ETA: 1s - loss: 0.1643 - accuracy: 0.7952 - jacard_coef: 0.069712/17 [====================>.........] - ETA: 0s - loss: 0.1639 - accuracy: 0.8021 - jacard_coef: 0.070113/17 [=====================>........] - ETA: 0s - loss: 0.1640 - accuracy: 0.7994 - jacard_coef: 0.071614/17 [=======================>......] - ETA: 0s - loss: 0.1639 - accuracy: 0.8045 - jacard_coef: 0.072615/17 [=========================>....] - ETA: 0s - loss: 0.1639 - accuracy: 0.8061 - jacard_coef: 0.076416/17 [===========================>..] - ETA: 0s - loss: 0.1638 - accuracy: 0.8132 - jacard_coef: 0.075817/17 [==============================] - 3s 184ms/step - loss: 0.1638 - accuracy: 0.8128 - jacard_coef: 0.0728 - val_loss: 0.2997 - val_accuracy: 0.8883 - val_jacard_coef: 0.0411 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1652 - accuracy: 0.9161 - jacard_coef: 0.0719 2/17 [==>...........................] - ETA: 2s - loss: 0.1632 - accuracy: 0.9099 - jacard_coef: 0.0793 3/17 [====>.........................] - ETA: 2s - loss: 0.1665 - accuracy: 0.9101 - jacard_coef: 0.0784 4/17 [======>.......................] - ETA: 2s - loss: 0.1647 - accuracy: 0.9095 - jacard_coef: 0.0799 5/17 [=======>......................] - ETA: 2s - loss: 0.1640 - accuracy: 0.9027 - jacard_coef: 0.0859 6/17 [=========>....................] - ETA: 2s - loss: 0.1638 - accuracy: 0.9007 - jacard_coef: 0.0878 7/17 [===========>..................] - ETA: 1s - loss: 0.1633 - accuracy: 0.9019 - jacard_coef: 0.0861 8/17 [=============>................] - ETA: 1s - loss: 0.1628 - accuracy: 0.9007 - jacard_coef: 0.0847 9/17 [==============>...............] - ETA: 1s - loss: 0.1624 - accuracy: 0.8960 - jacard_coef: 0.086610/17 [================>.............] - ETA: 1s - loss: 0.1620 - accuracy: 0.8961 - jacard_coef: 0.084911/17 [==================>...........] - ETA: 1s - loss: 0.1623 - accuracy: 0.8925 - jacard_coef: 0.083412/17 [====================>.........] - ETA: 0s - loss: 0.1653 - accuracy: 0.8517 - jacard_coef: 0.081113/17 [=====================>........] - ETA: 0s - loss: 0.1646 - accuracy: 0.8575 - jacard_coef: 0.079514/17 [=======================>......] - ETA: 0s - loss: 0.1646 - accuracy: 0.8597 - jacard_coef: 0.077915/17 [=========================>....] - ETA: 0s - loss: 0.1654 - accuracy: 0.8618 - jacard_coef: 0.076516/17 [===========================>..] - ETA: 0s - loss: 0.1660 - accuracy: 0.8618 - jacard_coef: 0.074817/17 [==============================] - 3s 185ms/step - loss: 0.1661 - accuracy: 0.8574 - jacard_coef: 0.0794 - val_loss: 1.1085 - val_accuracy: 0.9294 - val_jacard_coef: 0.0023 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1669 - accuracy: 0.7690 - jacard_coef: 0.0395 2/17 [==>...........................] - ETA: 2s - loss: 0.1678 - accuracy: 0.7555 - jacard_coef: 0.0602 3/17 [====>.........................] - ETA: 2s - loss: 0.1684 - accuracy: 0.7506 - jacard_coef: 0.0578 4/17 [======>.......................] - ETA: 2s - loss: 0.1748 - accuracy: 0.7759 - jacard_coef: 0.0625 5/17 [=======>......................] - ETA: 2s - loss: 0.1748 - accuracy: 0.7910 - jacard_coef: 0.0601 6/17 [=========>....................] - ETA: 2s - loss: 0.1730 - accuracy: 0.8051 - jacard_coef: 0.0637 7/17 [===========>..................] - ETA: 1s - loss: 0.1755 - accuracy: 0.8110 - jacard_coef: 0.0689 8/17 [=============>................] - ETA: 1s - loss: 0.1745 - accuracy: 0.8137 - jacard_coef: 0.0679 9/17 [==============>...............] - ETA: 1s - loss: 0.1733 - accuracy: 0.8211 - jacard_coef: 0.067710/17 [================>.............] - ETA: 1s - loss: 0.1723 - accuracy: 0.8258 - jacard_coef: 0.069511/17 [==================>...........] - ETA: 1s - loss: 0.1713 - accuracy: 0.8253 - jacard_coef: 0.073612/17 [====================>.........] - ETA: 0s - loss: 0.1702 - accuracy: 0.8294 - jacard_coef: 0.074613/17 [=====================>........] - ETA: 0s - loss: 0.1692 - accuracy: 0.8342 - jacard_coef: 0.073914/17 [=======================>......] - ETA: 0s - loss: 0.1686 - accuracy: 0.8236 - jacard_coef: 0.074815/17 [=========================>....] - ETA: 0s - loss: 0.1679 - accuracy: 0.8285 - jacard_coef: 0.074216/17 [===========================>..] - ETA: 0s - loss: 0.1671 - accuracy: 0.8325 - jacard_coef: 0.074417/17 [==============================] - 3s 185ms/step - loss: 0.1672 - accuracy: 0.8305 - jacard_coef: 0.0788 - val_loss: 1.1184 - val_accuracy: 0.9265 - val_jacard_coef: 1.9657e-04 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1710 - accuracy: 0.5883 - jacard_coef: 0.0530 2/17 [==>...........................] - ETA: 2s - loss: 0.1647 - accuracy: 0.6731 - jacard_coef: 0.0599 3/17 [====>.........................] - ETA: 2s - loss: 0.1655 - accuracy: 0.6486 - jacard_coef: 0.0668 4/17 [======>.......................] - ETA: 2s - loss: 0.1625 - accuracy: 0.7166 - jacard_coef: 0.0672 5/17 [=======>......................] - ETA: 2s - loss: 0.1615 - accuracy: 0.7515 - jacard_coef: 0.0732 6/17 [=========>....................] - ETA: 2s - loss: 0.1619 - accuracy: 0.7769 - jacard_coef: 0.0752 7/17 [===========>..................] - ETA: 1s - loss: 0.1613 - accuracy: 0.7951 - jacard_coef: 0.0741 8/17 [=============>................] - ETA: 1s - loss: 0.1605 - accuracy: 0.8043 - jacard_coef: 0.0778 9/17 [==============>...............] - ETA: 1s - loss: 0.1607 - accuracy: 0.8105 - jacard_coef: 0.077110/17 [================>.............] - ETA: 1s - loss: 0.1599 - accuracy: 0.8205 - jacard_coef: 0.075111/17 [==================>...........] - ETA: 1s - loss: 0.1594 - accuracy: 0.8245 - jacard_coef: 0.077412/17 [====================>.........] - ETA: 0s - loss: 0.1590 - accuracy: 0.8273 - jacard_coef: 0.075513/17 [=====================>........] - ETA: 0s - loss: 0.1591 - accuracy: 0.8327 - jacard_coef: 0.076414/17 [=======================>......] - ETA: 0s - loss: 0.1586 - accuracy: 0.8363 - jacard_coef: 0.076215/17 [=========================>....] - ETA: 0s - loss: 0.1591 - accuracy: 0.8229 - jacard_coef: 0.076816/17 [===========================>..] - ETA: 0s - loss: 0.1591 - accuracy: 0.8280 - jacard_coef: 0.075817/17 [==============================] - 3s 189ms/step - loss: 0.1591 - accuracy: 0.8288 - jacard_coef: 0.0716 - val_loss: 0.6211 - val_accuracy: 0.2409 - val_jacard_coef: 0.0614 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1657 - accuracy: 0.8492 - jacard_coef: 0.1101 2/17 [==>...........................] - ETA: 2s - loss: 0.1652 - accuracy: 0.8779 - jacard_coef: 0.0813 3/17 [====>.........................] - ETA: 2s - loss: 0.1661 - accuracy: 0.8780 - jacard_coef: 0.0827 4/17 [======>.......................] - ETA: 2s - loss: 0.1686 - accuracy: 0.8833 - jacard_coef: 0.0780 5/17 [=======>......................] - ETA: 2s - loss: 0.1686 - accuracy: 0.8899 - jacard_coef: 0.0735 6/17 [=========>....................] - ETA: 2s - loss: 0.1701 - accuracy: 0.8941 - jacard_coef: 0.0731 7/17 [===========>..................] - ETA: 1s - loss: 0.1703 - accuracy: 0.8935 - jacard_coef: 0.0762 8/17 [=============>................] - ETA: 1s - loss: 0.1725 - accuracy: 0.8959 - jacard_coef: 0.0760 9/17 [==============>...............] - ETA: 1s - loss: 0.1719 - accuracy: 0.8960 - jacard_coef: 0.077610/17 [================>.............] - ETA: 1s - loss: 0.1715 - accuracy: 0.9024 - jacard_coef: 0.073511/17 [==================>...........] - ETA: 1s - loss: 0.1707 - accuracy: 0.9055 - jacard_coef: 0.071712/17 [====================>.........] - ETA: 0s - loss: 0.1700 - accuracy: 0.9044 - jacard_coef: 0.073713/17 [=====================>........] - ETA: 0s - loss: 0.1697 - accuracy: 0.9030 - jacard_coef: 0.075814/17 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.9051 - jacard_coef: 0.074615/17 [=========================>....] - ETA: 0s - loss: 0.1686 - accuracy: 0.9066 - jacard_coef: 0.074016/17 [===========================>..] - ETA: 0s - loss: 0.1679 - accuracy: 0.9060 - jacard_coef: 0.075117/17 [==============================] - 3s 189ms/step - loss: 0.1678 - accuracy: 0.9058 - jacard_coef: 0.0762 - val_loss: 0.1225 - val_accuracy: 0.9202 - val_jacard_coef: 0.0609 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1610 - accuracy: 0.9093 - jacard_coef: 0.0799 2/17 [==>...........................] - ETA: 2s - loss: 0.1669 - accuracy: 0.9224 - jacard_coef: 0.0674 3/17 [====>.........................] - ETA: 2s - loss: 0.1639 - accuracy: 0.9213 - jacard_coef: 0.0685 4/17 [======>.......................] - ETA: 2s - loss: 0.1616 - accuracy: 0.9183 - jacard_coef: 0.0705 5/17 [=======>......................] - ETA: 2s - loss: 0.1595 - accuracy: 0.9165 - jacard_coef: 0.0710 6/17 [=========>....................] - ETA: 2s - loss: 0.1581 - accuracy: 0.9148 - jacard_coef: 0.0700 7/17 [===========>..................] - ETA: 1s - loss: 0.1582 - accuracy: 0.9164 - jacard_coef: 0.0687 8/17 [=============>................] - ETA: 1s - loss: 0.1577 - accuracy: 0.9129 - jacard_coef: 0.0718 9/17 [==============>...............] - ETA: 1s - loss: 0.1583 - accuracy: 0.9104 - jacard_coef: 0.072810/17 [================>.............] - ETA: 1s - loss: 0.1575 - accuracy: 0.9101 - jacard_coef: 0.071211/17 [==================>...........] - ETA: 1s - loss: 0.1567 - accuracy: 0.9084 - jacard_coef: 0.070912/17 [====================>.........] - ETA: 0s - loss: 0.1560 - accuracy: 0.9058 - jacard_coef: 0.071813/17 [=====================>........] - ETA: 0s - loss: 0.1559 - accuracy: 0.9021 - jacard_coef: 0.074814/17 [=======================>......] - ETA: 0s - loss: 0.1554 - accuracy: 0.9011 - jacard_coef: 0.075715/17 [=========================>....] - ETA: 0s - loss: 0.1552 - accuracy: 0.8947 - jacard_coef: 0.075816/17 [===========================>..] - ETA: 0s - loss: 0.1548 - accuracy: 0.8909 - jacard_coef: 0.075817/17 [==============================] - 3s 190ms/step - loss: 0.1547 - accuracy: 0.8899 - jacard_coef: 0.0715 - val_loss: 0.1653 - val_accuracy: 0.5677 - val_jacard_coef: 0.0628 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1561 - accuracy: 0.8358 - jacard_coef: 0.0665 2/17 [==>...........................] - ETA: 2s - loss: 0.1509 - accuracy: 0.8815 - jacard_coef: 0.0659 3/17 [====>.........................] - ETA: 2s - loss: 0.1507 - accuracy: 0.8829 - jacard_coef: 0.0767 4/17 [======>.......................] - ETA: 2s - loss: 0.1500 - accuracy: 0.8903 - jacard_coef: 0.0766 5/17 [=======>......................] - ETA: 2s - loss: 0.1522 - accuracy: 0.8955 - jacard_coef: 0.0757 6/17 [=========>....................] - ETA: 2s - loss: 0.1518 - accuracy: 0.8988 - jacard_coef: 0.0753 7/17 [===========>..................] - ETA: 1s - loss: 0.1522 - accuracy: 0.9006 - jacard_coef: 0.0757 8/17 [=============>................] - ETA: 1s - loss: 0.1519 - accuracy: 0.9036 - jacard_coef: 0.0741 9/17 [==============>...............] - ETA: 1s - loss: 0.1517 - accuracy: 0.9053 - jacard_coef: 0.073610/17 [================>.............] - ETA: 1s - loss: 0.1517 - accuracy: 0.9039 - jacard_coef: 0.075511/17 [==================>...........] - ETA: 1s - loss: 0.1512 - accuracy: 0.9054 - jacard_coef: 0.074912/17 [====================>.........] - ETA: 0s - loss: 0.1509 - accuracy: 0.9071 - jacard_coef: 0.074313/17 [=====================>........] - ETA: 0s - loss: 0.1506 - accuracy: 0.9082 - jacard_coef: 0.074114/17 [=======================>......] - ETA: 0s - loss: 0.1503 - accuracy: 0.9078 - jacard_coef: 0.075015/17 [=========================>....] - ETA: 0s - loss: 0.1504 - accuracy: 0.9077 - jacard_coef: 0.075516/17 [===========================>..] - ETA: 0s - loss: 0.1502 - accuracy: 0.9079 - jacard_coef: 0.075817/17 [==============================] - 3s 189ms/step - loss: 0.1501 - accuracy: 0.9083 - jacard_coef: 0.0717 - val_loss: 0.1351 - val_accuracy: 0.9234 - val_jacard_coef: 0.0632 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1446 - accuracy: 0.9271 - jacard_coef: 0.0656 2/17 [==>...........................] - ETA: 2s - loss: 0.1441 - accuracy: 0.9322 - jacard_coef: 0.0616 3/17 [====>.........................] - ETA: 2s - loss: 0.1447 - accuracy: 0.9230 - jacard_coef: 0.0695 4/17 [======>.......................] - ETA: 2s - loss: 0.1442 - accuracy: 0.9280 - jacard_coef: 0.0654 5/17 [=======>......................] - ETA: 2s - loss: 0.1443 - accuracy: 0.9224 - jacard_coef: 0.0699 6/17 [=========>....................] - ETA: 2s - loss: 0.1441 - accuracy: 0.9224 - jacard_coef: 0.0701 7/17 [===========>..................] - ETA: 1s - loss: 0.1435 - accuracy: 0.9264 - jacard_coef: 0.0663 8/17 [=============>................] - ETA: 1s - loss: 0.1437 - accuracy: 0.9235 - jacard_coef: 0.0682 9/17 [==============>...............] - ETA: 1s - loss: 0.1438 - accuracy: 0.9227 - jacard_coef: 0.068710/17 [================>.............] - ETA: 1s - loss: 0.1438 - accuracy: 0.9211 - jacard_coef: 0.069611/17 [==================>...........] - ETA: 1s - loss: 0.1439 - accuracy: 0.9178 - jacard_coef: 0.072112/17 [====================>.........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9137 - jacard_coef: 0.075513/17 [=====================>........] - ETA: 0s - loss: 0.1443 - accuracy: 0.9141 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1442 - accuracy: 0.9149 - jacard_coef: 0.072815/17 [=========================>....] - ETA: 0s - loss: 0.1441 - accuracy: 0.9142 - jacard_coef: 0.072416/17 [===========================>..] - ETA: 0s - loss: 0.1442 - accuracy: 0.9112 - jacard_coef: 0.074517/17 [==============================] - 3s 190ms/step - loss: 0.1447 - accuracy: 0.9084 - jacard_coef: 0.0776 - val_loss: 0.1538 - val_accuracy: 0.9304 - val_jacard_coef: 0.0630 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1451 - accuracy: 0.9099 - jacard_coef: 0.0788 2/17 [==>...........................] - ETA: 2s - loss: 0.1452 - accuracy: 0.9205 - jacard_coef: 0.0639 3/17 [====>.........................] - ETA: 2s - loss: 0.1467 - accuracy: 0.9143 - jacard_coef: 0.0686 4/17 [======>.......................] - ETA: 2s - loss: 0.1486 - accuracy: 0.9066 - jacard_coef: 0.0732 5/17 [=======>......................] - ETA: 2s - loss: 0.1532 - accuracy: 0.9035 - jacard_coef: 0.0758 6/17 [=========>....................] - ETA: 2s - loss: 0.1537 - accuracy: 0.9048 - jacard_coef: 0.0746 7/17 [===========>..................] - ETA: 1s - loss: 0.1542 - accuracy: 0.9043 - jacard_coef: 0.0750 8/17 [=============>................] - ETA: 1s - loss: 0.1543 - accuracy: 0.9016 - jacard_coef: 0.0774 9/17 [==============>...............] - ETA: 1s - loss: 0.1555 - accuracy: 0.9014 - jacard_coef: 0.077810/17 [================>.............] - ETA: 1s - loss: 0.1556 - accuracy: 0.9013 - jacard_coef: 0.078411/17 [==================>...........] - ETA: 1s - loss: 0.1569 - accuracy: 0.9020 - jacard_coef: 0.077112/17 [====================>.........] - ETA: 0s - loss: 0.1567 - accuracy: 0.9041 - jacard_coef: 0.075813/17 [=====================>........] - ETA: 0s - loss: 0.1576 - accuracy: 0.9035 - jacard_coef: 0.076914/17 [=======================>......] - ETA: 0s - loss: 0.1574 - accuracy: 0.9035 - jacard_coef: 0.077415/17 [=========================>....] - ETA: 0s - loss: 0.1573 - accuracy: 0.9053 - jacard_coef: 0.076316/17 [===========================>..] - ETA: 0s - loss: 0.1569 - accuracy: 0.9064 - jacard_coef: 0.075617/17 [==============================] - 3s 190ms/step - loss: 0.1569 - accuracy: 0.9056 - jacard_coef: 0.0744 - val_loss: 0.9763 - val_accuracy: 0.9303 - val_jacard_coef: 0.0213 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0682 (epoch 3)
  Final Val Loss: 0.9763
  Training Time: 0:01:48.644429
  Stability (std): 0.4312

Results saved to: hyperparameter_optimization_20250926_123742/exp_16_Attention_UNet_lr5e-4_bs8/Attention_UNet_lr0.0005_bs8_results.json

Experiment 16 completed in 144s
Progress: 16/36 completed
Estimated remaining time: 48 minutes

ðŸ”¬ EXPERIMENT 17/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758863839.695421 3251268 service.cc:145] XLA service 0x152d21d8f870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758863839.695458 3251268 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758863840.157135 3251268 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 8:00 - loss: 0.3436 - accuracy: 0.5018 - jacard_coef: 0.09912/9 [=====>........................] - ETA: 59s - loss: 0.3104 - accuracy: 0.4313 - jacard_coef: 0.0943 3/9 [=========>....................] - ETA: 26s - loss: 0.2871 - accuracy: 0.3474 - jacard_coef: 0.08214/9 [============>.................] - ETA: 15s - loss: 0.2707 - accuracy: 0.3204 - jacard_coef: 0.08135/9 [===============>..............] - ETA: 9s - loss: 0.2593 - accuracy: 0.3039 - jacard_coef: 0.0797 6/9 [===================>..........] - ETA: 5s - loss: 0.2494 - accuracy: 0.2992 - jacard_coef: 0.07917/9 [======================>.......] - ETA: 3s - loss: 0.2404 - accuracy: 0.3029 - jacard_coef: 0.07948/9 [=========================>....] - ETA: 1s - loss: 0.2325 - accuracy: 0.3153 - jacard_coef: 0.07659/9 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.3148 - jacard_coef: 0.07429/9 [==============================] - 79s 2s/step - loss: 0.2321 - accuracy: 0.3148 - jacard_coef: 0.0742 - val_loss: 0.2286 - val_accuracy: 0.9304 - val_jacard_coef: 0.0536 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.2584 - accuracy: 0.2974 - jacard_coef: 0.05882/9 [=====>........................] - ETA: 2s - loss: 0.2421 - accuracy: 0.2299 - jacard_coef: 0.05743/9 [=========>....................] - ETA: 2s - loss: 0.2275 - accuracy: 0.1941 - jacard_coef: 0.06524/9 [============>.................] - ETA: 1s - loss: 0.2221 - accuracy: 0.1768 - jacard_coef: 0.07315/9 [===============>..............] - ETA: 1s - loss: 0.2157 - accuracy: 0.1640 - jacard_coef: 0.07426/9 [===================>..........] - ETA: 1s - loss: 0.2109 - accuracy: 0.1570 - jacard_coef: 0.07437/9 [======================>.......] - ETA: 0s - loss: 0.2073 - accuracy: 0.1514 - jacard_coef: 0.07408/9 [=========================>....] - ETA: 0s - loss: 0.2042 - accuracy: 0.1535 - jacard_coef: 0.07659/9 [==============================] - 3s 334ms/step - loss: 0.2040 - accuracy: 0.1536 - jacard_coef: 0.0809 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1791 - accuracy: 0.4192 - jacard_coef: 0.08332/9 [=====>........................] - ETA: 2s - loss: 0.1793 - accuracy: 0.4142 - jacard_coef: 0.07973/9 [=========>....................] - ETA: 2s - loss: 0.1783 - accuracy: 0.4384 - jacard_coef: 0.07924/9 [============>.................] - ETA: 1s - loss: 0.1775 - accuracy: 0.4537 - jacard_coef: 0.08035/9 [===============>..............] - ETA: 1s - loss: 0.1772 - accuracy: 0.4525 - jacard_coef: 0.07576/9 [===================>..........] - ETA: 1s - loss: 0.1768 - accuracy: 0.4678 - jacard_coef: 0.07927/9 [======================>.......] - ETA: 0s - loss: 0.1767 - accuracy: 0.4890 - jacard_coef: 0.07908/9 [=========================>....] - ETA: 0s - loss: 0.1763 - accuracy: 0.4978 - jacard_coef: 0.07569/9 [==============================] - 3s 332ms/step - loss: 0.1775 - accuracy: 0.4961 - jacard_coef: 0.0806 - val_loss: 1.1706 - val_accuracy: 0.9270 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1789 - accuracy: 0.3660 - jacard_coef: 0.07362/9 [=====>........................] - ETA: 2s - loss: 0.1775 - accuracy: 0.3836 - jacard_coef: 0.07573/9 [=========>....................] - ETA: 2s - loss: 0.1774 - accuracy: 0.3912 - jacard_coef: 0.06734/9 [============>.................] - ETA: 1s - loss: 0.1772 - accuracy: 0.3883 - jacard_coef: 0.05965/9 [===============>..............] - ETA: 1s - loss: 0.1769 - accuracy: 0.3751 - jacard_coef: 0.06836/9 [===================>..........] - ETA: 1s - loss: 0.1768 - accuracy: 0.3777 - jacard_coef: 0.06777/9 [======================>.......] - ETA: 0s - loss: 0.1765 - accuracy: 0.3907 - jacard_coef: 0.07318/9 [=========================>....] - ETA: 0s - loss: 0.1761 - accuracy: 0.4146 - jacard_coef: 0.07599/9 [==============================] - 3s 332ms/step - loss: 0.1761 - accuracy: 0.4150 - jacard_coef: 0.0737 - val_loss: 1.1755 - val_accuracy: 0.9270 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1746 - accuracy: 0.5404 - jacard_coef: 0.05202/9 [=====>........................] - ETA: 2s - loss: 0.1743 - accuracy: 0.5643 - jacard_coef: 0.06193/9 [=========>....................] - ETA: 2s - loss: 0.1738 - accuracy: 0.5657 - jacard_coef: 0.05844/9 [============>.................] - ETA: 1s - loss: 0.1733 - accuracy: 0.6089 - jacard_coef: 0.06965/9 [===============>..............] - ETA: 1s - loss: 0.1726 - accuracy: 0.6424 - jacard_coef: 0.07246/9 [===================>..........] - ETA: 1s - loss: 0.1723 - accuracy: 0.6714 - jacard_coef: 0.07287/9 [======================>.......] - ETA: 0s - loss: 0.1719 - accuracy: 0.6874 - jacard_coef: 0.07728/9 [=========================>....] - ETA: 0s - loss: 0.1716 - accuracy: 0.6936 - jacard_coef: 0.07619/9 [==============================] - 3s 332ms/step - loss: 0.1720 - accuracy: 0.6900 - jacard_coef: 0.0715 - val_loss: 1.1762 - val_accuracy: 0.9270 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1707 - accuracy: 0.8000 - jacard_coef: 0.07112/9 [=====>........................] - ETA: 2s - loss: 0.1787 - accuracy: 0.4668 - jacard_coef: 0.07883/9 [=========>....................] - ETA: 2s - loss: 0.1776 - accuracy: 0.5177 - jacard_coef: 0.07354/9 [============>.................] - ETA: 1s - loss: 0.1759 - accuracy: 0.5821 - jacard_coef: 0.07215/9 [===============>..............] - ETA: 1s - loss: 0.1749 - accuracy: 0.6240 - jacard_coef: 0.07186/9 [===================>..........] - ETA: 1s - loss: 0.1741 - accuracy: 0.6573 - jacard_coef: 0.07117/9 [======================>.......] - ETA: 0s - loss: 0.1737 - accuracy: 0.6755 - jacard_coef: 0.07288/9 [=========================>....] - ETA: 0s - loss: 0.1733 - accuracy: 0.6899 - jacard_coef: 0.07529/9 [==============================] - 3s 332ms/step - loss: 0.1734 - accuracy: 0.6866 - jacard_coef: 0.0863 - val_loss: 1.1188 - val_accuracy: 0.9268 - val_jacard_coef: 0.0043 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.7913 - jacard_coef: 0.08992/9 [=====>........................] - ETA: 2s - loss: 0.1693 - accuracy: 0.8058 - jacard_coef: 0.08723/9 [=========>....................] - ETA: 2s - loss: 0.1691 - accuracy: 0.8010 - jacard_coef: 0.08684/9 [============>.................] - ETA: 1s - loss: 0.1688 - accuracy: 0.8123 - jacard_coef: 0.08755/9 [===============>..............] - ETA: 1s - loss: 0.1684 - accuracy: 0.8217 - jacard_coef: 0.08546/9 [===================>..........] - ETA: 1s - loss: 0.1685 - accuracy: 0.8237 - jacard_coef: 0.08427/9 [======================>.......] - ETA: 0s - loss: 0.1685 - accuracy: 0.8305 - jacard_coef: 0.07818/9 [=========================>....] - ETA: 0s - loss: 0.1682 - accuracy: 0.8370 - jacard_coef: 0.07649/9 [==============================] - 3s 332ms/step - loss: 0.1682 - accuracy: 0.8366 - jacard_coef: 0.0682 - val_loss: 0.3791 - val_accuracy: 0.8967 - val_jacard_coef: 0.0445 - lr: 5.0000e-04
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1654 - accuracy: 0.8646 - jacard_coef: 0.09162/9 [=====>........................] - ETA: 2s - loss: 0.1660 - accuracy: 0.8656 - jacard_coef: 0.08073/9 [=========>....................] - ETA: 2s - loss: 0.1660 - accuracy: 0.8644 - jacard_coef: 0.08144/9 [============>.................] - ETA: 1s - loss: 0.1662 - accuracy: 0.8720 - jacard_coef: 0.07605/9 [===============>..............] - ETA: 1s - loss: 0.1660 - accuracy: 0.8764 - jacard_coef: 0.07396/9 [===================>..........] - ETA: 1s - loss: 0.1661 - accuracy: 0.8738 - jacard_coef: 0.07817/9 [======================>.......] - ETA: 0s - loss: 0.1659 - accuracy: 0.8764 - jacard_coef: 0.07718/9 [=========================>....] - ETA: 0s - loss: 0.1658 - accuracy: 0.8781 - jacard_coef: 0.07649/9 [==============================] - 3s 341ms/step - loss: 0.1658 - accuracy: 0.8779 - jacard_coef: 0.0682 - val_loss: 0.1273 - val_accuracy: 0.8404 - val_jacard_coef: 0.0674 - lr: 5.0000e-04
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1653 - accuracy: 0.8977 - jacard_coef: 0.08442/9 [=====>........................] - ETA: 2s - loss: 0.1648 - accuracy: 0.8979 - jacard_coef: 0.07693/9 [=========>....................] - ETA: 2s - loss: 0.1647 - accuracy: 0.8994 - jacard_coef: 0.07824/9 [============>.................] - ETA: 1s - loss: 0.1646 - accuracy: 0.9066 - jacard_coef: 0.07395/9 [===============>..............] - ETA: 1s - loss: 0.1645 - accuracy: 0.9037 - jacard_coef: 0.07586/9 [===================>..........] - ETA: 1s - loss: 0.1646 - accuracy: 0.9020 - jacard_coef: 0.07657/9 [======================>.......] - ETA: 0s - loss: 0.1643 - accuracy: 0.8995 - jacard_coef: 0.07938/9 [=========================>....] - ETA: 0s - loss: 0.1641 - accuracy: 0.9044 - jacard_coef: 0.07519/9 [==============================] - 3s 334ms/step - loss: 0.1642 - accuracy: 0.9008 - jacard_coef: 0.0859 - val_loss: 0.1678 - val_accuracy: 0.8897 - val_jacard_coef: 0.0653 - lr: 5.0000e-04
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1622 - accuracy: 0.9455 - jacard_coef: 0.04062/9 [=====>........................] - ETA: 2s - loss: 0.1614 - accuracy: 0.9367 - jacard_coef: 0.04803/9 [=========>....................] - ETA: 2s - loss: 0.1617 - accuracy: 0.9233 - jacard_coef: 0.05714/9 [============>.................] - ETA: 1s - loss: 0.1624 - accuracy: 0.9081 - jacard_coef: 0.06495/9 [===============>..............] - ETA: 1s - loss: 0.1626 - accuracy: 0.8973 - jacard_coef: 0.06966/9 [===================>..........] - ETA: 1s - loss: 0.1625 - accuracy: 0.8894 - jacard_coef: 0.07127/9 [======================>.......] - ETA: 0s - loss: 0.1624 - accuracy: 0.8831 - jacard_coef: 0.07548/9 [=========================>....] - ETA: 0s - loss: 0.1624 - accuracy: 0.8797 - jacard_coef: 0.07529/9 [==============================] - 3s 334ms/step - loss: 0.1626 - accuracy: 0.8759 - jacard_coef: 0.0817 - val_loss: 0.1685 - val_accuracy: 0.7275 - val_jacard_coef: 0.0647 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1612 - accuracy: 0.8905 - jacard_coef: 0.08692/9 [=====>........................] - ETA: 2s - loss: 0.1626 - accuracy: 0.8928 - jacard_coef: 0.08653/9 [=========>....................] - ETA: 2s - loss: 0.1622 - accuracy: 0.8932 - jacard_coef: 0.08514/9 [============>.................] - ETA: 1s - loss: 0.1620 - accuracy: 0.8986 - jacard_coef: 0.08035/9 [===============>..............] - ETA: 1s - loss: 0.1621 - accuracy: 0.8869 - jacard_coef: 0.08046/9 [===================>..........] - ETA: 1s - loss: 0.1620 - accuracy: 0.8931 - jacard_coef: 0.07547/9 [======================>.......] - ETA: 0s - loss: 0.1617 - accuracy: 0.8942 - jacard_coef: 0.07708/9 [=========================>....] - ETA: 0s - loss: 0.1615 - accuracy: 0.8976 - jacard_coef: 0.07619/9 [==============================] - 3s 337ms/step - loss: 0.1617 - accuracy: 0.8947 - jacard_coef: 0.0718 - val_loss: 0.1579 - val_accuracy: 0.9173 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8767 - jacard_coef: 0.09482/9 [=====>........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.8803 - jacard_coef: 0.09143/9 [=========>....................] - ETA: 2s - loss: 0.1610 - accuracy: 0.8817 - jacard_coef: 0.09194/9 [============>.................] - ETA: 1s - loss: 0.1607 - accuracy: 0.8920 - jacard_coef: 0.08425/9 [===============>..............] - ETA: 1s - loss: 0.1606 - accuracy: 0.9010 - jacard_coef: 0.07356/9 [===================>..........] - ETA: 1s - loss: 0.1604 - accuracy: 0.9015 - jacard_coef: 0.07497/9 [======================>.......] - ETA: 0s - loss: 0.1603 - accuracy: 0.9025 - jacard_coef: 0.07538/9 [=========================>....] - ETA: 0s - loss: 0.1603 - accuracy: 0.9031 - jacard_coef: 0.07569/9 [==============================] - 3s 340ms/step - loss: 0.1603 - accuracy: 0.9032 - jacard_coef: 0.0733 - val_loss: 0.1594 - val_accuracy: 0.9228 - val_jacard_coef: 0.0647 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1590 - accuracy: 0.9057 - jacard_coef: 0.07832/9 [=====>........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.9075 - jacard_coef: 0.07683/9 [=========>....................] - ETA: 2s - loss: 0.1593 - accuracy: 0.8928 - jacard_coef: 0.07344/9 [============>.................] - ETA: 1s - loss: 0.1596 - accuracy: 0.8892 - jacard_coef: 0.07825/9 [===============>..............] - ETA: 1s - loss: 0.1596 - accuracy: 0.8894 - jacard_coef: 0.07606/9 [===================>..........] - ETA: 1s - loss: 0.1600 - accuracy: 0.8871 - jacard_coef: 0.07667/9 [======================>.......] - ETA: 0s - loss: 0.1599 - accuracy: 0.8882 - jacard_coef: 0.07578/9 [=========================>....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8900 - jacard_coef: 0.07619/9 [==============================] - 3s 340ms/step - loss: 0.1598 - accuracy: 0.8908 - jacard_coef: 0.0680 - val_loss: 0.1578 - val_accuracy: 0.9211 - val_jacard_coef: 0.0647 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1595 - accuracy: 0.9306 - jacard_coef: 0.06302/9 [=====>........................] - ETA: 2s - loss: 0.1588 - accuracy: 0.9249 - jacard_coef: 0.06793/9 [=========>....................] - ETA: 2s - loss: 0.1589 - accuracy: 0.9204 - jacard_coef: 0.07204/9 [============>.................] - ETA: 1s - loss: 0.1587 - accuracy: 0.9160 - jacard_coef: 0.07545/9 [===============>..............] - ETA: 1s - loss: 0.1587 - accuracy: 0.9165 - jacard_coef: 0.07526/9 [===================>..........] - ETA: 1s - loss: 0.1584 - accuracy: 0.9211 - jacard_coef: 0.07147/9 [======================>.......] - ETA: 0s - loss: 0.1584 - accuracy: 0.9162 - jacard_coef: 0.07528/9 [=========================>....] - ETA: 0s - loss: 0.1582 - accuracy: 0.9159 - jacard_coef: 0.07549/9 [==============================] - 3s 340ms/step - loss: 0.1589 - accuracy: 0.9129 - jacard_coef: 0.0829 - val_loss: 0.1613 - val_accuracy: 0.9091 - val_jacard_coef: 0.0648 - lr: 2.5000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1581 - accuracy: 0.9099 - jacard_coef: 0.07202/9 [=====>........................] - ETA: 2s - loss: 0.1592 - accuracy: 0.9045 - jacard_coef: 0.07093/9 [=========>....................] - ETA: 2s - loss: 0.1594 - accuracy: 0.9034 - jacard_coef: 0.07104/9 [============>.................] - ETA: 1s - loss: 0.1593 - accuracy: 0.8987 - jacard_coef: 0.07315/9 [===============>..............] - ETA: 1s - loss: 0.1597 - accuracy: 0.8945 - jacard_coef: 0.07496/9 [===================>..........] - ETA: 1s - loss: 0.1594 - accuracy: 0.8965 - jacard_coef: 0.07447/9 [======================>.......] - ETA: 0s - loss: 0.1593 - accuracy: 0.8986 - jacard_coef: 0.07318/9 [=========================>....] - ETA: 0s - loss: 0.1598 - accuracy: 0.8942 - jacard_coef: 0.07599/9 [==============================] - 3s 340ms/step - loss: 0.1600 - accuracy: 0.8895 - jacard_coef: 0.0735 - val_loss: 0.1590 - val_accuracy: 0.9242 - val_jacard_coef: 0.0651 - lr: 2.5000e-04
Epoch 16/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1595 - accuracy: 0.8904 - jacard_coef: 0.08002/9 [=====>........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.8956 - jacard_coef: 0.07373/9 [=========>....................] - ETA: 2s - loss: 0.1591 - accuracy: 0.8928 - jacard_coef: 0.07304/9 [============>.................] - ETA: 1s - loss: 0.1594 - accuracy: 0.8900 - jacard_coef: 0.07305/9 [===============>..............] - ETA: 1s - loss: 0.1594 - accuracy: 0.8921 - jacard_coef: 0.07226/9 [===================>..........] - ETA: 1s - loss: 0.1595 - accuracy: 0.8909 - jacard_coef: 0.07227/9 [======================>.......] - ETA: 0s - loss: 0.1596 - accuracy: 0.8886 - jacard_coef: 0.07328/9 [=========================>....] - ETA: 0s - loss: 0.1594 - accuracy: 0.8885 - jacard_coef: 0.07529/9 [==============================] - 3s 340ms/step - loss: 0.1595 - accuracy: 0.8880 - jacard_coef: 0.0817 - val_loss: 0.1568 - val_accuracy: 0.9232 - val_jacard_coef: 0.0648 - lr: 2.5000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1600 - accuracy: 0.8887 - jacard_coef: 0.08482/9 [=====>........................] - ETA: 2s - loss: 0.1590 - accuracy: 0.8960 - jacard_coef: 0.08013/9 [=========>....................] - ETA: 2s - loss: 0.1588 - accuracy: 0.9070 - jacard_coef: 0.07244/9 [============>.................] - ETA: 1s - loss: 0.1586 - accuracy: 0.9076 - jacard_coef: 0.07345/9 [===============>..............] - ETA: 1s - loss: 0.1586 - accuracy: 0.9077 - jacard_coef: 0.07436/9 [===================>..........] - ETA: 1s - loss: 0.1584 - accuracy: 0.9079 - jacard_coef: 0.07497/9 [======================>.......] - ETA: 0s - loss: 0.1585 - accuracy: 0.9085 - jacard_coef: 0.07498/9 [=========================>....] - ETA: 0s - loss: 0.1585 - accuracy: 0.9077 - jacard_coef: 0.07619/9 [==============================] - 3s 339ms/step - loss: 0.1585 - accuracy: 0.9079 - jacard_coef: 0.0730 - val_loss: 0.1561 - val_accuracy: 0.9233 - val_jacard_coef: 0.0648 - lr: 2.5000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1585 - accuracy: 0.8862 - jacard_coef: 0.09492/9 [=====>........................] - ETA: 2s - loss: 0.1576 - accuracy: 0.8919 - jacard_coef: 0.08663/9 [=========>....................] - ETA: 2s - loss: 0.1581 - accuracy: 0.8959 - jacard_coef: 0.08214/9 [============>.................] - ETA: 1s - loss: 0.1578 - accuracy: 0.8935 - jacard_coef: 0.08105/9 [===============>..............] - ETA: 1s - loss: 0.1576 - accuracy: 0.8957 - jacard_coef: 0.07796/9 [===================>..........] - ETA: 1s - loss: 0.1575 - accuracy: 0.8963 - jacard_coef: 0.07767/9 [======================>.......] - ETA: 0s - loss: 0.1573 - accuracy: 0.8949 - jacard_coef: 0.07858/9 [=========================>....] - ETA: 0s - loss: 0.1570 - accuracy: 0.8979 - jacard_coef: 0.07629/9 [==============================] - 3s 340ms/step - loss: 0.1571 - accuracy: 0.8952 - jacard_coef: 0.0686 - val_loss: 0.1555 - val_accuracy: 0.9237 - val_jacard_coef: 0.0648 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0674 (epoch 8)
  Final Val Loss: 0.1555
  Training Time: 0:02:12.121523
  Stability (std): 0.0044

Results saved to: hyperparameter_optimization_20250926_123742/exp_17_Attention_UNet_lr5e-4_bs16/Attention_UNet_lr0.0005_bs16_results.json

Experiment 17 completed in 168s
Progress: 17/36 completed
Estimated remaining time: 53 minutes

ðŸ”¬ EXPERIMENT 18/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758864013.604202 3257656 service.cc:145] XLA service 0x14843c5976b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758864013.604243 3257656 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758864013.993777 3257656 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:39 - loss: 0.3389 - accuracy: 0.5074 - jacard_coef: 0.08732/5 [===========>..................] - ETA: 45s - loss: 0.3005 - accuracy: 0.4050 - jacard_coef: 0.0761 3/5 [=================>............] - ETA: 15s - loss: 0.2713 - accuracy: 0.3357 - jacard_coef: 0.07764/5 [=======================>......] - ETA: 5s - loss: 0.2548 - accuracy: 0.2894 - jacard_coef: 0.0774 5/5 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.2877 - jacard_coef: 0.06315/5 [==============================] - 95s 6s/step - loss: 0.2544 - accuracy: 0.2877 - jacard_coef: 0.0631 - val_loss: 0.1430 - val_accuracy: 0.9235 - val_jacard_coef: 0.0652 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1973 - accuracy: 0.1438 - jacard_coef: 0.08192/5 [===========>..................] - ETA: 2s - loss: 0.1944 - accuracy: 0.1719 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1937 - accuracy: 0.1479 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 0s - loss: 0.1914 - accuracy: 0.1338 - jacard_coef: 0.07665/5 [==============================] - 3s 565ms/step - loss: 0.1913 - accuracy: 0.1362 - jacard_coef: 0.0722 - val_loss: 0.1159 - val_accuracy: 0.9271 - val_jacard_coef: 0.0534 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1736 - accuracy: 0.3751 - jacard_coef: 0.07202/5 [===========>..................] - ETA: 2s - loss: 0.1733 - accuracy: 0.3867 - jacard_coef: 0.07163/5 [=================>............] - ETA: 1s - loss: 0.1731 - accuracy: 0.4159 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.4465 - jacard_coef: 0.07675/5 [==============================] - 3s 567ms/step - loss: 0.1729 - accuracy: 0.4473 - jacard_coef: 0.0631 - val_loss: 0.2580 - val_accuracy: 0.9300 - val_jacard_coef: 0.0296 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1711 - accuracy: 0.6237 - jacard_coef: 0.08892/5 [===========>..................] - ETA: 2s - loss: 0.1709 - accuracy: 0.6426 - jacard_coef: 0.07823/5 [=================>............] - ETA: 1s - loss: 0.1708 - accuracy: 0.6273 - jacard_coef: 0.08394/5 [=======================>......] - ETA: 0s - loss: 0.1708 - accuracy: 0.5736 - jacard_coef: 0.07625/5 [==============================] - 3s 565ms/step - loss: 0.1708 - accuracy: 0.5738 - jacard_coef: 0.0666 - val_loss: 0.2914 - val_accuracy: 0.9301 - val_jacard_coef: 0.0243 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1716 - accuracy: 0.7064 - jacard_coef: 0.07462/5 [===========>..................] - ETA: 2s - loss: 0.1719 - accuracy: 0.7597 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1718 - accuracy: 0.7938 - jacard_coef: 0.07084/5 [=======================>......] - ETA: 0s - loss: 0.1715 - accuracy: 0.8126 - jacard_coef: 0.07545/5 [==============================] - 3s 566ms/step - loss: 0.1715 - accuracy: 0.8125 - jacard_coef: 0.0901 - val_loss: 0.3166 - val_accuracy: 0.9302 - val_jacard_coef: 0.0216 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1700 - accuracy: 0.8732 - jacard_coef: 0.07512/5 [===========>..................] - ETA: 2s - loss: 0.1698 - accuracy: 0.8592 - jacard_coef: 0.07623/5 [=================>............] - ETA: 1s - loss: 0.1697 - accuracy: 0.8450 - jacard_coef: 0.07384/5 [=======================>......] - ETA: 0s - loss: 0.1696 - accuracy: 0.8363 - jacard_coef: 0.07575/5 [==============================] - 3s 565ms/step - loss: 0.1696 - accuracy: 0.8359 - jacard_coef: 0.0868 - val_loss: 0.2500 - val_accuracy: 0.9301 - val_jacard_coef: 0.0303 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1687 - accuracy: 0.8443 - jacard_coef: 0.07332/5 [===========>..................] - ETA: 2s - loss: 0.1687 - accuracy: 0.8453 - jacard_coef: 0.07653/5 [=================>............] - ETA: 1s - loss: 0.1685 - accuracy: 0.8498 - jacard_coef: 0.07484/5 [=======================>......] - ETA: 0s - loss: 0.1683 - accuracy: 0.8458 - jacard_coef: 0.07635/5 [==============================] - 3s 565ms/step - loss: 0.1689 - accuracy: 0.8422 - jacard_coef: 0.0768 - val_loss: 0.1255 - val_accuracy: 0.9250 - val_jacard_coef: 0.0510 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1782 - accuracy: 0.4022 - jacard_coef: 0.07042/5 [===========>..................] - ETA: 2s - loss: 0.1732 - accuracy: 0.6259 - jacard_coef: 0.07873/5 [=================>............] - ETA: 1s - loss: 0.1716 - accuracy: 0.7104 - jacard_coef: 0.07964/5 [=======================>......] - ETA: 0s - loss: 0.1713 - accuracy: 0.7291 - jacard_coef: 0.07595/5 [==============================] - 3s 566ms/step - loss: 0.1713 - accuracy: 0.7286 - jacard_coef: 0.0866 - val_loss: 0.9722 - val_accuracy: 0.9301 - val_jacard_coef: 0.0080 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1732 - accuracy: 0.5856 - jacard_coef: 0.07372/5 [===========>..................] - ETA: 2s - loss: 0.1735 - accuracy: 0.5981 - jacard_coef: 0.07013/5 [=================>............] - ETA: 1s - loss: 0.1728 - accuracy: 0.6338 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1718 - accuracy: 0.6872 - jacard_coef: 0.07675/5 [==============================] - 3s 567ms/step - loss: 0.1717 - accuracy: 0.6887 - jacard_coef: 0.0619 - val_loss: 1.0530 - val_accuracy: 0.9274 - val_jacard_coef: 0.0036 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1674 - accuracy: 0.8623 - jacard_coef: 0.07602/5 [===========>..................] - ETA: 2s - loss: 0.1673 - accuracy: 0.8539 - jacard_coef: 0.07493/5 [=================>............] - ETA: 1s - loss: 0.1672 - accuracy: 0.8516 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 0s - loss: 0.1670 - accuracy: 0.8573 - jacard_coef: 0.07575/5 [==============================] - 3s 565ms/step - loss: 0.1670 - accuracy: 0.8569 - jacard_coef: 0.0851 - val_loss: 1.0940 - val_accuracy: 0.9304 - val_jacard_coef: 7.2618e-04 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1662 - accuracy: 0.8625 - jacard_coef: 0.06182/5 [===========>..................] - ETA: 2s - loss: 0.1663 - accuracy: 0.8652 - jacard_coef: 0.07473/5 [=================>............] - ETA: 1s - loss: 0.1662 - accuracy: 0.8743 - jacard_coef: 0.07214/5 [=======================>......] - ETA: 0s - loss: 0.1661 - accuracy: 0.8762 - jacard_coef: 0.07615/5 [==============================] - 3s 567ms/step - loss: 0.1661 - accuracy: 0.8765 - jacard_coef: 0.0709 - val_loss: 1.1002 - val_accuracy: 0.9304 - val_jacard_coef: 6.8991e-04 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0652 (epoch 1)
  Final Val Loss: 1.1002
  Training Time: 0:02:05.680514
  Stability (std): 0.4117

Results saved to: hyperparameter_optimization_20250926_123742/exp_18_Attention_UNet_lr5e-4_bs32/Attention_UNet_lr0.0005_bs32_results.json

Experiment 18 completed in 161s
Progress: 18/36 completed
Estimated remaining time: 48 minutes

ðŸ”¬ EXPERIMENT 19/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758864165.175718 3263907 service.cc:145] XLA service 0x1480fda476a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758864165.175801 3263907 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758864165.635363 3263907 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 14:55 - loss: 0.3391 - accuracy: 0.5308 - jacard_coef: 0.0647 2/17 [==>...........................] - ETA: 1:11 - loss: 0.3242 - accuracy: 0.6006 - jacard_coef: 0.0578  3/17 [====>.........................] - ETA: 34s - loss: 0.2990 - accuracy: 0.6183 - jacard_coef: 0.0682  4/17 [======>.......................] - ETA: 22s - loss: 0.2793 - accuracy: 0.5699 - jacard_coef: 0.0679 5/17 [=======>......................] - ETA: 15s - loss: 0.2646 - accuracy: 0.5559 - jacard_coef: 0.0670 6/17 [=========>....................] - ETA: 12s - loss: 0.2560 - accuracy: 0.5453 - jacard_coef: 0.0730 7/17 [===========>..................] - ETA: 9s - loss: 0.2485 - accuracy: 0.5535 - jacard_coef: 0.0720  8/17 [=============>................] - ETA: 7s - loss: 0.2427 - accuracy: 0.5718 - jacard_coef: 0.0735 9/17 [==============>...............] - ETA: 6s - loss: 0.2370 - accuracy: 0.5973 - jacard_coef: 0.070710/17 [================>.............] - ETA: 4s - loss: 0.2319 - accuracy: 0.6134 - jacard_coef: 0.072011/17 [==================>...........] - ETA: 3s - loss: 0.2271 - accuracy: 0.6244 - jacard_coef: 0.073812/17 [====================>.........] - ETA: 2s - loss: 0.2273 - accuracy: 0.6306 - jacard_coef: 0.073213/17 [=====================>........] - ETA: 2s - loss: 0.2241 - accuracy: 0.6419 - jacard_coef: 0.075814/17 [=======================>......] - ETA: 1s - loss: 0.2214 - accuracy: 0.6495 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 1s - loss: 0.2195 - accuracy: 0.6548 - jacard_coef: 0.077916/17 [===========================>..] - ETA: 0s - loss: 0.2176 - accuracy: 0.6621 - jacard_coef: 0.076317/17 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.6632 - jacard_coef: 0.075117/17 [==============================] - 70s 903ms/step - loss: 0.2173 - accuracy: 0.6632 - jacard_coef: 0.0751 - val_loss: 0.4074 - val_accuracy: 0.9298 - val_jacard_coef: 0.0166 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1856 - accuracy: 0.7831 - jacard_coef: 0.0815 2/17 [==>...........................] - ETA: 2s - loss: 0.1833 - accuracy: 0.8014 - jacard_coef: 0.0727 3/17 [====>.........................] - ETA: 2s - loss: 0.1826 - accuracy: 0.8195 - jacard_coef: 0.0629 4/17 [======>.......................] - ETA: 2s - loss: 0.1814 - accuracy: 0.8136 - jacard_coef: 0.0734 5/17 [=======>......................] - ETA: 2s - loss: 0.1804 - accuracy: 0.8291 - jacard_coef: 0.0684 6/17 [=========>....................] - ETA: 2s - loss: 0.1793 - accuracy: 0.8239 - jacard_coef: 0.0735 7/17 [===========>..................] - ETA: 1s - loss: 0.1778 - accuracy: 0.8187 - jacard_coef: 0.0715 8/17 [=============>................] - ETA: 1s - loss: 0.1767 - accuracy: 0.8137 - jacard_coef: 0.0689 9/17 [==============>...............] - ETA: 1s - loss: 0.1758 - accuracy: 0.8082 - jacard_coef: 0.072410/17 [================>.............] - ETA: 1s - loss: 0.1749 - accuracy: 0.8051 - jacard_coef: 0.068911/17 [==================>...........] - ETA: 1s - loss: 0.1864 - accuracy: 0.7918 - jacard_coef: 0.070512/17 [====================>.........] - ETA: 0s - loss: 0.1851 - accuracy: 0.7947 - jacard_coef: 0.072213/17 [=====================>........] - ETA: 0s - loss: 0.1841 - accuracy: 0.7979 - jacard_coef: 0.072514/17 [=======================>......] - ETA: 0s - loss: 0.1836 - accuracy: 0.8000 - jacard_coef: 0.074215/17 [=========================>....] - ETA: 0s - loss: 0.1832 - accuracy: 0.8061 - jacard_coef: 0.074316/17 [===========================>..] - ETA: 0s - loss: 0.1831 - accuracy: 0.8108 - jacard_coef: 0.075417/17 [==============================] - 3s 187ms/step - loss: 0.1830 - accuracy: 0.8113 - jacard_coef: 0.0770 - val_loss: 14.9204 - val_accuracy: 0.0730 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1779 - accuracy: 0.8899 - jacard_coef: 0.0868 2/17 [==>...........................] - ETA: 2s - loss: 0.1778 - accuracy: 0.8873 - jacard_coef: 0.0842 3/17 [====>.........................] - ETA: 2s - loss: 0.1772 - accuracy: 0.8888 - jacard_coef: 0.0818 4/17 [======>.......................] - ETA: 2s - loss: 0.1764 - accuracy: 0.8988 - jacard_coef: 0.0738 5/17 [=======>......................] - ETA: 2s - loss: 0.1756 - accuracy: 0.8979 - jacard_coef: 0.0738 6/17 [=========>....................] - ETA: 2s - loss: 0.1754 - accuracy: 0.8956 - jacard_coef: 0.0768 7/17 [===========>..................] - ETA: 1s - loss: 0.1752 - accuracy: 0.8953 - jacard_coef: 0.0786 8/17 [=============>................] - ETA: 1s - loss: 0.1755 - accuracy: 0.9032 - jacard_coef: 0.0732 9/17 [==============>...............] - ETA: 1s - loss: 0.1756 - accuracy: 0.8976 - jacard_coef: 0.078610/17 [================>.............] - ETA: 1s - loss: 0.1755 - accuracy: 0.8992 - jacard_coef: 0.077711/17 [==================>...........] - ETA: 1s - loss: 0.1753 - accuracy: 0.8996 - jacard_coef: 0.076212/17 [====================>.........] - ETA: 0s - loss: 0.1745 - accuracy: 0.8965 - jacard_coef: 0.075213/17 [=====================>........] - ETA: 0s - loss: 0.1743 - accuracy: 0.8944 - jacard_coef: 0.076114/17 [=======================>......] - ETA: 0s - loss: 0.1738 - accuracy: 0.8923 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 0s - loss: 0.1732 - accuracy: 0.8940 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1731 - accuracy: 0.8948 - jacard_coef: 0.074817/17 [==============================] - 3s 186ms/step - loss: 0.1732 - accuracy: 0.8942 - jacard_coef: 0.0787 - val_loss: 14.4105 - val_accuracy: 0.0732 - val_jacard_coef: 0.0683 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1622 - accuracy: 0.9193 - jacard_coef: 0.0523 2/17 [==>...........................] - ETA: 2s - loss: 0.1631 - accuracy: 0.8850 - jacard_coef: 0.0671 3/17 [====>.........................] - ETA: 2s - loss: 0.1654 - accuracy: 0.8710 - jacard_coef: 0.0820 4/17 [======>.......................] - ETA: 2s - loss: 0.1656 - accuracy: 0.8592 - jacard_coef: 0.0838 5/17 [=======>......................] - ETA: 2s - loss: 0.1648 - accuracy: 0.8419 - jacard_coef: 0.0761 6/17 [=========>....................] - ETA: 2s - loss: 0.1646 - accuracy: 0.8407 - jacard_coef: 0.0733 7/17 [===========>..................] - ETA: 1s - loss: 0.1644 - accuracy: 0.8346 - jacard_coef: 0.0743 8/17 [=============>................] - ETA: 1s - loss: 0.1642 - accuracy: 0.8261 - jacard_coef: 0.0760 9/17 [==============>...............] - ETA: 1s - loss: 0.1637 - accuracy: 0.8271 - jacard_coef: 0.074110/17 [================>.............] - ETA: 1s - loss: 0.1633 - accuracy: 0.8292 - jacard_coef: 0.073711/17 [==================>...........] - ETA: 1s - loss: 0.1630 - accuracy: 0.8322 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 0s - loss: 0.1627 - accuracy: 0.8324 - jacard_coef: 0.074913/17 [=====================>........] - ETA: 0s - loss: 0.1624 - accuracy: 0.8288 - jacard_coef: 0.076014/17 [=======================>......] - ETA: 0s - loss: 0.1622 - accuracy: 0.8244 - jacard_coef: 0.077415/17 [=========================>....] - ETA: 0s - loss: 0.1728 - accuracy: 0.8101 - jacard_coef: 0.074516/17 [===========================>..] - ETA: 0s - loss: 0.1730 - accuracy: 0.8147 - jacard_coef: 0.075917/17 [==============================] - 3s 183ms/step - loss: 0.1730 - accuracy: 0.8156 - jacard_coef: 0.0746 - val_loss: 0.2499 - val_accuracy: 0.5197 - val_jacard_coef: 0.0651 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1764 - accuracy: 0.9005 - jacard_coef: 0.0826 2/17 [==>...........................] - ETA: 2s - loss: 0.1743 - accuracy: 0.8902 - jacard_coef: 0.0918 3/17 [====>.........................] - ETA: 2s - loss: 0.1725 - accuracy: 0.8190 - jacard_coef: 0.0818 4/17 [======>.......................] - ETA: 2s - loss: 0.1712 - accuracy: 0.7909 - jacard_coef: 0.0853 5/17 [=======>......................] - ETA: 2s - loss: 0.1701 - accuracy: 0.8211 - jacard_coef: 0.0784 6/17 [=========>....................] - ETA: 2s - loss: 0.1695 - accuracy: 0.8346 - jacard_coef: 0.0790 7/17 [===========>..................] - ETA: 1s - loss: 0.1690 - accuracy: 0.8451 - jacard_coef: 0.0786 8/17 [=============>................] - ETA: 1s - loss: 0.1683 - accuracy: 0.8504 - jacard_coef: 0.0794 9/17 [==============>...............] - ETA: 1s - loss: 0.1679 - accuracy: 0.8578 - jacard_coef: 0.077210/17 [================>.............] - ETA: 1s - loss: 0.1675 - accuracy: 0.8628 - jacard_coef: 0.077211/17 [==================>...........] - ETA: 1s - loss: 0.1671 - accuracy: 0.8699 - jacard_coef: 0.074812/17 [====================>.........] - ETA: 0s - loss: 0.1666 - accuracy: 0.8728 - jacard_coef: 0.075513/17 [=====================>........] - ETA: 0s - loss: 0.1660 - accuracy: 0.8741 - jacard_coef: 0.077014/17 [=======================>......] - ETA: 0s - loss: 0.1654 - accuracy: 0.8790 - jacard_coef: 0.075115/17 [=========================>....] - ETA: 0s - loss: 0.1648 - accuracy: 0.8812 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1644 - accuracy: 0.8831 - jacard_coef: 0.074917/17 [==============================] - 3s 183ms/step - loss: 0.1653 - accuracy: 0.8814 - jacard_coef: 0.0800 - val_loss: 14.8412 - val_accuracy: 0.0731 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1811 - accuracy: 0.7802 - jacard_coef: 0.0772 2/17 [==>...........................] - ETA: 2s - loss: 0.1889 - accuracy: 0.7010 - jacard_coef: 0.0774 3/17 [====>.........................] - ETA: 2s - loss: 0.1806 - accuracy: 0.7654 - jacard_coef: 0.0827 4/17 [======>.......................] - ETA: 2s - loss: 0.1786 - accuracy: 0.8065 - jacard_coef: 0.0780 5/17 [=======>......................] - ETA: 2s - loss: 0.1786 - accuracy: 0.8294 - jacard_coef: 0.0766 6/17 [=========>....................] - ETA: 2s - loss: 0.1790 - accuracy: 0.8366 - jacard_coef: 0.0824 7/17 [===========>..................] - ETA: 1s - loss: 0.1791 - accuracy: 0.8474 - jacard_coef: 0.0818 8/17 [=============>................] - ETA: 1s - loss: 0.1795 - accuracy: 0.8553 - jacard_coef: 0.0815 9/17 [==============>...............] - ETA: 1s - loss: 0.1792 - accuracy: 0.8629 - jacard_coef: 0.080010/17 [================>.............] - ETA: 1s - loss: 0.1789 - accuracy: 0.8733 - jacard_coef: 0.074911/17 [==================>...........] - ETA: 1s - loss: 0.1790 - accuracy: 0.8738 - jacard_coef: 0.077712/17 [====================>.........] - ETA: 0s - loss: 0.1791 - accuracy: 0.8766 - jacard_coef: 0.078013/17 [=====================>........] - ETA: 0s - loss: 0.1786 - accuracy: 0.8787 - jacard_coef: 0.078314/17 [=======================>......] - ETA: 0s - loss: 0.1781 - accuracy: 0.8814 - jacard_coef: 0.077915/17 [=========================>....] - ETA: 0s - loss: 0.1775 - accuracy: 0.8845 - jacard_coef: 0.076816/17 [===========================>..] - ETA: 0s - loss: 0.1767 - accuracy: 0.8867 - jacard_coef: 0.075917/17 [==============================] - 3s 183ms/step - loss: 0.1765 - accuracy: 0.8870 - jacard_coef: 0.0735 - val_loss: 13.0165 - val_accuracy: 0.0733 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1676 - accuracy: 0.9143 - jacard_coef: 0.0686 2/17 [==>...........................] - ETA: 2s - loss: 0.1677 - accuracy: 0.8948 - jacard_coef: 0.0863 3/17 [====>.........................] - ETA: 2s - loss: 0.1646 - accuracy: 0.9123 - jacard_coef: 0.0684 4/17 [======>.......................] - ETA: 2s - loss: 0.1627 - accuracy: 0.9101 - jacard_coef: 0.0716 5/17 [=======>......................] - ETA: 2s - loss: 0.1622 - accuracy: 0.9088 - jacard_coef: 0.0717 6/17 [=========>....................] - ETA: 2s - loss: 0.1611 - accuracy: 0.9028 - jacard_coef: 0.0756 7/17 [===========>..................] - ETA: 1s - loss: 0.1605 - accuracy: 0.9003 - jacard_coef: 0.0783 8/17 [=============>................] - ETA: 1s - loss: 0.1593 - accuracy: 0.9042 - jacard_coef: 0.0749 9/17 [==============>...............] - ETA: 1s - loss: 0.1585 - accuracy: 0.9044 - jacard_coef: 0.075410/17 [================>.............] - ETA: 1s - loss: 0.1577 - accuracy: 0.9063 - jacard_coef: 0.074411/17 [==================>...........] - ETA: 1s - loss: 0.1570 - accuracy: 0.9048 - jacard_coef: 0.074912/17 [====================>.........] - ETA: 0s - loss: 0.1564 - accuracy: 0.9045 - jacard_coef: 0.075313/17 [=====================>........] - ETA: 0s - loss: 0.1557 - accuracy: 0.9059 - jacard_coef: 0.073914/17 [=======================>......] - ETA: 0s - loss: 0.1552 - accuracy: 0.9050 - jacard_coef: 0.074915/17 [=========================>....] - ETA: 0s - loss: 0.1547 - accuracy: 0.9059 - jacard_coef: 0.072916/17 [===========================>..] - ETA: 0s - loss: 0.1544 - accuracy: 0.9045 - jacard_coef: 0.074617/17 [==============================] - 3s 183ms/step - loss: 0.1551 - accuracy: 0.9023 - jacard_coef: 0.0773 - val_loss: 0.5177 - val_accuracy: 0.9304 - val_jacard_coef: 0.0168 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1518 - accuracy: 0.8387 - jacard_coef: 0.0452 2/17 [==>...........................] - ETA: 2s - loss: 0.1555 - accuracy: 0.7888 - jacard_coef: 0.0563 3/17 [====>.........................] - ETA: 2s - loss: 0.1591 - accuracy: 0.7770 - jacard_coef: 0.0647 4/17 [======>.......................] - ETA: 2s - loss: 0.1604 - accuracy: 0.7671 - jacard_coef: 0.0686 5/17 [=======>......................] - ETA: 2s - loss: 0.1616 - accuracy: 0.7620 - jacard_coef: 0.0715 6/17 [=========>....................] - ETA: 2s - loss: 0.1611 - accuracy: 0.7606 - jacard_coef: 0.0716 7/17 [===========>..................] - ETA: 1s - loss: 0.1606 - accuracy: 0.7689 - jacard_coef: 0.0717 8/17 [=============>................] - ETA: 1s - loss: 0.1600 - accuracy: 0.7739 - jacard_coef: 0.0730 9/17 [==============>...............] - ETA: 1s - loss: 0.1598 - accuracy: 0.7798 - jacard_coef: 0.074810/17 [================>.............] - ETA: 1s - loss: 0.1594 - accuracy: 0.7903 - jacard_coef: 0.074311/17 [==================>...........] - ETA: 1s - loss: 0.1591 - accuracy: 0.7973 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 0s - loss: 0.1594 - accuracy: 0.8052 - jacard_coef: 0.076413/17 [=====================>........] - ETA: 0s - loss: 0.1590 - accuracy: 0.8126 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1587 - accuracy: 0.8195 - jacard_coef: 0.076115/17 [=========================>....] - ETA: 0s - loss: 0.1582 - accuracy: 0.8261 - jacard_coef: 0.075416/17 [===========================>..] - ETA: 0s - loss: 0.1579 - accuracy: 0.8312 - jacard_coef: 0.075417/17 [==============================] - 3s 183ms/step - loss: 0.1578 - accuracy: 0.8320 - jacard_coef: 0.0742 - val_loss: 4.2532 - val_accuracy: 0.0818 - val_jacard_coef: 0.0680 - lr: 5.0000e-04
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1560 - accuracy: 0.9047 - jacard_coef: 0.0834 2/17 [==>...........................] - ETA: 2s - loss: 0.1564 - accuracy: 0.8955 - jacard_coef: 0.0910 3/17 [====>.........................] - ETA: 2s - loss: 0.1560 - accuracy: 0.8972 - jacard_coef: 0.0900 4/17 [======>.......................] - ETA: 2s - loss: 0.1565 - accuracy: 0.8980 - jacard_coef: 0.0895 5/17 [=======>......................] - ETA: 2s - loss: 0.1559 - accuracy: 0.9009 - jacard_coef: 0.0873 6/17 [=========>....................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9033 - jacard_coef: 0.0855 7/17 [===========>..................] - ETA: 1s - loss: 0.1537 - accuracy: 0.9113 - jacard_coef: 0.0786 8/17 [=============>................] - ETA: 1s - loss: 0.1532 - accuracy: 0.9130 - jacard_coef: 0.0773 9/17 [==============>...............] - ETA: 1s - loss: 0.1533 - accuracy: 0.9111 - jacard_coef: 0.079010/17 [================>.............] - ETA: 1s - loss: 0.1531 - accuracy: 0.9080 - jacard_coef: 0.081611/17 [==================>...........] - ETA: 1s - loss: 0.1523 - accuracy: 0.9123 - jacard_coef: 0.078012/17 [====================>.........] - ETA: 0s - loss: 0.1519 - accuracy: 0.9141 - jacard_coef: 0.076613/17 [=====================>........] - ETA: 0s - loss: 0.1515 - accuracy: 0.9147 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1512 - accuracy: 0.9155 - jacard_coef: 0.075515/17 [=========================>....] - ETA: 0s - loss: 0.1509 - accuracy: 0.9151 - jacard_coef: 0.075916/17 [===========================>..] - ETA: 0s - loss: 0.1506 - accuracy: 0.9160 - jacard_coef: 0.075217/17 [==============================] - 3s 185ms/step - loss: 0.1505 - accuracy: 0.9164 - jacard_coef: 0.0730 - val_loss: 0.2046 - val_accuracy: 0.2452 - val_jacard_coef: 0.0637 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1424 - accuracy: 0.9485 - jacard_coef: 0.0476 2/17 [==>...........................] - ETA: 2s - loss: 0.1450 - accuracy: 0.9215 - jacard_coef: 0.0708 3/17 [====>.........................] - ETA: 2s - loss: 0.1454 - accuracy: 0.9179 - jacard_coef: 0.0739 4/17 [======>.......................] - ETA: 2s - loss: 0.1447 - accuracy: 0.9225 - jacard_coef: 0.0700 5/17 [=======>......................] - ETA: 2s - loss: 0.1449 - accuracy: 0.9196 - jacard_coef: 0.0725 6/17 [=========>....................] - ETA: 2s - loss: 0.1444 - accuracy: 0.9234 - jacard_coef: 0.0693 7/17 [===========>..................] - ETA: 1s - loss: 0.1449 - accuracy: 0.9167 - jacard_coef: 0.0748 8/17 [=============>................] - ETA: 1s - loss: 0.1448 - accuracy: 0.9166 - jacard_coef: 0.0749 9/17 [==============>...............] - ETA: 1s - loss: 0.1448 - accuracy: 0.9156 - jacard_coef: 0.075810/17 [================>.............] - ETA: 1s - loss: 0.1445 - accuracy: 0.9172 - jacard_coef: 0.074111/17 [==================>...........] - ETA: 1s - loss: 0.1445 - accuracy: 0.9162 - jacard_coef: 0.074912/17 [====================>.........] - ETA: 0s - loss: 0.1446 - accuracy: 0.9145 - jacard_coef: 0.076413/17 [=====================>........] - ETA: 0s - loss: 0.1445 - accuracy: 0.9145 - jacard_coef: 0.076314/17 [=======================>......] - ETA: 0s - loss: 0.1443 - accuracy: 0.9153 - jacard_coef: 0.075515/17 [=========================>....] - ETA: 0s - loss: 0.1442 - accuracy: 0.9153 - jacard_coef: 0.075516/17 [===========================>..] - ETA: 0s - loss: 0.1442 - accuracy: 0.9152 - jacard_coef: 0.075517/17 [==============================] - 3s 184ms/step - loss: 0.1449 - accuracy: 0.9127 - jacard_coef: 0.0716 - val_loss: 0.0938 - val_accuracy: 0.9304 - val_jacard_coef: 0.0607 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1429 - accuracy: 0.9123 - jacard_coef: 0.0793 2/17 [==>...........................] - ETA: 2s - loss: 0.1429 - accuracy: 0.8927 - jacard_coef: 0.0689 3/17 [====>.........................] - ETA: 2s - loss: 0.1441 - accuracy: 0.8775 - jacard_coef: 0.0744 4/17 [======>.......................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8178 - jacard_coef: 0.0792 5/17 [=======>......................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8295 - jacard_coef: 0.0790 6/17 [=========>....................] - ETA: 2s - loss: 0.1578 - accuracy: 0.8374 - jacard_coef: 0.0824 7/17 [===========>..................] - ETA: 1s - loss: 0.1563 - accuracy: 0.8528 - jacard_coef: 0.0769 8/17 [=============>................] - ETA: 1s - loss: 0.1561 - accuracy: 0.8613 - jacard_coef: 0.0736 9/17 [==============>...............] - ETA: 1s - loss: 0.1561 - accuracy: 0.8650 - jacard_coef: 0.075010/17 [================>.............] - ETA: 1s - loss: 0.1563 - accuracy: 0.8660 - jacard_coef: 0.076511/17 [==================>...........] - ETA: 1s - loss: 0.1556 - accuracy: 0.8711 - jacard_coef: 0.075412/17 [====================>.........] - ETA: 0s - loss: 0.1547 - accuracy: 0.8772 - jacard_coef: 0.073013/17 [=====================>........] - ETA: 0s - loss: 0.1537 - accuracy: 0.8815 - jacard_coef: 0.071914/17 [=======================>......] - ETA: 0s - loss: 0.1530 - accuracy: 0.8850 - jacard_coef: 0.071215/17 [=========================>....] - ETA: 0s - loss: 0.1528 - accuracy: 0.8831 - jacard_coef: 0.074616/17 [===========================>..] - ETA: 0s - loss: 0.1522 - accuracy: 0.8856 - jacard_coef: 0.074317/17 [==============================] - 3s 184ms/step - loss: 0.1522 - accuracy: 0.8854 - jacard_coef: 0.0771 - val_loss: 0.1223 - val_accuracy: 0.9281 - val_jacard_coef: 0.0622 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1428 - accuracy: 0.9458 - jacard_coef: 0.0499 2/17 [==>...........................] - ETA: 2s - loss: 0.1411 - accuracy: 0.9535 - jacard_coef: 0.0429 3/17 [====>.........................] - ETA: 2s - loss: 0.1428 - accuracy: 0.9353 - jacard_coef: 0.0583 4/17 [======>.......................] - ETA: 2s - loss: 0.1433 - accuracy: 0.9299 - jacard_coef: 0.0633 5/17 [=======>......................] - ETA: 2s - loss: 0.1436 - accuracy: 0.9267 - jacard_coef: 0.0661 6/17 [=========>....................] - ETA: 2s - loss: 0.1442 - accuracy: 0.9233 - jacard_coef: 0.0690 7/17 [===========>..................] - ETA: 1s - loss: 0.1447 - accuracy: 0.9167 - jacard_coef: 0.0744 8/17 [=============>................] - ETA: 1s - loss: 0.1446 - accuracy: 0.9180 - jacard_coef: 0.0735 9/17 [==============>...............] - ETA: 1s - loss: 0.1449 - accuracy: 0.9152 - jacard_coef: 0.075810/17 [================>.............] - ETA: 1s - loss: 0.1446 - accuracy: 0.9170 - jacard_coef: 0.074411/17 [==================>...........] - ETA: 1s - loss: 0.1446 - accuracy: 0.9155 - jacard_coef: 0.075712/17 [====================>.........] - ETA: 0s - loss: 0.1443 - accuracy: 0.9170 - jacard_coef: 0.074513/17 [=====================>........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9148 - jacard_coef: 0.076314/17 [=======================>......] - ETA: 0s - loss: 0.1441 - accuracy: 0.9161 - jacard_coef: 0.075215/17 [=========================>....] - ETA: 0s - loss: 0.1439 - accuracy: 0.9165 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1437 - accuracy: 0.9165 - jacard_coef: 0.075017/17 [==============================] - 3s 184ms/step - loss: 0.1437 - accuracy: 0.9167 - jacard_coef: 0.0738 - val_loss: 0.1371 - val_accuracy: 0.9290 - val_jacard_coef: 0.0629 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1410 - accuracy: 0.9158 - jacard_coef: 0.0765 2/17 [==>...........................] - ETA: 2s - loss: 0.1400 - accuracy: 0.9244 - jacard_coef: 0.0694 3/17 [====>.........................] - ETA: 2s - loss: 0.1397 - accuracy: 0.9275 - jacard_coef: 0.0667 4/17 [======>.......................] - ETA: 2s - loss: 0.1396 - accuracy: 0.9255 - jacard_coef: 0.0685 5/17 [=======>......................] - ETA: 2s - loss: 0.1391 - accuracy: 0.9289 - jacard_coef: 0.0655 6/17 [=========>....................] - ETA: 2s - loss: 0.1398 - accuracy: 0.9214 - jacard_coef: 0.0717 7/17 [===========>..................] - ETA: 1s - loss: 0.1399 - accuracy: 0.9197 - jacard_coef: 0.0731 8/17 [=============>................] - ETA: 1s - loss: 0.1404 - accuracy: 0.9142 - jacard_coef: 0.0775 9/17 [==============>...............] - ETA: 1s - loss: 0.1403 - accuracy: 0.9152 - jacard_coef: 0.076710/17 [================>.............] - ETA: 1s - loss: 0.1403 - accuracy: 0.9148 - jacard_coef: 0.077111/17 [==================>...........] - ETA: 1s - loss: 0.1400 - accuracy: 0.9176 - jacard_coef: 0.074712/17 [====================>.........] - ETA: 0s - loss: 0.1398 - accuracy: 0.9194 - jacard_coef: 0.073213/17 [=====================>........] - ETA: 0s - loss: 0.1400 - accuracy: 0.9171 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1401 - accuracy: 0.9148 - jacard_coef: 0.076915/17 [=========================>....] - ETA: 0s - loss: 0.1399 - accuracy: 0.9163 - jacard_coef: 0.075716/17 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.9179 - jacard_coef: 0.074417/17 [==============================] - 3s 184ms/step - loss: 0.1400 - accuracy: 0.9154 - jacard_coef: 0.0780 - val_loss: 0.1315 - val_accuracy: 0.9291 - val_jacard_coef: 0.0625 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0683 (epoch 3)
  Final Val Loss: 0.1315
  Training Time: 0:01:48.878923
  Stability (std): 5.4363

Results saved to: hyperparameter_optimization_20250926_123742/exp_19_Attention_UNet_lr1e-3_bs8/Attention_UNet_lr0.001_bs8_results.json

Experiment 19 completed in 142s
Progress: 19/36 completed
Estimated remaining time: 40 minutes

ðŸ”¬ EXPERIMENT 20/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758864311.578211 3270087 service.cc:145] XLA service 0x1488ad9e3350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758864311.578257 3270087 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758864312.036347 3270087 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 7:59 - loss: 0.3408 - accuracy: 0.4905 - jacard_coef: 0.08562/9 [=====>........................] - ETA: 59s - loss: 0.3139 - accuracy: 0.4127 - jacard_coef: 0.0843 3/9 [=========>....................] - ETA: 26s - loss: 0.2916 - accuracy: 0.3490 - jacard_coef: 0.08204/9 [============>.................] - ETA: 15s - loss: 0.2724 - accuracy: 0.3081 - jacard_coef: 0.08275/9 [===============>..............] - ETA: 9s - loss: 0.2585 - accuracy: 0.3035 - jacard_coef: 0.0801 6/9 [===================>..........] - ETA: 5s - loss: 0.2525 - accuracy: 0.2748 - jacard_coef: 0.07497/9 [======================>.......] - ETA: 3s - loss: 0.2461 - accuracy: 0.2533 - jacard_coef: 0.07668/9 [=========================>....] - ETA: 1s - loss: 0.2400 - accuracy: 0.2355 - jacard_coef: 0.07699/9 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.2345 - jacard_coef: 0.07269/9 [==============================] - 79s 2s/step - loss: 0.2396 - accuracy: 0.2345 - jacard_coef: 0.0726 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1964 - accuracy: 0.0713 - jacard_coef: 0.05002/9 [=====>........................] - ETA: 2s - loss: 0.1960 - accuracy: 0.1171 - jacard_coef: 0.06513/9 [=========>....................] - ETA: 2s - loss: 0.1925 - accuracy: 0.1427 - jacard_coef: 0.07214/9 [============>.................] - ETA: 1s - loss: 0.1897 - accuracy: 0.1650 - jacard_coef: 0.07835/9 [===============>..............] - ETA: 1s - loss: 0.1894 - accuracy: 0.1869 - jacard_coef: 0.07616/9 [===================>..........] - ETA: 1s - loss: 0.1947 - accuracy: 0.2124 - jacard_coef: 0.07737/9 [======================>.......] - ETA: 0s - loss: 0.1939 - accuracy: 0.2357 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1931 - accuracy: 0.2459 - jacard_coef: 0.07639/9 [==============================] - 3s 330ms/step - loss: 0.1932 - accuracy: 0.2460 - jacard_coef: 0.0751 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1889 - accuracy: 0.2407 - jacard_coef: 0.08442/9 [=====>........................] - ETA: 2s - loss: 0.1884 - accuracy: 0.2248 - jacard_coef: 0.09513/9 [=========>....................] - ETA: 2s - loss: 0.1975 - accuracy: 0.2028 - jacard_coef: 0.08184/9 [============>.................] - ETA: 1s - loss: 0.1943 - accuracy: 0.1982 - jacard_coef: 0.08095/9 [===============>..............] - ETA: 1s - loss: 0.1924 - accuracy: 0.1933 - jacard_coef: 0.07876/9 [===================>..........] - ETA: 1s - loss: 0.1909 - accuracy: 0.1903 - jacard_coef: 0.07757/9 [======================>.......] - ETA: 0s - loss: 0.1906 - accuracy: 0.1921 - jacard_coef: 0.07748/9 [=========================>....] - ETA: 0s - loss: 0.1907 - accuracy: 0.1916 - jacard_coef: 0.07559/9 [==============================] - 3s 330ms/step - loss: 0.1907 - accuracy: 0.1926 - jacard_coef: 0.0847 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1850 - accuracy: 0.2281 - jacard_coef: 0.04582/9 [=====>........................] - ETA: 2s - loss: 0.1942 - accuracy: 0.2607 - jacard_coef: 0.06463/9 [=========>....................] - ETA: 2s - loss: 0.1936 - accuracy: 0.2385 - jacard_coef: 0.06624/9 [============>.................] - ETA: 1s - loss: 0.1906 - accuracy: 0.2537 - jacard_coef: 0.06965/9 [===============>..............] - ETA: 1s - loss: 0.1892 - accuracy: 0.2549 - jacard_coef: 0.07536/9 [===================>..........] - ETA: 1s - loss: 0.1884 - accuracy: 0.2616 - jacard_coef: 0.07717/9 [======================>.......] - ETA: 0s - loss: 0.1877 - accuracy: 0.2725 - jacard_coef: 0.07668/9 [=========================>....] - ETA: 0s - loss: 0.1867 - accuracy: 0.2844 - jacard_coef: 0.07559/9 [==============================] - 3s 332ms/step - loss: 0.1866 - accuracy: 0.2856 - jacard_coef: 0.0838 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1840 - accuracy: 0.3012 - jacard_coef: 0.08862/9 [=====>........................] - ETA: 2s - loss: 0.1823 - accuracy: 0.3033 - jacard_coef: 0.06573/9 [=========>....................] - ETA: 2s - loss: 0.1801 - accuracy: 0.3117 - jacard_coef: 0.07114/9 [============>.................] - ETA: 1s - loss: 0.1786 - accuracy: 0.3242 - jacard_coef: 0.07985/9 [===============>..............] - ETA: 1s - loss: 0.1782 - accuracy: 0.3129 - jacard_coef: 0.07936/9 [===================>..........] - ETA: 1s - loss: 0.1774 - accuracy: 0.3477 - jacard_coef: 0.07807/9 [======================>.......] - ETA: 0s - loss: 0.1779 - accuracy: 0.3729 - jacard_coef: 0.07928/9 [=========================>....] - ETA: 0s - loss: 0.1778 - accuracy: 0.3844 - jacard_coef: 0.07669/9 [==============================] - 3s 340ms/step - loss: 0.1778 - accuracy: 0.3842 - jacard_coef: 0.0684 - val_loss: 1.1051 - val_accuracy: 0.9304 - val_jacard_coef: 6.9341e-04 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1784 - accuracy: 0.6141 - jacard_coef: 0.05652/9 [=====>........................] - ETA: 2s - loss: 0.1773 - accuracy: 0.5848 - jacard_coef: 0.06953/9 [=========>....................] - ETA: 2s - loss: 0.1790 - accuracy: 0.5654 - jacard_coef: 0.07644/9 [============>.................] - ETA: 1s - loss: 0.1774 - accuracy: 0.5187 - jacard_coef: 0.07305/9 [===============>..............] - ETA: 1s - loss: 0.1764 - accuracy: 0.4935 - jacard_coef: 0.07766/9 [===================>..........] - ETA: 1s - loss: 0.1756 - accuracy: 0.4774 - jacard_coef: 0.07867/9 [======================>.......] - ETA: 0s - loss: 0.1749 - accuracy: 0.4794 - jacard_coef: 0.07738/9 [=========================>....] - ETA: 0s - loss: 0.1741 - accuracy: 0.5261 - jacard_coef: 0.07549/9 [==============================] - 3s 339ms/step - loss: 0.1741 - accuracy: 0.5250 - jacard_coef: 0.0812 - val_loss: 1.0160 - val_accuracy: 0.9298 - val_jacard_coef: 0.0019 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1747 - accuracy: 0.6836 - jacard_coef: 0.04972/9 [=====>........................] - ETA: 2s - loss: 0.1706 - accuracy: 0.7726 - jacard_coef: 0.06093/9 [=========>....................] - ETA: 2s - loss: 0.1689 - accuracy: 0.8042 - jacard_coef: 0.06354/9 [============>.................] - ETA: 1s - loss: 0.1695 - accuracy: 0.7451 - jacard_coef: 0.06925/9 [===============>..............] - ETA: 1s - loss: 0.1692 - accuracy: 0.7151 - jacard_coef: 0.07526/9 [===================>..........] - ETA: 1s - loss: 0.1724 - accuracy: 0.6445 - jacard_coef: 0.07677/9 [======================>.......] - ETA: 0s - loss: 0.1723 - accuracy: 0.6782 - jacard_coef: 0.07758/9 [=========================>....] - ETA: 0s - loss: 0.1725 - accuracy: 0.6735 - jacard_coef: 0.07549/9 [==============================] - 3s 357ms/step - loss: 0.1726 - accuracy: 0.6715 - jacard_coef: 0.0807 - val_loss: 1.0668 - val_accuracy: 0.9272 - val_jacard_coef: 0.0109 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1766 - accuracy: 0.5010 - jacard_coef: 0.07682/9 [=====>........................] - ETA: 2s - loss: 0.1769 - accuracy: 0.4334 - jacard_coef: 0.07343/9 [=========>....................] - ETA: 2s - loss: 0.1798 - accuracy: 0.4018 - jacard_coef: 0.07514/9 [============>.................] - ETA: 1s - loss: 0.1792 - accuracy: 0.3839 - jacard_coef: 0.07535/9 [===============>..............] - ETA: 1s - loss: 0.1776 - accuracy: 0.4023 - jacard_coef: 0.07576/9 [===================>..........] - ETA: 1s - loss: 0.1771 - accuracy: 0.4057 - jacard_coef: 0.07477/9 [======================>.......] - ETA: 0s - loss: 0.1760 - accuracy: 0.4249 - jacard_coef: 0.07878/9 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.4302 - jacard_coef: 0.07679/9 [==============================] - 3s 338ms/step - loss: 0.1775 - accuracy: 0.4292 - jacard_coef: 0.0685 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1727 - accuracy: 0.5837 - jacard_coef: 0.07702/9 [=====>........................] - ETA: 2s - loss: 0.1729 - accuracy: 0.5559 - jacard_coef: 0.07173/9 [=========>....................] - ETA: 2s - loss: 0.1778 - accuracy: 0.5500 - jacard_coef: 0.07764/9 [============>.................] - ETA: 1s - loss: 0.1806 - accuracy: 0.5172 - jacard_coef: 0.07665/9 [===============>..............] - ETA: 1s - loss: 0.1784 - accuracy: 0.5308 - jacard_coef: 0.07576/9 [===================>..........] - ETA: 1s - loss: 0.1785 - accuracy: 0.5134 - jacard_coef: 0.07657/9 [======================>.......] - ETA: 0s - loss: 0.1772 - accuracy: 0.5317 - jacard_coef: 0.07338/9 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.4990 - jacard_coef: 0.07619/9 [==============================] - 3s 338ms/step - loss: 0.1779 - accuracy: 0.4961 - jacard_coef: 0.0728 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4611e-05 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1696 - accuracy: 0.7314 - jacard_coef: 0.07312/9 [=====>........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.7400 - jacard_coef: 0.07313/9 [=========>....................] - ETA: 2s - loss: 0.1687 - accuracy: 0.7262 - jacard_coef: 0.07454/9 [============>.................] - ETA: 1s - loss: 0.1681 - accuracy: 0.7157 - jacard_coef: 0.07485/9 [===============>..............] - ETA: 1s - loss: 0.1685 - accuracy: 0.7083 - jacard_coef: 0.07626/9 [===================>..........] - ETA: 1s - loss: 0.1706 - accuracy: 0.7033 - jacard_coef: 0.07587/9 [======================>.......] - ETA: 0s - loss: 0.1699 - accuracy: 0.6982 - jacard_coef: 0.07408/9 [=========================>....] - ETA: 0s - loss: 0.1694 - accuracy: 0.6918 - jacard_coef: 0.07589/9 [==============================] - 3s 338ms/step - loss: 0.1694 - accuracy: 0.6908 - jacard_coef: 0.0808 - val_loss: 1.1148 - val_accuracy: 0.9302 - val_jacard_coef: 6.0264e-04 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1714 - accuracy: 0.3589 - jacard_coef: 0.08052/9 [=====>........................] - ETA: 2s - loss: 0.1666 - accuracy: 0.6242 - jacard_coef: 0.07373/9 [=========>....................] - ETA: 2s - loss: 0.1693 - accuracy: 0.6957 - jacard_coef: 0.07264/9 [============>.................] - ETA: 1s - loss: 0.1677 - accuracy: 0.7243 - jacard_coef: 0.07865/9 [===============>..............] - ETA: 1s - loss: 0.1672 - accuracy: 0.7290 - jacard_coef: 0.07336/9 [===================>..........] - ETA: 1s - loss: 0.1666 - accuracy: 0.7560 - jacard_coef: 0.07327/9 [======================>.......] - ETA: 0s - loss: 0.1662 - accuracy: 0.7765 - jacard_coef: 0.07368/9 [=========================>....] - ETA: 0s - loss: 0.1652 - accuracy: 0.7925 - jacard_coef: 0.07519/9 [==============================] - 3s 358ms/step - loss: 0.1653 - accuracy: 0.7906 - jacard_coef: 0.0814 - val_loss: 0.2491 - val_accuracy: 0.8770 - val_jacard_coef: 0.0579 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1617 - accuracy: 0.9087 - jacard_coef: 0.07722/9 [=====>........................] - ETA: 2s - loss: 0.1611 - accuracy: 0.9086 - jacard_coef: 0.07263/9 [=========>....................] - ETA: 2s - loss: 0.1636 - accuracy: 0.8999 - jacard_coef: 0.08054/9 [============>.................] - ETA: 1s - loss: 0.1632 - accuracy: 0.8986 - jacard_coef: 0.08135/9 [===============>..............] - ETA: 1s - loss: 0.1623 - accuracy: 0.8876 - jacard_coef: 0.07746/9 [===================>..........] - ETA: 1s - loss: 0.1616 - accuracy: 0.8921 - jacard_coef: 0.07647/9 [======================>.......] - ETA: 0s - loss: 0.1610 - accuracy: 0.8928 - jacard_coef: 0.07848/9 [=========================>....] - ETA: 0s - loss: 0.1602 - accuracy: 0.8982 - jacard_coef: 0.07559/9 [==============================] - 3s 339ms/step - loss: 0.1602 - accuracy: 0.8968 - jacard_coef: 0.0790 - val_loss: 0.4190 - val_accuracy: 0.9158 - val_jacard_coef: 0.0278 - lr: 0.0010
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1542 - accuracy: 0.9061 - jacard_coef: 0.07792/9 [=====>........................] - ETA: 2s - loss: 0.1575 - accuracy: 0.9061 - jacard_coef: 0.07763/9 [=========>....................] - ETA: 2s - loss: 0.1563 - accuracy: 0.9114 - jacard_coef: 0.07224/9 [============>.................] - ETA: 1s - loss: 0.1559 - accuracy: 0.9064 - jacard_coef: 0.07535/9 [===============>..............] - ETA: 1s - loss: 0.1556 - accuracy: 0.9070 - jacard_coef: 0.07396/9 [===================>..........] - ETA: 1s - loss: 0.1551 - accuracy: 0.9059 - jacard_coef: 0.07387/9 [======================>.......] - ETA: 0s - loss: 0.1556 - accuracy: 0.8586 - jacard_coef: 0.07728/9 [=========================>....] - ETA: 0s - loss: 0.1554 - accuracy: 0.8664 - jacard_coef: 0.07529/9 [==============================] - 3s 339ms/step - loss: 0.1555 - accuracy: 0.8645 - jacard_coef: 0.0836 - val_loss: 0.2501 - val_accuracy: 0.8857 - val_jacard_coef: 0.0567 - lr: 0.0010
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1556 - accuracy: 0.9026 - jacard_coef: 0.08392/9 [=====>........................] - ETA: 2s - loss: 0.1555 - accuracy: 0.9113 - jacard_coef: 0.07723/9 [=========>....................] - ETA: 2s - loss: 0.1545 - accuracy: 0.9090 - jacard_coef: 0.07844/9 [============>.................] - ETA: 1s - loss: 0.1545 - accuracy: 0.8979 - jacard_coef: 0.08035/9 [===============>..............] - ETA: 1s - loss: 0.1542 - accuracy: 0.8916 - jacard_coef: 0.08126/9 [===================>..........] - ETA: 1s - loss: 0.1537 - accuracy: 0.8899 - jacard_coef: 0.07857/9 [======================>.......] - ETA: 0s - loss: 0.1545 - accuracy: 0.8864 - jacard_coef: 0.07608/9 [=========================>....] - ETA: 0s - loss: 0.1542 - accuracy: 0.8807 - jacard_coef: 0.07609/9 [==============================] - 3s 359ms/step - loss: 0.1547 - accuracy: 0.8793 - jacard_coef: 0.0727 - val_loss: 0.1976 - val_accuracy: 0.4108 - val_jacard_coef: 0.0664 - lr: 0.0010
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1549 - accuracy: 0.8687 - jacard_coef: 0.04962/9 [=====>........................] - ETA: 2s - loss: 0.1550 - accuracy: 0.8689 - jacard_coef: 0.06123/9 [=========>....................] - ETA: 2s - loss: 0.1568 - accuracy: 0.8821 - jacard_coef: 0.06424/9 [============>.................] - ETA: 1s - loss: 0.1579 - accuracy: 0.8834 - jacard_coef: 0.07095/9 [===============>..............] - ETA: 1s - loss: 0.1566 - accuracy: 0.8893 - jacard_coef: 0.07186/9 [===================>..........] - ETA: 1s - loss: 0.1571 - accuracy: 0.8939 - jacard_coef: 0.07207/9 [======================>.......] - ETA: 0s - loss: 0.1567 - accuracy: 0.8981 - jacard_coef: 0.07178/9 [=========================>....] - ETA: 0s - loss: 0.1564 - accuracy: 0.8974 - jacard_coef: 0.07489/9 [==============================] - 3s 339ms/step - loss: 0.1564 - accuracy: 0.8956 - jacard_coef: 0.0829 - val_loss: 0.2123 - val_accuracy: 0.9091 - val_jacard_coef: 0.0655 - lr: 0.0010
Epoch 16/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1533 - accuracy: 0.9117 - jacard_coef: 0.07952/9 [=====>........................] - ETA: 2s - loss: 0.1540 - accuracy: 0.9238 - jacard_coef: 0.06753/9 [=========>....................] - ETA: 2s - loss: 0.1531 - accuracy: 0.9216 - jacard_coef: 0.06734/9 [============>.................] - ETA: 1s - loss: 0.1524 - accuracy: 0.9171 - jacard_coef: 0.07065/9 [===============>..............] - ETA: 1s - loss: 0.1527 - accuracy: 0.9199 - jacard_coef: 0.06846/9 [===================>..........] - ETA: 1s - loss: 0.1525 - accuracy: 0.9165 - jacard_coef: 0.07217/9 [======================>.......] - ETA: 0s - loss: 0.1521 - accuracy: 0.9134 - jacard_coef: 0.07538/9 [=========================>....] - ETA: 0s - loss: 0.1516 - accuracy: 0.9136 - jacard_coef: 0.07569/9 [==============================] - 3s 339ms/step - loss: 0.1516 - accuracy: 0.9136 - jacard_coef: 0.0715 - val_loss: 0.1670 - val_accuracy: 0.6185 - val_jacard_coef: 0.0637 - lr: 0.0010
Epoch 17/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1532 - accuracy: 0.9091 - jacard_coef: 0.07372/9 [=====>........................] - ETA: 2s - loss: 0.1523 - accuracy: 0.9085 - jacard_coef: 0.07463/9 [=========>....................] - ETA: 2s - loss: 0.1516 - accuracy: 0.9147 - jacard_coef: 0.06924/9 [============>.................] - ETA: 1s - loss: 0.1515 - accuracy: 0.9158 - jacard_coef: 0.06945/9 [===============>..............] - ETA: 1s - loss: 0.1508 - accuracy: 0.9138 - jacard_coef: 0.07236/9 [===================>..........] - ETA: 1s - loss: 0.1512 - accuracy: 0.9129 - jacard_coef: 0.07417/9 [======================>.......] - ETA: 0s - loss: 0.1508 - accuracy: 0.9128 - jacard_coef: 0.07498/9 [=========================>....] - ETA: 0s - loss: 0.1502 - accuracy: 0.9136 - jacard_coef: 0.07489/9 [==============================] - 3s 338ms/step - loss: 0.1503 - accuracy: 0.9111 - jacard_coef: 0.0822 - val_loss: 0.1630 - val_accuracy: 0.7032 - val_jacard_coef: 0.0639 - lr: 0.0010
Epoch 18/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1483 - accuracy: 0.8997 - jacard_coef: 0.09032/9 [=====>........................] - ETA: 2s - loss: 0.1481 - accuracy: 0.9040 - jacard_coef: 0.08643/9 [=========>....................] - ETA: 2s - loss: 0.1502 - accuracy: 0.9115 - jacard_coef: 0.07934/9 [============>.................] - ETA: 1s - loss: 0.1500 - accuracy: 0.9073 - jacard_coef: 0.08215/9 [===============>..............] - ETA: 1s - loss: 0.1495 - accuracy: 0.9078 - jacard_coef: 0.08106/9 [===================>..........] - ETA: 1s - loss: 0.1489 - accuracy: 0.9130 - jacard_coef: 0.07607/9 [======================>.......] - ETA: 0s - loss: 0.1487 - accuracy: 0.9131 - jacard_coef: 0.07558/9 [=========================>....] - ETA: 0s - loss: 0.1484 - accuracy: 0.9126 - jacard_coef: 0.07589/9 [==============================] - 3s 339ms/step - loss: 0.1484 - accuracy: 0.9131 - jacard_coef: 0.0681 - val_loss: 0.1493 - val_accuracy: 0.9242 - val_jacard_coef: 0.0626 - lr: 0.0010
Epoch 19/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1530 - accuracy: 0.9210 - jacard_coef: 0.07042/9 [=====>........................] - ETA: 2s - loss: 0.1496 - accuracy: 0.9168 - jacard_coef: 0.07393/9 [=========>....................] - ETA: 2s - loss: 0.1473 - accuracy: 0.9249 - jacard_coef: 0.06664/9 [============>.................] - ETA: 1s - loss: 0.1479 - accuracy: 0.9177 - jacard_coef: 0.07215/9 [===============>..............] - ETA: 1s - loss: 0.1476 - accuracy: 0.9151 - jacard_coef: 0.07426/9 [===================>..........] - ETA: 1s - loss: 0.1473 - accuracy: 0.9139 - jacard_coef: 0.07557/9 [======================>.......] - ETA: 0s - loss: 0.1467 - accuracy: 0.9138 - jacard_coef: 0.07588/9 [=========================>....] - ETA: 0s - loss: 0.1463 - accuracy: 0.9154 - jacard_coef: 0.07479/9 [==============================] - 3s 339ms/step - loss: 0.1464 - accuracy: 0.9146 - jacard_coef: 0.0837 - val_loss: 0.1574 - val_accuracy: 0.9304 - val_jacard_coef: 0.0649 - lr: 0.0010
Epoch 20/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1477 - accuracy: 0.8863 - jacard_coef: 0.09992/9 [=====>........................] - ETA: 2s - loss: 0.1440 - accuracy: 0.9146 - jacard_coef: 0.07673/9 [=========>....................] - ETA: 2s - loss: 0.1443 - accuracy: 0.9055 - jacard_coef: 0.08444/9 [============>.................] - ETA: 1s - loss: 0.1444 - accuracy: 0.9043 - jacard_coef: 0.08575/9 [===============>..............] - ETA: 1s - loss: 0.1449 - accuracy: 0.9119 - jacard_coef: 0.07936/9 [===================>..........] - ETA: 1s - loss: 0.1441 - accuracy: 0.9161 - jacard_coef: 0.07587/9 [======================>.......] - ETA: 0s - loss: 0.1439 - accuracy: 0.9161 - jacard_coef: 0.07588/9 [=========================>....] - ETA: 0s - loss: 0.1436 - accuracy: 0.9163 - jacard_coef: 0.07559/9 [==============================] - 3s 338ms/step - loss: 0.1436 - accuracy: 0.9169 - jacard_coef: 0.0678 - val_loss: 0.1571 - val_accuracy: 0.9304 - val_jacard_coef: 0.0647 - lr: 5.0000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1481 - accuracy: 0.9021 - jacard_coef: 0.08732/9 [=====>........................] - ETA: 2s - loss: 0.1439 - accuracy: 0.9179 - jacard_coef: 0.07403/9 [=========>....................] - ETA: 2s - loss: 0.1429 - accuracy: 0.9236 - jacard_coef: 0.06774/9 [============>.................] - ETA: 1s - loss: 0.1431 - accuracy: 0.9186 - jacard_coef: 0.07235/9 [===============>..............] - ETA: 1s - loss: 0.1432 - accuracy: 0.9153 - jacard_coef: 0.07556/9 [===================>..........] - ETA: 1s - loss: 0.1428 - accuracy: 0.9155 - jacard_coef: 0.07567/9 [======================>.......] - ETA: 0s - loss: 0.1425 - accuracy: 0.9159 - jacard_coef: 0.07548/9 [=========================>....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9160 - jacard_coef: 0.07569/9 [==============================] - 3s 339ms/step - loss: 0.1426 - accuracy: 0.9162 - jacard_coef: 0.0695 - val_loss: 0.1540 - val_accuracy: 0.9304 - val_jacard_coef: 0.0647 - lr: 5.0000e-04
Epoch 22/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1406 - accuracy: 0.9129 - jacard_coef: 0.07632/9 [=====>........................] - ETA: 2s - loss: 0.1412 - accuracy: 0.9134 - jacard_coef: 0.07623/9 [=========>....................] - ETA: 2s - loss: 0.1417 - accuracy: 0.9138 - jacard_coef: 0.07564/9 [============>.................] - ETA: 1s - loss: 0.1409 - accuracy: 0.9196 - jacard_coef: 0.07105/9 [===============>..............] - ETA: 1s - loss: 0.1410 - accuracy: 0.9186 - jacard_coef: 0.07226/9 [===================>..........] - ETA: 1s - loss: 0.1415 - accuracy: 0.9116 - jacard_coef: 0.07827/9 [======================>.......] - ETA: 0s - loss: 0.1412 - accuracy: 0.9133 - jacard_coef: 0.07718/9 [=========================>....] - ETA: 0s - loss: 0.1415 - accuracy: 0.9154 - jacard_coef: 0.07559/9 [==============================] - 3s 339ms/step - loss: 0.1416 - accuracy: 0.9153 - jacard_coef: 0.0676 - val_loss: 0.1534 - val_accuracy: 0.9304 - val_jacard_coef: 0.0647 - lr: 5.0000e-04
Epoch 23/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1414 - accuracy: 0.9131 - jacard_coef: 0.07892/9 [=====>........................] - ETA: 2s - loss: 0.1396 - accuracy: 0.9237 - jacard_coef: 0.07003/9 [=========>....................] - ETA: 2s - loss: 0.1407 - accuracy: 0.9262 - jacard_coef: 0.06774/9 [============>.................] - ETA: 1s - loss: 0.1400 - accuracy: 0.9272 - jacard_coef: 0.06675/9 [===============>..............] - ETA: 1s - loss: 0.1404 - accuracy: 0.9216 - jacard_coef: 0.07116/9 [===================>..........] - ETA: 1s - loss: 0.1410 - accuracy: 0.9161 - jacard_coef: 0.07317/9 [======================>.......] - ETA: 0s - loss: 0.1406 - accuracy: 0.9175 - jacard_coef: 0.07228/9 [=========================>....] - ETA: 0s - loss: 0.1407 - accuracy: 0.9144 - jacard_coef: 0.07519/9 [==============================] - 3s 339ms/step - loss: 0.1407 - accuracy: 0.9144 - jacard_coef: 0.0740 - val_loss: 0.1544 - val_accuracy: 0.9304 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
Epoch 24/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1394 - accuracy: 0.9182 - jacard_coef: 0.07482/9 [=====>........................] - ETA: 2s - loss: 0.1391 - accuracy: 0.9218 - jacard_coef: 0.07183/9 [=========>....................] - ETA: 2s - loss: 0.1386 - accuracy: 0.9241 - jacard_coef: 0.06994/9 [============>.................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9230 - jacard_coef: 0.07075/9 [===============>..............] - ETA: 1s - loss: 0.1384 - accuracy: 0.9263 - jacard_coef: 0.06796/9 [===================>..........] - ETA: 1s - loss: 0.1382 - accuracy: 0.9263 - jacard_coef: 0.06797/9 [======================>.......] - ETA: 0s - loss: 0.1392 - accuracy: 0.9192 - jacard_coef: 0.07368/9 [=========================>....] - ETA: 0s - loss: 0.1392 - accuracy: 0.9171 - jacard_coef: 0.07529/9 [==============================] - 3s 339ms/step - loss: 0.1393 - accuracy: 0.9157 - jacard_coef: 0.0709 - val_loss: 0.1505 - val_accuracy: 0.9304 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0664 (epoch 14)
  Final Val Loss: 0.1505
  Training Time: 0:02:31.171633
  Stability (std): 0.0176

Results saved to: hyperparameter_optimization_20250926_123742/exp_20_Attention_UNet_lr1e-3_bs16/Attention_UNet_lr0.001_bs16_results.json

Experiment 20 completed in 188s
Progress: 20/36 completed
Estimated remaining time: 50 minutes

ðŸ”¬ EXPERIMENT 21/36
================================================
Architecture: Attention_UNet
Learning Rate: 1e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758864502.501082 3276744 service.cc:145] XLA service 0x149634a43820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758864502.501120 3276744 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758864502.891196 3276744 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:22 - loss: 0.3379 - accuracy: 0.4989 - jacard_coef: 0.07312/5 [===========>..................] - ETA: 47s - loss: 0.3113 - accuracy: 0.4372 - jacard_coef: 0.0766 3/5 [=================>............] - ETA: 16s - loss: 0.2838 - accuracy: 0.3609 - jacard_coef: 0.07894/5 [=======================>......] - ETA: 5s - loss: 0.2667 - accuracy: 0.3277 - jacard_coef: 0.0775 5/5 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.3258 - jacard_coef: 0.06615/5 [==============================] - 91s 6s/step - loss: 0.2663 - accuracy: 0.3258 - jacard_coef: 0.0661 - val_loss: 1.1043 - val_accuracy: 0.9304 - val_jacard_coef: 7.4220e-05 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1997 - accuracy: 0.1313 - jacard_coef: 0.07322/5 [===========>..................] - ETA: 2s - loss: 0.1957 - accuracy: 0.1498 - jacard_coef: 0.06763/5 [=================>............] - ETA: 1s - loss: 0.1911 - accuracy: 0.2016 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 0s - loss: 0.1883 - accuracy: 0.2432 - jacard_coef: 0.07605/5 [==============================] - 3s 567ms/step - loss: 0.1889 - accuracy: 0.2437 - jacard_coef: 0.0891 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.2239 - accuracy: 0.1872 - jacard_coef: 0.08282/5 [===========>..................] - ETA: 2s - loss: 0.2143 - accuracy: 0.1814 - jacard_coef: 0.07163/5 [=================>............] - ETA: 1s - loss: 0.2058 - accuracy: 0.2266 - jacard_coef: 0.07224/5 [=======================>......] - ETA: 0s - loss: 0.1998 - accuracy: 0.2383 - jacard_coef: 0.07675/5 [==============================] - 3s 567ms/step - loss: 0.1997 - accuracy: 0.2391 - jacard_coef: 0.0622 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1804 - accuracy: 0.3614 - jacard_coef: 0.07352/5 [===========>..................] - ETA: 2s - loss: 0.1812 - accuracy: 0.3732 - jacard_coef: 0.08513/5 [=================>............] - ETA: 1s - loss: 0.1826 - accuracy: 0.3302 - jacard_coef: 0.07614/5 [=======================>......] - ETA: 0s - loss: 0.1812 - accuracy: 0.3087 - jacard_coef: 0.07575/5 [==============================] - 3s 568ms/step - loss: 0.1812 - accuracy: 0.3081 - jacard_coef: 0.0807 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.2037 - accuracy: 0.2815 - jacard_coef: 0.08012/5 [===========>..................] - ETA: 2s - loss: 0.1916 - accuracy: 0.2870 - jacard_coef: 0.07513/5 [=================>............] - ETA: 1s - loss: 0.1903 - accuracy: 0.2883 - jacard_coef: 0.08014/5 [=======================>......] - ETA: 0s - loss: 0.1871 - accuracy: 0.3238 - jacard_coef: 0.07655/5 [==============================] - 3s 580ms/step - loss: 0.1871 - accuracy: 0.3236 - jacard_coef: 0.0721 - val_loss: 1.2310 - val_accuracy: 0.9019 - val_jacard_coef: 0.0118 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1761 - accuracy: 0.5403 - jacard_coef: 0.09262/5 [===========>..................] - ETA: 2s - loss: 0.1782 - accuracy: 0.5159 - jacard_coef: 0.08253/5 [=================>............] - ETA: 1s - loss: 0.1789 - accuracy: 0.4837 - jacard_coef: 0.07844/5 [=======================>......] - ETA: 0s - loss: 0.1780 - accuracy: 0.4619 - jacard_coef: 0.07625/5 [==============================] - 3s 567ms/step - loss: 0.1786 - accuracy: 0.4616 - jacard_coef: 0.0683 - val_loss: 1.1224 - val_accuracy: 0.9232 - val_jacard_coef: 0.0021 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1735 - accuracy: 0.5430 - jacard_coef: 0.07192/5 [===========>..................] - ETA: 2s - loss: 0.1755 - accuracy: 0.5236 - jacard_coef: 0.07233/5 [=================>............] - ETA: 1s - loss: 0.1754 - accuracy: 0.5135 - jacard_coef: 0.07554/5 [=======================>......] - ETA: 0s - loss: 0.1758 - accuracy: 0.5268 - jacard_coef: 0.07575/5 [==============================] - 3s 587ms/step - loss: 0.1759 - accuracy: 0.5250 - jacard_coef: 0.0871 - val_loss: 13.7057 - val_accuracy: 0.0904 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1750 - accuracy: 0.4779 - jacard_coef: 0.06652/5 [===========>..................] - ETA: 2s - loss: 0.1754 - accuracy: 0.3816 - jacard_coef: 0.07273/5 [=================>............] - ETA: 1s - loss: 0.1756 - accuracy: 0.4762 - jacard_coef: 0.07584/5 [=======================>......] - ETA: 0s - loss: 0.1744 - accuracy: 0.5741 - jacard_coef: 0.07635/5 [==============================] - 3s 569ms/step - loss: 0.1744 - accuracy: 0.5709 - jacard_coef: 0.0732 - val_loss: 0.9858 - val_accuracy: 0.9265 - val_jacard_coef: 0.0014 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1694 - accuracy: 0.8754 - jacard_coef: 0.08902/5 [===========>..................] - ETA: 2s - loss: 0.1727 - accuracy: 0.6467 - jacard_coef: 0.07293/5 [=================>............] - ETA: 1s - loss: 0.1730 - accuracy: 0.6018 - jacard_coef: 0.07444/5 [=======================>......] - ETA: 0s - loss: 0.1727 - accuracy: 0.5710 - jacard_coef: 0.07645/5 [==============================] - 3s 575ms/step - loss: 0.1731 - accuracy: 0.5685 - jacard_coef: 0.0617 - val_loss: 1.1134 - val_accuracy: 0.9303 - val_jacard_coef: 2.6765e-04 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.2042 - accuracy: 0.1720 - jacard_coef: 0.06202/5 [===========>..................] - ETA: 2s - loss: 0.2109 - accuracy: 0.1672 - jacard_coef: 0.07213/5 [=================>............] - ETA: 1s - loss: 0.2060 - accuracy: 0.1642 - jacard_coef: 0.07784/5 [=======================>......] - ETA: 0s - loss: 0.2046 - accuracy: 0.1547 - jacard_coef: 0.07675/5 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.1540 - jacard_coef: 0.06195/5 [==============================] - 3s 580ms/step - loss: 0.2046 - accuracy: 0.1540 - jacard_coef: 0.0619 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1950 - accuracy: 0.1206 - jacard_coef: 0.05612/5 [===========>..................] - ETA: 2s - loss: 0.1959 - accuracy: 0.1254 - jacard_coef: 0.06303/5 [=================>............] - ETA: 1s - loss: 0.1962 - accuracy: 0.1323 - jacard_coef: 0.06864/5 [=======================>......] - ETA: 0s - loss: 0.1952 - accuracy: 0.1379 - jacard_coef: 0.07585/5 [==============================] - 3s 579ms/step - loss: 0.1954 - accuracy: 0.1379 - jacard_coef: 0.0796 - val_loss: 1.1217 - val_accuracy: 0.9304 - val_jacard_coef: 1.4632e-05 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1958 - accuracy: 0.1187 - jacard_coef: 0.08052/5 [===========>..................] - ETA: 2s - loss: 0.1890 - accuracy: 0.1265 - jacard_coef: 0.07363/5 [=================>............] - ETA: 1s - loss: 0.1893 - accuracy: 0.1413 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1865 - accuracy: 0.1554 - jacard_coef: 0.07685/5 [==============================] - 3s 579ms/step - loss: 0.1865 - accuracy: 0.1552 - jacard_coef: 0.0620 - val_loss: 1.1166 - val_accuracy: 0.9303 - val_jacard_coef: 2.4572e-04 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1787 - accuracy: 0.2374 - jacard_coef: 0.06172/5 [===========>..................] - ETA: 2s - loss: 0.1808 - accuracy: 0.3556 - jacard_coef: 0.07143/5 [=================>............] - ETA: 1s - loss: 0.1819 - accuracy: 0.4146 - jacard_coef: 0.07424/5 [=======================>......] - ETA: 0s - loss: 0.1803 - accuracy: 0.4027 - jacard_coef: 0.07575/5 [==============================] - 3s 579ms/step - loss: 0.1804 - accuracy: 0.4015 - jacard_coef: 0.0900 - val_loss: 1.0909 - val_accuracy: 0.9301 - val_jacard_coef: 0.0011 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1745 - accuracy: 0.4066 - jacard_coef: 0.08172/5 [===========>..................] - ETA: 2s - loss: 0.1776 - accuracy: 0.4421 - jacard_coef: 0.07853/5 [=================>............] - ETA: 1s - loss: 0.1760 - accuracy: 0.4435 - jacard_coef: 0.07474/5 [=======================>......] - ETA: 0s - loss: 0.1746 - accuracy: 0.4535 - jacard_coef: 0.07595/5 [==============================] - 3s 580ms/step - loss: 0.1747 - accuracy: 0.4530 - jacard_coef: 0.0852 - val_loss: 0.7497 - val_accuracy: 0.9274 - val_jacard_coef: 0.0050 - lr: 5.0000e-04
Epoch 15/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1720 - accuracy: 0.4521 - jacard_coef: 0.05882/5 [===========>..................] - ETA: 2s - loss: 0.1724 - accuracy: 0.4404 - jacard_coef: 0.07233/5 [=================>............] - ETA: 1s - loss: 0.1755 - accuracy: 0.4510 - jacard_coef: 0.07674/5 [=======================>......] - ETA: 0s - loss: 0.1747 - accuracy: 0.4529 - jacard_coef: 0.07645/5 [==============================] - 3s 580ms/step - loss: 0.1747 - accuracy: 0.4510 - jacard_coef: 0.0724 - val_loss: 0.6795 - val_accuracy: 0.9271 - val_jacard_coef: 0.0110 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1725 - accuracy: 0.4831 - jacard_coef: 0.07482/5 [===========>..................] - ETA: 2s - loss: 0.1728 - accuracy: 0.4908 - jacard_coef: 0.08423/5 [=================>............] - ETA: 1s - loss: 0.1756 - accuracy: 0.5467 - jacard_coef: 0.07834/5 [=======================>......] - ETA: 0s - loss: 0.1750 - accuracy: 0.5616 - jacard_coef: 0.07645/5 [==============================] - 3s 580ms/step - loss: 0.1751 - accuracy: 0.5586 - jacard_coef: 0.0715 - val_loss: 0.1752 - val_accuracy: 0.9297 - val_jacard_coef: 0.0483 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1746 - accuracy: 0.6442 - jacard_coef: 0.06502/5 [===========>..................] - ETA: 2s - loss: 0.1762 - accuracy: 0.7025 - jacard_coef: 0.06493/5 [=================>............] - ETA: 1s - loss: 0.1743 - accuracy: 0.7217 - jacard_coef: 0.07164/5 [=======================>......] - ETA: 0s - loss: 0.1736 - accuracy: 0.7269 - jacard_coef: 0.07675/5 [==============================] - 3s 578ms/step - loss: 0.1736 - accuracy: 0.7241 - jacard_coef: 0.0627 - val_loss: 0.1027 - val_accuracy: 0.9262 - val_jacard_coef: 0.0625 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0701 (epoch 7)
  Final Val Loss: 0.1027
  Training Time: 0:02:20.612407
  Stability (std): 0.3757

Results saved to: hyperparameter_optimization_20250926_123742/exp_21_Attention_UNet_lr1e-3_bs32/Attention_UNet_lr0.001_bs32_results.json

Experiment 21 completed in 176s
Progress: 21/36 completed
Estimated remaining time: 44 minutes

ðŸ”¬ EXPERIMENT 22/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758864672.615502 3283265 service.cc:145] XLA service 0x1524a9d10270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758864672.615577 3283265 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758864673.082565 3283265 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 14:54 - loss: 0.3329 - accuracy: 0.4954 - jacard_coef: 0.0620 2/17 [==>...........................] - ETA: 1:12 - loss: 0.2880 - accuracy: 0.4432 - jacard_coef: 0.0908  3/17 [====>.........................] - ETA: 35s - loss: 0.2624 - accuracy: 0.3396 - jacard_coef: 0.0742  4/17 [======>.......................] - ETA: 22s - loss: 0.2479 - accuracy: 0.3398 - jacard_coef: 0.0750 5/17 [=======>......................] - ETA: 16s - loss: 0.2466 - accuracy: 0.3271 - jacard_coef: 0.0700 6/17 [=========>....................] - ETA: 12s - loss: 0.2408 - accuracy: 0.2999 - jacard_coef: 0.0704 7/17 [===========>..................] - ETA: 9s - loss: 0.2348 - accuracy: 0.2795 - jacard_coef: 0.0710  8/17 [=============>................] - ETA: 7s - loss: 0.2301 - accuracy: 0.2704 - jacard_coef: 0.0764 9/17 [==============>...............] - ETA: 6s - loss: 0.2259 - accuracy: 0.2664 - jacard_coef: 0.075910/17 [================>.............] - ETA: 4s - loss: 0.2220 - accuracy: 0.2624 - jacard_coef: 0.073911/17 [==================>...........] - ETA: 3s - loss: 0.2184 - accuracy: 0.2618 - jacard_coef: 0.075712/17 [====================>.........] - ETA: 3s - loss: 0.2158 - accuracy: 0.2591 - jacard_coef: 0.075613/17 [=====================>........] - ETA: 2s - loss: 0.2135 - accuracy: 0.2590 - jacard_coef: 0.075814/17 [=======================>......] - ETA: 1s - loss: 0.2112 - accuracy: 0.2594 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 1s - loss: 0.2094 - accuracy: 0.2601 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.2081 - accuracy: 0.2640 - jacard_coef: 0.076217/17 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.2652 - jacard_coef: 0.079617/17 [==============================] - 71s 913ms/step - loss: 0.2079 - accuracy: 0.2652 - jacard_coef: 0.0796 - val_loss: 1.0876 - val_accuracy: 0.9304 - val_jacard_coef: 0.0021 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1823 - accuracy: 0.4449 - jacard_coef: 0.0993 2/17 [==>...........................] - ETA: 2s - loss: 0.1827 - accuracy: 0.4019 - jacard_coef: 0.0951 3/17 [====>.........................] - ETA: 2s - loss: 0.1812 - accuracy: 0.3893 - jacard_coef: 0.0844 4/17 [======>.......................] - ETA: 2s - loss: 0.1818 - accuracy: 0.3651 - jacard_coef: 0.0799 5/17 [=======>......................] - ETA: 2s - loss: 0.1818 - accuracy: 0.3561 - jacard_coef: 0.0807 6/17 [=========>....................] - ETA: 2s - loss: 0.1810 - accuracy: 0.3529 - jacard_coef: 0.0821 7/17 [===========>..................] - ETA: 1s - loss: 0.1808 - accuracy: 0.3624 - jacard_coef: 0.0804 8/17 [=============>................] - ETA: 1s - loss: 0.1800 - accuracy: 0.3885 - jacard_coef: 0.0832 9/17 [==============>...............] - ETA: 1s - loss: 0.1792 - accuracy: 0.4234 - jacard_coef: 0.079010/17 [================>.............] - ETA: 1s - loss: 0.1785 - accuracy: 0.4536 - jacard_coef: 0.077811/17 [==================>...........] - ETA: 1s - loss: 0.1818 - accuracy: 0.4406 - jacard_coef: 0.079812/17 [====================>.........] - ETA: 0s - loss: 0.1814 - accuracy: 0.4206 - jacard_coef: 0.079813/17 [=====================>........] - ETA: 0s - loss: 0.1812 - accuracy: 0.4005 - jacard_coef: 0.077714/17 [=======================>......] - ETA: 0s - loss: 0.1810 - accuracy: 0.3868 - jacard_coef: 0.079015/17 [=========================>....] - ETA: 0s - loss: 0.1808 - accuracy: 0.3720 - jacard_coef: 0.077816/17 [===========================>..] - ETA: 0s - loss: 0.1805 - accuracy: 0.3634 - jacard_coef: 0.076317/17 [==============================] - 3s 188ms/step - loss: 0.1805 - accuracy: 0.3620 - jacard_coef: 0.0722 - val_loss: 0.9782 - val_accuracy: 0.9265 - val_jacard_coef: 0.0101 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1764 - accuracy: 0.2336 - jacard_coef: 0.0657 2/17 [==>...........................] - ETA: 2s - loss: 0.1773 - accuracy: 0.2761 - jacard_coef: 0.0756 3/17 [====>.........................] - ETA: 2s - loss: 0.1766 - accuracy: 0.3077 - jacard_coef: 0.0739 4/17 [======>.......................] - ETA: 2s - loss: 0.1768 - accuracy: 0.3363 - jacard_coef: 0.0826 5/17 [=======>......................] - ETA: 2s - loss: 0.1763 - accuracy: 0.3625 - jacard_coef: 0.0836 6/17 [=========>....................] - ETA: 2s - loss: 0.1759 - accuracy: 0.4192 - jacard_coef: 0.0840 7/17 [===========>..................] - ETA: 1s - loss: 0.1756 - accuracy: 0.4592 - jacard_coef: 0.0780 8/17 [=============>................] - ETA: 1s - loss: 0.1753 - accuracy: 0.5003 - jacard_coef: 0.0772 9/17 [==============>...............] - ETA: 1s - loss: 0.1753 - accuracy: 0.5053 - jacard_coef: 0.078410/17 [================>.............] - ETA: 1s - loss: 0.1749 - accuracy: 0.5347 - jacard_coef: 0.079511/17 [==================>...........] - ETA: 1s - loss: 0.1748 - accuracy: 0.5603 - jacard_coef: 0.079112/17 [====================>.........] - ETA: 0s - loss: 0.1746 - accuracy: 0.5835 - jacard_coef: 0.077513/17 [=====================>........] - ETA: 0s - loss: 0.1743 - accuracy: 0.6029 - jacard_coef: 0.077114/17 [=======================>......] - ETA: 0s - loss: 0.1741 - accuracy: 0.6178 - jacard_coef: 0.078015/17 [=========================>....] - ETA: 0s - loss: 0.1739 - accuracy: 0.6299 - jacard_coef: 0.078316/17 [===========================>..] - ETA: 0s - loss: 0.1737 - accuracy: 0.6435 - jacard_coef: 0.076017/17 [==============================] - 3s 188ms/step - loss: 0.1737 - accuracy: 0.6449 - jacard_coef: 0.0746 - val_loss: 0.5270 - val_accuracy: 0.7929 - val_jacard_coef: 0.0477 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1687 - accuracy: 0.8483 - jacard_coef: 0.0833 2/17 [==>...........................] - ETA: 2s - loss: 0.1691 - accuracy: 0.8694 - jacard_coef: 0.0720 3/17 [====>.........................] - ETA: 2s - loss: 0.1685 - accuracy: 0.8683 - jacard_coef: 0.0778 4/17 [======>.......................] - ETA: 2s - loss: 0.1681 - accuracy: 0.8713 - jacard_coef: 0.0762 5/17 [=======>......................] - ETA: 2s - loss: 0.1682 - accuracy: 0.8765 - jacard_coef: 0.0724 6/17 [=========>....................] - ETA: 2s - loss: 0.1682 - accuracy: 0.8760 - jacard_coef: 0.0731 7/17 [===========>..................] - ETA: 1s - loss: 0.1681 - accuracy: 0.8769 - jacard_coef: 0.0714 8/17 [=============>................] - ETA: 1s - loss: 0.1678 - accuracy: 0.8772 - jacard_coef: 0.0702 9/17 [==============>...............] - ETA: 1s - loss: 0.1675 - accuracy: 0.8778 - jacard_coef: 0.070010/17 [================>.............] - ETA: 1s - loss: 0.1675 - accuracy: 0.8767 - jacard_coef: 0.072311/17 [==================>...........] - ETA: 1s - loss: 0.1672 - accuracy: 0.8771 - jacard_coef: 0.072912/17 [====================>.........] - ETA: 0s - loss: 0.1670 - accuracy: 0.8770 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 0s - loss: 0.1669 - accuracy: 0.8725 - jacard_coef: 0.075814/17 [=======================>......] - ETA: 0s - loss: 0.1666 - accuracy: 0.8665 - jacard_coef: 0.076815/17 [=========================>....] - ETA: 0s - loss: 0.1664 - accuracy: 0.8627 - jacard_coef: 0.077216/17 [===========================>..] - ETA: 0s - loss: 0.1664 - accuracy: 0.8627 - jacard_coef: 0.076217/17 [==============================] - 3s 187ms/step - loss: 0.1663 - accuracy: 0.8635 - jacard_coef: 0.0720 - val_loss: 0.2737 - val_accuracy: 0.5271 - val_jacard_coef: 0.0654 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1622 - accuracy: 0.9006 - jacard_coef: 0.0585 2/17 [==>...........................] - ETA: 2s - loss: 0.1618 - accuracy: 0.9009 - jacard_coef: 0.0621 3/17 [====>.........................] - ETA: 2s - loss: 0.1620 - accuracy: 0.8949 - jacard_coef: 0.0690 4/17 [======>.......................] - ETA: 2s - loss: 0.1621 - accuracy: 0.8958 - jacard_coef: 0.0699 5/17 [=======>......................] - ETA: 2s - loss: 0.1620 - accuracy: 0.8978 - jacard_coef: 0.0706 6/17 [=========>....................] - ETA: 2s - loss: 0.1619 - accuracy: 0.8916 - jacard_coef: 0.0759 7/17 [===========>..................] - ETA: 1s - loss: 0.1616 - accuracy: 0.8937 - jacard_coef: 0.0764 8/17 [=============>................] - ETA: 1s - loss: 0.1613 - accuracy: 0.8944 - jacard_coef: 0.0774 9/17 [==============>...............] - ETA: 1s - loss: 0.1612 - accuracy: 0.8933 - jacard_coef: 0.079810/17 [================>.............] - ETA: 1s - loss: 0.1610 - accuracy: 0.8992 - jacard_coef: 0.075911/17 [==================>...........] - ETA: 1s - loss: 0.1608 - accuracy: 0.9006 - jacard_coef: 0.075112/17 [====================>.........] - ETA: 0s - loss: 0.1608 - accuracy: 0.8989 - jacard_coef: 0.077213/17 [=====================>........] - ETA: 0s - loss: 0.1605 - accuracy: 0.9009 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1602 - accuracy: 0.9017 - jacard_coef: 0.073215/17 [=========================>....] - ETA: 0s - loss: 0.1602 - accuracy: 0.8998 - jacard_coef: 0.074916/17 [===========================>..] - ETA: 0s - loss: 0.1600 - accuracy: 0.9008 - jacard_coef: 0.074817/17 [==============================] - 3s 183ms/step - loss: 0.1600 - accuracy: 0.9002 - jacard_coef: 0.0792 - val_loss: 0.1673 - val_accuracy: 0.9042 - val_jacard_coef: 0.0629 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1588 - accuracy: 0.8924 - jacard_coef: 0.0922 2/17 [==>...........................] - ETA: 2s - loss: 0.1583 - accuracy: 0.9153 - jacard_coef: 0.0706 3/17 [====>.........................] - ETA: 2s - loss: 0.1580 - accuracy: 0.9081 - jacard_coef: 0.0756 4/17 [======>.......................] - ETA: 2s - loss: 0.1577 - accuracy: 0.9073 - jacard_coef: 0.0756 5/17 [=======>......................] - ETA: 2s - loss: 0.1576 - accuracy: 0.9083 - jacard_coef: 0.0742 6/17 [=========>....................] - ETA: 2s - loss: 0.1577 - accuracy: 0.9042 - jacard_coef: 0.0763 7/17 [===========>..................] - ETA: 1s - loss: 0.1576 - accuracy: 0.9039 - jacard_coef: 0.0765 8/17 [=============>................] - ETA: 1s - loss: 0.1572 - accuracy: 0.9100 - jacard_coef: 0.0720 9/17 [==============>...............] - ETA: 1s - loss: 0.1573 - accuracy: 0.9065 - jacard_coef: 0.075810/17 [================>.............] - ETA: 1s - loss: 0.1570 - accuracy: 0.9079 - jacard_coef: 0.074111/17 [==================>...........] - ETA: 1s - loss: 0.1569 - accuracy: 0.9048 - jacard_coef: 0.076712/17 [====================>.........] - ETA: 0s - loss: 0.1567 - accuracy: 0.9089 - jacard_coef: 0.073813/17 [=====================>........] - ETA: 0s - loss: 0.1565 - accuracy: 0.9100 - jacard_coef: 0.073614/17 [=======================>......] - ETA: 0s - loss: 0.1564 - accuracy: 0.9115 - jacard_coef: 0.072915/17 [=========================>....] - ETA: 0s - loss: 0.1564 - accuracy: 0.9098 - jacard_coef: 0.074816/17 [===========================>..] - ETA: 0s - loss: 0.1563 - accuracy: 0.9091 - jacard_coef: 0.075817/17 [==============================] - 3s 183ms/step - loss: 0.1570 - accuracy: 0.9051 - jacard_coef: 0.0716 - val_loss: 0.1094 - val_accuracy: 0.9085 - val_jacard_coef: 0.0576 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1565 - accuracy: 0.8951 - jacard_coef: 0.0703 2/17 [==>...........................] - ETA: 2s - loss: 0.1572 - accuracy: 0.8833 - jacard_coef: 0.0663 3/17 [====>.........................] - ETA: 2s - loss: 0.1567 - accuracy: 0.8831 - jacard_coef: 0.0655 4/17 [======>.......................] - ETA: 2s - loss: 0.1561 - accuracy: 0.8772 - jacard_coef: 0.0659 5/17 [=======>......................] - ETA: 2s - loss: 0.1560 - accuracy: 0.8763 - jacard_coef: 0.0663 6/17 [=========>....................] - ETA: 2s - loss: 0.1559 - accuracy: 0.8744 - jacard_coef: 0.0658 7/17 [===========>..................] - ETA: 1s - loss: 0.1564 - accuracy: 0.8675 - jacard_coef: 0.0685 8/17 [=============>................] - ETA: 1s - loss: 0.1565 - accuracy: 0.8687 - jacard_coef: 0.0698 9/17 [==============>...............] - ETA: 1s - loss: 0.1563 - accuracy: 0.8755 - jacard_coef: 0.067910/17 [================>.............] - ETA: 1s - loss: 0.1562 - accuracy: 0.8784 - jacard_coef: 0.069311/17 [==================>...........] - ETA: 1s - loss: 0.1563 - accuracy: 0.8791 - jacard_coef: 0.071812/17 [====================>.........] - ETA: 0s - loss: 0.1561 - accuracy: 0.8828 - jacard_coef: 0.071513/17 [=====================>........] - ETA: 0s - loss: 0.1562 - accuracy: 0.8845 - jacard_coef: 0.072314/17 [=======================>......] - ETA: 0s - loss: 0.1562 - accuracy: 0.8840 - jacard_coef: 0.074715/17 [=========================>....] - ETA: 0s - loss: 0.1561 - accuracy: 0.8850 - jacard_coef: 0.075416/17 [===========================>..] - ETA: 0s - loss: 0.1562 - accuracy: 0.8866 - jacard_coef: 0.075717/17 [==============================] - 3s 184ms/step - loss: 0.1563 - accuracy: 0.8871 - jacard_coef: 0.0742 - val_loss: 0.1349 - val_accuracy: 0.8521 - val_jacard_coef: 0.0597 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1563 - accuracy: 0.8984 - jacard_coef: 0.0807 2/17 [==>...........................] - ETA: 2s - loss: 0.1554 - accuracy: 0.8884 - jacard_coef: 0.0893 3/17 [====>.........................] - ETA: 2s - loss: 0.1542 - accuracy: 0.8928 - jacard_coef: 0.0883 4/17 [======>.......................] - ETA: 2s - loss: 0.1536 - accuracy: 0.9026 - jacard_coef: 0.0813 5/17 [=======>......................] - ETA: 2s - loss: 0.1535 - accuracy: 0.9056 - jacard_coef: 0.0796 6/17 [=========>....................] - ETA: 2s - loss: 0.1535 - accuracy: 0.9091 - jacard_coef: 0.0775 7/17 [===========>..................] - ETA: 1s - loss: 0.1536 - accuracy: 0.9053 - jacard_coef: 0.0813 8/17 [=============>................] - ETA: 1s - loss: 0.1534 - accuracy: 0.9065 - jacard_coef: 0.0808 9/17 [==============>...............] - ETA: 1s - loss: 0.1531 - accuracy: 0.9088 - jacard_coef: 0.079010/17 [================>.............] - ETA: 1s - loss: 0.1526 - accuracy: 0.9136 - jacard_coef: 0.075111/17 [==================>...........] - ETA: 1s - loss: 0.1525 - accuracy: 0.9135 - jacard_coef: 0.075212/17 [====================>.........] - ETA: 0s - loss: 0.1522 - accuracy: 0.9147 - jacard_coef: 0.074113/17 [=====================>........] - ETA: 0s - loss: 0.1520 - accuracy: 0.9157 - jacard_coef: 0.073114/17 [=======================>......] - ETA: 0s - loss: 0.1518 - accuracy: 0.9146 - jacard_coef: 0.074215/17 [=========================>....] - ETA: 0s - loss: 0.1517 - accuracy: 0.9143 - jacard_coef: 0.074616/17 [===========================>..] - ETA: 0s - loss: 0.1516 - accuracy: 0.9132 - jacard_coef: 0.075117/17 [==============================] - 3s 184ms/step - loss: 0.1516 - accuracy: 0.9129 - jacard_coef: 0.0769 - val_loss: 0.1545 - val_accuracy: 0.9267 - val_jacard_coef: 0.0626 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1475 - accuracy: 0.9159 - jacard_coef: 0.0619 2/17 [==>...........................] - ETA: 2s - loss: 0.1478 - accuracy: 0.9091 - jacard_coef: 0.0738 3/17 [====>.........................] - ETA: 2s - loss: 0.1472 - accuracy: 0.9139 - jacard_coef: 0.0725 4/17 [======>.......................] - ETA: 2s - loss: 0.1481 - accuracy: 0.9076 - jacard_coef: 0.0788 5/17 [=======>......................] - ETA: 2s - loss: 0.1481 - accuracy: 0.9071 - jacard_coef: 0.0800 6/17 [=========>....................] - ETA: 2s - loss: 0.1475 - accuracy: 0.9127 - jacard_coef: 0.0744 7/17 [===========>..................] - ETA: 1s - loss: 0.1476 - accuracy: 0.9121 - jacard_coef: 0.0742 8/17 [=============>................] - ETA: 1s - loss: 0.1474 - accuracy: 0.9095 - jacard_coef: 0.0739 9/17 [==============>...............] - ETA: 1s - loss: 0.1473 - accuracy: 0.9100 - jacard_coef: 0.074010/17 [================>.............] - ETA: 1s - loss: 0.1473 - accuracy: 0.9104 - jacard_coef: 0.074211/17 [==================>...........] - ETA: 1s - loss: 0.1471 - accuracy: 0.9121 - jacard_coef: 0.073412/17 [====================>.........] - ETA: 0s - loss: 0.1469 - accuracy: 0.9123 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 0s - loss: 0.1469 - accuracy: 0.9095 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1468 - accuracy: 0.9104 - jacard_coef: 0.075715/17 [=========================>....] - ETA: 0s - loss: 0.1468 - accuracy: 0.9086 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1466 - accuracy: 0.9093 - jacard_coef: 0.075917/17 [==============================] - 3s 186ms/step - loss: 0.1465 - accuracy: 0.9099 - jacard_coef: 0.0716 - val_loss: 0.1424 - val_accuracy: 0.9271 - val_jacard_coef: 0.0625 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9119 - jacard_coef: 0.0782 2/17 [==>...........................] - ETA: 2s - loss: 0.1438 - accuracy: 0.9076 - jacard_coef: 0.0819 3/17 [====>.........................] - ETA: 2s - loss: 0.1444 - accuracy: 0.9079 - jacard_coef: 0.0822 4/17 [======>.......................] - ETA: 2s - loss: 0.1443 - accuracy: 0.9073 - jacard_coef: 0.0829 5/17 [=======>......................] - ETA: 2s - loss: 0.1440 - accuracy: 0.9128 - jacard_coef: 0.0784 6/17 [=========>....................] - ETA: 2s - loss: 0.1435 - accuracy: 0.9174 - jacard_coef: 0.0747 7/17 [===========>..................] - ETA: 1s - loss: 0.1438 - accuracy: 0.9104 - jacard_coef: 0.0711 8/17 [=============>................] - ETA: 1s - loss: 0.1437 - accuracy: 0.9114 - jacard_coef: 0.0712 9/17 [==============>...............] - ETA: 1s - loss: 0.1438 - accuracy: 0.9106 - jacard_coef: 0.072310/17 [================>.............] - ETA: 1s - loss: 0.1440 - accuracy: 0.9071 - jacard_coef: 0.074011/17 [==================>...........] - ETA: 1s - loss: 0.1443 - accuracy: 0.9033 - jacard_coef: 0.076312/17 [====================>.........] - ETA: 0s - loss: 0.1441 - accuracy: 0.9060 - jacard_coef: 0.074013/17 [=====================>........] - ETA: 0s - loss: 0.1442 - accuracy: 0.9042 - jacard_coef: 0.076014/17 [=======================>......] - ETA: 0s - loss: 0.1440 - accuracy: 0.9069 - jacard_coef: 0.074415/17 [=========================>....] - ETA: 0s - loss: 0.1440 - accuracy: 0.9061 - jacard_coef: 0.075716/17 [===========================>..] - ETA: 0s - loss: 0.1439 - accuracy: 0.9079 - jacard_coef: 0.074617/17 [==============================] - 3s 190ms/step - loss: 0.1439 - accuracy: 0.9073 - jacard_coef: 0.0784 - val_loss: 0.1423 - val_accuracy: 0.9272 - val_jacard_coef: 0.0626 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1433 - accuracy: 0.9179 - jacard_coef: 0.0748 2/17 [==>...........................] - ETA: 2s - loss: 0.1427 - accuracy: 0.9218 - jacard_coef: 0.0714 3/17 [====>.........................] - ETA: 2s - loss: 0.1425 - accuracy: 0.9228 - jacard_coef: 0.0706 4/17 [======>.......................] - ETA: 2s - loss: 0.1420 - accuracy: 0.9272 - jacard_coef: 0.0667 5/17 [=======>......................] - ETA: 2s - loss: 0.1419 - accuracy: 0.9299 - jacard_coef: 0.0644 6/17 [=========>....................] - ETA: 2s - loss: 0.1421 - accuracy: 0.9272 - jacard_coef: 0.0666 7/17 [===========>..................] - ETA: 1s - loss: 0.1425 - accuracy: 0.9222 - jacard_coef: 0.0708 8/17 [=============>................] - ETA: 1s - loss: 0.1426 - accuracy: 0.9195 - jacard_coef: 0.0730 9/17 [==============>...............] - ETA: 1s - loss: 0.1425 - accuracy: 0.9196 - jacard_coef: 0.072810/17 [================>.............] - ETA: 1s - loss: 0.1428 - accuracy: 0.9149 - jacard_coef: 0.076411/17 [==================>...........] - ETA: 1s - loss: 0.1425 - accuracy: 0.9160 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 0s - loss: 0.1426 - accuracy: 0.9145 - jacard_coef: 0.076513/17 [=====================>........] - ETA: 0s - loss: 0.1424 - accuracy: 0.9150 - jacard_coef: 0.076014/17 [=======================>......] - ETA: 0s - loss: 0.1423 - accuracy: 0.9166 - jacard_coef: 0.074715/17 [=========================>....] - ETA: 0s - loss: 0.1422 - accuracy: 0.9169 - jacard_coef: 0.074516/17 [===========================>..] - ETA: 0s - loss: 0.1421 - accuracy: 0.9171 - jacard_coef: 0.074417/17 [==============================] - 3s 190ms/step - loss: 0.1421 - accuracy: 0.9163 - jacard_coef: 0.0787 - val_loss: 0.1409 - val_accuracy: 0.9260 - val_jacard_coef: 0.0625 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 2s - loss: 0.1404 - accuracy: 0.9321 - jacard_coef: 0.0628 2/17 [==>...........................] - ETA: 2s - loss: 0.1408 - accuracy: 0.9186 - jacard_coef: 0.0743 3/17 [====>.........................] - ETA: 2s - loss: 0.1414 - accuracy: 0.9122 - jacard_coef: 0.0795 4/17 [======>.......................] - ETA: 2s - loss: 0.1413 - accuracy: 0.9113 - jacard_coef: 0.0803 5/17 [=======>......................] - ETA: 2s - loss: 0.1427 - accuracy: 0.8899 - jacard_coef: 0.0789 6/17 [=========>....................] - ETA: 2s - loss: 0.1426 - accuracy: 0.8925 - jacard_coef: 0.0797 7/17 [===========>..................] - ETA: 1s - loss: 0.1428 - accuracy: 0.8913 - jacard_coef: 0.0826 8/17 [=============>................] - ETA: 1s - loss: 0.1426 - accuracy: 0.8942 - jacard_coef: 0.0817 9/17 [==============>...............] - ETA: 1s - loss: 0.1426 - accuracy: 0.8969 - jacard_coef: 0.080610/17 [================>.............] - ETA: 1s - loss: 0.1425 - accuracy: 0.8996 - jacard_coef: 0.078911/17 [==================>...........] - ETA: 1s - loss: 0.1422 - accuracy: 0.9031 - jacard_coef: 0.076712/17 [====================>.........] - ETA: 0s - loss: 0.1420 - accuracy: 0.9057 - jacard_coef: 0.074913/17 [=====================>........] - ETA: 0s - loss: 0.1422 - accuracy: 0.9039 - jacard_coef: 0.076914/17 [=======================>......] - ETA: 0s - loss: 0.1422 - accuracy: 0.9045 - jacard_coef: 0.076915/17 [=========================>....] - ETA: 0s - loss: 0.1420 - accuracy: 0.9066 - jacard_coef: 0.075616/17 [===========================>..] - ETA: 0s - loss: 0.1420 - accuracy: 0.9076 - jacard_coef: 0.075317/17 [==============================] - 3s 190ms/step - loss: 0.1420 - accuracy: 0.9079 - jacard_coef: 0.0740 - val_loss: 0.1428 - val_accuracy: 0.9277 - val_jacard_coef: 0.0629 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1427 - accuracy: 0.9250 - jacard_coef: 0.0683 2/17 [==>...........................] - ETA: 2s - loss: 0.1431 - accuracy: 0.9133 - jacard_coef: 0.0782 3/17 [====>.........................] - ETA: 2s - loss: 0.1426 - accuracy: 0.9139 - jacard_coef: 0.0776 4/17 [======>.......................] - ETA: 2s - loss: 0.1424 - accuracy: 0.9127 - jacard_coef: 0.0785 5/17 [=======>......................] - ETA: 2s - loss: 0.1421 - accuracy: 0.9142 - jacard_coef: 0.0772 6/17 [=========>....................] - ETA: 2s - loss: 0.1421 - accuracy: 0.9147 - jacard_coef: 0.0768 7/17 [===========>..................] - ETA: 1s - loss: 0.1414 - accuracy: 0.9210 - jacard_coef: 0.0712 8/17 [=============>................] - ETA: 1s - loss: 0.1413 - accuracy: 0.9198 - jacard_coef: 0.0722 9/17 [==============>...............] - ETA: 1s - loss: 0.1411 - accuracy: 0.9205 - jacard_coef: 0.071610/17 [================>.............] - ETA: 1s - loss: 0.1413 - accuracy: 0.9175 - jacard_coef: 0.074111/17 [==================>...........] - ETA: 1s - loss: 0.1413 - accuracy: 0.9173 - jacard_coef: 0.074312/17 [====================>.........] - ETA: 0s - loss: 0.1411 - accuracy: 0.9184 - jacard_coef: 0.073413/17 [=====================>........] - ETA: 0s - loss: 0.1413 - accuracy: 0.9172 - jacard_coef: 0.074514/17 [=======================>......] - ETA: 0s - loss: 0.1412 - accuracy: 0.9169 - jacard_coef: 0.074815/17 [=========================>....] - ETA: 0s - loss: 0.1414 - accuracy: 0.9152 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.1411 - accuracy: 0.9172 - jacard_coef: 0.074517/17 [==============================] - 3s 189ms/step - loss: 0.1411 - accuracy: 0.9167 - jacard_coef: 0.0777 - val_loss: 0.1367 - val_accuracy: 0.9278 - val_jacard_coef: 0.0627 - lr: 5.0000e-04
Epoch 14/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1409 - accuracy: 0.9108 - jacard_coef: 0.0803 2/17 [==>...........................] - ETA: 2s - loss: 0.1405 - accuracy: 0.9114 - jacard_coef: 0.0797 3/17 [====>.........................] - ETA: 2s - loss: 0.1397 - accuracy: 0.9213 - jacard_coef: 0.0714 4/17 [======>.......................] - ETA: 2s - loss: 0.1388 - accuracy: 0.9300 - jacard_coef: 0.0638 5/17 [=======>......................] - ETA: 2s - loss: 0.1390 - accuracy: 0.9253 - jacard_coef: 0.0676 6/17 [=========>....................] - ETA: 2s - loss: 0.1385 - accuracy: 0.9303 - jacard_coef: 0.0629 7/17 [===========>..................] - ETA: 1s - loss: 0.1384 - accuracy: 0.9290 - jacard_coef: 0.0641 8/17 [=============>................] - ETA: 1s - loss: 0.1386 - accuracy: 0.9256 - jacard_coef: 0.0668 9/17 [==============>...............] - ETA: 1s - loss: 0.1387 - accuracy: 0.9234 - jacard_coef: 0.068610/17 [================>.............] - ETA: 1s - loss: 0.1388 - accuracy: 0.9212 - jacard_coef: 0.070511/17 [==================>...........] - ETA: 1s - loss: 0.1382 - accuracy: 0.9265 - jacard_coef: 0.065812/17 [====================>.........] - ETA: 0s - loss: 0.1386 - accuracy: 0.9222 - jacard_coef: 0.069513/17 [=====================>........] - ETA: 0s - loss: 0.1386 - accuracy: 0.9212 - jacard_coef: 0.070414/17 [=======================>......] - ETA: 0s - loss: 0.1387 - accuracy: 0.9202 - jacard_coef: 0.071215/17 [=========================>....] - ETA: 0s - loss: 0.1387 - accuracy: 0.9191 - jacard_coef: 0.072316/17 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.9171 - jacard_coef: 0.073917/17 [==============================] - 3s 189ms/step - loss: 0.1389 - accuracy: 0.9164 - jacard_coef: 0.0783 - val_loss: 0.1366 - val_accuracy: 0.9285 - val_jacard_coef: 0.0627 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0654 (epoch 4)
  Final Val Loss: 0.1366
  Training Time: 0:01:52.879415
  Stability (std): 0.0140

Results saved to: hyperparameter_optimization_20250926_123742/exp_22_Attention_UNet_lr5e-3_bs8/Attention_UNet_lr0.005_bs8_results.json

Experiment 22 completed in 148s
Progress: 22/36 completed
Estimated remaining time: 34 minutes

ðŸ”¬ EXPERIMENT 23/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758864822.197499 3289499 service.cc:145] XLA service 0x14f781539ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758864822.197570 3289499 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758864822.658242 3289499 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 8:06 - loss: 0.3386 - accuracy: 0.4950 - jacard_coef: 0.07882/9 [=====>........................] - ETA: 58s - loss: 0.2944 - accuracy: 0.4190 - jacard_coef: 0.0905 3/9 [=========>....................] - ETA: 26s - loss: 0.2662 - accuracy: 0.3511 - jacard_coef: 0.08554/9 [============>.................] - ETA: 15s - loss: 0.2493 - accuracy: 0.3087 - jacard_coef: 0.08785/9 [===============>..............] - ETA: 9s - loss: 0.2392 - accuracy: 0.2756 - jacard_coef: 0.0831 6/9 [===================>..........] - ETA: 5s - loss: 0.2303 - accuracy: 0.2685 - jacard_coef: 0.08407/9 [======================>.......] - ETA: 3s - loss: 0.2231 - accuracy: 0.2762 - jacard_coef: 0.08208/9 [=========================>....] - ETA: 1s - loss: 0.2179 - accuracy: 0.2743 - jacard_coef: 0.07749/9 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.2744 - jacard_coef: 0.07339/9 [==============================] - 80s 2s/step - loss: 0.2176 - accuracy: 0.2744 - jacard_coef: 0.0733 - val_loss: 0.1412 - val_accuracy: 0.9298 - val_jacard_coef: 0.0630 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1779 - accuracy: 0.4320 - jacard_coef: 0.06732/9 [=====>........................] - ETA: 2s - loss: 0.1779 - accuracy: 0.4357 - jacard_coef: 0.06923/9 [=========>....................] - ETA: 2s - loss: 0.1776 - accuracy: 0.4393 - jacard_coef: 0.07394/9 [============>.................] - ETA: 1s - loss: 0.1773 - accuracy: 0.4381 - jacard_coef: 0.07175/9 [===============>..............] - ETA: 1s - loss: 0.1770 - accuracy: 0.4338 - jacard_coef: 0.07086/9 [===================>..........] - ETA: 1s - loss: 0.1767 - accuracy: 0.4302 - jacard_coef: 0.07577/9 [======================>.......] - ETA: 0s - loss: 0.1765 - accuracy: 0.4195 - jacard_coef: 0.07658/9 [=========================>....] - ETA: 0s - loss: 0.1763 - accuracy: 0.4156 - jacard_coef: 0.07669/9 [==============================] - 3s 331ms/step - loss: 0.1766 - accuracy: 0.4134 - jacard_coef: 0.0741 - val_loss: 0.2237 - val_accuracy: 0.9298 - val_jacard_coef: 0.0605 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1737 - accuracy: 0.3822 - jacard_coef: 0.07322/9 [=====>........................] - ETA: 2s - loss: 0.1734 - accuracy: 0.4409 - jacard_coef: 0.08163/9 [=========>....................] - ETA: 2s - loss: 0.1729 - accuracy: 0.4895 - jacard_coef: 0.07984/9 [============>.................] - ETA: 1s - loss: 0.1727 - accuracy: 0.5160 - jacard_coef: 0.07975/9 [===============>..............] - ETA: 1s - loss: 0.1725 - accuracy: 0.5377 - jacard_coef: 0.08066/9 [===================>..........] - ETA: 1s - loss: 0.1724 - accuracy: 0.5484 - jacard_coef: 0.07657/9 [======================>.......] - ETA: 0s - loss: 0.1723 - accuracy: 0.5592 - jacard_coef: 0.07708/9 [=========================>....] - ETA: 0s - loss: 0.1721 - accuracy: 0.5701 - jacard_coef: 0.07619/9 [==============================] - 3s 340ms/step - loss: 0.1722 - accuracy: 0.5693 - jacard_coef: 0.0779 - val_loss: 0.1862 - val_accuracy: 0.9304 - val_jacard_coef: 0.0642 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1716 - accuracy: 0.6346 - jacard_coef: 0.08662/9 [=====>........................] - ETA: 2s - loss: 0.1712 - accuracy: 0.6580 - jacard_coef: 0.07793/9 [=========>....................] - ETA: 2s - loss: 0.1709 - accuracy: 0.6791 - jacard_coef: 0.07874/9 [============>.................] - ETA: 1s - loss: 0.1707 - accuracy: 0.7026 - jacard_coef: 0.07755/9 [===============>..............] - ETA: 1s - loss: 0.1704 - accuracy: 0.7215 - jacard_coef: 0.07586/9 [===================>..........] - ETA: 1s - loss: 0.1701 - accuracy: 0.7359 - jacard_coef: 0.07397/9 [======================>.......] - ETA: 0s - loss: 0.1700 - accuracy: 0.7461 - jacard_coef: 0.07518/9 [=========================>....] - ETA: 0s - loss: 0.1698 - accuracy: 0.7578 - jacard_coef: 0.07659/9 [==============================] - 3s 333ms/step - loss: 0.1698 - accuracy: 0.7591 - jacard_coef: 0.0683 - val_loss: 0.1265 - val_accuracy: 0.9302 - val_jacard_coef: 0.0585 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1680 - accuracy: 0.8866 - jacard_coef: 0.06972/9 [=====>........................] - ETA: 2s - loss: 0.1678 - accuracy: 0.8896 - jacard_coef: 0.06983/9 [=========>....................] - ETA: 2s - loss: 0.1676 - accuracy: 0.8918 - jacard_coef: 0.07314/9 [============>.................] - ETA: 1s - loss: 0.1674 - accuracy: 0.8953 - jacard_coef: 0.07245/9 [===============>..............] - ETA: 1s - loss: 0.1672 - accuracy: 0.8980 - jacard_coef: 0.07246/9 [===================>..........] - ETA: 1s - loss: 0.1670 - accuracy: 0.9012 - jacard_coef: 0.07217/9 [======================>.......] - ETA: 0s - loss: 0.1670 - accuracy: 0.8990 - jacard_coef: 0.07578/9 [=========================>....] - ETA: 0s - loss: 0.1668 - accuracy: 0.9006 - jacard_coef: 0.07579/9 [==============================] - 3s 333ms/step - loss: 0.1669 - accuracy: 0.8997 - jacard_coef: 0.0807 - val_loss: 0.1011 - val_accuracy: 0.9304 - val_jacard_coef: 0.0578 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1648 - accuracy: 0.9192 - jacard_coef: 0.07192/9 [=====>........................] - ETA: 2s - loss: 0.1646 - accuracy: 0.9228 - jacard_coef: 0.06813/9 [=========>....................] - ETA: 2s - loss: 0.1649 - accuracy: 0.9174 - jacard_coef: 0.07244/9 [============>.................] - ETA: 1s - loss: 0.1648 - accuracy: 0.9157 - jacard_coef: 0.07415/9 [===============>..............] - ETA: 1s - loss: 0.1647 - accuracy: 0.9176 - jacard_coef: 0.07256/9 [===================>..........] - ETA: 1s - loss: 0.1645 - accuracy: 0.9169 - jacard_coef: 0.07357/9 [======================>.......] - ETA: 0s - loss: 0.1646 - accuracy: 0.8937 - jacard_coef: 0.07468/9 [=========================>....] - ETA: 0s - loss: 0.1646 - accuracy: 0.8874 - jacard_coef: 0.07569/9 [==============================] - 3s 333ms/step - loss: 0.1647 - accuracy: 0.8866 - jacard_coef: 0.0817 - val_loss: 0.0861 - val_accuracy: 0.9304 - val_jacard_coef: 0.0604 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1638 - accuracy: 0.8654 - jacard_coef: 0.08922/9 [=====>........................] - ETA: 2s - loss: 0.1627 - accuracy: 0.8825 - jacard_coef: 0.07893/9 [=========>....................] - ETA: 2s - loss: 0.1819 - accuracy: 0.7397 - jacard_coef: 0.07694/9 [============>.................] - ETA: 1s - loss: 0.1889 - accuracy: 0.5999 - jacard_coef: 0.07605/9 [===============>..............] - ETA: 1s - loss: 0.1885 - accuracy: 0.5059 - jacard_coef: 0.07286/9 [===================>..........] - ETA: 1s - loss: 0.1900 - accuracy: 0.4626 - jacard_coef: 0.07167/9 [======================>.......] - ETA: 0s - loss: 0.1908 - accuracy: 0.4430 - jacard_coef: 0.07818/9 [=========================>....] - ETA: 0s - loss: 0.1913 - accuracy: 0.4166 - jacard_coef: 0.07619/9 [==============================] - 3s 332ms/step - loss: 0.1914 - accuracy: 0.4157 - jacard_coef: 0.0840 - val_loss: 1.0764 - val_accuracy: 0.9304 - val_jacard_coef: 0.0012 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1916 - accuracy: 0.6096 - jacard_coef: 0.08872/9 [=====>........................] - ETA: 2s - loss: 0.1913 - accuracy: 0.5715 - jacard_coef: 0.08003/9 [=========>....................] - ETA: 2s - loss: 0.1884 - accuracy: 0.5468 - jacard_coef: 0.07704/9 [============>.................] - ETA: 1s - loss: 0.1852 - accuracy: 0.5344 - jacard_coef: 0.07845/9 [===============>..............] - ETA: 1s - loss: 0.1842 - accuracy: 0.4706 - jacard_coef: 0.08056/9 [===================>..........] - ETA: 1s - loss: 0.1830 - accuracy: 0.5065 - jacard_coef: 0.07807/9 [======================>.......] - ETA: 0s - loss: 0.1837 - accuracy: 0.5335 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1833 - accuracy: 0.5480 - jacard_coef: 0.07619/9 [==============================] - 3s 341ms/step - loss: 0.1832 - accuracy: 0.5466 - jacard_coef: 0.0825 - val_loss: 14.9380 - val_accuracy: 0.0731 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1763 - accuracy: 0.4980 - jacard_coef: 0.06742/9 [=====>........................] - ETA: 2s - loss: 0.1756 - accuracy: 0.3825 - jacard_coef: 0.07453/9 [=========>....................] - ETA: 2s - loss: 0.1758 - accuracy: 0.3617 - jacard_coef: 0.07894/9 [============>.................] - ETA: 1s - loss: 0.1771 - accuracy: 0.3550 - jacard_coef: 0.07465/9 [===============>..............] - ETA: 1s - loss: 0.1757 - accuracy: 0.3827 - jacard_coef: 0.07556/9 [===================>..........] - ETA: 1s - loss: 0.1747 - accuracy: 0.4056 - jacard_coef: 0.07877/9 [======================>.......] - ETA: 0s - loss: 0.1739 - accuracy: 0.4632 - jacard_coef: 0.07698/9 [=========================>....] - ETA: 0s - loss: 0.1727 - accuracy: 0.5146 - jacard_coef: 0.07659/9 [==============================] - 3s 352ms/step - loss: 0.1727 - accuracy: 0.5145 - jacard_coef: 0.0742 - val_loss: 14.8183 - val_accuracy: 0.0799 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8939 - jacard_coef: 0.07722/9 [=====>........................] - ETA: 2s - loss: 0.1715 - accuracy: 0.6408 - jacard_coef: 0.08903/9 [=========>....................] - ETA: 2s - loss: 0.1693 - accuracy: 0.7297 - jacard_coef: 0.07814/9 [============>.................] - ETA: 1s - loss: 0.1688 - accuracy: 0.7712 - jacard_coef: 0.07415/9 [===============>..............] - ETA: 1s - loss: 0.1682 - accuracy: 0.7930 - jacard_coef: 0.07436/9 [===================>..........] - ETA: 1s - loss: 0.1687 - accuracy: 0.8080 - jacard_coef: 0.07487/9 [======================>.......] - ETA: 0s - loss: 0.1682 - accuracy: 0.8165 - jacard_coef: 0.07768/9 [=========================>....] - ETA: 0s - loss: 0.1685 - accuracy: 0.8267 - jacard_coef: 0.07639/9 [==============================] - 3s 339ms/step - loss: 0.1685 - accuracy: 0.8242 - jacard_coef: 0.0706 - val_loss: 7.3153 - val_accuracy: 0.2017 - val_jacard_coef: 0.0688 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1733 - accuracy: 0.6245 - jacard_coef: 0.09352/9 [=====>........................] - ETA: 2s - loss: 0.1694 - accuracy: 0.6649 - jacard_coef: 0.08763/9 [=========>....................] - ETA: 2s - loss: 0.1683 - accuracy: 0.7398 - jacard_coef: 0.08574/9 [============>.................] - ETA: 1s - loss: 0.1676 - accuracy: 0.7837 - jacard_coef: 0.07745/9 [===============>..............] - ETA: 1s - loss: 0.1670 - accuracy: 0.8059 - jacard_coef: 0.07596/9 [===================>..........] - ETA: 1s - loss: 0.1662 - accuracy: 0.8144 - jacard_coef: 0.07947/9 [======================>.......] - ETA: 0s - loss: 0.1654 - accuracy: 0.8216 - jacard_coef: 0.07708/9 [=========================>....] - ETA: 0s - loss: 0.1648 - accuracy: 0.8267 - jacard_coef: 0.07559/9 [==============================] - 3s 356ms/step - loss: 0.1648 - accuracy: 0.8255 - jacard_coef: 0.0812 - val_loss: 12.5637 - val_accuracy: 0.0912 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1611 - accuracy: 0.8462 - jacard_coef: 0.08342/9 [=====>........................] - ETA: 2s - loss: 0.1594 - accuracy: 0.8661 - jacard_coef: 0.07353/9 [=========>....................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8708 - jacard_coef: 0.07934/9 [============>.................] - ETA: 1s - loss: 0.1587 - accuracy: 0.8847 - jacard_coef: 0.07645/9 [===============>..............] - ETA: 1s - loss: 0.1586 - accuracy: 0.8882 - jacard_coef: 0.07876/9 [===================>..........] - ETA: 1s - loss: 0.1580 - accuracy: 0.8975 - jacard_coef: 0.07437/9 [======================>.......] - ETA: 0s - loss: 0.1578 - accuracy: 0.8969 - jacard_coef: 0.07718/9 [=========================>....] - ETA: 0s - loss: 0.1574 - accuracy: 0.9004 - jacard_coef: 0.07599/9 [==============================] - 3s 340ms/step - loss: 0.1574 - accuracy: 0.9007 - jacard_coef: 0.0738 - val_loss: 5.1559 - val_accuracy: 0.1133 - val_jacard_coef: 0.0680 - lr: 0.0010
Epoch 13/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1524 - accuracy: 0.9090 - jacard_coef: 0.07572/9 [=====>........................] - ETA: 2s - loss: 0.1528 - accuracy: 0.9086 - jacard_coef: 0.07773/9 [=========>....................] - ETA: 2s - loss: 0.1527 - accuracy: 0.9046 - jacard_coef: 0.07634/9 [============>.................] - ETA: 1s - loss: 0.1527 - accuracy: 0.8821 - jacard_coef: 0.07385/9 [===============>..............] - ETA: 1s - loss: 0.1529 - accuracy: 0.8832 - jacard_coef: 0.07796/9 [===================>..........] - ETA: 1s - loss: 0.1524 - accuracy: 0.8933 - jacard_coef: 0.07217/9 [======================>.......] - ETA: 0s - loss: 0.1525 - accuracy: 0.8945 - jacard_coef: 0.07458/9 [=========================>....] - ETA: 0s - loss: 0.1524 - accuracy: 0.8953 - jacard_coef: 0.07589/9 [==============================] - 3s 340ms/step - loss: 0.1526 - accuracy: 0.8924 - jacard_coef: 0.0713 - val_loss: 0.3512 - val_accuracy: 0.3999 - val_jacard_coef: 0.0679 - lr: 0.0010
Epoch 14/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1501 - accuracy: 0.9202 - jacard_coef: 0.06332/9 [=====>........................] - ETA: 2s - loss: 0.1522 - accuracy: 0.9052 - jacard_coef: 0.07483/9 [=========>....................] - ETA: 2s - loss: 0.1529 - accuracy: 0.8956 - jacard_coef: 0.08094/9 [============>.................] - ETA: 1s - loss: 0.1525 - accuracy: 0.9040 - jacard_coef: 0.07715/9 [===============>..............] - ETA: 1s - loss: 0.1527 - accuracy: 0.9018 - jacard_coef: 0.08096/9 [===================>..........] - ETA: 1s - loss: 0.1525 - accuracy: 0.9077 - jacard_coef: 0.07717/9 [======================>.......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9103 - jacard_coef: 0.07608/9 [=========================>....] - ETA: 0s - loss: 0.1523 - accuracy: 0.9124 - jacard_coef: 0.07499/9 [==============================] - 3s 341ms/step - loss: 0.1523 - accuracy: 0.9115 - jacard_coef: 0.0824 - val_loss: 0.1023 - val_accuracy: 0.9083 - val_jacard_coef: 0.0605 - lr: 0.0010
Epoch 15/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1494 - accuracy: 0.9344 - jacard_coef: 0.06132/9 [=====>........................] - ETA: 2s - loss: 0.1490 - accuracy: 0.9366 - jacard_coef: 0.05933/9 [=========>....................] - ETA: 2s - loss: 0.1501 - accuracy: 0.9263 - jacard_coef: 0.06774/9 [============>.................] - ETA: 1s - loss: 0.1509 - accuracy: 0.9174 - jacard_coef: 0.07125/9 [===============>..............] - ETA: 1s - loss: 0.1511 - accuracy: 0.9085 - jacard_coef: 0.07626/9 [===================>..........] - ETA: 1s - loss: 0.1515 - accuracy: 0.9040 - jacard_coef: 0.07837/9 [======================>.......] - ETA: 0s - loss: 0.1511 - accuracy: 0.9057 - jacard_coef: 0.07668/9 [=========================>....] - ETA: 0s - loss: 0.1509 - accuracy: 0.9072 - jacard_coef: 0.07489/9 [==============================] - 3s 338ms/step - loss: 0.1511 - accuracy: 0.9033 - jacard_coef: 0.0821 - val_loss: 0.1138 - val_accuracy: 0.9274 - val_jacard_coef: 0.0539 - lr: 0.0010
Epoch 16/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1499 - accuracy: 0.9139 - jacard_coef: 0.07042/9 [=====>........................] - ETA: 2s - loss: 0.1514 - accuracy: 0.8973 - jacard_coef: 0.07723/9 [=========>....................] - ETA: 2s - loss: 0.1528 - accuracy: 0.8909 - jacard_coef: 0.08454/9 [============>.................] - ETA: 1s - loss: 0.1538 - accuracy: 0.9002 - jacard_coef: 0.07985/9 [===============>..............] - ETA: 1s - loss: 0.1542 - accuracy: 0.9034 - jacard_coef: 0.07916/9 [===================>..........] - ETA: 1s - loss: 0.1533 - accuracy: 0.9082 - jacard_coef: 0.07647/9 [======================>.......] - ETA: 0s - loss: 0.1526 - accuracy: 0.9090 - jacard_coef: 0.07688/9 [=========================>....] - ETA: 0s - loss: 0.1549 - accuracy: 0.8645 - jacard_coef: 0.07519/9 [==============================] - 3s 339ms/step - loss: 0.1549 - accuracy: 0.8644 - jacard_coef: 0.0809 - val_loss: 0.6574 - val_accuracy: 0.9296 - val_jacard_coef: 0.0398 - lr: 0.0010
Epoch 17/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1520 - accuracy: 0.9183 - jacard_coef: 0.07452/9 [=====>........................] - ETA: 2s - loss: 0.1519 - accuracy: 0.9109 - jacard_coef: 0.08083/9 [=========>....................] - ETA: 2s - loss: 0.1522 - accuracy: 0.9112 - jacard_coef: 0.08054/9 [============>.................] - ETA: 1s - loss: 0.1518 - accuracy: 0.9174 - jacard_coef: 0.07525/9 [===============>..............] - ETA: 1s - loss: 0.1521 - accuracy: 0.9194 - jacard_coef: 0.07336/9 [===================>..........] - ETA: 1s - loss: 0.1528 - accuracy: 0.9184 - jacard_coef: 0.07417/9 [======================>.......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9220 - jacard_coef: 0.07098/9 [=========================>....] - ETA: 0s - loss: 0.1526 - accuracy: 0.9157 - jacard_coef: 0.07599/9 [==============================] - 3s 340ms/step - loss: 0.1526 - accuracy: 0.9162 - jacard_coef: 0.0679 - val_loss: 0.1516 - val_accuracy: 0.9304 - val_jacard_coef: 0.0617 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1506 - accuracy: 0.9317 - jacard_coef: 0.06182/9 [=====>........................] - ETA: 2s - loss: 0.1513 - accuracy: 0.9104 - jacard_coef: 0.07953/9 [=========>....................] - ETA: 2s - loss: 0.1529 - accuracy: 0.9053 - jacard_coef: 0.08364/9 [============>.................] - ETA: 1s - loss: 0.1525 - accuracy: 0.9129 - jacard_coef: 0.07725/9 [===============>..............] - ETA: 1s - loss: 0.1521 - accuracy: 0.9131 - jacard_coef: 0.07726/9 [===================>..........] - ETA: 1s - loss: 0.1517 - accuracy: 0.9155 - jacard_coef: 0.07527/9 [======================>.......] - ETA: 0s - loss: 0.1517 - accuracy: 0.9154 - jacard_coef: 0.07538/9 [=========================>....] - ETA: 0s - loss: 0.1513 - accuracy: 0.9147 - jacard_coef: 0.07609/9 [==============================] - 3s 338ms/step - loss: 0.1513 - accuracy: 0.9152 - jacard_coef: 0.0680 - val_loss: 0.1479 - val_accuracy: 0.9304 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1509 - accuracy: 0.9292 - jacard_coef: 0.06432/9 [=====>........................] - ETA: 2s - loss: 0.1504 - accuracy: 0.9174 - jacard_coef: 0.07373/9 [=========>....................] - ETA: 2s - loss: 0.1496 - accuracy: 0.9132 - jacard_coef: 0.07754/9 [============>.................] - ETA: 1s - loss: 0.1496 - accuracy: 0.9140 - jacard_coef: 0.07725/9 [===============>..............] - ETA: 1s - loss: 0.1488 - accuracy: 0.9220 - jacard_coef: 0.07046/9 [===================>..........] - ETA: 1s - loss: 0.1483 - accuracy: 0.9243 - jacard_coef: 0.06857/9 [======================>.......] - ETA: 0s - loss: 0.1482 - accuracy: 0.9205 - jacard_coef: 0.07188/9 [=========================>....] - ETA: 0s - loss: 0.1482 - accuracy: 0.9155 - jacard_coef: 0.07589/9 [==============================] - 3s 339ms/step - loss: 0.1482 - accuracy: 0.9159 - jacard_coef: 0.0676 - val_loss: 0.1475 - val_accuracy: 0.9304 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1464 - accuracy: 0.9131 - jacard_coef: 0.07872/9 [=====>........................] - ETA: 2s - loss: 0.1460 - accuracy: 0.9104 - jacard_coef: 0.08063/9 [=========>....................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9115 - jacard_coef: 0.07784/9 [============>.................] - ETA: 1s - loss: 0.1461 - accuracy: 0.9104 - jacard_coef: 0.07955/9 [===============>..............] - ETA: 1s - loss: 0.1458 - accuracy: 0.9146 - jacard_coef: 0.07346/9 [===================>..........] - ETA: 1s - loss: 0.1457 - accuracy: 0.9136 - jacard_coef: 0.07497/9 [======================>.......] - ETA: 0s - loss: 0.1451 - accuracy: 0.9162 - jacard_coef: 0.07318/9 [=========================>....] - ETA: 0s - loss: 0.1452 - accuracy: 0.9135 - jacard_coef: 0.07559/9 [==============================] - 3s 341ms/step - loss: 0.1453 - accuracy: 0.9135 - jacard_coef: 0.0732 - val_loss: 0.1466 - val_accuracy: 0.9304 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
Epoch 21/30
1/9 [==>...........................] - ETA: 2s - loss: 0.1431 - accuracy: 0.9185 - jacard_coef: 0.07442/9 [=====>........................] - ETA: 2s - loss: 0.1430 - accuracy: 0.9175 - jacard_coef: 0.07493/9 [=========>....................] - ETA: 2s - loss: 0.1432 - accuracy: 0.9175 - jacard_coef: 0.07444/9 [============>.................] - ETA: 1s - loss: 0.1441 - accuracy: 0.9079 - jacard_coef: 0.08225/9 [===============>..............] - ETA: 1s - loss: 0.1439 - accuracy: 0.9071 - jacard_coef: 0.08316/9 [===================>..........] - ETA: 1s - loss: 0.1435 - accuracy: 0.9104 - jacard_coef: 0.08057/9 [======================>.......] - ETA: 0s - loss: 0.1428 - accuracy: 0.9174 - jacard_coef: 0.07458/9 [=========================>....] - ETA: 0s - loss: 0.1428 - accuracy: 0.9163 - jacard_coef: 0.07569/9 [==============================] - 3s 341ms/step - loss: 0.1428 - accuracy: 0.9167 - jacard_coef: 0.0678 - val_loss: 0.1453 - val_accuracy: 0.9304 - val_jacard_coef: 0.0645 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0697 (epoch 11)
  Final Val Loss: 0.1453
  Training Time: 0:02:22.051797
  Stability (std): 1.4901

Results saved to: hyperparameter_optimization_20250926_123742/exp_23_Attention_UNet_lr5e-3_bs16/Attention_UNet_lr0.005_bs16_results.json

Experiment 23 completed in 177s
Progress: 23/36 completed
Estimated remaining time: 38 minutes

ðŸ”¬ EXPERIMENT 24/36
================================================
Architecture: Attention_UNet
Learning Rate: 5e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_UNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758865004.427766 3296041 service.cc:145] XLA service 0x14d52d522fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758865004.427798 3296041 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758865004.801838 3296041 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:21 - loss: 0.3394 - accuracy: 0.5093 - jacard_coef: 0.07682/5 [===========>..................] - ETA: 45s - loss: 0.2936 - accuracy: 0.4342 - jacard_coef: 0.0759 3/5 [=================>............] - ETA: 16s - loss: 0.2685 - accuracy: 0.4056 - jacard_coef: 0.07344/5 [=======================>......] - ETA: 5s - loss: 0.2537 - accuracy: 0.3819 - jacard_coef: 0.0768 5/5 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.3809 - jacard_coef: 0.07345/5 [==============================] - 91s 6s/step - loss: 0.2534 - accuracy: 0.3809 - jacard_coef: 0.0734 - val_loss: 0.1579 - val_accuracy: 0.9294 - val_jacard_coef: 0.0648 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1976 - accuracy: 0.2070 - jacard_coef: 0.07862/5 [===========>..................] - ETA: 2s - loss: 0.1944 - accuracy: 0.1791 - jacard_coef: 0.08383/5 [=================>............] - ETA: 1s - loss: 0.1937 - accuracy: 0.1546 - jacard_coef: 0.07664/5 [=======================>......] - ETA: 0s - loss: 0.1937 - accuracy: 0.1481 - jacard_coef: 0.07615/5 [==============================] - 3s 567ms/step - loss: 0.1937 - accuracy: 0.1483 - jacard_coef: 0.0821 - val_loss: 0.1444 - val_accuracy: 0.9304 - val_jacard_coef: 0.0403 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1904 - accuracy: 0.1589 - jacard_coef: 0.08642/5 [===========>..................] - ETA: 2s - loss: 0.1928 - accuracy: 0.1931 - jacard_coef: 0.07263/5 [=================>............] - ETA: 1s - loss: 0.1918 - accuracy: 0.1741 - jacard_coef: 0.07094/5 [=======================>......] - ETA: 0s - loss: 0.1911 - accuracy: 0.1699 - jacard_coef: 0.07595/5 [==============================] - 3s 566ms/step - loss: 0.1911 - accuracy: 0.1702 - jacard_coef: 0.0869 - val_loss: 0.3608 - val_accuracy: 0.9304 - val_jacard_coef: 0.0129 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1883 - accuracy: 0.1528 - jacard_coef: 0.08462/5 [===========>..................] - ETA: 2s - loss: 0.1880 - accuracy: 0.1498 - jacard_coef: 0.07863/5 [=================>............] - ETA: 1s - loss: 0.1887 - accuracy: 0.1478 - jacard_coef: 0.07494/5 [=======================>......] - ETA: 0s - loss: 0.1882 - accuracy: 0.1532 - jacard_coef: 0.07675/5 [==============================] - 3s 565ms/step - loss: 0.1882 - accuracy: 0.1531 - jacard_coef: 0.0720 - val_loss: 0.8171 - val_accuracy: 0.9304 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1874 - accuracy: 0.1545 - jacard_coef: 0.06612/5 [===========>..................] - ETA: 2s - loss: 0.1861 - accuracy: 0.1608 - jacard_coef: 0.07373/5 [=================>............] - ETA: 1s - loss: 0.1860 - accuracy: 0.1595 - jacard_coef: 0.07514/5 [=======================>......] - ETA: 0s - loss: 0.1861 - accuracy: 0.1561 - jacard_coef: 0.07595/5 [==============================] - 3s 565ms/step - loss: 0.1861 - accuracy: 0.1566 - jacard_coef: 0.0911 - val_loss: 0.8849 - val_accuracy: 0.9304 - val_jacard_coef: 0.0031 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1840 - accuracy: 0.1397 - jacard_coef: 0.07312/5 [===========>..................] - ETA: 2s - loss: 0.1841 - accuracy: 0.1518 - jacard_coef: 0.08283/5 [=================>............] - ETA: 1s - loss: 0.1840 - accuracy: 0.1531 - jacard_coef: 0.08024/5 [=======================>......] - ETA: 0s - loss: 0.1837 - accuracy: 0.1534 - jacard_coef: 0.07655/5 [==============================] - 3s 568ms/step - loss: 0.1837 - accuracy: 0.1535 - jacard_coef: 0.0729 - val_loss: 0.8006 - val_accuracy: 0.9304 - val_jacard_coef: 0.0037 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1803 - accuracy: 0.1981 - jacard_coef: 0.08552/5 [===========>..................] - ETA: 2s - loss: 0.1797 - accuracy: 0.1971 - jacard_coef: 0.08363/5 [=================>............] - ETA: 1s - loss: 0.1798 - accuracy: 0.1941 - jacard_coef: 0.08014/5 [=======================>......] - ETA: 0s - loss: 0.1796 - accuracy: 0.1905 - jacard_coef: 0.07565/5 [==============================] - 3s 569ms/step - loss: 0.1796 - accuracy: 0.1911 - jacard_coef: 0.0937 - val_loss: 0.3792 - val_accuracy: 0.9304 - val_jacard_coef: 0.0139 - lr: 5.0000e-04
Epoch 8/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1829 - accuracy: 0.2103 - jacard_coef: 0.08622/5 [===========>..................] - ETA: 2s - loss: 0.1799 - accuracy: 0.2016 - jacard_coef: 0.07793/5 [=================>............] - ETA: 1s - loss: 0.1791 - accuracy: 0.1959 - jacard_coef: 0.08224/5 [=======================>......] - ETA: 0s - loss: 0.1787 - accuracy: 0.1837 - jacard_coef: 0.07605/5 [==============================] - 3s 568ms/step - loss: 0.1787 - accuracy: 0.1836 - jacard_coef: 0.0853 - val_loss: 0.1874 - val_accuracy: 0.9301 - val_jacard_coef: 0.0315 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1786 - accuracy: 0.1555 - jacard_coef: 0.06352/5 [===========>..................] - ETA: 2s - loss: 0.1781 - accuracy: 0.1637 - jacard_coef: 0.07213/5 [=================>............] - ETA: 1s - loss: 0.1779 - accuracy: 0.1692 - jacard_coef: 0.07334/5 [=======================>......] - ETA: 0s - loss: 0.1776 - accuracy: 0.1852 - jacard_coef: 0.07675/5 [==============================] - 3s 579ms/step - loss: 0.1776 - accuracy: 0.1846 - jacard_coef: 0.0622 - val_loss: 0.1922 - val_accuracy: 0.9300 - val_jacard_coef: 0.0310 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1767 - accuracy: 0.2700 - jacard_coef: 0.07692/5 [===========>..................] - ETA: 2s - loss: 0.1762 - accuracy: 0.2921 - jacard_coef: 0.07493/5 [=================>............] - ETA: 1s - loss: 0.1763 - accuracy: 0.3028 - jacard_coef: 0.06814/5 [=======================>......] - ETA: 0s - loss: 0.1762 - accuracy: 0.3193 - jacard_coef: 0.07635/5 [==============================] - 3s 580ms/step - loss: 0.1762 - accuracy: 0.3205 - jacard_coef: 0.0696 - val_loss: 0.1817 - val_accuracy: 0.9299 - val_jacard_coef: 0.0336 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 2s - loss: 0.1746 - accuracy: 0.3979 - jacard_coef: 0.07132/5 [===========>..................] - ETA: 2s - loss: 0.1745 - accuracy: 0.4091 - jacard_coef: 0.07283/5 [=================>............] - ETA: 1s - loss: 0.1742 - accuracy: 0.4205 - jacard_coef: 0.07474/5 [=======================>......] - ETA: 0s - loss: 0.1742 - accuracy: 0.4279 - jacard_coef: 0.07675/5 [==============================] - 3s 579ms/step - loss: 0.1742 - accuracy: 0.4282 - jacard_coef: 0.0692 - val_loss: 0.1328 - val_accuracy: 0.9296 - val_jacard_coef: 0.0429 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0648 (epoch 1)
  Final Val Loss: 0.1328
  Training Time: 0:02:01.721791
  Stability (std): 0.2905

Results saved to: hyperparameter_optimization_20250926_123742/exp_24_Attention_UNet_lr5e-3_bs32/Attention_UNet_lr0.005_bs32_results.json

Experiment 24 completed in 157s
Progress: 24/36 completed
Estimated remaining time: 31 minutes

ðŸ”¬ EXPERIMENT 25/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758865159.111828 3302575 service.cc:145] XLA service 0x14eb195a4170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758865159.111869 3302575 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758865159.499685 3302575 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 16:40 - loss: 0.3330 - accuracy: 0.5113 - jacard_coef: 0.1049 2/17 [==>...........................] - ETA: 1:10 - loss: 0.3050 - accuracy: 0.4374 - jacard_coef: 0.1061  3/17 [====>.........................] - ETA: 34s - loss: 0.2786 - accuracy: 0.3713 - jacard_coef: 0.0954  4/17 [======>.......................] - ETA: 22s - loss: 0.2670 - accuracy: 0.3114 - jacard_coef: 0.0874 5/17 [=======>......................] - ETA: 16s - loss: 0.2594 - accuracy: 0.2803 - jacard_coef: 0.0840 6/17 [=========>....................] - ETA: 12s - loss: 0.2514 - accuracy: 0.2714 - jacard_coef: 0.0840 7/17 [===========>..................] - ETA: 9s - loss: 0.2445 - accuracy: 0.2614 - jacard_coef: 0.0752  8/17 [=============>................] - ETA: 7s - loss: 0.2389 - accuracy: 0.2592 - jacard_coef: 0.0813 9/17 [==============>...............] - ETA: 6s - loss: 0.2417 - accuracy: 0.2491 - jacard_coef: 0.079210/17 [================>.............] - ETA: 4s - loss: 0.2435 - accuracy: 0.2456 - jacard_coef: 0.079011/17 [==================>...........] - ETA: 3s - loss: 0.2417 - accuracy: 0.2395 - jacard_coef: 0.079112/17 [====================>.........] - ETA: 3s - loss: 0.2388 - accuracy: 0.2431 - jacard_coef: 0.080913/17 [=====================>........] - ETA: 2s - loss: 0.2358 - accuracy: 0.2396 - jacard_coef: 0.079014/17 [=======================>......] - ETA: 1s - loss: 0.2363 - accuracy: 0.2370 - jacard_coef: 0.078215/17 [=========================>....] - ETA: 1s - loss: 0.2346 - accuracy: 0.2366 - jacard_coef: 0.076816/17 [===========================>..] - ETA: 0s - loss: 0.2328 - accuracy: 0.2381 - jacard_coef: 0.076617/17 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.2383 - jacard_coef: 0.072217/17 [==============================] - 78s 996ms/step - loss: 0.2326 - accuracy: 0.2383 - jacard_coef: 0.0722 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 3.4109e-05 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1947 - accuracy: 0.1458 - jacard_coef: 0.0845 2/17 [==>...........................] - ETA: 3s - loss: 0.1973 - accuracy: 0.1375 - jacard_coef: 0.0830 3/17 [====>.........................] - ETA: 2s - loss: 0.1953 - accuracy: 0.1286 - jacard_coef: 0.0761 4/17 [======>.......................] - ETA: 2s - loss: 0.1948 - accuracy: 0.1393 - jacard_coef: 0.0777 5/17 [=======>......................] - ETA: 2s - loss: 0.1962 - accuracy: 0.1532 - jacard_coef: 0.0814 6/17 [=========>....................] - ETA: 2s - loss: 0.1960 - accuracy: 0.1531 - jacard_coef: 0.0812 7/17 [===========>..................] - ETA: 2s - loss: 0.1942 - accuracy: 0.1512 - jacard_coef: 0.0799 8/17 [=============>................] - ETA: 1s - loss: 0.1930 - accuracy: 0.1529 - jacard_coef: 0.0790 9/17 [==============>...............] - ETA: 1s - loss: 0.1923 - accuracy: 0.1505 - jacard_coef: 0.076410/17 [================>.............] - ETA: 1s - loss: 0.1911 - accuracy: 0.1528 - jacard_coef: 0.077511/17 [==================>...........] - ETA: 1s - loss: 0.1901 - accuracy: 0.1557 - jacard_coef: 0.078312/17 [====================>.........] - ETA: 1s - loss: 0.1889 - accuracy: 0.1755 - jacard_coef: 0.076913/17 [=====================>........] - ETA: 0s - loss: 0.1878 - accuracy: 0.2010 - jacard_coef: 0.077614/17 [=======================>......] - ETA: 0s - loss: 0.1872 - accuracy: 0.2203 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1871 - accuracy: 0.2451 - jacard_coef: 0.076616/17 [===========================>..] - ETA: 0s - loss: 0.1872 - accuracy: 0.2618 - jacard_coef: 0.076217/17 [==============================] - 4s 214ms/step - loss: 0.1871 - accuracy: 0.2638 - jacard_coef: 0.0795 - val_loss: 1.2381 - val_accuracy: 0.9198 - val_jacard_coef: 0.0057 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1782 - accuracy: 0.5880 - jacard_coef: 0.0569 2/17 [==>...........................] - ETA: 3s - loss: 0.1769 - accuracy: 0.5865 - jacard_coef: 0.0613 3/17 [====>.........................] - ETA: 2s - loss: 0.1797 - accuracy: 0.5715 - jacard_coef: 0.0662 4/17 [======>.......................] - ETA: 2s - loss: 0.1777 - accuracy: 0.5654 - jacard_coef: 0.0757 5/17 [=======>......................] - ETA: 2s - loss: 0.1768 - accuracy: 0.5442 - jacard_coef: 0.0733 6/17 [=========>....................] - ETA: 2s - loss: 0.1765 - accuracy: 0.5346 - jacard_coef: 0.0767 7/17 [===========>..................] - ETA: 2s - loss: 0.1765 - accuracy: 0.5215 - jacard_coef: 0.0698 8/17 [=============>................] - ETA: 1s - loss: 0.1760 - accuracy: 0.5189 - jacard_coef: 0.0690 9/17 [==============>...............] - ETA: 1s - loss: 0.1755 - accuracy: 0.5221 - jacard_coef: 0.072610/17 [================>.............] - ETA: 1s - loss: 0.1749 - accuracy: 0.5327 - jacard_coef: 0.070611/17 [==================>...........] - ETA: 1s - loss: 0.1745 - accuracy: 0.5459 - jacard_coef: 0.071212/17 [====================>.........] - ETA: 1s - loss: 0.1741 - accuracy: 0.5530 - jacard_coef: 0.074713/17 [=====================>........] - ETA: 0s - loss: 0.1736 - accuracy: 0.5494 - jacard_coef: 0.076514/17 [=======================>......] - ETA: 0s - loss: 0.1734 - accuracy: 0.5480 - jacard_coef: 0.075415/17 [=========================>....] - ETA: 0s - loss: 0.1731 - accuracy: 0.5643 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1728 - accuracy: 0.5716 - jacard_coef: 0.076417/17 [==============================] - 4s 214ms/step - loss: 0.1728 - accuracy: 0.5710 - jacard_coef: 0.0722 - val_loss: 4.8536 - val_accuracy: 0.1075 - val_jacard_coef: 0.0680 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1676 - accuracy: 0.7804 - jacard_coef: 0.0675 2/17 [==>...........................] - ETA: 3s - loss: 0.1690 - accuracy: 0.7474 - jacard_coef: 0.0813 3/17 [====>.........................] - ETA: 2s - loss: 0.1686 - accuracy: 0.7496 - jacard_coef: 0.0847 4/17 [======>.......................] - ETA: 2s - loss: 0.1705 - accuracy: 0.6635 - jacard_coef: 0.0794 5/17 [=======>......................] - ETA: 2s - loss: 0.1695 - accuracy: 0.7005 - jacard_coef: 0.0766 6/17 [=========>....................] - ETA: 2s - loss: 0.1691 - accuracy: 0.7220 - jacard_coef: 0.0786 7/17 [===========>..................] - ETA: 2s - loss: 0.1690 - accuracy: 0.7299 - jacard_coef: 0.0800 8/17 [=============>................] - ETA: 1s - loss: 0.1692 - accuracy: 0.7246 - jacard_coef: 0.0821 9/17 [==============>...............] - ETA: 1s - loss: 0.1692 - accuracy: 0.7256 - jacard_coef: 0.084310/17 [================>.............] - ETA: 1s - loss: 0.1691 - accuracy: 0.7345 - jacard_coef: 0.080711/17 [==================>...........] - ETA: 1s - loss: 0.1694 - accuracy: 0.7364 - jacard_coef: 0.080012/17 [====================>.........] - ETA: 1s - loss: 0.1691 - accuracy: 0.7439 - jacard_coef: 0.079513/17 [=====================>........] - ETA: 0s - loss: 0.1688 - accuracy: 0.7489 - jacard_coef: 0.079414/17 [=======================>......] - ETA: 0s - loss: 0.1684 - accuracy: 0.7574 - jacard_coef: 0.077715/17 [=========================>....] - ETA: 0s - loss: 0.1685 - accuracy: 0.7590 - jacard_coef: 0.077916/17 [===========================>..] - ETA: 0s - loss: 0.1688 - accuracy: 0.7480 - jacard_coef: 0.076317/17 [==============================] - 4s 209ms/step - loss: 0.1688 - accuracy: 0.7480 - jacard_coef: 0.0741 - val_loss: 0.1055 - val_accuracy: 0.8772 - val_jacard_coef: 0.0621 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1710 - accuracy: 0.8049 - jacard_coef: 0.0896 2/17 [==>...........................] - ETA: 3s - loss: 0.1689 - accuracy: 0.7269 - jacard_coef: 0.0763 3/17 [====>.........................] - ETA: 2s - loss: 0.1679 - accuracy: 0.7256 - jacard_coef: 0.0838 4/17 [======>.......................] - ETA: 2s - loss: 0.1694 - accuracy: 0.7744 - jacard_coef: 0.0782 5/17 [=======>......................] - ETA: 2s - loss: 0.1683 - accuracy: 0.8027 - jacard_coef: 0.0745 6/17 [=========>....................] - ETA: 2s - loss: 0.1675 - accuracy: 0.8145 - jacard_coef: 0.0716 7/17 [===========>..................] - ETA: 2s - loss: 0.1669 - accuracy: 0.8223 - jacard_coef: 0.0764 8/17 [=============>................] - ETA: 1s - loss: 0.1664 - accuracy: 0.8249 - jacard_coef: 0.0788 9/17 [==============>...............] - ETA: 1s - loss: 0.1658 - accuracy: 0.8311 - jacard_coef: 0.079210/17 [================>.............] - ETA: 1s - loss: 0.1655 - accuracy: 0.8296 - jacard_coef: 0.079511/17 [==================>...........] - ETA: 1s - loss: 0.1650 - accuracy: 0.8354 - jacard_coef: 0.078312/17 [====================>.........] - ETA: 1s - loss: 0.1647 - accuracy: 0.8332 - jacard_coef: 0.080613/17 [=====================>........] - ETA: 0s - loss: 0.1644 - accuracy: 0.8323 - jacard_coef: 0.080714/17 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.8279 - jacard_coef: 0.078515/17 [=========================>....] - ETA: 0s - loss: 0.1638 - accuracy: 0.8331 - jacard_coef: 0.076416/17 [===========================>..] - ETA: 0s - loss: 0.1636 - accuracy: 0.8363 - jacard_coef: 0.075717/17 [==============================] - 4s 210ms/step - loss: 0.1636 - accuracy: 0.8355 - jacard_coef: 0.0746 - val_loss: 0.2123 - val_accuracy: 0.1262 - val_jacard_coef: 0.0640 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1607 - accuracy: 0.9015 - jacard_coef: 0.0797 2/17 [==>...........................] - ETA: 3s - loss: 0.1614 - accuracy: 0.8720 - jacard_coef: 0.0858 3/17 [====>.........................] - ETA: 2s - loss: 0.1613 - accuracy: 0.8651 - jacard_coef: 0.0887 4/17 [======>.......................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8747 - jacard_coef: 0.0846 5/17 [=======>......................] - ETA: 2s - loss: 0.1604 - accuracy: 0.8723 - jacard_coef: 0.0807 6/17 [=========>....................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8660 - jacard_coef: 0.0801 7/17 [===========>..................] - ETA: 2s - loss: 0.1599 - accuracy: 0.8640 - jacard_coef: 0.0774 8/17 [=============>................] - ETA: 1s - loss: 0.1599 - accuracy: 0.8572 - jacard_coef: 0.0784 9/17 [==============>...............] - ETA: 1s - loss: 0.1596 - accuracy: 0.8590 - jacard_coef: 0.078610/17 [================>.............] - ETA: 1s - loss: 0.1593 - accuracy: 0.8621 - jacard_coef: 0.077111/17 [==================>...........] - ETA: 1s - loss: 0.1591 - accuracy: 0.8587 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 1s - loss: 0.1588 - accuracy: 0.8584 - jacard_coef: 0.076213/17 [=====================>........] - ETA: 0s - loss: 0.1586 - accuracy: 0.8553 - jacard_coef: 0.077114/17 [=======================>......] - ETA: 0s - loss: 0.1586 - accuracy: 0.8495 - jacard_coef: 0.078215/17 [=========================>....] - ETA: 0s - loss: 0.1586 - accuracy: 0.8533 - jacard_coef: 0.077716/17 [===========================>..] - ETA: 0s - loss: 0.1584 - accuracy: 0.8569 - jacard_coef: 0.076217/17 [==============================] - 4s 210ms/step - loss: 0.1586 - accuracy: 0.8536 - jacard_coef: 0.0731 - val_loss: 0.1520 - val_accuracy: 0.9304 - val_jacard_coef: 0.0631 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1603 - accuracy: 0.8897 - jacard_coef: 0.0723 2/17 [==>...........................] - ETA: 3s - loss: 0.1619 - accuracy: 0.8745 - jacard_coef: 0.0867 3/17 [====>.........................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8884 - jacard_coef: 0.0768 4/17 [======>.......................] - ETA: 2s - loss: 0.1630 - accuracy: 0.8799 - jacard_coef: 0.0786 5/17 [=======>......................] - ETA: 2s - loss: 0.1634 - accuracy: 0.8835 - jacard_coef: 0.0782 6/17 [=========>....................] - ETA: 2s - loss: 0.1670 - accuracy: 0.8603 - jacard_coef: 0.0749 7/17 [===========>..................] - ETA: 2s - loss: 0.1673 - accuracy: 0.8035 - jacard_coef: 0.0778 8/17 [=============>................] - ETA: 1s - loss: 0.1680 - accuracy: 0.7742 - jacard_coef: 0.0762 9/17 [==============>...............] - ETA: 1s - loss: 0.1677 - accuracy: 0.7855 - jacard_coef: 0.078310/17 [================>.............] - ETA: 1s - loss: 0.1673 - accuracy: 0.7944 - jacard_coef: 0.076111/17 [==================>...........] - ETA: 1s - loss: 0.1688 - accuracy: 0.7957 - jacard_coef: 0.080112/17 [====================>.........] - ETA: 1s - loss: 0.1682 - accuracy: 0.7984 - jacard_coef: 0.079413/17 [=====================>........] - ETA: 0s - loss: 0.1677 - accuracy: 0.7997 - jacard_coef: 0.078014/17 [=======================>......] - ETA: 0s - loss: 0.1672 - accuracy: 0.8031 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 0s - loss: 0.1666 - accuracy: 0.8059 - jacard_coef: 0.076316/17 [===========================>..] - ETA: 0s - loss: 0.1665 - accuracy: 0.8061 - jacard_coef: 0.076217/17 [==============================] - 4s 210ms/step - loss: 0.1665 - accuracy: 0.8067 - jacard_coef: 0.0721 - val_loss: 0.1579 - val_accuracy: 0.9304 - val_jacard_coef: 0.0632 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1596 - accuracy: 0.8705 - jacard_coef: 0.0699 2/17 [==>...........................] - ETA: 3s - loss: 0.1593 - accuracy: 0.8808 - jacard_coef: 0.0737 3/17 [====>.........................] - ETA: 2s - loss: 0.1583 - accuracy: 0.8868 - jacard_coef: 0.0710 4/17 [======>.......................] - ETA: 2s - loss: 0.1579 - accuracy: 0.8857 - jacard_coef: 0.0777 5/17 [=======>......................] - ETA: 2s - loss: 0.1574 - accuracy: 0.8899 - jacard_coef: 0.0750 6/17 [=========>....................] - ETA: 2s - loss: 0.1571 - accuracy: 0.8922 - jacard_coef: 0.0743 7/17 [===========>..................] - ETA: 2s - loss: 0.1567 - accuracy: 0.8915 - jacard_coef: 0.0732 8/17 [=============>................] - ETA: 1s - loss: 0.1571 - accuracy: 0.8792 - jacard_coef: 0.0765 9/17 [==============>...............] - ETA: 1s - loss: 0.1574 - accuracy: 0.8760 - jacard_coef: 0.077810/17 [================>.............] - ETA: 1s - loss: 0.1571 - accuracy: 0.8739 - jacard_coef: 0.078211/17 [==================>...........] - ETA: 1s - loss: 0.1566 - accuracy: 0.8758 - jacard_coef: 0.077712/17 [====================>.........] - ETA: 1s - loss: 0.1561 - accuracy: 0.8773 - jacard_coef: 0.077913/17 [=====================>........] - ETA: 0s - loss: 0.1559 - accuracy: 0.8780 - jacard_coef: 0.077014/17 [=======================>......] - ETA: 0s - loss: 0.1565 - accuracy: 0.8795 - jacard_coef: 0.076615/17 [=========================>....] - ETA: 0s - loss: 0.1561 - accuracy: 0.8811 - jacard_coef: 0.076516/17 [===========================>..] - ETA: 0s - loss: 0.1556 - accuracy: 0.8835 - jacard_coef: 0.075317/17 [==============================] - 4s 210ms/step - loss: 0.1556 - accuracy: 0.8826 - jacard_coef: 0.0790 - val_loss: 0.1568 - val_accuracy: 0.9304 - val_jacard_coef: 0.0632 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1506 - accuracy: 0.9105 - jacard_coef: 0.0519 2/17 [==>...........................] - ETA: 3s - loss: 0.1504 - accuracy: 0.9117 - jacard_coef: 0.0542 3/17 [====>.........................] - ETA: 2s - loss: 0.1508 - accuracy: 0.9036 - jacard_coef: 0.0687 4/17 [======>.......................] - ETA: 2s - loss: 0.1515 - accuracy: 0.8946 - jacard_coef: 0.0804 5/17 [=======>......................] - ETA: 2s - loss: 0.1516 - accuracy: 0.8921 - jacard_coef: 0.0849 6/17 [=========>....................] - ETA: 2s - loss: 0.1517 - accuracy: 0.8917 - jacard_coef: 0.0866 7/17 [===========>..................] - ETA: 2s - loss: 0.1517 - accuracy: 0.8898 - jacard_coef: 0.0886 8/17 [=============>................] - ETA: 1s - loss: 0.1516 - accuracy: 0.8943 - jacard_coef: 0.0858 9/17 [==============>...............] - ETA: 1s - loss: 0.1518 - accuracy: 0.8931 - jacard_coef: 0.085410/17 [================>.............] - ETA: 1s - loss: 0.1516 - accuracy: 0.8964 - jacard_coef: 0.082111/17 [==================>...........] - ETA: 1s - loss: 0.1513 - accuracy: 0.8988 - jacard_coef: 0.078412/17 [====================>.........] - ETA: 1s - loss: 0.1508 - accuracy: 0.9025 - jacard_coef: 0.075013/17 [=====================>........] - ETA: 0s - loss: 0.1507 - accuracy: 0.9011 - jacard_coef: 0.076314/17 [=======================>......] - ETA: 0s - loss: 0.1506 - accuracy: 0.9029 - jacard_coef: 0.075115/17 [=========================>....] - ETA: 0s - loss: 0.1510 - accuracy: 0.9041 - jacard_coef: 0.074916/17 [===========================>..] - ETA: 0s - loss: 0.1510 - accuracy: 0.9016 - jacard_coef: 0.074617/17 [==============================] - 4s 210ms/step - loss: 0.1511 - accuracy: 0.8997 - jacard_coef: 0.0796 - val_loss: 0.1543 - val_accuracy: 0.9304 - val_jacard_coef: 0.0631 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1477 - accuracy: 0.9100 - jacard_coef: 0.0730 2/17 [==>...........................] - ETA: 3s - loss: 0.1495 - accuracy: 0.8895 - jacard_coef: 0.0850 3/17 [====>.........................] - ETA: 2s - loss: 0.1512 - accuracy: 0.8951 - jacard_coef: 0.0738 4/17 [======>.......................] - ETA: 2s - loss: 0.1510 - accuracy: 0.8931 - jacard_coef: 0.0767 5/17 [=======>......................] - ETA: 2s - loss: 0.1528 - accuracy: 0.8982 - jacard_coef: 0.0728 6/17 [=========>....................] - ETA: 2s - loss: 0.1522 - accuracy: 0.9007 - jacard_coef: 0.0698 7/17 [===========>..................] - ETA: 2s - loss: 0.1520 - accuracy: 0.8991 - jacard_coef: 0.0727 8/17 [=============>................] - ETA: 1s - loss: 0.1516 - accuracy: 0.9022 - jacard_coef: 0.0723 9/17 [==============>...............] - ETA: 1s - loss: 0.1513 - accuracy: 0.9043 - jacard_coef: 0.071610/17 [================>.............] - ETA: 1s - loss: 0.1518 - accuracy: 0.9063 - jacard_coef: 0.070811/17 [==================>...........] - ETA: 1s - loss: 0.1515 - accuracy: 0.9068 - jacard_coef: 0.071412/17 [====================>.........] - ETA: 1s - loss: 0.1512 - accuracy: 0.9065 - jacard_coef: 0.072613/17 [=====================>........] - ETA: 0s - loss: 0.1510 - accuracy: 0.9063 - jacard_coef: 0.073414/17 [=======================>......] - ETA: 0s - loss: 0.1507 - accuracy: 0.9065 - jacard_coef: 0.073915/17 [=========================>....] - ETA: 0s - loss: 0.1507 - accuracy: 0.9048 - jacard_coef: 0.075716/17 [===========================>..] - ETA: 0s - loss: 0.1504 - accuracy: 0.9051 - jacard_coef: 0.075917/17 [==============================] - 4s 210ms/step - loss: 0.1504 - accuracy: 0.9053 - jacard_coef: 0.0732 - val_loss: 0.1541 - val_accuracy: 0.9304 - val_jacard_coef: 0.0631 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1485 - accuracy: 0.9079 - jacard_coef: 0.0729 2/17 [==>...........................] - ETA: 3s - loss: 0.1475 - accuracy: 0.9117 - jacard_coef: 0.0732 3/17 [====>.........................] - ETA: 2s - loss: 0.1470 - accuracy: 0.9124 - jacard_coef: 0.0725 4/17 [======>.......................] - ETA: 2s - loss: 0.1470 - accuracy: 0.9122 - jacard_coef: 0.0729 5/17 [=======>......................] - ETA: 2s - loss: 0.1466 - accuracy: 0.9139 - jacard_coef: 0.0720 6/17 [=========>....................] - ETA: 2s - loss: 0.1466 - accuracy: 0.9129 - jacard_coef: 0.0719 7/17 [===========>..................] - ETA: 2s - loss: 0.1461 - accuracy: 0.9175 - jacard_coef: 0.0684 8/17 [=============>................] - ETA: 1s - loss: 0.1465 - accuracy: 0.9117 - jacard_coef: 0.0736 9/17 [==============>...............] - ETA: 1s - loss: 0.1462 - accuracy: 0.9143 - jacard_coef: 0.071410/17 [================>.............] - ETA: 1s - loss: 0.1464 - accuracy: 0.9110 - jacard_coef: 0.074411/17 [==================>...........] - ETA: 1s - loss: 0.1468 - accuracy: 0.9069 - jacard_coef: 0.077312/17 [====================>.........] - ETA: 1s - loss: 0.1465 - accuracy: 0.9089 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.1469 - accuracy: 0.9078 - jacard_coef: 0.077214/17 [=======================>......] - ETA: 0s - loss: 0.1467 - accuracy: 0.9092 - jacard_coef: 0.076115/17 [=========================>....] - ETA: 0s - loss: 0.1468 - accuracy: 0.9090 - jacard_coef: 0.075916/17 [===========================>..] - ETA: 0s - loss: 0.1469 - accuracy: 0.9090 - jacard_coef: 0.076117/17 [==============================] - 4s 210ms/step - loss: 0.1469 - accuracy: 0.9094 - jacard_coef: 0.0727 - val_loss: 0.1518 - val_accuracy: 0.9304 - val_jacard_coef: 0.0631 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1424 - accuracy: 0.9215 - jacard_coef: 0.0602 2/17 [==>...........................] - ETA: 3s - loss: 0.1431 - accuracy: 0.9095 - jacard_coef: 0.0711 3/17 [====>.........................] - ETA: 2s - loss: 0.1450 - accuracy: 0.8914 - jacard_coef: 0.0828 4/17 [======>.......................] - ETA: 2s - loss: 0.1449 - accuracy: 0.8931 - jacard_coef: 0.0823 5/17 [=======>......................] - ETA: 2s - loss: 0.1452 - accuracy: 0.8926 - jacard_coef: 0.0848 6/17 [=========>....................] - ETA: 2s - loss: 0.1451 - accuracy: 0.8948 - jacard_coef: 0.0844 7/17 [===========>..................] - ETA: 2s - loss: 0.1445 - accuracy: 0.9008 - jacard_coef: 0.0803 8/17 [=============>................] - ETA: 1s - loss: 0.1445 - accuracy: 0.9032 - jacard_coef: 0.0781 9/17 [==============>...............] - ETA: 1s - loss: 0.1441 - accuracy: 0.9071 - jacard_coef: 0.075510/17 [================>.............] - ETA: 1s - loss: 0.1438 - accuracy: 0.9092 - jacard_coef: 0.074311/17 [==================>...........] - ETA: 1s - loss: 0.1441 - accuracy: 0.9051 - jacard_coef: 0.077912/17 [====================>.........] - ETA: 1s - loss: 0.1439 - accuracy: 0.9061 - jacard_coef: 0.077513/17 [=====================>........] - ETA: 0s - loss: 0.1437 - accuracy: 0.9072 - jacard_coef: 0.076614/17 [=======================>......] - ETA: 0s - loss: 0.1435 - accuracy: 0.9086 - jacard_coef: 0.074715/17 [=========================>....] - ETA: 0s - loss: 0.1438 - accuracy: 0.9075 - jacard_coef: 0.076016/17 [===========================>..] - ETA: 0s - loss: 0.1439 - accuracy: 0.9083 - jacard_coef: 0.075317/17 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9079 - jacard_coef: 0.077017/17 [==============================] - 4s 215ms/step - loss: 0.1440 - accuracy: 0.9079 - jacard_coef: 0.0770 - val_loss: 0.1539 - val_accuracy: 0.9304 - val_jacard_coef: 0.0631 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1427 - accuracy: 0.8968 - jacard_coef: 0.0927 2/17 [==>...........................] - ETA: 3s - loss: 0.1414 - accuracy: 0.9182 - jacard_coef: 0.0720 3/17 [====>.........................] - ETA: 3s - loss: 0.1413 - accuracy: 0.9208 - jacard_coef: 0.0694 4/17 [======>.......................] - ETA: 2s - loss: 0.1420 - accuracy: 0.9111 - jacard_coef: 0.0776 5/17 [=======>......................] - ETA: 2s - loss: 0.1429 - accuracy: 0.9167 - jacard_coef: 0.0734 6/17 [=========>....................] - ETA: 2s - loss: 0.1430 - accuracy: 0.8996 - jacard_coef: 0.0723 7/17 [===========>..................] - ETA: 2s - loss: 0.1429 - accuracy: 0.8987 - jacard_coef: 0.0754 8/17 [=============>................] - ETA: 1s - loss: 0.1430 - accuracy: 0.8987 - jacard_coef: 0.0768 9/17 [==============>...............] - ETA: 1s - loss: 0.1428 - accuracy: 0.9019 - jacard_coef: 0.075510/17 [================>.............] - ETA: 1s - loss: 0.1425 - accuracy: 0.9067 - jacard_coef: 0.072511/17 [==================>...........] - ETA: 1s - loss: 0.1427 - accuracy: 0.9043 - jacard_coef: 0.075612/17 [====================>.........] - ETA: 1s - loss: 0.1427 - accuracy: 0.9045 - jacard_coef: 0.076313/17 [=====================>........] - ETA: 0s - loss: 0.1426 - accuracy: 0.9061 - jacard_coef: 0.075714/17 [=======================>......] - ETA: 0s - loss: 0.1424 - accuracy: 0.9072 - jacard_coef: 0.074715/17 [=========================>....] - ETA: 0s - loss: 0.1425 - accuracy: 0.9054 - jacard_coef: 0.075416/17 [===========================>..] - ETA: 0s - loss: 0.1424 - accuracy: 0.9052 - jacard_coef: 0.075517/17 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9053 - jacard_coef: 0.074217/17 [==============================] - 4s 215ms/step - loss: 0.1424 - accuracy: 0.9053 - jacard_coef: 0.0742 - val_loss: 0.1486 - val_accuracy: 0.9304 - val_jacard_coef: 0.0630 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0680 (epoch 3)
  Final Val Loss: 0.1486
  Training Time: 0:02:02.791379
  Stability (std): 0.0241

Results saved to: hyperparameter_optimization_20250926_123742/exp_25_Attention_ResUNet_lr1e-4_bs8/Attention_ResUNet_lr0.0001_bs8_results.json

Experiment 25 completed in 160s
Progress: 25/36 completed
Estimated remaining time: 29 minutes

ðŸ”¬ EXPERIMENT 26/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758865323.634221 3310163 service.cc:145] XLA service 0x15133c904e40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758865323.634279 3310163 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758865324.096056 3310163 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 9:14 - loss: 0.3307 - accuracy: 0.4671 - jacard_coef: 0.07092/9 [=====>........................] - ETA: 58s - loss: 0.3022 - accuracy: 0.3847 - jacard_coef: 0.0847 3/9 [=========>....................] - ETA: 26s - loss: 0.2767 - accuracy: 0.3374 - jacard_coef: 0.08844/9 [============>.................] - ETA: 15s - loss: 0.2589 - accuracy: 0.3149 - jacard_coef: 0.08515/9 [===============>..............] - ETA: 9s - loss: 0.2471 - accuracy: 0.2834 - jacard_coef: 0.0816 6/9 [===================>..........] - ETA: 5s - loss: 0.2391 - accuracy: 0.2557 - jacard_coef: 0.07767/9 [======================>.......] - ETA: 3s - loss: 0.2330 - accuracy: 0.2383 - jacard_coef: 0.07738/9 [=========================>....] - ETA: 1s - loss: 0.2279 - accuracy: 0.2281 - jacard_coef: 0.07729/9 [==============================] - ETA: 0s - loss: 0.2275 - accuracy: 0.2277 - jacard_coef: 0.07469/9 [==============================] - 90s 3s/step - loss: 0.2275 - accuracy: 0.2277 - jacard_coef: 0.0746 - val_loss: 0.8667 - val_accuracy: 0.9304 - val_jacard_coef: 0.0048 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1892 - accuracy: 0.1409 - jacard_coef: 0.07482/9 [=====>........................] - ETA: 2s - loss: 0.1901 - accuracy: 0.1430 - jacard_coef: 0.07173/9 [=========>....................] - ETA: 2s - loss: 0.1895 - accuracy: 0.1810 - jacard_coef: 0.08154/9 [============>.................] - ETA: 2s - loss: 0.1893 - accuracy: 0.2116 - jacard_coef: 0.07895/9 [===============>..............] - ETA: 1s - loss: 0.1871 - accuracy: 0.2695 - jacard_coef: 0.08296/9 [===================>..........] - ETA: 1s - loss: 0.1950 - accuracy: 0.2869 - jacard_coef: 0.08337/9 [======================>.......] - ETA: 0s - loss: 0.2001 - accuracy: 0.2818 - jacard_coef: 0.07998/9 [=========================>....] - ETA: 0s - loss: 0.2000 - accuracy: 0.2695 - jacard_coef: 0.07609/9 [==============================] - 4s 388ms/step - loss: 0.1999 - accuracy: 0.2694 - jacard_coef: 0.0828 - val_loss: 1.2094 - val_accuracy: 0.9157 - val_jacard_coef: 0.0050 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1919 - accuracy: 0.1986 - jacard_coef: 0.07172/9 [=====>........................] - ETA: 2s - loss: 0.1912 - accuracy: 0.2217 - jacard_coef: 0.07623/9 [=========>....................] - ETA: 2s - loss: 0.1914 - accuracy: 0.2255 - jacard_coef: 0.06544/9 [============>.................] - ETA: 2s - loss: 0.1901 - accuracy: 0.2359 - jacard_coef: 0.07175/9 [===============>..............] - ETA: 1s - loss: 0.1893 - accuracy: 0.2402 - jacard_coef: 0.07806/9 [===================>..........] - ETA: 1s - loss: 0.1891 - accuracy: 0.2379 - jacard_coef: 0.07587/9 [======================>.......] - ETA: 0s - loss: 0.1884 - accuracy: 0.2383 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1883 - accuracy: 0.2346 - jacard_coef: 0.07659/9 [==============================] - 4s 388ms/step - loss: 0.1883 - accuracy: 0.2343 - jacard_coef: 0.0710 - val_loss: 1.7396 - val_accuracy: 0.8517 - val_jacard_coef: 0.0143 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1816 - accuracy: 0.2400 - jacard_coef: 0.07802/9 [=====>........................] - ETA: 2s - loss: 0.1830 - accuracy: 0.2745 - jacard_coef: 0.07983/9 [=========>....................] - ETA: 2s - loss: 0.1823 - accuracy: 0.3008 - jacard_coef: 0.07774/9 [============>.................] - ETA: 2s - loss: 0.1820 - accuracy: 0.3165 - jacard_coef: 0.07855/9 [===============>..............] - ETA: 1s - loss: 0.1817 - accuracy: 0.3277 - jacard_coef: 0.07506/9 [===================>..........] - ETA: 1s - loss: 0.1814 - accuracy: 0.3392 - jacard_coef: 0.07517/9 [======================>.......] - ETA: 0s - loss: 0.1807 - accuracy: 0.3448 - jacard_coef: 0.07508/9 [=========================>....] - ETA: 0s - loss: 0.1802 - accuracy: 0.3509 - jacard_coef: 0.07699/9 [==============================] - 3s 385ms/step - loss: 0.1802 - accuracy: 0.3506 - jacard_coef: 0.0689 - val_loss: 2.1866 - val_accuracy: 0.8236 - val_jacard_coef: 0.0258 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1746 - accuracy: 0.4617 - jacard_coef: 0.08142/9 [=====>........................] - ETA: 2s - loss: 0.1738 - accuracy: 0.4729 - jacard_coef: 0.07133/9 [=========>....................] - ETA: 2s - loss: 0.1736 - accuracy: 0.5039 - jacard_coef: 0.07314/9 [============>.................] - ETA: 2s - loss: 0.1728 - accuracy: 0.5404 - jacard_coef: 0.07245/9 [===============>..............] - ETA: 1s - loss: 0.1724 - accuracy: 0.5609 - jacard_coef: 0.07516/9 [===================>..........] - ETA: 1s - loss: 0.1720 - accuracy: 0.5788 - jacard_coef: 0.07477/9 [======================>.......] - ETA: 0s - loss: 0.1716 - accuracy: 0.5920 - jacard_coef: 0.07518/9 [=========================>....] - ETA: 0s - loss: 0.1712 - accuracy: 0.6011 - jacard_coef: 0.07669/9 [==============================] - 3s 386ms/step - loss: 0.1712 - accuracy: 0.6014 - jacard_coef: 0.0706 - val_loss: 13.7736 - val_accuracy: 0.0797 - val_jacard_coef: 0.0700 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1680 - accuracy: 0.5395 - jacard_coef: 0.07792/9 [=====>........................] - ETA: 2s - loss: 0.1673 - accuracy: 0.5985 - jacard_coef: 0.07063/9 [=========>....................] - ETA: 2s - loss: 0.1675 - accuracy: 0.6345 - jacard_coef: 0.07444/9 [============>.................] - ETA: 2s - loss: 0.1675 - accuracy: 0.6628 - jacard_coef: 0.07755/9 [===============>..............] - ETA: 1s - loss: 0.1673 - accuracy: 0.6860 - jacard_coef: 0.08026/9 [===================>..........] - ETA: 1s - loss: 0.1670 - accuracy: 0.7041 - jacard_coef: 0.07537/9 [======================>.......] - ETA: 0s - loss: 0.1667 - accuracy: 0.7138 - jacard_coef: 0.07698/9 [=========================>....] - ETA: 0s - loss: 0.1664 - accuracy: 0.7206 - jacard_coef: 0.07579/9 [==============================] - 3s 381ms/step - loss: 0.1666 - accuracy: 0.7192 - jacard_coef: 0.0819 - val_loss: 0.6968 - val_accuracy: 0.7014 - val_jacard_coef: 0.0533 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1672 - accuracy: 0.6846 - jacard_coef: 0.08292/9 [=====>........................] - ETA: 2s - loss: 0.1679 - accuracy: 0.6371 - jacard_coef: 0.08603/9 [=========>....................] - ETA: 2s - loss: 0.1683 - accuracy: 0.6041 - jacard_coef: 0.08304/9 [============>.................] - ETA: 2s - loss: 0.1693 - accuracy: 0.5822 - jacard_coef: 0.08085/9 [===============>..............] - ETA: 1s - loss: 0.1691 - accuracy: 0.5771 - jacard_coef: 0.08276/9 [===================>..........] - ETA: 1s - loss: 0.1690 - accuracy: 0.5746 - jacard_coef: 0.08427/9 [======================>.......] - ETA: 0s - loss: 0.1689 - accuracy: 0.5799 - jacard_coef: 0.07878/9 [=========================>....] - ETA: 0s - loss: 0.1688 - accuracy: 0.5908 - jacard_coef: 0.07559/9 [==============================] - 3s 381ms/step - loss: 0.1688 - accuracy: 0.5914 - jacard_coef: 0.0825 - val_loss: 0.8358 - val_accuracy: 0.9109 - val_jacard_coef: 0.0352 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1683 - accuracy: 0.7902 - jacard_coef: 0.07152/9 [=====>........................] - ETA: 2s - loss: 0.1684 - accuracy: 0.8015 - jacard_coef: 0.07663/9 [=========>....................] - ETA: 2s - loss: 0.1674 - accuracy: 0.8172 - jacard_coef: 0.07024/9 [============>.................] - ETA: 2s - loss: 0.1675 - accuracy: 0.8135 - jacard_coef: 0.07755/9 [===============>..............] - ETA: 1s - loss: 0.1674 - accuracy: 0.8155 - jacard_coef: 0.07676/9 [===================>..........] - ETA: 1s - loss: 0.1672 - accuracy: 0.8166 - jacard_coef: 0.07927/9 [======================>.......] - ETA: 0s - loss: 0.1670 - accuracy: 0.8179 - jacard_coef: 0.07778/9 [=========================>....] - ETA: 0s - loss: 0.1667 - accuracy: 0.8154 - jacard_coef: 0.07619/9 [==============================] - 3s 381ms/step - loss: 0.1668 - accuracy: 0.8145 - jacard_coef: 0.0729 - val_loss: 0.7263 - val_accuracy: 0.9172 - val_jacard_coef: 0.0382 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1638 - accuracy: 0.8074 - jacard_coef: 0.04492/9 [=====>........................] - ETA: 2s - loss: 0.1636 - accuracy: 0.7995 - jacard_coef: 0.06913/9 [=========>....................] - ETA: 2s - loss: 0.1633 - accuracy: 0.7968 - jacard_coef: 0.07474/9 [============>.................] - ETA: 2s - loss: 0.1629 - accuracy: 0.8011 - jacard_coef: 0.07275/9 [===============>..............] - ETA: 1s - loss: 0.1629 - accuracy: 0.7967 - jacard_coef: 0.07806/9 [===================>..........] - ETA: 1s - loss: 0.1627 - accuracy: 0.7972 - jacard_coef: 0.07497/9 [======================>.......] - ETA: 0s - loss: 0.1627 - accuracy: 0.7954 - jacard_coef: 0.07568/9 [=========================>....] - ETA: 0s - loss: 0.1627 - accuracy: 0.7941 - jacard_coef: 0.07559/9 [==============================] - 3s 381ms/step - loss: 0.1627 - accuracy: 0.7935 - jacard_coef: 0.0823 - val_loss: 0.4048 - val_accuracy: 0.9051 - val_jacard_coef: 0.0509 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1598 - accuracy: 0.8023 - jacard_coef: 0.08052/9 [=====>........................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8087 - jacard_coef: 0.07053/9 [=========>....................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8157 - jacard_coef: 0.07194/9 [============>.................] - ETA: 2s - loss: 0.1601 - accuracy: 0.8187 - jacard_coef: 0.07415/9 [===============>..............] - ETA: 1s - loss: 0.1596 - accuracy: 0.8243 - jacard_coef: 0.07356/9 [===================>..........] - ETA: 1s - loss: 0.1596 - accuracy: 0.8225 - jacard_coef: 0.07577/9 [======================>.......] - ETA: 0s - loss: 0.1596 - accuracy: 0.8229 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1596 - accuracy: 0.8250 - jacard_coef: 0.07639/9 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.8246 - jacard_coef: 0.07329/9 [==============================] - 3s 386ms/step - loss: 0.1596 - accuracy: 0.8246 - jacard_coef: 0.0732 - val_loss: 0.1588 - val_accuracy: 0.8731 - val_jacard_coef: 0.0609 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1581 - accuracy: 0.8453 - jacard_coef: 0.09872/9 [=====>........................] - ETA: 2s - loss: 0.1570 - accuracy: 0.8601 - jacard_coef: 0.08483/9 [=========>....................] - ETA: 2s - loss: 0.1570 - accuracy: 0.8550 - jacard_coef: 0.08084/9 [============>.................] - ETA: 2s - loss: 0.1570 - accuracy: 0.8590 - jacard_coef: 0.07755/9 [===============>..............] - ETA: 1s - loss: 0.1571 - accuracy: 0.8622 - jacard_coef: 0.07546/9 [===================>..........] - ETA: 1s - loss: 0.1569 - accuracy: 0.8559 - jacard_coef: 0.07367/9 [======================>.......] - ETA: 0s - loss: 0.1570 - accuracy: 0.8533 - jacard_coef: 0.07398/9 [=========================>....] - ETA: 0s - loss: 0.1570 - accuracy: 0.8506 - jacard_coef: 0.07649/9 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.8505 - jacard_coef: 0.07069/9 [==============================] - 4s 388ms/step - loss: 0.1570 - accuracy: 0.8505 - jacard_coef: 0.0706 - val_loss: 0.1402 - val_accuracy: 0.9086 - val_jacard_coef: 0.0637 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1559 - accuracy: 0.8568 - jacard_coef: 0.09012/9 [=====>........................] - ETA: 2s - loss: 0.1555 - accuracy: 0.8596 - jacard_coef: 0.08443/9 [=========>....................] - ETA: 2s - loss: 0.1552 - accuracy: 0.8563 - jacard_coef: 0.07914/9 [============>.................] - ETA: 2s - loss: 0.1548 - accuracy: 0.8693 - jacard_coef: 0.07475/9 [===============>..............] - ETA: 1s - loss: 0.1550 - accuracy: 0.8675 - jacard_coef: 0.07706/9 [===================>..........] - ETA: 1s - loss: 0.1550 - accuracy: 0.8737 - jacard_coef: 0.07577/9 [======================>.......] - ETA: 0s - loss: 0.1551 - accuracy: 0.8714 - jacard_coef: 0.07888/9 [=========================>....] - ETA: 0s - loss: 0.1549 - accuracy: 0.8655 - jacard_coef: 0.07639/9 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.8644 - jacard_coef: 0.07399/9 [==============================] - 4s 389ms/step - loss: 0.1549 - accuracy: 0.8644 - jacard_coef: 0.0739 - val_loss: 0.1227 - val_accuracy: 0.9147 - val_jacard_coef: 0.0632 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1540 - accuracy: 0.9095 - jacard_coef: 0.06792/9 [=====>........................] - ETA: 2s - loss: 0.1546 - accuracy: 0.8327 - jacard_coef: 0.06853/9 [=========>....................] - ETA: 2s - loss: 0.1545 - accuracy: 0.8506 - jacard_coef: 0.07224/9 [============>.................] - ETA: 2s - loss: 0.1547 - accuracy: 0.8629 - jacard_coef: 0.07425/9 [===============>..............] - ETA: 1s - loss: 0.1546 - accuracy: 0.8728 - jacard_coef: 0.07346/9 [===================>..........] - ETA: 1s - loss: 0.1548 - accuracy: 0.8767 - jacard_coef: 0.07557/9 [======================>.......] - ETA: 0s - loss: 0.1546 - accuracy: 0.8849 - jacard_coef: 0.07308/9 [=========================>....] - ETA: 0s - loss: 0.1547 - accuracy: 0.8859 - jacard_coef: 0.07559/9 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.8823 - jacard_coef: 0.08089/9 [==============================] - 4s 388ms/step - loss: 0.1553 - accuracy: 0.8823 - jacard_coef: 0.0808 - val_loss: 0.1514 - val_accuracy: 0.9267 - val_jacard_coef: 0.0644 - lr: 5.0000e-04
Epoch 14/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1544 - accuracy: 0.9352 - jacard_coef: 0.05292/9 [=====>........................] - ETA: 2s - loss: 0.1569 - accuracy: 0.8999 - jacard_coef: 0.07343/9 [=========>....................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8717 - jacard_coef: 0.08224/9 [============>.................] - ETA: 2s - loss: 0.1593 - accuracy: 0.8688 - jacard_coef: 0.08145/9 [===============>..............] - ETA: 1s - loss: 0.1616 - accuracy: 0.8096 - jacard_coef: 0.08156/9 [===================>..........] - ETA: 1s - loss: 0.1611 - accuracy: 0.8111 - jacard_coef: 0.07907/9 [======================>.......] - ETA: 0s - loss: 0.1609 - accuracy: 0.8173 - jacard_coef: 0.07658/9 [=========================>....] - ETA: 0s - loss: 0.1605 - accuracy: 0.8245 - jacard_coef: 0.07539/9 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.8242 - jacard_coef: 0.08109/9 [==============================] - 4s 389ms/step - loss: 0.1606 - accuracy: 0.8242 - jacard_coef: 0.0810 - val_loss: 0.1547 - val_accuracy: 0.9304 - val_jacard_coef: 0.0647 - lr: 5.0000e-04
Epoch 15/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1591 - accuracy: 0.8260 - jacard_coef: 0.09942/9 [=====>........................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8447 - jacard_coef: 0.08783/9 [=========>....................] - ETA: 2s - loss: 0.1584 - accuracy: 0.8495 - jacard_coef: 0.08224/9 [============>.................] - ETA: 2s - loss: 0.1585 - accuracy: 0.8527 - jacard_coef: 0.07775/9 [===============>..............] - ETA: 1s - loss: 0.1579 - accuracy: 0.8632 - jacard_coef: 0.07376/9 [===================>..........] - ETA: 1s - loss: 0.1580 - accuracy: 0.8614 - jacard_coef: 0.07957/9 [======================>.......] - ETA: 0s - loss: 0.1580 - accuracy: 0.8675 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1577 - accuracy: 0.8719 - jacard_coef: 0.07609/9 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.8680 - jacard_coef: 0.06799/9 [==============================] - 4s 388ms/step - loss: 0.1579 - accuracy: 0.8680 - jacard_coef: 0.0679 - val_loss: 0.1652 - val_accuracy: 0.9304 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0700 (epoch 5)
  Final Val Loss: 0.1652
  Training Time: 0:02:19.879433
  Stability (std): 0.2729

Results saved to: hyperparameter_optimization_20250926_123742/exp_26_Attention_ResUNet_lr1e-4_bs16/Attention_ResUNet_lr0.0001_bs16_results.json

Experiment 26 completed in 178s
Progress: 26/36 completed
Estimated remaining time: 29 minutes

ðŸ”¬ EXPERIMENT 27/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758865506.370376 3317814 service.cc:145] XLA service 0x150fa545d420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758865506.370413 3317814 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758865506.746322 3317814 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 4:56 - loss: 0.3419 - accuracy: 0.4979 - jacard_coef: 0.06262/5 [===========>..................] - ETA: 46s - loss: 0.3146 - accuracy: 0.5502 - jacard_coef: 0.0717 3/5 [=================>............] - ETA: 16s - loss: 0.2870 - accuracy: 0.4668 - jacard_coef: 0.07594/5 [=======================>......] - ETA: 5s - loss: 0.2698 - accuracy: 0.4035 - jacard_coef: 0.0781 5/5 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.4023 - jacard_coef: 0.07575/5 [==============================] - 101s 7s/step - loss: 0.2695 - accuracy: 0.4023 - jacard_coef: 0.0757 - val_loss: 1.1153 - val_accuracy: 0.9304 - val_jacard_coef: 4.2127e-05 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1977 - accuracy: 0.2937 - jacard_coef: 0.07852/5 [===========>..................] - ETA: 2s - loss: 0.1944 - accuracy: 0.2812 - jacard_coef: 0.07563/5 [=================>............] - ETA: 1s - loss: 0.1907 - accuracy: 0.3085 - jacard_coef: 0.07374/5 [=======================>......] - ETA: 0s - loss: 0.1874 - accuracy: 0.3377 - jacard_coef: 0.07605/5 [==============================] - 3s 647ms/step - loss: 0.1879 - accuracy: 0.3380 - jacard_coef: 0.0951 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1927 - accuracy: 0.1428 - jacard_coef: 0.08102/5 [===========>..................] - ETA: 2s - loss: 0.1967 - accuracy: 0.1391 - jacard_coef: 0.08163/5 [=================>............] - ETA: 1s - loss: 0.1963 - accuracy: 0.1378 - jacard_coef: 0.07974/5 [=======================>......] - ETA: 0s - loss: 0.1960 - accuracy: 0.1369 - jacard_coef: 0.07725/5 [==============================] - 3s 647ms/step - loss: 0.1961 - accuracy: 0.1374 - jacard_coef: 0.0866 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1894 - accuracy: 0.1574 - jacard_coef: 0.07732/5 [===========>..................] - ETA: 2s - loss: 0.1898 - accuracy: 0.1715 - jacard_coef: 0.07553/5 [=================>............] - ETA: 1s - loss: 0.1895 - accuracy: 0.1641 - jacard_coef: 0.07874/5 [=======================>......] - ETA: 0s - loss: 0.1913 - accuracy: 0.1543 - jacard_coef: 0.07655/5 [==============================] - 3s 647ms/step - loss: 0.1913 - accuracy: 0.1548 - jacard_coef: 0.0904 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1934 - accuracy: 0.1336 - jacard_coef: 0.08092/5 [===========>..................] - ETA: 2s - loss: 0.1901 - accuracy: 0.1336 - jacard_coef: 0.07753/5 [=================>............] - ETA: 1s - loss: 0.1899 - accuracy: 0.1345 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.1416 - jacard_coef: 0.07715/5 [==============================] - 3s 669ms/step - loss: 0.1888 - accuracy: 0.1416 - jacard_coef: 0.0692 - val_loss: 1.1772 - val_accuracy: 0.9268 - val_jacard_coef: 0.0013 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1849 - accuracy: 0.1936 - jacard_coef: 0.08582/5 [===========>..................] - ETA: 2s - loss: 0.1837 - accuracy: 0.1886 - jacard_coef: 0.07783/5 [=================>............] - ETA: 1s - loss: 0.1835 - accuracy: 0.1904 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1824 - accuracy: 0.1974 - jacard_coef: 0.07685/5 [==============================] - 3s 668ms/step - loss: 0.1826 - accuracy: 0.1980 - jacard_coef: 0.0781 - val_loss: 14.8395 - val_accuracy: 0.0767 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1788 - accuracy: 0.3314 - jacard_coef: 0.08062/5 [===========>..................] - ETA: 2s - loss: 0.1764 - accuracy: 0.4821 - jacard_coef: 0.07203/5 [=================>............] - ETA: 1s - loss: 0.1776 - accuracy: 0.4427 - jacard_coef: 0.08084/5 [=======================>......] - ETA: 0s - loss: 0.1767 - accuracy: 0.5360 - jacard_coef: 0.07675/5 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.5370 - jacard_coef: 0.06915/5 [==============================] - 3s 658ms/step - loss: 0.1767 - accuracy: 0.5370 - jacard_coef: 0.0691 - val_loss: 14.9161 - val_accuracy: 0.0697 - val_jacard_coef: 0.0696 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1765 - accuracy: 0.7910 - jacard_coef: 0.07072/5 [===========>..................] - ETA: 2s - loss: 0.1753 - accuracy: 0.7072 - jacard_coef: 0.07443/5 [=================>............] - ETA: 1s - loss: 0.1745 - accuracy: 0.6081 - jacard_coef: 0.07364/5 [=======================>......] - ETA: 0s - loss: 0.1739 - accuracy: 0.5534 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.5523 - jacard_coef: 0.08595/5 [==============================] - 3s 660ms/step - loss: 0.1739 - accuracy: 0.5523 - jacard_coef: 0.0859 - val_loss: 1.0407 - val_accuracy: 0.9304 - val_jacard_coef: 0.0019 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1715 - accuracy: 0.3687 - jacard_coef: 0.08922/5 [===========>..................] - ETA: 2s - loss: 0.1711 - accuracy: 0.3640 - jacard_coef: 0.08073/5 [=================>............] - ETA: 1s - loss: 0.1708 - accuracy: 0.3651 - jacard_coef: 0.07744/5 [=======================>......] - ETA: 0s - loss: 0.1708 - accuracy: 0.3666 - jacard_coef: 0.07715/5 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.3660 - jacard_coef: 0.07235/5 [==============================] - 4s 697ms/step - loss: 0.1708 - accuracy: 0.3660 - jacard_coef: 0.0723 - val_loss: 9.1947 - val_accuracy: 0.0762 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1669 - accuracy: 0.7621 - jacard_coef: 0.07482/5 [===========>..................] - ETA: 2s - loss: 0.1667 - accuracy: 0.7801 - jacard_coef: 0.08273/5 [=================>............] - ETA: 1s - loss: 0.1668 - accuracy: 0.7976 - jacard_coef: 0.07544/5 [=======================>......] - ETA: 0s - loss: 0.1668 - accuracy: 0.8023 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.8021 - jacard_coef: 0.06955/5 [==============================] - 3s 660ms/step - loss: 0.1668 - accuracy: 0.8021 - jacard_coef: 0.0695 - val_loss: 13.1315 - val_accuracy: 0.0735 - val_jacard_coef: 0.0700 - lr: 0.0010
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1649 - accuracy: 0.7368 - jacard_coef: 0.07812/5 [===========>..................] - ETA: 2s - loss: 0.1639 - accuracy: 0.7928 - jacard_coef: 0.06933/5 [=================>............] - ETA: 1s - loss: 0.1643 - accuracy: 0.7538 - jacard_coef: 0.07274/5 [=======================>......] - ETA: 0s - loss: 0.1640 - accuracy: 0.7660 - jacard_coef: 0.07555/5 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.7649 - jacard_coef: 0.09065/5 [==============================] - 4s 705ms/step - loss: 0.1640 - accuracy: 0.7649 - jacard_coef: 0.0906 - val_loss: 9.1058 - val_accuracy: 0.0749 - val_jacard_coef: 0.0701 - lr: 0.0010
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1626 - accuracy: 0.7559 - jacard_coef: 0.07962/5 [===========>..................] - ETA: 2s - loss: 0.1625 - accuracy: 0.7654 - jacard_coef: 0.07683/5 [=================>............] - ETA: 1s - loss: 0.1624 - accuracy: 0.7588 - jacard_coef: 0.07524/5 [=======================>......] - ETA: 0s - loss: 0.1623 - accuracy: 0.7505 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.7505 - jacard_coef: 0.07455/5 [==============================] - 3s 661ms/step - loss: 0.1623 - accuracy: 0.7505 - jacard_coef: 0.0745 - val_loss: 2.8285 - val_accuracy: 0.0916 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1628 - accuracy: 0.7063 - jacard_coef: 0.07672/5 [===========>..................] - ETA: 2s - loss: 0.1619 - accuracy: 0.7582 - jacard_coef: 0.07403/5 [=================>............] - ETA: 1s - loss: 0.1615 - accuracy: 0.7928 - jacard_coef: 0.07044/5 [=======================>......] - ETA: 0s - loss: 0.1614 - accuracy: 0.8052 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.8030 - jacard_coef: 0.08645/5 [==============================] - 3s 660ms/step - loss: 0.1620 - accuracy: 0.8030 - jacard_coef: 0.0864 - val_loss: 1.4956 - val_accuracy: 0.1154 - val_jacard_coef: 0.0689 - lr: 0.0010
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1645 - accuracy: 0.8796 - jacard_coef: 0.08362/5 [===========>..................] - ETA: 2s - loss: 0.1656 - accuracy: 0.8806 - jacard_coef: 0.07813/5 [=================>............] - ETA: 1s - loss: 0.1667 - accuracy: 0.8750 - jacard_coef: 0.07834/5 [=======================>......] - ETA: 0s - loss: 0.1674 - accuracy: 0.8762 - jacard_coef: 0.07545/5 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.8754 - jacard_coef: 0.08795/5 [==============================] - 3s 660ms/step - loss: 0.1675 - accuracy: 0.8754 - jacard_coef: 0.0879 - val_loss: 0.2586 - val_accuracy: 0.9213 - val_jacard_coef: 0.0264 - lr: 0.0010
Epoch 15/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1699 - accuracy: 0.8741 - jacard_coef: 0.07772/5 [===========>..................] - ETA: 2s - loss: 0.1705 - accuracy: 0.8620 - jacard_coef: 0.08003/5 [=================>............] - ETA: 1s - loss: 0.1710 - accuracy: 0.8618 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 0s - loss: 0.1707 - accuracy: 0.8634 - jacard_coef: 0.07625/5 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.8589 - jacard_coef: 0.08045/5 [==============================] - 3s 659ms/step - loss: 0.1708 - accuracy: 0.8589 - jacard_coef: 0.0804 - val_loss: 0.2272 - val_accuracy: 0.9303 - val_jacard_coef: 0.0342 - lr: 5.0000e-04
Epoch 16/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1700 - accuracy: 0.9017 - jacard_coef: 0.06342/5 [===========>..................] - ETA: 2s - loss: 0.1698 - accuracy: 0.8350 - jacard_coef: 0.07653/5 [=================>............] - ETA: 1s - loss: 0.1694 - accuracy: 0.8291 - jacard_coef: 0.07834/5 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.8393 - jacard_coef: 0.07605/5 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.8392 - jacard_coef: 0.08685/5 [==============================] - 3s 660ms/step - loss: 0.1690 - accuracy: 0.8392 - jacard_coef: 0.0868 - val_loss: 0.1214 - val_accuracy: 0.9288 - val_jacard_coef: 0.0550 - lr: 5.0000e-04
Epoch 17/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1668 - accuracy: 0.8821 - jacard_coef: 0.07842/5 [===========>..................] - ETA: 2s - loss: 0.1665 - accuracy: 0.8952 - jacard_coef: 0.07263/5 [=================>............] - ETA: 1s - loss: 0.1663 - accuracy: 0.8919 - jacard_coef: 0.07344/5 [=======================>......] - ETA: 0s - loss: 0.1666 - accuracy: 0.8897 - jacard_coef: 0.07615/5 [==============================] - 3s 660ms/step - loss: 0.1666 - accuracy: 0.8862 - jacard_coef: 0.0862 - val_loss: 0.0905 - val_accuracy: 0.9248 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 18/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1661 - accuracy: 0.9064 - jacard_coef: 0.07822/5 [===========>..................] - ETA: 2s - loss: 0.1652 - accuracy: 0.9027 - jacard_coef: 0.08223/5 [=================>............] - ETA: 1s - loss: 0.1647 - accuracy: 0.9027 - jacard_coef: 0.08314/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.9108 - jacard_coef: 0.07685/5 [==============================] - 3s 660ms/step - loss: 0.1642 - accuracy: 0.9114 - jacard_coef: 0.0631 - val_loss: 0.1274 - val_accuracy: 0.9224 - val_jacard_coef: 0.0644 - lr: 5.0000e-04
Epoch 19/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1629 - accuracy: 0.9061 - jacard_coef: 0.08112/5 [===========>..................] - ETA: 2s - loss: 0.1620 - accuracy: 0.9126 - jacard_coef: 0.07593/5 [=================>............] - ETA: 1s - loss: 0.1619 - accuracy: 0.9119 - jacard_coef: 0.07604/5 [=======================>......] - ETA: 0s - loss: 0.1617 - accuracy: 0.9125 - jacard_coef: 0.07585/5 [==============================] - 3s 659ms/step - loss: 0.1617 - accuracy: 0.9119 - jacard_coef: 0.0873 - val_loss: 0.1564 - val_accuracy: 0.9130 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 20/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1611 - accuracy: 0.9053 - jacard_coef: 0.08182/5 [===========>..................] - ETA: 2s - loss: 0.1608 - accuracy: 0.8950 - jacard_coef: 0.09063/5 [=================>............] - ETA: 1s - loss: 0.1604 - accuracy: 0.9074 - jacard_coef: 0.08044/5 [=======================>......] - ETA: 0s - loss: 0.1599 - accuracy: 0.9125 - jacard_coef: 0.07615/5 [==============================] - 3s 660ms/step - loss: 0.1600 - accuracy: 0.9085 - jacard_coef: 0.0713 - val_loss: 0.1662 - val_accuracy: 0.8855 - val_jacard_coef: 0.0650 - lr: 2.5000e-04
Epoch 21/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1597 - accuracy: 0.9061 - jacard_coef: 0.08182/5 [===========>..................] - ETA: 2s - loss: 0.1591 - accuracy: 0.9158 - jacard_coef: 0.07373/5 [=================>............] - ETA: 1s - loss: 0.1588 - accuracy: 0.9185 - jacard_coef: 0.07134/5 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.9135 - jacard_coef: 0.07545/5 [==============================] - ETA: 0s - loss: 0.1589 - accuracy: 0.9126 - jacard_coef: 0.09025/5 [==============================] - 3s 659ms/step - loss: 0.1589 - accuracy: 0.9126 - jacard_coef: 0.0902 - val_loss: 0.1692 - val_accuracy: 0.8959 - val_jacard_coef: 0.0651 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0701 (epoch 11)
  Final Val Loss: 0.1692
  Training Time: 0:02:51.137730
  Stability (std): 0.8540

Results saved to: hyperparameter_optimization_20250926_123742/exp_27_Attention_ResUNet_lr1e-4_bs32/Attention_ResUNet_lr0.0001_bs32_results.json

Experiment 27 completed in 209s
Progress: 27/36 completed
Estimated remaining time: 31 minutes

ðŸ”¬ EXPERIMENT 28/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-4
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758865707.100642 3325976 service.cc:145] XLA service 0x1462616e22a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758865707.100700 3325976 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758865707.565321 3325976 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 16:56 - loss: 0.3424 - accuracy: 0.5207 - jacard_coef: 0.0807 2/17 [==>...........................] - ETA: 1:10 - loss: 0.3152 - accuracy: 0.4925 - jacard_coef: 0.0861  3/17 [====>.........................] - ETA: 34s - loss: 0.2885 - accuracy: 0.4734 - jacard_coef: 0.0917  4/17 [======>.......................] - ETA: 22s - loss: 0.2690 - accuracy: 0.4894 - jacard_coef: 0.0821 5/17 [=======>......................] - ETA: 15s - loss: 0.2556 - accuracy: 0.5287 - jacard_coef: 0.0828 6/17 [=========>....................] - ETA: 12s - loss: 0.2455 - accuracy: 0.5534 - jacard_coef: 0.0812 7/17 [===========>..................] - ETA: 9s - loss: 0.2375 - accuracy: 0.5732 - jacard_coef: 0.0776  8/17 [=============>................] - ETA: 7s - loss: 0.2301 - accuracy: 0.5813 - jacard_coef: 0.0770 9/17 [==============>...............] - ETA: 6s - loss: 0.2386 - accuracy: 0.5795 - jacard_coef: 0.073010/17 [================>.............] - ETA: 4s - loss: 0.2380 - accuracy: 0.5775 - jacard_coef: 0.074911/17 [==================>...........] - ETA: 3s - loss: 0.2354 - accuracy: 0.5872 - jacard_coef: 0.076512/17 [====================>.........] - ETA: 3s - loss: 0.2319 - accuracy: 0.5976 - jacard_coef: 0.077813/17 [=====================>........] - ETA: 2s - loss: 0.2287 - accuracy: 0.6061 - jacard_coef: 0.076414/17 [=======================>......] - ETA: 1s - loss: 0.2255 - accuracy: 0.6095 - jacard_coef: 0.076715/17 [=========================>....] - ETA: 1s - loss: 0.2227 - accuracy: 0.6126 - jacard_coef: 0.077316/17 [===========================>..] - ETA: 0s - loss: 0.2202 - accuracy: 0.6150 - jacard_coef: 0.076417/17 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.6154 - jacard_coef: 0.074017/17 [==============================] - 79s 993ms/step - loss: 0.2198 - accuracy: 0.6154 - jacard_coef: 0.0740 - val_loss: 1.1794 - val_accuracy: 0.9257 - val_jacard_coef: 0.0031 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1759 - accuracy: 0.7138 - jacard_coef: 0.0691 2/17 [==>...........................] - ETA: 3s - loss: 0.1757 - accuracy: 0.7256 - jacard_coef: 0.0806 3/17 [====>.........................] - ETA: 2s - loss: 0.1758 - accuracy: 0.7444 - jacard_coef: 0.0823 4/17 [======>.......................] - ETA: 2s - loss: 0.1747 - accuracy: 0.7336 - jacard_coef: 0.0868 5/17 [=======>......................] - ETA: 2s - loss: 0.1736 - accuracy: 0.7285 - jacard_coef: 0.0803 6/17 [=========>....................] - ETA: 2s - loss: 0.1732 - accuracy: 0.7073 - jacard_coef: 0.0773 7/17 [===========>..................] - ETA: 2s - loss: 0.1727 - accuracy: 0.7126 - jacard_coef: 0.0785 8/17 [=============>................] - ETA: 1s - loss: 0.1725 - accuracy: 0.7285 - jacard_coef: 0.0764 9/17 [==============>...............] - ETA: 1s - loss: 0.1728 - accuracy: 0.7356 - jacard_coef: 0.077510/17 [================>.............] - ETA: 1s - loss: 0.1729 - accuracy: 0.7426 - jacard_coef: 0.076711/17 [==================>...........] - ETA: 1s - loss: 0.1728 - accuracy: 0.7503 - jacard_coef: 0.073712/17 [====================>.........] - ETA: 1s - loss: 0.1730 - accuracy: 0.7572 - jacard_coef: 0.075713/17 [=====================>........] - ETA: 0s - loss: 0.1731 - accuracy: 0.7631 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1729 - accuracy: 0.7659 - jacard_coef: 0.076215/17 [=========================>....] - ETA: 0s - loss: 0.1726 - accuracy: 0.7679 - jacard_coef: 0.075116/17 [===========================>..] - ETA: 0s - loss: 0.1724 - accuracy: 0.7667 - jacard_coef: 0.075217/17 [==============================] - 4s 212ms/step - loss: 0.1732 - accuracy: 0.7656 - jacard_coef: 0.0790 - val_loss: 12.9416 - val_accuracy: 0.1303 - val_jacard_coef: 0.0673 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1736 - accuracy: 0.7914 - jacard_coef: 0.1130 2/17 [==>...........................] - ETA: 3s - loss: 0.1853 - accuracy: 0.7901 - jacard_coef: 0.0851 3/17 [====>.........................] - ETA: 2s - loss: 0.1809 - accuracy: 0.7859 - jacard_coef: 0.0821 4/17 [======>.......................] - ETA: 2s - loss: 0.1786 - accuracy: 0.7954 - jacard_coef: 0.0750 5/17 [=======>......................] - ETA: 2s - loss: 0.1772 - accuracy: 0.8001 - jacard_coef: 0.0747 6/17 [=========>....................] - ETA: 2s - loss: 0.1760 - accuracy: 0.8017 - jacard_coef: 0.0732 7/17 [===========>..................] - ETA: 2s - loss: 0.1755 - accuracy: 0.8041 - jacard_coef: 0.0710 8/17 [=============>................] - ETA: 1s - loss: 0.1752 - accuracy: 0.8026 - jacard_coef: 0.0721 9/17 [==============>...............] - ETA: 1s - loss: 0.1747 - accuracy: 0.8062 - jacard_coef: 0.070910/17 [================>.............] - ETA: 1s - loss: 0.1740 - accuracy: 0.8053 - jacard_coef: 0.072611/17 [==================>...........] - ETA: 1s - loss: 0.1736 - accuracy: 0.8100 - jacard_coef: 0.073312/17 [====================>.........] - ETA: 1s - loss: 0.1733 - accuracy: 0.8132 - jacard_coef: 0.074613/17 [=====================>........] - ETA: 0s - loss: 0.1727 - accuracy: 0.8142 - jacard_coef: 0.075114/17 [=======================>......] - ETA: 0s - loss: 0.1723 - accuracy: 0.8174 - jacard_coef: 0.075215/17 [=========================>....] - ETA: 0s - loss: 0.1719 - accuracy: 0.8178 - jacard_coef: 0.074616/17 [===========================>..] - ETA: 0s - loss: 0.1715 - accuracy: 0.8088 - jacard_coef: 0.075817/17 [==============================] - 4s 211ms/step - loss: 0.1715 - accuracy: 0.8086 - jacard_coef: 0.0743 - val_loss: 1.5093 - val_accuracy: 0.3537 - val_jacard_coef: 0.0705 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1628 - accuracy: 0.7339 - jacard_coef: 0.0817 2/17 [==>...........................] - ETA: 3s - loss: 0.1630 - accuracy: 0.7816 - jacard_coef: 0.0827 3/17 [====>.........................] - ETA: 2s - loss: 0.1635 - accuracy: 0.8073 - jacard_coef: 0.0835 4/17 [======>.......................] - ETA: 2s - loss: 0.1630 - accuracy: 0.8177 - jacard_coef: 0.0770 5/17 [=======>......................] - ETA: 2s - loss: 0.1629 - accuracy: 0.8205 - jacard_coef: 0.0786 6/17 [=========>....................] - ETA: 2s - loss: 0.1627 - accuracy: 0.8140 - jacard_coef: 0.0820 7/17 [===========>..................] - ETA: 2s - loss: 0.1630 - accuracy: 0.8048 - jacard_coef: 0.0883 8/17 [=============>................] - ETA: 1s - loss: 0.1631 - accuracy: 0.8059 - jacard_coef: 0.0903 9/17 [==============>...............] - ETA: 1s - loss: 0.1627 - accuracy: 0.8081 - jacard_coef: 0.085110/17 [================>.............] - ETA: 1s - loss: 0.1625 - accuracy: 0.8155 - jacard_coef: 0.083711/17 [==================>...........] - ETA: 1s - loss: 0.1623 - accuracy: 0.8166 - jacard_coef: 0.086112/17 [====================>.........] - ETA: 1s - loss: 0.1621 - accuracy: 0.8195 - jacard_coef: 0.083913/17 [=====================>........] - ETA: 0s - loss: 0.1618 - accuracy: 0.8212 - jacard_coef: 0.080614/17 [=======================>......] - ETA: 0s - loss: 0.1616 - accuracy: 0.8199 - jacard_coef: 0.078515/17 [=========================>....] - ETA: 0s - loss: 0.1614 - accuracy: 0.8229 - jacard_coef: 0.077716/17 [===========================>..] - ETA: 0s - loss: 0.1611 - accuracy: 0.8264 - jacard_coef: 0.075917/17 [==============================] - 4s 209ms/step - loss: 0.1611 - accuracy: 0.8268 - jacard_coef: 0.0718 - val_loss: 1.2060 - val_accuracy: 0.2179 - val_jacard_coef: 0.0680 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1605 - accuracy: 0.7913 - jacard_coef: 0.1086 2/17 [==>...........................] - ETA: 3s - loss: 0.1594 - accuracy: 0.8367 - jacard_coef: 0.0921 3/17 [====>.........................] - ETA: 2s - loss: 0.1586 - accuracy: 0.8634 - jacard_coef: 0.0859 4/17 [======>.......................] - ETA: 2s - loss: 0.1584 - accuracy: 0.8808 - jacard_coef: 0.0793 5/17 [=======>......................] - ETA: 2s - loss: 0.1580 - accuracy: 0.8926 - jacard_coef: 0.0745 6/17 [=========>....................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8928 - jacard_coef: 0.0777 7/17 [===========>..................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8914 - jacard_coef: 0.0811 8/17 [=============>................] - ETA: 1s - loss: 0.1580 - accuracy: 0.8927 - jacard_coef: 0.0821 9/17 [==============>...............] - ETA: 1s - loss: 0.1579 - accuracy: 0.8953 - jacard_coef: 0.081410/17 [================>.............] - ETA: 1s - loss: 0.1638 - accuracy: 0.8725 - jacard_coef: 0.078311/17 [==================>...........] - ETA: 1s - loss: 0.1663 - accuracy: 0.8733 - jacard_coef: 0.078212/17 [====================>.........] - ETA: 1s - loss: 0.1655 - accuracy: 0.8760 - jacard_coef: 0.077213/17 [=====================>........] - ETA: 0s - loss: 0.1650 - accuracy: 0.8778 - jacard_coef: 0.076514/17 [=======================>......] - ETA: 0s - loss: 0.1646 - accuracy: 0.8782 - jacard_coef: 0.076015/17 [=========================>....] - ETA: 0s - loss: 0.1642 - accuracy: 0.8807 - jacard_coef: 0.074316/17 [===========================>..] - ETA: 0s - loss: 0.1638 - accuracy: 0.8813 - jacard_coef: 0.074917/17 [==============================] - 4s 209ms/step - loss: 0.1639 - accuracy: 0.8809 - jacard_coef: 0.0781 - val_loss: 0.3427 - val_accuracy: 0.3435 - val_jacard_coef: 0.0686 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1612 - accuracy: 0.8795 - jacard_coef: 0.0945 2/17 [==>...........................] - ETA: 3s - loss: 0.1599 - accuracy: 0.9131 - jacard_coef: 0.0692 3/17 [====>.........................] - ETA: 2s - loss: 0.1610 - accuracy: 0.8691 - jacard_coef: 0.0713 4/17 [======>.......................] - ETA: 2s - loss: 0.1605 - accuracy: 0.8779 - jacard_coef: 0.0719 5/17 [=======>......................] - ETA: 2s - loss: 0.1599 - accuracy: 0.8805 - jacard_coef: 0.0660 6/17 [=========>....................] - ETA: 2s - loss: 0.1594 - accuracy: 0.8902 - jacard_coef: 0.0642 7/17 [===========>..................] - ETA: 2s - loss: 0.1593 - accuracy: 0.8947 - jacard_coef: 0.0647 8/17 [=============>................] - ETA: 1s - loss: 0.1590 - accuracy: 0.8949 - jacard_coef: 0.0672 9/17 [==============>...............] - ETA: 1s - loss: 0.1594 - accuracy: 0.8940 - jacard_coef: 0.070010/17 [================>.............] - ETA: 1s - loss: 0.1592 - accuracy: 0.8947 - jacard_coef: 0.069511/17 [==================>...........] - ETA: 1s - loss: 0.1589 - accuracy: 0.8933 - jacard_coef: 0.069112/17 [====================>.........] - ETA: 1s - loss: 0.1583 - accuracy: 0.8946 - jacard_coef: 0.067813/17 [=====================>........] - ETA: 0s - loss: 0.1582 - accuracy: 0.8930 - jacard_coef: 0.068314/17 [=======================>......] - ETA: 0s - loss: 0.1584 - accuracy: 0.8891 - jacard_coef: 0.072315/17 [=========================>....] - ETA: 0s - loss: 0.1583 - accuracy: 0.8888 - jacard_coef: 0.073516/17 [===========================>..] - ETA: 0s - loss: 0.1583 - accuracy: 0.8881 - jacard_coef: 0.075617/17 [==============================] - ETA: 0s - loss: 0.1583 - accuracy: 0.8889 - jacard_coef: 0.071517/17 [==============================] - 4s 213ms/step - loss: 0.1583 - accuracy: 0.8889 - jacard_coef: 0.0715 - val_loss: 0.1486 - val_accuracy: 0.9217 - val_jacard_coef: 0.0628 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1500 - accuracy: 0.9525 - jacard_coef: 0.0419 2/17 [==>...........................] - ETA: 3s - loss: 0.1528 - accuracy: 0.9156 - jacard_coef: 0.0670 3/17 [====>.........................] - ETA: 2s - loss: 0.1520 - accuracy: 0.9201 - jacard_coef: 0.0653 4/17 [======>.......................] - ETA: 2s - loss: 0.1519 - accuracy: 0.9250 - jacard_coef: 0.0626 5/17 [=======>......................] - ETA: 2s - loss: 0.1521 - accuracy: 0.9238 - jacard_coef: 0.0643 6/17 [=========>....................] - ETA: 2s - loss: 0.1522 - accuracy: 0.9153 - jacard_coef: 0.0690 7/17 [===========>..................] - ETA: 2s - loss: 0.1526 - accuracy: 0.9110 - jacard_coef: 0.0734 8/17 [=============>................] - ETA: 1s - loss: 0.1527 - accuracy: 0.9110 - jacard_coef: 0.0724 9/17 [==============>...............] - ETA: 1s - loss: 0.1524 - accuracy: 0.9090 - jacard_coef: 0.071810/17 [================>.............] - ETA: 1s - loss: 0.1521 - accuracy: 0.9057 - jacard_coef: 0.071211/17 [==================>...........] - ETA: 1s - loss: 0.1521 - accuracy: 0.9044 - jacard_coef: 0.072512/17 [====================>.........] - ETA: 1s - loss: 0.1519 - accuracy: 0.9039 - jacard_coef: 0.071013/17 [=====================>........] - ETA: 0s - loss: 0.1518 - accuracy: 0.9003 - jacard_coef: 0.074314/17 [=======================>......] - ETA: 0s - loss: 0.1518 - accuracy: 0.8997 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1516 - accuracy: 0.9008 - jacard_coef: 0.075616/17 [===========================>..] - ETA: 0s - loss: 0.1516 - accuracy: 0.9016 - jacard_coef: 0.075717/17 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.8984 - jacard_coef: 0.071417/17 [==============================] - 4s 215ms/step - loss: 0.1523 - accuracy: 0.8984 - jacard_coef: 0.0714 - val_loss: 0.1429 - val_accuracy: 0.9188 - val_jacard_coef: 0.0627 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1531 - accuracy: 0.8624 - jacard_coef: 0.0945 2/17 [==>...........................] - ETA: 3s - loss: 0.1553 - accuracy: 0.8293 - jacard_coef: 0.0979 3/17 [====>.........................] - ETA: 2s - loss: 0.1551 - accuracy: 0.8320 - jacard_coef: 0.0926 4/17 [======>.......................] - ETA: 2s - loss: 0.1542 - accuracy: 0.8367 - jacard_coef: 0.0827 5/17 [=======>......................] - ETA: 2s - loss: 0.1539 - accuracy: 0.8377 - jacard_coef: 0.0811 6/17 [=========>....................] - ETA: 2s - loss: 0.1535 - accuracy: 0.8435 - jacard_coef: 0.0797 7/17 [===========>..................] - ETA: 2s - loss: 0.1524 - accuracy: 0.8555 - jacard_coef: 0.0750 8/17 [=============>................] - ETA: 1s - loss: 0.1529 - accuracy: 0.8605 - jacard_coef: 0.0730 9/17 [==============>...............] - ETA: 1s - loss: 0.1530 - accuracy: 0.8615 - jacard_coef: 0.076210/17 [================>.............] - ETA: 1s - loss: 0.1528 - accuracy: 0.8641 - jacard_coef: 0.078411/17 [==================>...........] - ETA: 1s - loss: 0.1523 - accuracy: 0.8682 - jacard_coef: 0.077812/17 [====================>.........] - ETA: 1s - loss: 0.1519 - accuracy: 0.8717 - jacard_coef: 0.076913/17 [=====================>........] - ETA: 0s - loss: 0.1518 - accuracy: 0.8734 - jacard_coef: 0.078114/17 [=======================>......] - ETA: 0s - loss: 0.1513 - accuracy: 0.8805 - jacard_coef: 0.074215/17 [=========================>....] - ETA: 0s - loss: 0.1510 - accuracy: 0.8830 - jacard_coef: 0.074016/17 [===========================>..] - ETA: 0s - loss: 0.1509 - accuracy: 0.8834 - jacard_coef: 0.075217/17 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.8838 - jacard_coef: 0.074617/17 [==============================] - 4s 215ms/step - loss: 0.1509 - accuracy: 0.8838 - jacard_coef: 0.0746 - val_loss: 0.1860 - val_accuracy: 0.9252 - val_jacard_coef: 0.0481 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1469 - accuracy: 0.8935 - jacard_coef: 0.0896 2/17 [==>...........................] - ETA: 3s - loss: 0.1466 - accuracy: 0.9071 - jacard_coef: 0.0808 3/17 [====>.........................] - ETA: 2s - loss: 0.1467 - accuracy: 0.9066 - jacard_coef: 0.0815 4/17 [======>.......................] - ETA: 2s - loss: 0.1458 - accuracy: 0.9194 - jacard_coef: 0.0708 5/17 [=======>......................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9181 - jacard_coef: 0.0722 6/17 [=========>....................] - ETA: 2s - loss: 0.1456 - accuracy: 0.9214 - jacard_coef: 0.0696 7/17 [===========>..................] - ETA: 2s - loss: 0.1452 - accuracy: 0.9233 - jacard_coef: 0.0676 8/17 [=============>................] - ETA: 1s - loss: 0.1445 - accuracy: 0.9278 - jacard_coef: 0.0639 9/17 [==============>...............] - ETA: 1s - loss: 0.1447 - accuracy: 0.9252 - jacard_coef: 0.066310/17 [================>.............] - ETA: 1s - loss: 0.1447 - accuracy: 0.9227 - jacard_coef: 0.068211/17 [==================>...........] - ETA: 1s - loss: 0.1448 - accuracy: 0.9231 - jacard_coef: 0.067912/17 [====================>.........] - ETA: 1s - loss: 0.1451 - accuracy: 0.9197 - jacard_coef: 0.070813/17 [=====================>........] - ETA: 0s - loss: 0.1450 - accuracy: 0.9189 - jacard_coef: 0.071614/17 [=======================>......] - ETA: 0s - loss: 0.1449 - accuracy: 0.9183 - jacard_coef: 0.072215/17 [=========================>....] - ETA: 0s - loss: 0.1452 - accuracy: 0.9149 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1453 - accuracy: 0.9146 - jacard_coef: 0.075417/17 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9152 - jacard_coef: 0.071417/17 [==============================] - 4s 215ms/step - loss: 0.1453 - accuracy: 0.9152 - jacard_coef: 0.0714 - val_loss: 0.1359 - val_accuracy: 0.9266 - val_jacard_coef: 0.0579 - lr: 5.0000e-04
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1448 - accuracy: 0.8934 - jacard_coef: 0.0930 2/17 [==>...........................] - ETA: 3s - loss: 0.1436 - accuracy: 0.9124 - jacard_coef: 0.0777 3/17 [====>.........................] - ETA: 2s - loss: 0.1442 - accuracy: 0.9081 - jacard_coef: 0.0818 4/17 [======>.......................] - ETA: 2s - loss: 0.1432 - accuracy: 0.9187 - jacard_coef: 0.0729 5/17 [=======>......................] - ETA: 2s - loss: 0.1430 - accuracy: 0.9218 - jacard_coef: 0.0705 6/17 [=========>....................] - ETA: 2s - loss: 0.1428 - accuracy: 0.9253 - jacard_coef: 0.0676 7/17 [===========>..................] - ETA: 2s - loss: 0.1426 - accuracy: 0.9265 - jacard_coef: 0.0667 8/17 [=============>................] - ETA: 1s - loss: 0.1430 - accuracy: 0.9205 - jacard_coef: 0.0716 9/17 [==============>...............] - ETA: 1s - loss: 0.1431 - accuracy: 0.9185 - jacard_coef: 0.073210/17 [================>.............] - ETA: 1s - loss: 0.1435 - accuracy: 0.9139 - jacard_coef: 0.077011/17 [==================>...........] - ETA: 1s - loss: 0.1433 - accuracy: 0.9165 - jacard_coef: 0.074912/17 [====================>.........] - ETA: 1s - loss: 0.1431 - accuracy: 0.9169 - jacard_coef: 0.074713/17 [=====================>........] - ETA: 0s - loss: 0.1430 - accuracy: 0.9185 - jacard_coef: 0.073314/17 [=======================>......] - ETA: 0s - loss: 0.1430 - accuracy: 0.9184 - jacard_coef: 0.073415/17 [=========================>....] - ETA: 0s - loss: 0.1431 - accuracy: 0.9166 - jacard_coef: 0.074716/17 [===========================>..] - ETA: 0s - loss: 0.1473 - accuracy: 0.8961 - jacard_coef: 0.074417/17 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.8944 - jacard_coef: 0.078517/17 [==============================] - 4s 215ms/step - loss: 0.1477 - accuracy: 0.8944 - jacard_coef: 0.0785 - val_loss: 0.1430 - val_accuracy: 0.9209 - val_jacard_coef: 0.0593 - lr: 5.0000e-04
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1580 - accuracy: 0.9035 - jacard_coef: 0.0669 2/17 [==>...........................] - ETA: 3s - loss: 0.1670 - accuracy: 0.8821 - jacard_coef: 0.0881 3/17 [====>.........................] - ETA: 2s - loss: 0.1693 - accuracy: 0.8802 - jacard_coef: 0.0912 4/17 [======>.......................] - ETA: 2s - loss: 0.1675 - accuracy: 0.8862 - jacard_coef: 0.0879 5/17 [=======>......................] - ETA: 2s - loss: 0.1659 - accuracy: 0.8940 - jacard_coef: 0.0823 6/17 [=========>....................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8955 - jacard_coef: 0.0815 7/17 [===========>..................] - ETA: 2s - loss: 0.1636 - accuracy: 0.8999 - jacard_coef: 0.0783 8/17 [=============>................] - ETA: 1s - loss: 0.1625 - accuracy: 0.9014 - jacard_coef: 0.0773 9/17 [==============>...............] - ETA: 1s - loss: 0.1618 - accuracy: 0.8996 - jacard_coef: 0.078810/17 [================>.............] - ETA: 1s - loss: 0.1611 - accuracy: 0.9021 - jacard_coef: 0.076911/17 [==================>...........] - ETA: 1s - loss: 0.1606 - accuracy: 0.9025 - jacard_coef: 0.076712/17 [====================>.........] - ETA: 1s - loss: 0.1594 - accuracy: 0.9074 - jacard_coef: 0.073013/17 [=====================>........] - ETA: 0s - loss: 0.1589 - accuracy: 0.9070 - jacard_coef: 0.073914/17 [=======================>......] - ETA: 0s - loss: 0.1583 - accuracy: 0.9080 - jacard_coef: 0.073615/17 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.9077 - jacard_coef: 0.074316/17 [===========================>..] - ETA: 0s - loss: 0.1575 - accuracy: 0.9081 - jacard_coef: 0.074417/17 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9082 - jacard_coef: 0.073917/17 [==============================] - 4s 215ms/step - loss: 0.1575 - accuracy: 0.9082 - jacard_coef: 0.0739 - val_loss: 0.1192 - val_accuracy: 0.9303 - val_jacard_coef: 0.0622 - lr: 5.0000e-04
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1471 - accuracy: 0.9215 - jacard_coef: 0.0695 2/17 [==>...........................] - ETA: 3s - loss: 0.1492 - accuracy: 0.9094 - jacard_coef: 0.0786 3/17 [====>.........................] - ETA: 2s - loss: 0.1487 - accuracy: 0.9099 - jacard_coef: 0.0783 4/17 [======>.......................] - ETA: 2s - loss: 0.1489 - accuracy: 0.9082 - jacard_coef: 0.0784 5/17 [=======>......................] - ETA: 2s - loss: 0.1500 - accuracy: 0.8971 - jacard_coef: 0.0851 6/17 [=========>....................] - ETA: 2s - loss: 0.1491 - accuracy: 0.8976 - jacard_coef: 0.0839 7/17 [===========>..................] - ETA: 2s - loss: 0.1489 - accuracy: 0.8964 - jacard_coef: 0.0840 8/17 [=============>................] - ETA: 1s - loss: 0.1485 - accuracy: 0.8973 - jacard_coef: 0.0835 9/17 [==============>...............] - ETA: 1s - loss: 0.1482 - accuracy: 0.8975 - jacard_coef: 0.083910/17 [================>.............] - ETA: 1s - loss: 0.1478 - accuracy: 0.8996 - jacard_coef: 0.082411/17 [==================>...........] - ETA: 1s - loss: 0.1471 - accuracy: 0.9046 - jacard_coef: 0.078312/17 [====================>.........] - ETA: 1s - loss: 0.1470 - accuracy: 0.9041 - jacard_coef: 0.078613/17 [=====================>........] - ETA: 0s - loss: 0.1466 - accuracy: 0.9069 - jacard_coef: 0.076314/17 [=======================>......] - ETA: 0s - loss: 0.1464 - accuracy: 0.9069 - jacard_coef: 0.076515/17 [=========================>....] - ETA: 0s - loss: 0.1461 - accuracy: 0.9070 - jacard_coef: 0.076416/17 [===========================>..] - ETA: 0s - loss: 0.1459 - accuracy: 0.9082 - jacard_coef: 0.075317/17 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9087 - jacard_coef: 0.071217/17 [==============================] - 4s 215ms/step - loss: 0.1459 - accuracy: 0.9087 - jacard_coef: 0.0712 - val_loss: 0.1171 - val_accuracy: 0.9304 - val_jacard_coef: 0.0618 - lr: 5.0000e-04
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1415 - accuracy: 0.9122 - jacard_coef: 0.0750 2/17 [==>...........................] - ETA: 3s - loss: 0.1401 - accuracy: 0.9295 - jacard_coef: 0.0594 3/17 [====>.........................] - ETA: 2s - loss: 0.1420 - accuracy: 0.9184 - jacard_coef: 0.0687 4/17 [======>.......................] - ETA: 2s - loss: 0.1412 - accuracy: 0.9183 - jacard_coef: 0.0689 5/17 [=======>......................] - ETA: 2s - loss: 0.1414 - accuracy: 0.9150 - jacard_coef: 0.0717 6/17 [=========>....................] - ETA: 2s - loss: 0.1411 - accuracy: 0.9143 - jacard_coef: 0.0728 7/17 [===========>..................] - ETA: 2s - loss: 0.1409 - accuracy: 0.9144 - jacard_coef: 0.0726 8/17 [=============>................] - ETA: 1s - loss: 0.1411 - accuracy: 0.9123 - jacard_coef: 0.0731 9/17 [==============>...............] - ETA: 1s - loss: 0.1410 - accuracy: 0.9117 - jacard_coef: 0.072310/17 [================>.............] - ETA: 1s - loss: 0.1408 - accuracy: 0.9110 - jacard_coef: 0.072711/17 [==================>...........] - ETA: 1s - loss: 0.1408 - accuracy: 0.9097 - jacard_coef: 0.073412/17 [====================>.........] - ETA: 1s - loss: 0.1407 - accuracy: 0.9112 - jacard_coef: 0.072413/17 [=====================>........] - ETA: 0s - loss: 0.1409 - accuracy: 0.9092 - jacard_coef: 0.074414/17 [=======================>......] - ETA: 0s - loss: 0.1409 - accuracy: 0.9096 - jacard_coef: 0.074615/17 [=========================>....] - ETA: 0s - loss: 0.1411 - accuracy: 0.9085 - jacard_coef: 0.075816/17 [===========================>..] - ETA: 0s - loss: 0.1408 - accuracy: 0.9107 - jacard_coef: 0.074317/17 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9097 - jacard_coef: 0.079317/17 [==============================] - 4s 215ms/step - loss: 0.1409 - accuracy: 0.9097 - jacard_coef: 0.0793 - val_loss: 0.1193 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0705 (epoch 3)
  Final Val Loss: 0.1193
  Training Time: 0:02:04.089290
  Stability (std): 0.3197

Results saved to: hyperparameter_optimization_20250926_123742/exp_28_Attention_ResUNet_lr5e-4_bs8/Attention_ResUNet_lr0.0005_bs8_results.json

Experiment 28 completed in 161s
Progress: 28/36 completed
Estimated remaining time: 21 minutes

ðŸ”¬ EXPERIMENT 29/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-4
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758865869.377522 3333943 service.cc:145] XLA service 0x154025e0be50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758865869.377576 3333943 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758865869.834734 3333943 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 9:11 - loss: 0.3435 - accuracy: 0.5120 - jacard_coef: 0.08882/9 [=====>........................] - ETA: 58s - loss: 0.3186 - accuracy: 0.4760 - jacard_coef: 0.0773 3/9 [=========>....................] - ETA: 26s - loss: 0.2922 - accuracy: 0.5121 - jacard_coef: 0.07654/9 [============>.................] - ETA: 15s - loss: 0.2733 - accuracy: 0.5416 - jacard_coef: 0.07585/9 [===============>..............] - ETA: 9s - loss: 0.2582 - accuracy: 0.5551 - jacard_coef: 0.0792 6/9 [===================>..........] - ETA: 5s - loss: 0.2556 - accuracy: 0.5611 - jacard_coef: 0.07857/9 [======================>.......] - ETA: 3s - loss: 0.2476 - accuracy: 0.5880 - jacard_coef: 0.07398/9 [=========================>....] - ETA: 1s - loss: 0.2407 - accuracy: 0.6070 - jacard_coef: 0.07569/9 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.6076 - jacard_coef: 0.08209/9 [==============================] - 89s 3s/step - loss: 0.2404 - accuracy: 0.6076 - jacard_coef: 0.0820 - val_loss: 14.3699 - val_accuracy: 0.0981 - val_jacard_coef: 0.0693 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1844 - accuracy: 0.5873 - jacard_coef: 0.09052/9 [=====>........................] - ETA: 2s - loss: 0.1815 - accuracy: 0.5636 - jacard_coef: 0.08493/9 [=========>....................] - ETA: 2s - loss: 0.1839 - accuracy: 0.5508 - jacard_coef: 0.07154/9 [============>.................] - ETA: 2s - loss: 0.1829 - accuracy: 0.5886 - jacard_coef: 0.06965/9 [===============>..............] - ETA: 1s - loss: 0.1824 - accuracy: 0.6196 - jacard_coef: 0.07076/9 [===================>..........] - ETA: 1s - loss: 0.1827 - accuracy: 0.6423 - jacard_coef: 0.07157/9 [======================>.......] - ETA: 0s - loss: 0.1831 - accuracy: 0.6603 - jacard_coef: 0.07428/9 [=========================>....] - ETA: 0s - loss: 0.1832 - accuracy: 0.6763 - jacard_coef: 0.07669/9 [==============================] - 4s 391ms/step - loss: 0.1831 - accuracy: 0.6773 - jacard_coef: 0.0688 - val_loss: 14.7803 - val_accuracy: 0.0829 - val_jacard_coef: 0.0695 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1790 - accuracy: 0.8264 - jacard_coef: 0.05512/9 [=====>........................] - ETA: 2s - loss: 0.1785 - accuracy: 0.8286 - jacard_coef: 0.06933/9 [=========>....................] - ETA: 2s - loss: 0.1777 - accuracy: 0.8212 - jacard_coef: 0.07904/9 [============>.................] - ETA: 2s - loss: 0.1766 - accuracy: 0.8069 - jacard_coef: 0.07945/9 [===============>..............] - ETA: 1s - loss: 0.1759 - accuracy: 0.7928 - jacard_coef: 0.08076/9 [===================>..........] - ETA: 1s - loss: 0.1748 - accuracy: 0.7775 - jacard_coef: 0.07937/9 [======================>.......] - ETA: 0s - loss: 0.1742 - accuracy: 0.7688 - jacard_coef: 0.07598/9 [=========================>....] - ETA: 0s - loss: 0.1736 - accuracy: 0.7611 - jacard_coef: 0.07559/9 [==============================] - 4s 390ms/step - loss: 0.1736 - accuracy: 0.7611 - jacard_coef: 0.0809 - val_loss: 14.7684 - val_accuracy: 0.0831 - val_jacard_coef: 0.0695 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1686 - accuracy: 0.6531 - jacard_coef: 0.09552/9 [=====>........................] - ETA: 2s - loss: 0.1679 - accuracy: 0.6555 - jacard_coef: 0.07373/9 [=========>....................] - ETA: 2s - loss: 0.1677 - accuracy: 0.6797 - jacard_coef: 0.06644/9 [============>.................] - ETA: 2s - loss: 0.1675 - accuracy: 0.6797 - jacard_coef: 0.07765/9 [===============>..............] - ETA: 1s - loss: 0.1673 - accuracy: 0.6935 - jacard_coef: 0.07536/9 [===================>..........] - ETA: 1s - loss: 0.1671 - accuracy: 0.7073 - jacard_coef: 0.07417/9 [======================>.......] - ETA: 0s - loss: 0.1669 - accuracy: 0.7163 - jacard_coef: 0.07578/9 [=========================>....] - ETA: 0s - loss: 0.1667 - accuracy: 0.7300 - jacard_coef: 0.07629/9 [==============================] - 3s 380ms/step - loss: 0.1667 - accuracy: 0.7309 - jacard_coef: 0.0722 - val_loss: 4.6661 - val_accuracy: 0.2743 - val_jacard_coef: 0.0656 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1672 - accuracy: 0.7072 - jacard_coef: 0.10972/9 [=====>........................] - ETA: 2s - loss: 0.1661 - accuracy: 0.7146 - jacard_coef: 0.09013/9 [=========>....................] - ETA: 2s - loss: 0.1653 - accuracy: 0.7217 - jacard_coef: 0.08244/9 [============>.................] - ETA: 2s - loss: 0.1651 - accuracy: 0.7316 - jacard_coef: 0.08165/9 [===============>..............] - ETA: 1s - loss: 0.1647 - accuracy: 0.7330 - jacard_coef: 0.07806/9 [===================>..........] - ETA: 1s - loss: 0.1645 - accuracy: 0.7450 - jacard_coef: 0.07817/9 [======================>.......] - ETA: 0s - loss: 0.1643 - accuracy: 0.7496 - jacard_coef: 0.07928/9 [=========================>....] - ETA: 0s - loss: 0.1641 - accuracy: 0.7592 - jacard_coef: 0.07669/9 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.7606 - jacard_coef: 0.06849/9 [==============================] - 3s 384ms/step - loss: 0.1641 - accuracy: 0.7606 - jacard_coef: 0.0684 - val_loss: 0.5318 - val_accuracy: 0.4438 - val_jacard_coef: 0.0665 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1626 - accuracy: 0.8537 - jacard_coef: 0.07862/9 [=====>........................] - ETA: 2s - loss: 0.1620 - accuracy: 0.8574 - jacard_coef: 0.07463/9 [=========>....................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8503 - jacard_coef: 0.07554/9 [============>.................] - ETA: 2s - loss: 0.1618 - accuracy: 0.8516 - jacard_coef: 0.07995/9 [===============>..............] - ETA: 1s - loss: 0.1619 - accuracy: 0.8458 - jacard_coef: 0.08226/9 [===================>..........] - ETA: 1s - loss: 0.1615 - accuracy: 0.8464 - jacard_coef: 0.07707/9 [======================>.......] - ETA: 0s - loss: 0.1614 - accuracy: 0.8420 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1612 - accuracy: 0.8448 - jacard_coef: 0.07559/9 [==============================] - ETA: 0s - loss: 0.1612 - accuracy: 0.8438 - jacard_coef: 0.08339/9 [==============================] - 4s 387ms/step - loss: 0.1612 - accuracy: 0.8438 - jacard_coef: 0.0833 - val_loss: 0.1178 - val_accuracy: 0.8873 - val_jacard_coef: 0.0635 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1594 - accuracy: 0.8192 - jacard_coef: 0.08212/9 [=====>........................] - ETA: 2s - loss: 0.1603 - accuracy: 0.7964 - jacard_coef: 0.08323/9 [=========>....................] - ETA: 2s - loss: 0.1604 - accuracy: 0.7890 - jacard_coef: 0.08054/9 [============>.................] - ETA: 2s - loss: 0.1602 - accuracy: 0.7870 - jacard_coef: 0.07865/9 [===============>..............] - ETA: 1s - loss: 0.1602 - accuracy: 0.7826 - jacard_coef: 0.07666/9 [===================>..........] - ETA: 1s - loss: 0.1602 - accuracy: 0.7912 - jacard_coef: 0.07827/9 [======================>.......] - ETA: 0s - loss: 0.1602 - accuracy: 0.8033 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1601 - accuracy: 0.8159 - jacard_coef: 0.07679/9 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.8148 - jacard_coef: 0.06859/9 [==============================] - 4s 388ms/step - loss: 0.1607 - accuracy: 0.8148 - jacard_coef: 0.0685 - val_loss: 0.1127 - val_accuracy: 0.9153 - val_jacard_coef: 0.0627 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1603 - accuracy: 0.9460 - jacard_coef: 0.04212/9 [=====>........................] - ETA: 2s - loss: 0.1624 - accuracy: 0.9251 - jacard_coef: 0.05903/9 [=========>....................] - ETA: 2s - loss: 0.1632 - accuracy: 0.9129 - jacard_coef: 0.06574/9 [============>.................] - ETA: 2s - loss: 0.1648 - accuracy: 0.8958 - jacard_coef: 0.07465/9 [===============>..............] - ETA: 1s - loss: 0.1646 - accuracy: 0.8906 - jacard_coef: 0.07246/9 [===================>..........] - ETA: 1s - loss: 0.1648 - accuracy: 0.8843 - jacard_coef: 0.07207/9 [======================>.......] - ETA: 0s - loss: 0.1651 - accuracy: 0.8725 - jacard_coef: 0.07618/9 [=========================>....] - ETA: 0s - loss: 0.1651 - accuracy: 0.8687 - jacard_coef: 0.07619/9 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.8688 - jacard_coef: 0.06829/9 [==============================] - 4s 387ms/step - loss: 0.1651 - accuracy: 0.8688 - jacard_coef: 0.0682 - val_loss: 0.1874 - val_accuracy: 0.4148 - val_jacard_coef: 0.0649 - lr: 5.0000e-04
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1652 - accuracy: 0.8480 - jacard_coef: 0.08392/9 [=====>........................] - ETA: 2s - loss: 0.1642 - accuracy: 0.8571 - jacard_coef: 0.08253/9 [=========>....................] - ETA: 2s - loss: 0.1635 - accuracy: 0.8669 - jacard_coef: 0.07974/9 [============>.................] - ETA: 2s - loss: 0.1627 - accuracy: 0.8721 - jacard_coef: 0.07735/9 [===============>..............] - ETA: 1s - loss: 0.1626 - accuracy: 0.8731 - jacard_coef: 0.08066/9 [===================>..........] - ETA: 1s - loss: 0.1624 - accuracy: 0.8796 - jacard_coef: 0.07827/9 [======================>.......] - ETA: 0s - loss: 0.1619 - accuracy: 0.8833 - jacard_coef: 0.07768/9 [=========================>....] - ETA: 0s - loss: 0.1617 - accuracy: 0.8875 - jacard_coef: 0.07629/9 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.8879 - jacard_coef: 0.06849/9 [==============================] - 4s 388ms/step - loss: 0.1621 - accuracy: 0.8879 - jacard_coef: 0.0684 - val_loss: 0.1457 - val_accuracy: 0.9120 - val_jacard_coef: 0.0640 - lr: 5.0000e-04
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1588 - accuracy: 0.8972 - jacard_coef: 0.07792/9 [=====>........................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8885 - jacard_coef: 0.08783/9 [=========>....................] - ETA: 2s - loss: 0.1605 - accuracy: 0.8942 - jacard_coef: 0.08194/9 [============>.................] - ETA: 2s - loss: 0.1605 - accuracy: 0.9037 - jacard_coef: 0.07475/9 [===============>..............] - ETA: 1s - loss: 0.1602 - accuracy: 0.9051 - jacard_coef: 0.07406/9 [===================>..........] - ETA: 1s - loss: 0.1603 - accuracy: 0.9043 - jacard_coef: 0.07537/9 [======================>.......] - ETA: 0s - loss: 0.1602 - accuracy: 0.9045 - jacard_coef: 0.07598/9 [=========================>....] - ETA: 0s - loss: 0.1599 - accuracy: 0.9048 - jacard_coef: 0.07599/9 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9049 - jacard_coef: 0.07409/9 [==============================] - 4s 387ms/step - loss: 0.1599 - accuracy: 0.9049 - jacard_coef: 0.0740 - val_loss: 0.1423 - val_accuracy: 0.9232 - val_jacard_coef: 0.0641 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1598 - accuracy: 0.8712 - jacard_coef: 0.10332/9 [=====>........................] - ETA: 2s - loss: 0.1588 - accuracy: 0.8930 - jacard_coef: 0.08793/9 [=========>....................] - ETA: 2s - loss: 0.1580 - accuracy: 0.9065 - jacard_coef: 0.07734/9 [============>.................] - ETA: 2s - loss: 0.1579 - accuracy: 0.9053 - jacard_coef: 0.07915/9 [===============>..............] - ETA: 1s - loss: 0.1573 - accuracy: 0.9086 - jacard_coef: 0.07676/9 [===================>..........] - ETA: 1s - loss: 0.1567 - accuracy: 0.9141 - jacard_coef: 0.07197/9 [======================>.......] - ETA: 0s - loss: 0.1571 - accuracy: 0.9122 - jacard_coef: 0.07308/9 [=========================>....] - ETA: 0s - loss: 0.1571 - accuracy: 0.9110 - jacard_coef: 0.07469/9 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9100 - jacard_coef: 0.08509/9 [==============================] - 4s 387ms/step - loss: 0.1572 - accuracy: 0.9100 - jacard_coef: 0.0850 - val_loss: 0.1558 - val_accuracy: 0.9247 - val_jacard_coef: 0.0641 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1575 - accuracy: 0.9143 - jacard_coef: 0.07712/9 [=====>........................] - ETA: 2s - loss: 0.1575 - accuracy: 0.9010 - jacard_coef: 0.08823/9 [=========>....................] - ETA: 2s - loss: 0.1569 - accuracy: 0.9070 - jacard_coef: 0.08344/9 [============>.................] - ETA: 2s - loss: 0.1565 - accuracy: 0.9118 - jacard_coef: 0.07945/9 [===============>..............] - ETA: 1s - loss: 0.1566 - accuracy: 0.9081 - jacard_coef: 0.08266/9 [===================>..........] - ETA: 1s - loss: 0.1563 - accuracy: 0.9105 - jacard_coef: 0.08057/9 [======================>.......] - ETA: 0s - loss: 0.1560 - accuracy: 0.9134 - jacard_coef: 0.07818/9 [=========================>....] - ETA: 0s - loss: 0.1557 - accuracy: 0.9157 - jacard_coef: 0.07619/9 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9160 - jacard_coef: 0.07019/9 [==============================] - 4s 389ms/step - loss: 0.1557 - accuracy: 0.9160 - jacard_coef: 0.0701 - val_loss: 0.1572 - val_accuracy: 0.9220 - val_jacard_coef: 0.0643 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1538 - accuracy: 0.9220 - jacard_coef: 0.06922/9 [=====>........................] - ETA: 2s - loss: 0.1547 - accuracy: 0.9058 - jacard_coef: 0.08233/9 [=========>....................] - ETA: 2s - loss: 0.1545 - accuracy: 0.9089 - jacard_coef: 0.07984/9 [============>.................] - ETA: 2s - loss: 0.1545 - accuracy: 0.9106 - jacard_coef: 0.07885/9 [===============>..............] - ETA: 1s - loss: 0.1543 - accuracy: 0.9134 - jacard_coef: 0.07676/9 [===================>..........] - ETA: 1s - loss: 0.1542 - accuracy: 0.9131 - jacard_coef: 0.07737/9 [======================>.......] - ETA: 0s - loss: 0.1541 - accuracy: 0.9132 - jacard_coef: 0.07738/9 [=========================>....] - ETA: 0s - loss: 0.1540 - accuracy: 0.9149 - jacard_coef: 0.07619/9 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9153 - jacard_coef: 0.07179/9 [==============================] - 4s 388ms/step - loss: 0.1540 - accuracy: 0.9153 - jacard_coef: 0.0717 - val_loss: 0.1552 - val_accuracy: 0.9257 - val_jacard_coef: 0.0645 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0695 (epoch 3)
  Final Val Loss: 0.1552
  Training Time: 0:02:12.561912
  Stability (std): 1.3480

Results saved to: hyperparameter_optimization_20250926_123742/exp_29_Attention_ResUNet_lr5e-4_bs16/Attention_ResUNet_lr0.0005_bs16_results.json

Experiment 29 completed in 169s
Progress: 29/36 completed
Estimated remaining time: 19 minutes

ðŸ”¬ EXPERIMENT 30/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-4
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.0005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758866043.333952 3341475 service.cc:145] XLA service 0x1494b9cda980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758866043.333990 3341475 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758866043.722597 3341475 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 5:00 - loss: 0.3391 - accuracy: 0.5010 - jacard_coef: 0.07452/5 [===========>..................] - ETA: 46s - loss: 0.3176 - accuracy: 0.4182 - jacard_coef: 0.0763 3/5 [=================>............] - ETA: 16s - loss: 0.2915 - accuracy: 0.3470 - jacard_coef: 0.07524/5 [=======================>......] - ETA: 5s - loss: 0.2802 - accuracy: 0.3302 - jacard_coef: 0.0766 5/5 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.3288 - jacard_coef: 0.06195/5 [==============================] - 102s 7s/step - loss: 0.2799 - accuracy: 0.3288 - jacard_coef: 0.0619 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2171 - accuracy: 0.2143 - jacard_coef: 0.06402/5 [===========>..................] - ETA: 2s - loss: 0.2090 - accuracy: 0.3244 - jacard_coef: 0.07233/5 [=================>............] - ETA: 1s - loss: 0.2037 - accuracy: 0.3059 - jacard_coef: 0.07384/5 [=======================>......] - ETA: 0s - loss: 0.1997 - accuracy: 0.3849 - jacard_coef: 0.07575/5 [==============================] - 3s 660ms/step - loss: 0.1997 - accuracy: 0.3851 - jacard_coef: 0.0871 - val_loss: 1.1218 - val_accuracy: 0.9304 - val_jacard_coef: 1.4615e-05 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2124 - accuracy: 0.4100 - jacard_coef: 0.07772/5 [===========>..................] - ETA: 2s - loss: 0.1939 - accuracy: 0.5096 - jacard_coef: 0.06853/5 [=================>............] - ETA: 1s - loss: 0.1876 - accuracy: 0.5695 - jacard_coef: 0.07424/5 [=======================>......] - ETA: 0s - loss: 0.1844 - accuracy: 0.5966 - jacard_coef: 0.07605/5 [==============================] - 3s 662ms/step - loss: 0.1843 - accuracy: 0.5966 - jacard_coef: 0.0807 - val_loss: 1.1555 - val_accuracy: 0.9270 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1729 - accuracy: 0.6459 - jacard_coef: 0.08492/5 [===========>..................] - ETA: 2s - loss: 0.1726 - accuracy: 0.6837 - jacard_coef: 0.08283/5 [=================>............] - ETA: 1s - loss: 0.1721 - accuracy: 0.7185 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 0s - loss: 0.1718 - accuracy: 0.7398 - jacard_coef: 0.07595/5 [==============================] - 3s 657ms/step - loss: 0.1718 - accuracy: 0.7401 - jacard_coef: 0.0856 - val_loss: 4.4303 - val_accuracy: 0.2324 - val_jacard_coef: 0.0758 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1695 - accuracy: 0.7820 - jacard_coef: 0.07922/5 [===========>..................] - ETA: 2s - loss: 0.1696 - accuracy: 0.7522 - jacard_coef: 0.07373/5 [=================>............] - ETA: 1s - loss: 0.1697 - accuracy: 0.7288 - jacard_coef: 0.07604/5 [=======================>......] - ETA: 0s - loss: 0.1695 - accuracy: 0.7176 - jacard_coef: 0.07585/5 [==============================] - 3s 646ms/step - loss: 0.1696 - accuracy: 0.7166 - jacard_coef: 0.0892 - val_loss: 11.6443 - val_accuracy: 0.0989 - val_jacard_coef: 0.0707 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1676 - accuracy: 0.7822 - jacard_coef: 0.08362/5 [===========>..................] - ETA: 2s - loss: 0.1674 - accuracy: 0.7963 - jacard_coef: 0.07513/5 [=================>............] - ETA: 1s - loss: 0.1676 - accuracy: 0.8011 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1674 - accuracy: 0.8122 - jacard_coef: 0.07645/5 [==============================] - 3s 646ms/step - loss: 0.1674 - accuracy: 0.8124 - jacard_coef: 0.0629 - val_loss: 11.5702 - val_accuracy: 0.0731 - val_jacard_coef: 0.0697 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1660 - accuracy: 0.7690 - jacard_coef: 0.08572/5 [===========>..................] - ETA: 2s - loss: 0.1657 - accuracy: 0.7664 - jacard_coef: 0.07703/5 [=================>............] - ETA: 1s - loss: 0.1657 - accuracy: 0.7650 - jacard_coef: 0.07484/5 [=======================>......] - ETA: 0s - loss: 0.1656 - accuracy: 0.7679 - jacard_coef: 0.07575/5 [==============================] - 3s 650ms/step - loss: 0.1656 - accuracy: 0.7682 - jacard_coef: 0.0875 - val_loss: 2.3002 - val_accuracy: 0.1149 - val_jacard_coef: 0.0711 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1646 - accuracy: 0.8119 - jacard_coef: 0.06512/5 [===========>..................] - ETA: 2s - loss: 0.1646 - accuracy: 0.8054 - jacard_coef: 0.07333/5 [=================>............] - ETA: 1s - loss: 0.1644 - accuracy: 0.8103 - jacard_coef: 0.07544/5 [=======================>......] - ETA: 0s - loss: 0.1642 - accuracy: 0.8105 - jacard_coef: 0.07595/5 [==============================] - 3s 649ms/step - loss: 0.1643 - accuracy: 0.8107 - jacard_coef: 0.0853 - val_loss: 1.6055 - val_accuracy: 0.0769 - val_jacard_coef: 0.0686 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1630 - accuracy: 0.8359 - jacard_coef: 0.06572/5 [===========>..................] - ETA: 2s - loss: 0.1631 - accuracy: 0.8309 - jacard_coef: 0.07033/5 [=================>............] - ETA: 1s - loss: 0.1629 - accuracy: 0.8325 - jacard_coef: 0.07084/5 [=======================>......] - ETA: 0s - loss: 0.1630 - accuracy: 0.8274 - jacard_coef: 0.07675/5 [==============================] - 3s 649ms/step - loss: 0.1630 - accuracy: 0.8278 - jacard_coef: 0.0627 - val_loss: 0.3923 - val_accuracy: 0.1404 - val_jacard_coef: 0.0680 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1625 - accuracy: 0.8207 - jacard_coef: 0.08752/5 [===========>..................] - ETA: 2s - loss: 0.1623 - accuracy: 0.8321 - jacard_coef: 0.07893/5 [=================>............] - ETA: 1s - loss: 0.1620 - accuracy: 0.8357 - jacard_coef: 0.07514/5 [=======================>......] - ETA: 0s - loss: 0.1620 - accuracy: 0.8360 - jacard_coef: 0.07605/5 [==============================] - 3s 649ms/step - loss: 0.1620 - accuracy: 0.8356 - jacard_coef: 0.0837 - val_loss: 0.2488 - val_accuracy: 0.1636 - val_jacard_coef: 0.0674 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1616 - accuracy: 0.8367 - jacard_coef: 0.06932/5 [===========>..................] - ETA: 2s - loss: 0.1615 - accuracy: 0.8376 - jacard_coef: 0.07743/5 [=================>............] - ETA: 1s - loss: 0.1615 - accuracy: 0.8370 - jacard_coef: 0.07774/5 [=======================>......] - ETA: 0s - loss: 0.1614 - accuracy: 0.8368 - jacard_coef: 0.07675/5 [==============================] - 3s 649ms/step - loss: 0.1614 - accuracy: 0.8369 - jacard_coef: 0.0675 - val_loss: 0.2081 - val_accuracy: 0.2475 - val_jacard_coef: 0.0672 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1612 - accuracy: 0.8247 - jacard_coef: 0.07982/5 [===========>..................] - ETA: 2s - loss: 0.1610 - accuracy: 0.8306 - jacard_coef: 0.07993/5 [=================>............] - ETA: 1s - loss: 0.1609 - accuracy: 0.8326 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1608 - accuracy: 0.8343 - jacard_coef: 0.07675/5 [==============================] - 3s 650ms/step - loss: 0.1608 - accuracy: 0.8349 - jacard_coef: 0.0687 - val_loss: 0.2103 - val_accuracy: 0.2197 - val_jacard_coef: 0.0666 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1606 - accuracy: 0.8329 - jacard_coef: 0.08762/5 [===========>..................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8408 - jacard_coef: 0.08003/5 [=================>............] - ETA: 1s - loss: 0.1604 - accuracy: 0.8391 - jacard_coef: 0.08074/5 [=======================>......] - ETA: 0s - loss: 0.1603 - accuracy: 0.8429 - jacard_coef: 0.07615/5 [==============================] - 3s 650ms/step - loss: 0.1603 - accuracy: 0.8430 - jacard_coef: 0.0805 - val_loss: 0.1649 - val_accuracy: 0.5143 - val_jacard_coef: 0.0660 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1598 - accuracy: 0.8452 - jacard_coef: 0.07842/5 [===========>..................] - ETA: 2s - loss: 0.1598 - accuracy: 0.8401 - jacard_coef: 0.07783/5 [=================>............] - ETA: 1s - loss: 0.1598 - accuracy: 0.8351 - jacard_coef: 0.07964/5 [=======================>......] - ETA: 0s - loss: 0.1596 - accuracy: 0.8400 - jacard_coef: 0.07595/5 [==============================] - 3s 648ms/step - loss: 0.1597 - accuracy: 0.8394 - jacard_coef: 0.0908 - val_loss: 0.1525 - val_accuracy: 0.8886 - val_jacard_coef: 0.0656 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0758 (epoch 4)
  Final Val Loss: 0.1525
  Training Time: 0:02:26.874994
  Stability (std): 4.4325

Results saved to: hyperparameter_optimization_20250926_123742/exp_30_Attention_ResUNet_lr5e-4_bs32/Attention_ResUNet_lr0.0005_bs32_results.json

Experiment 30 completed in 182s
Progress: 30/36 completed
Estimated remaining time: 18 minutes

ðŸ”¬ EXPERIMENT 31/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758866216.034712 3349098 service.cc:145] XLA service 0x151c41873610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758866216.034771 3349098 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758866216.505373 3349098 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 16:40 - loss: 0.3385 - accuracy: 0.5037 - jacard_coef: 0.0911 2/17 [==>...........................] - ETA: 1:10 - loss: 0.3046 - accuracy: 0.4107 - jacard_coef: 0.0732  3/17 [====>.........................] - ETA: 34s - loss: 0.2789 - accuracy: 0.3515 - jacard_coef: 0.0634  4/17 [======>.......................] - ETA: 22s - loss: 0.2602 - accuracy: 0.3263 - jacard_coef: 0.0641 5/17 [=======>......................] - ETA: 15s - loss: 0.2543 - accuracy: 0.3165 - jacard_coef: 0.0613 6/17 [=========>....................] - ETA: 12s - loss: 0.2443 - accuracy: 0.2977 - jacard_coef: 0.0669 7/17 [===========>..................] - ETA: 9s - loss: 0.2379 - accuracy: 0.2900 - jacard_coef: 0.0678  8/17 [=============>................] - ETA: 7s - loss: 0.2321 - accuracy: 0.2874 - jacard_coef: 0.0696 9/17 [==============>...............] - ETA: 6s - loss: 0.2275 - accuracy: 0.2782 - jacard_coef: 0.068610/17 [================>.............] - ETA: 4s - loss: 0.2235 - accuracy: 0.2745 - jacard_coef: 0.071311/17 [==================>...........] - ETA: 3s - loss: 0.2204 - accuracy: 0.2666 - jacard_coef: 0.070512/17 [====================>.........] - ETA: 3s - loss: 0.2172 - accuracy: 0.2643 - jacard_coef: 0.073113/17 [=====================>........] - ETA: 2s - loss: 0.2143 - accuracy: 0.2755 - jacard_coef: 0.075514/17 [=======================>......] - ETA: 1s - loss: 0.2117 - accuracy: 0.2934 - jacard_coef: 0.074915/17 [=========================>....] - ETA: 1s - loss: 0.2096 - accuracy: 0.2959 - jacard_coef: 0.073216/17 [===========================>..] - ETA: 0s - loss: 0.2076 - accuracy: 0.3133 - jacard_coef: 0.076217/17 [==============================] - ETA: 0s - loss: 0.2086 - accuracy: 0.3126 - jacard_coef: 0.078117/17 [==============================] - 78s 991ms/step - loss: 0.2086 - accuracy: 0.3126 - jacard_coef: 0.0781 - val_loss: 1.1623 - val_accuracy: 0.9274 - val_jacard_coef: 0.0025 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.2157 - accuracy: 0.1870 - jacard_coef: 0.1137 2/17 [==>...........................] - ETA: 3s - loss: 0.2081 - accuracy: 0.1661 - jacard_coef: 0.0812 3/17 [====>.........................] - ETA: 2s - loss: 0.2069 - accuracy: 0.1820 - jacard_coef: 0.0852 4/17 [======>.......................] - ETA: 2s - loss: 0.2003 - accuracy: 0.2555 - jacard_coef: 0.0809 5/17 [=======>......................] - ETA: 2s - loss: 0.1968 - accuracy: 0.3064 - jacard_coef: 0.0717 6/17 [=========>....................] - ETA: 2s - loss: 0.1944 - accuracy: 0.3364 - jacard_coef: 0.0777 7/17 [===========>..................] - ETA: 2s - loss: 0.1927 - accuracy: 0.3479 - jacard_coef: 0.0727 8/17 [=============>................] - ETA: 1s - loss: 0.1907 - accuracy: 0.3568 - jacard_coef: 0.0707 9/17 [==============>...............] - ETA: 1s - loss: 0.1892 - accuracy: 0.3668 - jacard_coef: 0.073210/17 [================>.............] - ETA: 1s - loss: 0.1884 - accuracy: 0.3757 - jacard_coef: 0.073511/17 [==================>...........] - ETA: 1s - loss: 0.1874 - accuracy: 0.3824 - jacard_coef: 0.074712/17 [====================>.........] - ETA: 1s - loss: 0.1865 - accuracy: 0.3870 - jacard_coef: 0.076213/17 [=====================>........] - ETA: 0s - loss: 0.1854 - accuracy: 0.3905 - jacard_coef: 0.078914/17 [=======================>......] - ETA: 0s - loss: 0.1848 - accuracy: 0.3951 - jacard_coef: 0.078515/17 [=========================>....] - ETA: 0s - loss: 0.1841 - accuracy: 0.4014 - jacard_coef: 0.078116/17 [===========================>..] - ETA: 0s - loss: 0.1835 - accuracy: 0.4081 - jacard_coef: 0.076417/17 [==============================] - 4s 215ms/step - loss: 0.1835 - accuracy: 0.4079 - jacard_coef: 0.0751 - val_loss: 1.1675 - val_accuracy: 0.9270 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1714 - accuracy: 0.4866 - jacard_coef: 0.0785 2/17 [==>...........................] - ETA: 3s - loss: 0.1711 - accuracy: 0.5068 - jacard_coef: 0.0886 3/17 [====>.........................] - ETA: 2s - loss: 0.1718 - accuracy: 0.5069 - jacard_coef: 0.0818 4/17 [======>.......................] - ETA: 2s - loss: 0.1713 - accuracy: 0.5161 - jacard_coef: 0.0883 5/17 [=======>......................] - ETA: 2s - loss: 0.1712 - accuracy: 0.5220 - jacard_coef: 0.0830 6/17 [=========>....................] - ETA: 2s - loss: 0.1710 - accuracy: 0.5349 - jacard_coef: 0.0765 7/17 [===========>..................] - ETA: 2s - loss: 0.1713 - accuracy: 0.5531 - jacard_coef: 0.0724 8/17 [=============>................] - ETA: 1s - loss: 0.1709 - accuracy: 0.5605 - jacard_coef: 0.0741 9/17 [==============>...............] - ETA: 1s - loss: 0.1705 - accuracy: 0.5757 - jacard_coef: 0.070810/17 [================>.............] - ETA: 1s - loss: 0.1702 - accuracy: 0.5840 - jacard_coef: 0.070511/17 [==================>...........] - ETA: 1s - loss: 0.1701 - accuracy: 0.5861 - jacard_coef: 0.073212/17 [====================>.........] - ETA: 1s - loss: 0.1702 - accuracy: 0.5885 - jacard_coef: 0.073513/17 [=====================>........] - ETA: 0s - loss: 0.1700 - accuracy: 0.5898 - jacard_coef: 0.071914/17 [=======================>......] - ETA: 0s - loss: 0.1701 - accuracy: 0.5862 - jacard_coef: 0.073815/17 [=========================>....] - ETA: 0s - loss: 0.1700 - accuracy: 0.5842 - jacard_coef: 0.075816/17 [===========================>..] - ETA: 0s - loss: 0.1699 - accuracy: 0.5998 - jacard_coef: 0.076117/17 [==============================] - 4s 214ms/step - loss: 0.1701 - accuracy: 0.5985 - jacard_coef: 0.0752 - val_loss: 0.2481 - val_accuracy: 0.9095 - val_jacard_coef: 0.0469 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1679 - accuracy: 0.8716 - jacard_coef: 0.0677 2/17 [==>...........................] - ETA: 3s - loss: 0.1677 - accuracy: 0.8430 - jacard_coef: 0.0769 3/17 [====>.........................] - ETA: 2s - loss: 0.1704 - accuracy: 0.6217 - jacard_coef: 0.0775 4/17 [======>.......................] - ETA: 2s - loss: 0.1702 - accuracy: 0.6280 - jacard_coef: 0.0765 5/17 [=======>......................] - ETA: 2s - loss: 0.1698 - accuracy: 0.6546 - jacard_coef: 0.0680 6/17 [=========>....................] - ETA: 2s - loss: 0.1697 - accuracy: 0.6743 - jacard_coef: 0.0662 7/17 [===========>..................] - ETA: 2s - loss: 0.1697 - accuracy: 0.6903 - jacard_coef: 0.0665 8/17 [=============>................] - ETA: 1s - loss: 0.1697 - accuracy: 0.7020 - jacard_coef: 0.0685 9/17 [==============>...............] - ETA: 1s - loss: 0.1700 - accuracy: 0.7105 - jacard_coef: 0.072210/17 [================>.............] - ETA: 1s - loss: 0.1697 - accuracy: 0.7179 - jacard_coef: 0.076211/17 [==================>...........] - ETA: 1s - loss: 0.1696 - accuracy: 0.7272 - jacard_coef: 0.076012/17 [====================>.........] - ETA: 1s - loss: 0.1697 - accuracy: 0.7358 - jacard_coef: 0.077513/17 [=====================>........] - ETA: 0s - loss: 0.1696 - accuracy: 0.7447 - jacard_coef: 0.077214/17 [=======================>......] - ETA: 0s - loss: 0.1692 - accuracy: 0.7531 - jacard_coef: 0.076715/17 [=========================>....] - ETA: 0s - loss: 0.1689 - accuracy: 0.7603 - jacard_coef: 0.077316/17 [===========================>..] - ETA: 0s - loss: 0.1686 - accuracy: 0.7693 - jacard_coef: 0.075617/17 [==============================] - 4s 212ms/step - loss: 0.1687 - accuracy: 0.7656 - jacard_coef: 0.0774 - val_loss: 0.1648 - val_accuracy: 0.9304 - val_jacard_coef: 0.0632 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1661 - accuracy: 0.8823 - jacard_coef: 0.0940 2/17 [==>...........................] - ETA: 3s - loss: 0.1640 - accuracy: 0.8869 - jacard_coef: 0.0666 3/17 [====>.........................] - ETA: 2s - loss: 0.1649 - accuracy: 0.8697 - jacard_coef: 0.0769 4/17 [======>.......................] - ETA: 2s - loss: 0.1646 - accuracy: 0.8606 - jacard_coef: 0.0791 5/17 [=======>......................] - ETA: 2s - loss: 0.1644 - accuracy: 0.8601 - jacard_coef: 0.0775 6/17 [=========>....................] - ETA: 2s - loss: 0.1642 - accuracy: 0.8618 - jacard_coef: 0.0760 7/17 [===========>..................] - ETA: 2s - loss: 0.1642 - accuracy: 0.8644 - jacard_coef: 0.0742 8/17 [=============>................] - ETA: 1s - loss: 0.1642 - accuracy: 0.8641 - jacard_coef: 0.0775 9/17 [==============>...............] - ETA: 1s - loss: 0.1639 - accuracy: 0.8628 - jacard_coef: 0.081410/17 [================>.............] - ETA: 1s - loss: 0.1638 - accuracy: 0.8655 - jacard_coef: 0.081311/17 [==================>...........] - ETA: 1s - loss: 0.1635 - accuracy: 0.8679 - jacard_coef: 0.080312/17 [====================>.........] - ETA: 1s - loss: 0.1633 - accuracy: 0.8707 - jacard_coef: 0.078713/17 [=====================>........] - ETA: 0s - loss: 0.1630 - accuracy: 0.8682 - jacard_coef: 0.078114/17 [=======================>......] - ETA: 0s - loss: 0.1626 - accuracy: 0.8736 - jacard_coef: 0.075215/17 [=========================>....] - ETA: 0s - loss: 0.1623 - accuracy: 0.8772 - jacard_coef: 0.073816/17 [===========================>..] - ETA: 0s - loss: 0.1621 - accuracy: 0.8774 - jacard_coef: 0.075717/17 [==============================] - 4s 209ms/step - loss: 0.1621 - accuracy: 0.8781 - jacard_coef: 0.0729 - val_loss: 0.1604 - val_accuracy: 0.9197 - val_jacard_coef: 0.0632 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1576 - accuracy: 0.9222 - jacard_coef: 0.0682 2/17 [==>...........................] - ETA: 3s - loss: 0.1584 - accuracy: 0.9108 - jacard_coef: 0.0786 3/17 [====>.........................] - ETA: 2s - loss: 0.1585 - accuracy: 0.9074 - jacard_coef: 0.0797 4/17 [======>.......................] - ETA: 2s - loss: 0.1582 - accuracy: 0.8981 - jacard_coef: 0.0853 5/17 [=======>......................] - ETA: 2s - loss: 0.1584 - accuracy: 0.9034 - jacard_coef: 0.0810 6/17 [=========>....................] - ETA: 2s - loss: 0.1580 - accuracy: 0.8994 - jacard_coef: 0.0829 7/17 [===========>..................] - ETA: 2s - loss: 0.1577 - accuracy: 0.9013 - jacard_coef: 0.0820 8/17 [=============>................] - ETA: 1s - loss: 0.1576 - accuracy: 0.9070 - jacard_coef: 0.0772 9/17 [==============>...............] - ETA: 1s - loss: 0.1575 - accuracy: 0.9058 - jacard_coef: 0.078710/17 [================>.............] - ETA: 1s - loss: 0.1572 - accuracy: 0.9085 - jacard_coef: 0.075811/17 [==================>...........] - ETA: 1s - loss: 0.1571 - accuracy: 0.9060 - jacard_coef: 0.076912/17 [====================>.........] - ETA: 1s - loss: 0.1571 - accuracy: 0.9026 - jacard_coef: 0.078213/17 [=====================>........] - ETA: 0s - loss: 0.1569 - accuracy: 0.9022 - jacard_coef: 0.078514/17 [=======================>......] - ETA: 0s - loss: 0.1568 - accuracy: 0.9028 - jacard_coef: 0.078215/17 [=========================>....] - ETA: 0s - loss: 0.1566 - accuracy: 0.9034 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1564 - accuracy: 0.9044 - jacard_coef: 0.075917/17 [==============================] - 4s 212ms/step - loss: 0.1567 - accuracy: 0.9013 - jacard_coef: 0.0734 - val_loss: 0.1566 - val_accuracy: 0.9213 - val_jacard_coef: 0.0633 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1557 - accuracy: 0.9314 - jacard_coef: 0.0628 2/17 [==>...........................] - ETA: 3s - loss: 0.1621 - accuracy: 0.8079 - jacard_coef: 0.0837 3/17 [====>.........................] - ETA: 2s - loss: 0.1601 - accuracy: 0.8374 - jacard_coef: 0.0864 4/17 [======>.......................] - ETA: 2s - loss: 0.1603 - accuracy: 0.8475 - jacard_coef: 0.0880 5/17 [=======>......................] - ETA: 2s - loss: 0.1600 - accuracy: 0.8666 - jacard_coef: 0.0784 6/17 [=========>....................] - ETA: 2s - loss: 0.1599 - accuracy: 0.8684 - jacard_coef: 0.0815 7/17 [===========>..................] - ETA: 2s - loss: 0.1599 - accuracy: 0.8667 - jacard_coef: 0.0841 8/17 [=============>................] - ETA: 1s - loss: 0.1594 - accuracy: 0.8683 - jacard_coef: 0.0850 9/17 [==============>...............] - ETA: 1s - loss: 0.1595 - accuracy: 0.8694 - jacard_coef: 0.085010/17 [================>.............] - ETA: 1s - loss: 0.1591 - accuracy: 0.8708 - jacard_coef: 0.085211/17 [==================>...........] - ETA: 1s - loss: 0.1584 - accuracy: 0.8771 - jacard_coef: 0.081612/17 [====================>.........] - ETA: 1s - loss: 0.1580 - accuracy: 0.8823 - jacard_coef: 0.079313/17 [=====================>........] - ETA: 0s - loss: 0.1576 - accuracy: 0.8846 - jacard_coef: 0.079314/17 [=======================>......] - ETA: 0s - loss: 0.1573 - accuracy: 0.8888 - jacard_coef: 0.077315/17 [=========================>....] - ETA: 0s - loss: 0.1570 - accuracy: 0.8901 - jacard_coef: 0.077616/17 [===========================>..] - ETA: 0s - loss: 0.1567 - accuracy: 0.8947 - jacard_coef: 0.074817/17 [==============================] - 4s 212ms/step - loss: 0.1568 - accuracy: 0.8927 - jacard_coef: 0.0792 - val_loss: 0.1920 - val_accuracy: 0.0734 - val_jacard_coef: 0.0636 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1540 - accuracy: 0.8941 - jacard_coef: 0.0946 2/17 [==>...........................] - ETA: 3s - loss: 0.1520 - accuracy: 0.9190 - jacard_coef: 0.0739 3/17 [====>.........................] - ETA: 2s - loss: 0.1534 - accuracy: 0.9122 - jacard_coef: 0.0793 4/17 [======>.......................] - ETA: 2s - loss: 0.1537 - accuracy: 0.9123 - jacard_coef: 0.0791 5/17 [=======>......................] - ETA: 2s - loss: 0.1536 - accuracy: 0.9103 - jacard_coef: 0.0801 6/17 [=========>....................] - ETA: 2s - loss: 0.1534 - accuracy: 0.9076 - jacard_coef: 0.0813 7/17 [===========>..................] - ETA: 2s - loss: 0.1532 - accuracy: 0.9044 - jacard_coef: 0.0834 8/17 [=============>................] - ETA: 1s - loss: 0.1531 - accuracy: 0.9041 - jacard_coef: 0.0833 9/17 [==============>...............] - ETA: 1s - loss: 0.1528 - accuracy: 0.9045 - jacard_coef: 0.082910/17 [================>.............] - ETA: 1s - loss: 0.1523 - accuracy: 0.9106 - jacard_coef: 0.077811/17 [==================>...........] - ETA: 1s - loss: 0.1520 - accuracy: 0.9130 - jacard_coef: 0.076012/17 [====================>.........] - ETA: 1s - loss: 0.1519 - accuracy: 0.9150 - jacard_coef: 0.074513/17 [=====================>........] - ETA: 0s - loss: 0.1518 - accuracy: 0.9135 - jacard_coef: 0.075914/17 [=======================>......] - ETA: 0s - loss: 0.1514 - accuracy: 0.9151 - jacard_coef: 0.074715/17 [=========================>....] - ETA: 0s - loss: 0.1515 - accuracy: 0.9146 - jacard_coef: 0.075216/17 [===========================>..] - ETA: 0s - loss: 0.1513 - accuracy: 0.9145 - jacard_coef: 0.075417/17 [==============================] - 4s 213ms/step - loss: 0.1514 - accuracy: 0.9119 - jacard_coef: 0.0749 - val_loss: 0.2097 - val_accuracy: 0.0776 - val_jacard_coef: 0.0639 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1584 - accuracy: 0.7427 - jacard_coef: 0.0768 2/17 [==>...........................] - ETA: 3s - loss: 0.1544 - accuracy: 0.8220 - jacard_coef: 0.0808 3/17 [====>.........................] - ETA: 2s - loss: 0.1521 - accuracy: 0.8439 - jacard_coef: 0.0827 4/17 [======>.......................] - ETA: 2s - loss: 0.1519 - accuracy: 0.8544 - jacard_coef: 0.0837 5/17 [=======>......................] - ETA: 2s - loss: 0.1512 - accuracy: 0.8628 - jacard_coef: 0.0783 6/17 [=========>....................] - ETA: 2s - loss: 0.1512 - accuracy: 0.8642 - jacard_coef: 0.0782 7/17 [===========>..................] - ETA: 2s - loss: 0.1506 - accuracy: 0.8752 - jacard_coef: 0.0738 8/17 [=============>................] - ETA: 1s - loss: 0.1506 - accuracy: 0.8757 - jacard_coef: 0.0773 9/17 [==============>...............] - ETA: 1s - loss: 0.1505 - accuracy: 0.8782 - jacard_coef: 0.078710/17 [================>.............] - ETA: 1s - loss: 0.1503 - accuracy: 0.8795 - jacard_coef: 0.080511/17 [==================>...........] - ETA: 1s - loss: 0.1501 - accuracy: 0.8839 - jacard_coef: 0.079212/17 [====================>.........] - ETA: 1s - loss: 0.1498 - accuracy: 0.8880 - jacard_coef: 0.077813/17 [=====================>........] - ETA: 0s - loss: 0.1496 - accuracy: 0.8904 - jacard_coef: 0.077514/17 [=======================>......] - ETA: 0s - loss: 0.1495 - accuracy: 0.8932 - jacard_coef: 0.076615/17 [=========================>....] - ETA: 0s - loss: 0.1491 - accuracy: 0.8953 - jacard_coef: 0.076116/17 [===========================>..] - ETA: 0s - loss: 0.1490 - accuracy: 0.8969 - jacard_coef: 0.075817/17 [==============================] - 4s 209ms/step - loss: 0.1491 - accuracy: 0.8951 - jacard_coef: 0.0716 - val_loss: 0.1499 - val_accuracy: 0.9302 - val_jacard_coef: 0.0630 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1463 - accuracy: 0.9206 - jacard_coef: 0.0725 2/17 [==>...........................] - ETA: 3s - loss: 0.1472 - accuracy: 0.9040 - jacard_coef: 0.0852 3/17 [====>.........................] - ETA: 2s - loss: 0.1468 - accuracy: 0.9110 - jacard_coef: 0.0789 4/17 [======>.......................] - ETA: 2s - loss: 0.1464 - accuracy: 0.9099 - jacard_coef: 0.0798 5/17 [=======>......................] - ETA: 2s - loss: 0.1464 - accuracy: 0.9069 - jacard_coef: 0.0823 6/17 [=========>....................] - ETA: 2s - loss: 0.1460 - accuracy: 0.9095 - jacard_coef: 0.0803 7/17 [===========>..................] - ETA: 2s - loss: 0.1457 - accuracy: 0.9140 - jacard_coef: 0.0766 8/17 [=============>................] - ETA: 1s - loss: 0.1454 - accuracy: 0.9151 - jacard_coef: 0.0758 9/17 [==============>...............] - ETA: 1s - loss: 0.1452 - accuracy: 0.9169 - jacard_coef: 0.074410/17 [================>.............] - ETA: 1s - loss: 0.1451 - accuracy: 0.9193 - jacard_coef: 0.072411/17 [==================>...........] - ETA: 1s - loss: 0.1453 - accuracy: 0.9167 - jacard_coef: 0.074612/17 [====================>.........] - ETA: 1s - loss: 0.1449 - accuracy: 0.9203 - jacard_coef: 0.071513/17 [=====================>........] - ETA: 0s - loss: 0.1449 - accuracy: 0.9183 - jacard_coef: 0.073114/17 [=======================>......] - ETA: 0s - loss: 0.1446 - accuracy: 0.9207 - jacard_coef: 0.070915/17 [=========================>....] - ETA: 0s - loss: 0.1447 - accuracy: 0.9171 - jacard_coef: 0.073716/17 [===========================>..] - ETA: 0s - loss: 0.1448 - accuracy: 0.9150 - jacard_coef: 0.075117/17 [==============================] - 4s 209ms/step - loss: 0.1448 - accuracy: 0.9153 - jacard_coef: 0.0730 - val_loss: 0.1562 - val_accuracy: 0.9304 - val_jacard_coef: 0.0631 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1418 - accuracy: 0.9335 - jacard_coef: 0.0573 2/17 [==>...........................] - ETA: 3s - loss: 0.1411 - accuracy: 0.9331 - jacard_coef: 0.0584 3/17 [====>.........................] - ETA: 2s - loss: 0.1408 - accuracy: 0.9392 - jacard_coef: 0.0540 4/17 [======>.......................] - ETA: 2s - loss: 0.1420 - accuracy: 0.9262 - jacard_coef: 0.0661 5/17 [=======>......................] - ETA: 2s - loss: 0.1422 - accuracy: 0.9188 - jacard_coef: 0.0726 6/17 [=========>....................] - ETA: 2s - loss: 0.1416 - accuracy: 0.9219 - jacard_coef: 0.0702 7/17 [===========>..................] - ETA: 2s - loss: 0.1413 - accuracy: 0.9229 - jacard_coef: 0.0696 8/17 [=============>................] - ETA: 1s - loss: 0.1418 - accuracy: 0.9160 - jacard_coef: 0.0752 9/17 [==============>...............] - ETA: 1s - loss: 0.1419 - accuracy: 0.9135 - jacard_coef: 0.077410/17 [================>.............] - ETA: 1s - loss: 0.1416 - accuracy: 0.9152 - jacard_coef: 0.076111/17 [==================>...........] - ETA: 1s - loss: 0.1416 - accuracy: 0.9173 - jacard_coef: 0.074312/17 [====================>.........] - ETA: 1s - loss: 0.1415 - accuracy: 0.9170 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 0s - loss: 0.1417 - accuracy: 0.9139 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1413 - accuracy: 0.9162 - jacard_coef: 0.074415/17 [=========================>....] - ETA: 0s - loss: 0.1412 - accuracy: 0.9156 - jacard_coef: 0.074916/17 [===========================>..] - ETA: 0s - loss: 0.1412 - accuracy: 0.9162 - jacard_coef: 0.074617/17 [==============================] - 4s 209ms/step - loss: 0.1412 - accuracy: 0.9156 - jacard_coef: 0.0779 - val_loss: 0.1431 - val_accuracy: 0.9304 - val_jacard_coef: 0.0629 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1425 - accuracy: 0.8667 - jacard_coef: 0.1044 2/17 [==>...........................] - ETA: 3s - loss: 0.1403 - accuracy: 0.8953 - jacard_coef: 0.0830 3/17 [====>.........................] - ETA: 2s - loss: 0.1399 - accuracy: 0.8992 - jacard_coef: 0.0830 4/17 [======>.......................] - ETA: 2s - loss: 0.1398 - accuracy: 0.9011 - jacard_coef: 0.0832 5/17 [=======>......................] - ETA: 2s - loss: 0.1391 - accuracy: 0.9063 - jacard_coef: 0.0801 6/17 [=========>....................] - ETA: 2s - loss: 0.1389 - accuracy: 0.9091 - jacard_coef: 0.0785 7/17 [===========>..................] - ETA: 2s - loss: 0.1396 - accuracy: 0.9059 - jacard_coef: 0.0815 8/17 [=============>................] - ETA: 1s - loss: 0.1391 - accuracy: 0.9101 - jacard_coef: 0.0784 9/17 [==============>...............] - ETA: 1s - loss: 0.1392 - accuracy: 0.9087 - jacard_coef: 0.079910/17 [================>.............] - ETA: 1s - loss: 0.1393 - accuracy: 0.9082 - jacard_coef: 0.080411/17 [==================>...........] - ETA: 1s - loss: 0.1390 - accuracy: 0.9105 - jacard_coef: 0.078712/17 [====================>.........] - ETA: 1s - loss: 0.1417 - accuracy: 0.8967 - jacard_coef: 0.077413/17 [=====================>........] - ETA: 0s - loss: 0.1413 - accuracy: 0.8995 - jacard_coef: 0.076014/17 [=======================>......] - ETA: 0s - loss: 0.1410 - accuracy: 0.9014 - jacard_coef: 0.074915/17 [=========================>....] - ETA: 0s - loss: 0.1409 - accuracy: 0.9016 - jacard_coef: 0.075216/17 [===========================>..] - ETA: 0s - loss: 0.1409 - accuracy: 0.9024 - jacard_coef: 0.075017/17 [==============================] - 4s 209ms/step - loss: 0.1409 - accuracy: 0.9025 - jacard_coef: 0.0737 - val_loss: 0.1391 - val_accuracy: 0.9304 - val_jacard_coef: 0.0628 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1383 - accuracy: 0.9279 - jacard_coef: 0.0650 2/17 [==>...........................] - ETA: 3s - loss: 0.1394 - accuracy: 0.9183 - jacard_coef: 0.0736 3/17 [====>.........................] - ETA: 2s - loss: 0.1385 - accuracy: 0.9240 - jacard_coef: 0.0692 4/17 [======>.......................] - ETA: 2s - loss: 0.1391 - accuracy: 0.9195 - jacard_coef: 0.0731 5/17 [=======>......................] - ETA: 2s - loss: 0.1388 - accuracy: 0.9193 - jacard_coef: 0.0735 6/17 [=========>....................] - ETA: 2s - loss: 0.1388 - accuracy: 0.9182 - jacard_coef: 0.0746 7/17 [===========>..................] - ETA: 2s - loss: 0.1392 - accuracy: 0.9110 - jacard_coef: 0.0803 8/17 [=============>................] - ETA: 1s - loss: 0.1392 - accuracy: 0.9119 - jacard_coef: 0.0796 9/17 [==============>...............] - ETA: 1s - loss: 0.1384 - accuracy: 0.9180 - jacard_coef: 0.074310/17 [================>.............] - ETA: 1s - loss: 0.1379 - accuracy: 0.9207 - jacard_coef: 0.072111/17 [==================>...........] - ETA: 1s - loss: 0.1379 - accuracy: 0.9199 - jacard_coef: 0.072712/17 [====================>.........] - ETA: 1s - loss: 0.1378 - accuracy: 0.9209 - jacard_coef: 0.071813/17 [=====================>........] - ETA: 0s - loss: 0.1379 - accuracy: 0.9182 - jacard_coef: 0.073814/17 [=======================>......] - ETA: 0s - loss: 0.1377 - accuracy: 0.9185 - jacard_coef: 0.073415/17 [=========================>....] - ETA: 0s - loss: 0.1377 - accuracy: 0.9179 - jacard_coef: 0.073716/17 [===========================>..] - ETA: 0s - loss: 0.1378 - accuracy: 0.9162 - jacard_coef: 0.075017/17 [==============================] - 4s 209ms/step - loss: 0.1378 - accuracy: 0.9163 - jacard_coef: 0.0742 - val_loss: 0.1400 - val_accuracy: 0.9304 - val_jacard_coef: 0.0628 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1374 - accuracy: 0.9038 - jacard_coef: 0.0822 2/17 [==>...........................] - ETA: 3s - loss: 0.1362 - accuracy: 0.9074 - jacard_coef: 0.0799 3/17 [====>.........................] - ETA: 2s - loss: 0.1356 - accuracy: 0.9084 - jacard_coef: 0.0794 4/17 [======>.......................] - ETA: 2s - loss: 0.1366 - accuracy: 0.9020 - jacard_coef: 0.0843 5/17 [=======>......................] - ETA: 2s - loss: 0.1362 - accuracy: 0.9047 - jacard_coef: 0.0822 6/17 [=========>....................] - ETA: 2s - loss: 0.1364 - accuracy: 0.9044 - jacard_coef: 0.0829 7/17 [===========>..................] - ETA: 2s - loss: 0.1365 - accuracy: 0.9023 - jacard_coef: 0.0849 8/17 [=============>................] - ETA: 1s - loss: 0.1361 - accuracy: 0.9042 - jacard_coef: 0.0836 9/17 [==============>...............] - ETA: 1s - loss: 0.1362 - accuracy: 0.9017 - jacard_coef: 0.085710/17 [================>.............] - ETA: 1s - loss: 0.1361 - accuracy: 0.9020 - jacard_coef: 0.085711/17 [==================>...........] - ETA: 1s - loss: 0.1357 - accuracy: 0.9048 - jacard_coef: 0.083512/17 [====================>.........] - ETA: 1s - loss: 0.1355 - accuracy: 0.9074 - jacard_coef: 0.081413/17 [=====================>........] - ETA: 0s - loss: 0.1353 - accuracy: 0.9100 - jacard_coef: 0.079314/17 [=======================>......] - ETA: 0s - loss: 0.1350 - accuracy: 0.9125 - jacard_coef: 0.077315/17 [=========================>....] - ETA: 0s - loss: 0.1348 - accuracy: 0.9125 - jacard_coef: 0.077516/17 [===========================>..] - ETA: 0s - loss: 0.1344 - accuracy: 0.9163 - jacard_coef: 0.074217/17 [==============================] - 4s 209ms/step - loss: 0.1345 - accuracy: 0.9158 - jacard_coef: 0.0775 - val_loss: 0.1357 - val_accuracy: 0.9304 - val_jacard_coef: 0.0627 - lr: 5.0000e-04
Epoch 15/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1328 - accuracy: 0.9276 - jacard_coef: 0.0664 2/17 [==>...........................] - ETA: 3s - loss: 0.1320 - accuracy: 0.9333 - jacard_coef: 0.0613 3/17 [====>.........................] - ETA: 2s - loss: 0.1337 - accuracy: 0.9130 - jacard_coef: 0.0778 4/17 [======>.......................] - ETA: 2s - loss: 0.1335 - accuracy: 0.9143 - jacard_coef: 0.0769 5/17 [=======>......................] - ETA: 2s - loss: 0.1337 - accuracy: 0.9108 - jacard_coef: 0.0800 6/17 [=========>....................] - ETA: 2s - loss: 0.1333 - accuracy: 0.9160 - jacard_coef: 0.0757 7/17 [===========>..................] - ETA: 2s - loss: 0.1338 - accuracy: 0.9108 - jacard_coef: 0.0798 8/17 [=============>................] - ETA: 1s - loss: 0.1341 - accuracy: 0.9093 - jacard_coef: 0.0810 9/17 [==============>...............] - ETA: 1s - loss: 0.1338 - accuracy: 0.9123 - jacard_coef: 0.078510/17 [================>.............] - ETA: 1s - loss: 0.1333 - accuracy: 0.9157 - jacard_coef: 0.075711/17 [==================>...........] - ETA: 1s - loss: 0.1329 - accuracy: 0.9193 - jacard_coef: 0.072612/17 [====================>.........] - ETA: 1s - loss: 0.1326 - accuracy: 0.9216 - jacard_coef: 0.070713/17 [=====================>........] - ETA: 0s - loss: 0.1326 - accuracy: 0.9201 - jacard_coef: 0.071914/17 [=======================>......] - ETA: 0s - loss: 0.1326 - accuracy: 0.9196 - jacard_coef: 0.072315/17 [=========================>....] - ETA: 0s - loss: 0.1326 - accuracy: 0.9189 - jacard_coef: 0.072916/17 [===========================>..] - ETA: 0s - loss: 0.1327 - accuracy: 0.9166 - jacard_coef: 0.074817/17 [==============================] - 4s 211ms/step - loss: 0.1327 - accuracy: 0.9168 - jacard_coef: 0.0735 - val_loss: 0.1329 - val_accuracy: 0.9304 - val_jacard_coef: 0.0626 - lr: 5.0000e-04
Epoch 16/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1340 - accuracy: 0.9113 - jacard_coef: 0.0796 2/17 [==>...........................] - ETA: 3s - loss: 0.1344 - accuracy: 0.8981 - jacard_coef: 0.0903 3/17 [====>.........................] - ETA: 2s - loss: 0.1331 - accuracy: 0.9131 - jacard_coef: 0.0779 4/17 [======>.......................] - ETA: 2s - loss: 0.1320 - accuracy: 0.9174 - jacard_coef: 0.0745 5/17 [=======>......................] - ETA: 2s - loss: 0.1320 - accuracy: 0.9145 - jacard_coef: 0.0772 6/17 [=========>....................] - ETA: 2s - loss: 0.1318 - accuracy: 0.9144 - jacard_coef: 0.0773 7/17 [===========>..................] - ETA: 2s - loss: 0.1323 - accuracy: 0.9126 - jacard_coef: 0.0788 8/17 [=============>................] - ETA: 1s - loss: 0.1319 - accuracy: 0.9151 - jacard_coef: 0.0767 9/17 [==============>...............] - ETA: 1s - loss: 0.1316 - accuracy: 0.9164 - jacard_coef: 0.075710/17 [================>.............] - ETA: 1s - loss: 0.1314 - accuracy: 0.9180 - jacard_coef: 0.074511/17 [==================>...........] - ETA: 1s - loss: 0.1315 - accuracy: 0.9162 - jacard_coef: 0.075912/17 [====================>.........] - ETA: 1s - loss: 0.1312 - accuracy: 0.9176 - jacard_coef: 0.074813/17 [=====================>........] - ETA: 0s - loss: 0.1310 - accuracy: 0.9186 - jacard_coef: 0.074014/17 [=======================>......] - ETA: 0s - loss: 0.1308 - accuracy: 0.9197 - jacard_coef: 0.073115/17 [=========================>....] - ETA: 0s - loss: 0.1309 - accuracy: 0.9189 - jacard_coef: 0.073816/17 [===========================>..] - ETA: 0s - loss: 0.1311 - accuracy: 0.9170 - jacard_coef: 0.075217/17 [==============================] - 4s 210ms/step - loss: 0.1311 - accuracy: 0.9173 - jacard_coef: 0.0737 - val_loss: 0.1345 - val_accuracy: 0.9304 - val_jacard_coef: 0.0627 - lr: 5.0000e-04
Epoch 17/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1304 - accuracy: 0.9266 - jacard_coef: 0.0676 2/17 [==>...........................] - ETA: 3s - loss: 0.1302 - accuracy: 0.9228 - jacard_coef: 0.0706 3/17 [====>.........................] - ETA: 2s - loss: 0.1301 - accuracy: 0.9202 - jacard_coef: 0.0727 4/17 [======>.......................] - ETA: 2s - loss: 0.1309 - accuracy: 0.9165 - jacard_coef: 0.0757 5/17 [=======>......................] - ETA: 2s - loss: 0.1304 - accuracy: 0.9152 - jacard_coef: 0.0720 6/17 [=========>....................] - ETA: 2s - loss: 0.1306 - accuracy: 0.9148 - jacard_coef: 0.0730 7/17 [===========>..................] - ETA: 2s - loss: 0.1303 - accuracy: 0.9165 - jacard_coef: 0.0722 8/17 [=============>................] - ETA: 1s - loss: 0.1304 - accuracy: 0.9151 - jacard_coef: 0.0736 9/17 [==============>...............] - ETA: 1s - loss: 0.1303 - accuracy: 0.9145 - jacard_coef: 0.074510/17 [================>.............] - ETA: 1s - loss: 0.1310 - accuracy: 0.9079 - jacard_coef: 0.079911/17 [==================>...........] - ETA: 1s - loss: 0.1307 - accuracy: 0.9091 - jacard_coef: 0.079212/17 [====================>.........] - ETA: 1s - loss: 0.1303 - accuracy: 0.9113 - jacard_coef: 0.077613/17 [=====================>........] - ETA: 0s - loss: 0.1303 - accuracy: 0.9116 - jacard_coef: 0.077514/17 [=======================>......] - ETA: 0s - loss: 0.1300 - accuracy: 0.9137 - jacard_coef: 0.075915/17 [=========================>....] - ETA: 0s - loss: 0.1298 - accuracy: 0.9150 - jacard_coef: 0.075016/17 [===========================>..] - ETA: 0s - loss: 0.1296 - accuracy: 0.9158 - jacard_coef: 0.074517/17 [==============================] - 4s 210ms/step - loss: 0.1298 - accuracy: 0.9144 - jacard_coef: 0.0773 - val_loss: 0.1293 - val_accuracy: 0.9304 - val_jacard_coef: 0.0625 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1279 - accuracy: 0.9158 - jacard_coef: 0.0761 2/17 [==>...........................] - ETA: 3s - loss: 0.1312 - accuracy: 0.8915 - jacard_coef: 0.0932 3/17 [====>.........................] - ETA: 2s - loss: 0.1310 - accuracy: 0.8955 - jacard_coef: 0.0896 4/17 [======>.......................] - ETA: 2s - loss: 0.1310 - accuracy: 0.8961 - jacard_coef: 0.0883 5/17 [=======>......................] - ETA: 2s - loss: 0.1301 - accuracy: 0.9018 - jacard_coef: 0.0826 6/17 [=========>....................] - ETA: 2s - loss: 0.1306 - accuracy: 0.8984 - jacard_coef: 0.0846 7/17 [===========>..................] - ETA: 2s - loss: 0.1304 - accuracy: 0.8999 - jacard_coef: 0.0834 8/17 [=============>................] - ETA: 1s - loss: 0.1302 - accuracy: 0.9053 - jacard_coef: 0.0791 9/17 [==============>...............] - ETA: 1s - loss: 0.1298 - accuracy: 0.9079 - jacard_coef: 0.077110/17 [================>.............] - ETA: 1s - loss: 0.1292 - accuracy: 0.9116 - jacard_coef: 0.074311/17 [==================>...........] - ETA: 1s - loss: 0.1291 - accuracy: 0.9126 - jacard_coef: 0.073812/17 [====================>.........] - ETA: 1s - loss: 0.1295 - accuracy: 0.9104 - jacard_coef: 0.075913/17 [=====================>........] - ETA: 0s - loss: 0.1292 - accuracy: 0.9133 - jacard_coef: 0.073714/17 [=======================>......] - ETA: 0s - loss: 0.1290 - accuracy: 0.9153 - jacard_coef: 0.072315/17 [=========================>....] - ETA: 0s - loss: 0.1292 - accuracy: 0.9130 - jacard_coef: 0.074416/17 [===========================>..] - ETA: 0s - loss: 0.1292 - accuracy: 0.9128 - jacard_coef: 0.074817/17 [==============================] - 4s 211ms/step - loss: 0.1292 - accuracy: 0.9134 - jacard_coef: 0.0707 - val_loss: 0.1235 - val_accuracy: 0.9304 - val_jacard_coef: 0.0622 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0639 (epoch 8)
  Final Val Loss: 0.1235
  Training Time: 0:02:19.967030
  Stability (std): 0.0091

Results saved to: hyperparameter_optimization_20250926_123742/exp_31_Attention_ResUNet_lr1e-3_bs8/Attention_ResUNet_lr0.001_bs8_results.json

Experiment 31 completed in 175s
Progress: 31/36 completed
Estimated remaining time: 14 minutes

ðŸ”¬ EXPERIMENT 32/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758866395.630361 3356800 service.cc:145] XLA service 0x148e4de2e600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758866395.630417 3356800 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758866396.091830 3356800 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 9:15 - loss: 0.3371 - accuracy: 0.4884 - jacard_coef: 0.08052/9 [=====>........................] - ETA: 59s - loss: 0.3014 - accuracy: 0.3697 - jacard_coef: 0.0626 3/9 [=========>....................] - ETA: 26s - loss: 0.2719 - accuracy: 0.3204 - jacard_coef: 0.07014/9 [============>.................] - ETA: 15s - loss: 0.2574 - accuracy: 0.2862 - jacard_coef: 0.07185/9 [===============>..............] - ETA: 9s - loss: 0.2462 - accuracy: 0.2679 - jacard_coef: 0.0752 6/9 [===================>..........] - ETA: 6s - loss: 0.2381 - accuracy: 0.2575 - jacard_coef: 0.07867/9 [======================>.......] - ETA: 3s - loss: 0.2319 - accuracy: 0.2454 - jacard_coef: 0.07798/9 [=========================>....] - ETA: 1s - loss: 0.2275 - accuracy: 0.2335 - jacard_coef: 0.07739/9 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.2322 - jacard_coef: 0.06889/9 [==============================] - 90s 3s/step - loss: 0.2273 - accuracy: 0.2322 - jacard_coef: 0.0688 - val_loss: 1.1357 - val_accuracy: 0.9270 - val_jacard_coef: 0.0035 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1938 - accuracy: 0.1164 - jacard_coef: 0.07122/9 [=====>........................] - ETA: 2s - loss: 0.1919 - accuracy: 0.1228 - jacard_coef: 0.07033/9 [=========>....................] - ETA: 2s - loss: 0.1898 - accuracy: 0.1378 - jacard_coef: 0.07694/9 [============>.................] - ETA: 2s - loss: 0.1891 - accuracy: 0.1449 - jacard_coef: 0.07685/9 [===============>..............] - ETA: 1s - loss: 0.1877 - accuracy: 0.1562 - jacard_coef: 0.07756/9 [===================>..........] - ETA: 1s - loss: 0.1861 - accuracy: 0.1756 - jacard_coef: 0.07977/9 [======================>.......] - ETA: 0s - loss: 0.1864 - accuracy: 0.1911 - jacard_coef: 0.07638/9 [=========================>....] - ETA: 0s - loss: 0.1859 - accuracy: 0.2185 - jacard_coef: 0.07679/9 [==============================] - 4s 388ms/step - loss: 0.1859 - accuracy: 0.2196 - jacard_coef: 0.0740 - val_loss: 1.1485 - val_accuracy: 0.9267 - val_jacard_coef: 0.0040 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1796 - accuracy: 0.4180 - jacard_coef: 0.07392/9 [=====>........................] - ETA: 2s - loss: 0.1812 - accuracy: 0.3845 - jacard_coef: 0.06413/9 [=========>....................] - ETA: 2s - loss: 0.1806 - accuracy: 0.3831 - jacard_coef: 0.06634/9 [============>.................] - ETA: 2s - loss: 0.1793 - accuracy: 0.3959 - jacard_coef: 0.07125/9 [===============>..............] - ETA: 1s - loss: 0.1783 - accuracy: 0.4066 - jacard_coef: 0.07476/9 [===================>..........] - ETA: 1s - loss: 0.1773 - accuracy: 0.4260 - jacard_coef: 0.07647/9 [======================>.......] - ETA: 0s - loss: 0.1768 - accuracy: 0.4455 - jacard_coef: 0.07778/9 [=========================>....] - ETA: 0s - loss: 0.1762 - accuracy: 0.4617 - jacard_coef: 0.07699/9 [==============================] - 3s 379ms/step - loss: 0.1770 - accuracy: 0.4620 - jacard_coef: 0.0716 - val_loss: 1.0951 - val_accuracy: 0.9270 - val_jacard_coef: 0.0026 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1779 - accuracy: 0.2980 - jacard_coef: 0.08452/9 [=====>........................] - ETA: 2s - loss: 0.1801 - accuracy: 0.2744 - jacard_coef: 0.09073/9 [=========>....................] - ETA: 2s - loss: 0.1809 - accuracy: 0.2594 - jacard_coef: 0.08554/9 [============>.................] - ETA: 2s - loss: 0.1818 - accuracy: 0.2440 - jacard_coef: 0.08205/9 [===============>..............] - ETA: 1s - loss: 0.1815 - accuracy: 0.2499 - jacard_coef: 0.08276/9 [===================>..........] - ETA: 1s - loss: 0.1802 - accuracy: 0.2785 - jacard_coef: 0.08107/9 [======================>.......] - ETA: 0s - loss: 0.1794 - accuracy: 0.3031 - jacard_coef: 0.07848/9 [=========================>....] - ETA: 0s - loss: 0.1787 - accuracy: 0.3300 - jacard_coef: 0.07699/9 [==============================] - 3s 380ms/step - loss: 0.1786 - accuracy: 0.3317 - jacard_coef: 0.0746 - val_loss: 1.0894 - val_accuracy: 0.9303 - val_jacard_coef: 0.0013 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1735 - accuracy: 0.6020 - jacard_coef: 0.09762/9 [=====>........................] - ETA: 2s - loss: 0.1727 - accuracy: 0.6250 - jacard_coef: 0.07973/9 [=========>....................] - ETA: 2s - loss: 0.1719 - accuracy: 0.6177 - jacard_coef: 0.08174/9 [============>.................] - ETA: 2s - loss: 0.1715 - accuracy: 0.5890 - jacard_coef: 0.08075/9 [===============>..............] - ETA: 1s - loss: 0.1711 - accuracy: 0.5726 - jacard_coef: 0.07946/9 [===================>..........] - ETA: 1s - loss: 0.1708 - accuracy: 0.5614 - jacard_coef: 0.07727/9 [======================>.......] - ETA: 0s - loss: 0.1706 - accuracy: 0.5653 - jacard_coef: 0.07608/9 [=========================>....] - ETA: 0s - loss: 0.1703 - accuracy: 0.5697 - jacard_coef: 0.07599/9 [==============================] - 4s 390ms/step - loss: 0.1703 - accuracy: 0.5703 - jacard_coef: 0.0801 - val_loss: 0.3753 - val_accuracy: 0.9275 - val_jacard_coef: 0.0313 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1677 - accuracy: 0.7155 - jacard_coef: 0.06652/9 [=====>........................] - ETA: 2s - loss: 0.1675 - accuracy: 0.6542 - jacard_coef: 0.06383/9 [=========>....................] - ETA: 2s - loss: 0.1674 - accuracy: 0.6294 - jacard_coef: 0.07154/9 [============>.................] - ETA: 2s - loss: 0.1672 - accuracy: 0.6341 - jacard_coef: 0.07305/9 [===============>..............] - ETA: 1s - loss: 0.1669 - accuracy: 0.6570 - jacard_coef: 0.07116/9 [===================>..........] - ETA: 1s - loss: 0.1666 - accuracy: 0.6867 - jacard_coef: 0.07077/9 [======================>.......] - ETA: 0s - loss: 0.1664 - accuracy: 0.7041 - jacard_coef: 0.07308/9 [=========================>....] - ETA: 0s - loss: 0.1663 - accuracy: 0.7094 - jacard_coef: 0.07629/9 [==============================] - 4s 388ms/step - loss: 0.1663 - accuracy: 0.7096 - jacard_coef: 0.0734 - val_loss: 0.1284 - val_accuracy: 0.9074 - val_jacard_coef: 0.0557 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1646 - accuracy: 0.7159 - jacard_coef: 0.07922/9 [=====>........................] - ETA: 2s - loss: 0.1644 - accuracy: 0.7191 - jacard_coef: 0.07003/9 [=========>....................] - ETA: 2s - loss: 0.1642 - accuracy: 0.7172 - jacard_coef: 0.06874/9 [============>.................] - ETA: 2s - loss: 0.1641 - accuracy: 0.7225 - jacard_coef: 0.06865/9 [===============>..............] - ETA: 1s - loss: 0.1639 - accuracy: 0.7387 - jacard_coef: 0.07026/9 [===================>..........] - ETA: 1s - loss: 0.1638 - accuracy: 0.7587 - jacard_coef: 0.07247/9 [======================>.......] - ETA: 0s - loss: 0.1637 - accuracy: 0.7759 - jacard_coef: 0.07488/9 [=========================>....] - ETA: 0s - loss: 0.1636 - accuracy: 0.7894 - jacard_coef: 0.07579/9 [==============================] - 4s 406ms/step - loss: 0.1636 - accuracy: 0.7896 - jacard_coef: 0.0817 - val_loss: 0.0832 - val_accuracy: 0.9138 - val_jacard_coef: 0.0592 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1617 - accuracy: 0.8940 - jacard_coef: 0.07302/9 [=====>........................] - ETA: 2s - loss: 0.1616 - accuracy: 0.8459 - jacard_coef: 0.06933/9 [=========>....................] - ETA: 2s - loss: 0.1616 - accuracy: 0.8253 - jacard_coef: 0.07524/9 [============>.................] - ETA: 2s - loss: 0.1612 - accuracy: 0.8258 - jacard_coef: 0.07045/9 [===============>..............] - ETA: 1s - loss: 0.1613 - accuracy: 0.8201 - jacard_coef: 0.07226/9 [===================>..........] - ETA: 1s - loss: 0.1612 - accuracy: 0.8300 - jacard_coef: 0.07257/9 [======================>.......] - ETA: 0s - loss: 0.1610 - accuracy: 0.8386 - jacard_coef: 0.07258/9 [=========================>....] - ETA: 0s - loss: 0.1610 - accuracy: 0.8428 - jacard_coef: 0.07559/9 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.8423 - jacard_coef: 0.08349/9 [==============================] - 4s 387ms/step - loss: 0.1610 - accuracy: 0.8423 - jacard_coef: 0.0834 - val_loss: 0.0841 - val_accuracy: 0.9205 - val_jacard_coef: 0.0577 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1587 - accuracy: 0.9102 - jacard_coef: 0.06432/9 [=====>........................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8518 - jacard_coef: 0.07533/9 [=========>....................] - ETA: 2s - loss: 0.1592 - accuracy: 0.8335 - jacard_coef: 0.07204/9 [============>.................] - ETA: 2s - loss: 0.1590 - accuracy: 0.8261 - jacard_coef: 0.07005/9 [===============>..............] - ETA: 1s - loss: 0.1590 - accuracy: 0.8154 - jacard_coef: 0.06976/9 [===================>..........] - ETA: 1s - loss: 0.1592 - accuracy: 0.8162 - jacard_coef: 0.07417/9 [======================>.......] - ETA: 0s - loss: 0.1590 - accuracy: 0.8192 - jacard_coef: 0.07618/9 [=========================>....] - ETA: 0s - loss: 0.1588 - accuracy: 0.8280 - jacard_coef: 0.07559/9 [==============================] - ETA: 0s - loss: 0.1588 - accuracy: 0.8276 - jacard_coef: 0.08379/9 [==============================] - 4s 387ms/step - loss: 0.1588 - accuracy: 0.8276 - jacard_coef: 0.0837 - val_loss: 0.0807 - val_accuracy: 0.9195 - val_jacard_coef: 0.0579 - lr: 0.0010
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1574 - accuracy: 0.8770 - jacard_coef: 0.08242/9 [=====>........................] - ETA: 2s - loss: 0.1568 - accuracy: 0.8752 - jacard_coef: 0.07543/9 [=========>....................] - ETA: 2s - loss: 0.1569 - accuracy: 0.8617 - jacard_coef: 0.07454/9 [============>.................] - ETA: 2s - loss: 0.1570 - accuracy: 0.8422 - jacard_coef: 0.07905/9 [===============>..............] - ETA: 1s - loss: 0.1570 - accuracy: 0.8350 - jacard_coef: 0.07716/9 [===================>..........] - ETA: 1s - loss: 0.1568 - accuracy: 0.8383 - jacard_coef: 0.07617/9 [======================>.......] - ETA: 0s - loss: 0.1569 - accuracy: 0.8357 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1567 - accuracy: 0.8453 - jacard_coef: 0.07669/9 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.8412 - jacard_coef: 0.07119/9 [==============================] - 4s 411ms/step - loss: 0.1570 - accuracy: 0.8412 - jacard_coef: 0.0711 - val_loss: 0.1238 - val_accuracy: 0.9157 - val_jacard_coef: 0.0664 - lr: 0.0010
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1566 - accuracy: 0.9245 - jacard_coef: 0.06782/9 [=====>........................] - ETA: 2s - loss: 0.1590 - accuracy: 0.8947 - jacard_coef: 0.06043/9 [=========>....................] - ETA: 2s - loss: 0.1589 - accuracy: 0.8953 - jacard_coef: 0.06734/9 [============>.................] - ETA: 2s - loss: 0.1599 - accuracy: 0.8573 - jacard_coef: 0.07125/9 [===============>..............] - ETA: 1s - loss: 0.1599 - accuracy: 0.8589 - jacard_coef: 0.07586/9 [===================>..........] - ETA: 1s - loss: 0.1597 - accuracy: 0.8650 - jacard_coef: 0.07527/9 [======================>.......] - ETA: 0s - loss: 0.1594 - accuracy: 0.8685 - jacard_coef: 0.07578/9 [=========================>....] - ETA: 0s - loss: 0.1591 - accuracy: 0.8709 - jacard_coef: 0.07649/9 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.8704 - jacard_coef: 0.06849/9 [==============================] - 4s 388ms/step - loss: 0.1591 - accuracy: 0.8704 - jacard_coef: 0.0684 - val_loss: 0.1683 - val_accuracy: 0.6409 - val_jacard_coef: 0.0649 - lr: 0.0010
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1585 - accuracy: 0.8632 - jacard_coef: 0.09352/9 [=====>........................] - ETA: 2s - loss: 0.1564 - accuracy: 0.8973 - jacard_coef: 0.07213/9 [=========>....................] - ETA: 2s - loss: 0.1565 - accuracy: 0.8943 - jacard_coef: 0.07414/9 [============>.................] - ETA: 2s - loss: 0.1562 - accuracy: 0.8976 - jacard_coef: 0.07335/9 [===============>..............] - ETA: 1s - loss: 0.1567 - accuracy: 0.8853 - jacard_coef: 0.07466/9 [===================>..........] - ETA: 1s - loss: 0.1562 - accuracy: 0.8886 - jacard_coef: 0.06987/9 [======================>.......] - ETA: 0s - loss: 0.1561 - accuracy: 0.8847 - jacard_coef: 0.07118/9 [=========================>....] - ETA: 0s - loss: 0.1561 - accuracy: 0.8813 - jacard_coef: 0.07579/9 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.8805 - jacard_coef: 0.07049/9 [==============================] - 4s 388ms/step - loss: 0.1561 - accuracy: 0.8805 - jacard_coef: 0.0704 - val_loss: 0.1911 - val_accuracy: 0.1633 - val_jacard_coef: 0.0657 - lr: 0.0010
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1540 - accuracy: 0.9213 - jacard_coef: 0.06702/9 [=====>........................] - ETA: 2s - loss: 0.1538 - accuracy: 0.9150 - jacard_coef: 0.07363/9 [=========>....................] - ETA: 2s - loss: 0.1536 - accuracy: 0.9126 - jacard_coef: 0.07584/9 [============>.................] - ETA: 2s - loss: 0.1535 - accuracy: 0.9108 - jacard_coef: 0.07775/9 [===============>..............] - ETA: 1s - loss: 0.1533 - accuracy: 0.9115 - jacard_coef: 0.07736/9 [===================>..........] - ETA: 1s - loss: 0.1533 - accuracy: 0.9070 - jacard_coef: 0.07697/9 [======================>.......] - ETA: 0s - loss: 0.1531 - accuracy: 0.9071 - jacard_coef: 0.07768/9 [=========================>....] - ETA: 0s - loss: 0.1527 - accuracy: 0.9098 - jacard_coef: 0.07609/9 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9101 - jacard_coef: 0.07309/9 [==============================] - 4s 388ms/step - loss: 0.1527 - accuracy: 0.9101 - jacard_coef: 0.0730 - val_loss: 0.1894 - val_accuracy: 0.1343 - val_jacard_coef: 0.0655 - lr: 0.0010
Epoch 14/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1515 - accuracy: 0.8807 - jacard_coef: 0.06742/9 [=====>........................] - ETA: 2s - loss: 0.1507 - accuracy: 0.9013 - jacard_coef: 0.06813/9 [=========>....................] - ETA: 2s - loss: 0.1503 - accuracy: 0.9103 - jacard_coef: 0.06674/9 [============>.................] - ETA: 2s - loss: 0.1502 - accuracy: 0.9133 - jacard_coef: 0.06755/9 [===============>..............] - ETA: 1s - loss: 0.1504 - accuracy: 0.9038 - jacard_coef: 0.06906/9 [===================>..........] - ETA: 1s - loss: 0.1501 - accuracy: 0.9073 - jacard_coef: 0.06887/9 [======================>.......] - ETA: 0s - loss: 0.1502 - accuracy: 0.9059 - jacard_coef: 0.07198/9 [=========================>....] - ETA: 0s - loss: 0.1503 - accuracy: 0.9030 - jacard_coef: 0.07589/9 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9018 - jacard_coef: 0.07179/9 [==============================] - 4s 388ms/step - loss: 0.1503 - accuracy: 0.9018 - jacard_coef: 0.0717 - val_loss: 0.1845 - val_accuracy: 0.1484 - val_jacard_coef: 0.0652 - lr: 0.0010
Epoch 15/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1480 - accuracy: 0.9288 - jacard_coef: 0.06462/9 [=====>........................] - ETA: 2s - loss: 0.1482 - accuracy: 0.9252 - jacard_coef: 0.06653/9 [=========>....................] - ETA: 2s - loss: 0.1481 - accuracy: 0.9259 - jacard_coef: 0.06484/9 [============>.................] - ETA: 2s - loss: 0.1481 - accuracy: 0.9253 - jacard_coef: 0.06575/9 [===============>..............] - ETA: 1s - loss: 0.1484 - accuracy: 0.9217 - jacard_coef: 0.06926/9 [===================>..........] - ETA: 1s - loss: 0.1486 - accuracy: 0.9174 - jacard_coef: 0.07337/9 [======================>.......] - ETA: 0s - loss: 0.1486 - accuracy: 0.9169 - jacard_coef: 0.07418/9 [=========================>....] - ETA: 0s - loss: 0.1486 - accuracy: 0.9158 - jacard_coef: 0.07529/9 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9154 - jacard_coef: 0.08049/9 [==============================] - 4s 389ms/step - loss: 0.1486 - accuracy: 0.9154 - jacard_coef: 0.0804 - val_loss: 0.1593 - val_accuracy: 0.9216 - val_jacard_coef: 0.0651 - lr: 0.0010
Epoch 16/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1489 - accuracy: 0.8946 - jacard_coef: 0.09502/9 [=====>........................] - ETA: 2s - loss: 0.1482 - accuracy: 0.9082 - jacard_coef: 0.08363/9 [=========>....................] - ETA: 2s - loss: 0.1476 - accuracy: 0.9178 - jacard_coef: 0.07534/9 [============>.................] - ETA: 2s - loss: 0.1478 - accuracy: 0.9144 - jacard_coef: 0.07825/9 [===============>..............] - ETA: 1s - loss: 0.1479 - accuracy: 0.9126 - jacard_coef: 0.07976/9 [===================>..........] - ETA: 1s - loss: 0.1474 - accuracy: 0.9181 - jacard_coef: 0.07497/9 [======================>.......] - ETA: 0s - loss: 0.1476 - accuracy: 0.9144 - jacard_coef: 0.07808/9 [=========================>....] - ETA: 0s - loss: 0.1474 - accuracy: 0.9167 - jacard_coef: 0.07609/9 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9174 - jacard_coef: 0.06799/9 [==============================] - 4s 388ms/step - loss: 0.1474 - accuracy: 0.9174 - jacard_coef: 0.0679 - val_loss: 0.1623 - val_accuracy: 0.9182 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 17/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1471 - accuracy: 0.9103 - jacard_coef: 0.08132/9 [=====>........................] - ETA: 2s - loss: 0.1466 - accuracy: 0.9158 - jacard_coef: 0.07673/9 [=========>....................] - ETA: 2s - loss: 0.1465 - accuracy: 0.9191 - jacard_coef: 0.07374/9 [============>.................] - ETA: 2s - loss: 0.1465 - accuracy: 0.9179 - jacard_coef: 0.07475/9 [===============>..............] - ETA: 1s - loss: 0.1466 - accuracy: 0.9163 - jacard_coef: 0.07606/9 [===================>..........] - ETA: 1s - loss: 0.1468 - accuracy: 0.9130 - jacard_coef: 0.07877/9 [======================>.......] - ETA: 0s - loss: 0.1466 - accuracy: 0.9145 - jacard_coef: 0.07758/9 [=========================>....] - ETA: 0s - loss: 0.1465 - accuracy: 0.9165 - jacard_coef: 0.07599/9 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9168 - jacard_coef: 0.07179/9 [==============================] - 4s 388ms/step - loss: 0.1464 - accuracy: 0.9168 - jacard_coef: 0.0717 - val_loss: 0.1617 - val_accuracy: 0.9300 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 18/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1459 - accuracy: 0.9196 - jacard_coef: 0.07362/9 [=====>........................] - ETA: 2s - loss: 0.1458 - accuracy: 0.9114 - jacard_coef: 0.08093/9 [=========>....................] - ETA: 2s - loss: 0.1458 - accuracy: 0.9092 - jacard_coef: 0.07904/9 [============>.................] - ETA: 2s - loss: 0.1452 - accuracy: 0.9138 - jacard_coef: 0.07375/9 [===============>..............] - ETA: 1s - loss: 0.1453 - accuracy: 0.9101 - jacard_coef: 0.07366/9 [===================>..........] - ETA: 1s - loss: 0.1455 - accuracy: 0.9069 - jacard_coef: 0.07527/9 [======================>.......] - ETA: 0s - loss: 0.1455 - accuracy: 0.9059 - jacard_coef: 0.07538/9 [=========================>....] - ETA: 0s - loss: 0.1455 - accuracy: 0.9071 - jacard_coef: 0.07539/9 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9065 - jacard_coef: 0.08149/9 [==============================] - 4s 388ms/step - loss: 0.1455 - accuracy: 0.9065 - jacard_coef: 0.0814 - val_loss: 0.1591 - val_accuracy: 0.9304 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 19/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1446 - accuracy: 0.9205 - jacard_coef: 0.07292/9 [=====>........................] - ETA: 2s - loss: 0.1457 - accuracy: 0.9051 - jacard_coef: 0.08563/9 [=========>....................] - ETA: 2s - loss: 0.1447 - accuracy: 0.9173 - jacard_coef: 0.07534/9 [============>.................] - ETA: 2s - loss: 0.1445 - accuracy: 0.9203 - jacard_coef: 0.07295/9 [===============>..............] - ETA: 1s - loss: 0.1447 - accuracy: 0.9167 - jacard_coef: 0.07596/9 [===================>..........] - ETA: 1s - loss: 0.1447 - accuracy: 0.9166 - jacard_coef: 0.07617/9 [======================>.......] - ETA: 0s - loss: 0.1446 - accuracy: 0.9174 - jacard_coef: 0.07558/9 [=========================>....] - ETA: 0s - loss: 0.1445 - accuracy: 0.9178 - jacard_coef: 0.07529/9 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.9170 - jacard_coef: 0.08049/9 [==============================] - 4s 388ms/step - loss: 0.1445 - accuracy: 0.9170 - jacard_coef: 0.0804 - val_loss: 0.1585 - val_accuracy: 0.9304 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
Epoch 20/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1448 - accuracy: 0.9041 - jacard_coef: 0.08702/9 [=====>........................] - ETA: 2s - loss: 0.1452 - accuracy: 0.9000 - jacard_coef: 0.09023/9 [=========>....................] - ETA: 2s - loss: 0.1447 - accuracy: 0.9056 - jacard_coef: 0.08564/9 [============>.................] - ETA: 2s - loss: 0.1442 - accuracy: 0.9130 - jacard_coef: 0.07935/9 [===============>..............] - ETA: 1s - loss: 0.1436 - accuracy: 0.9188 - jacard_coef: 0.07446/9 [===================>..........] - ETA: 1s - loss: 0.1437 - accuracy: 0.9181 - jacard_coef: 0.07507/9 [======================>.......] - ETA: 0s - loss: 0.1436 - accuracy: 0.9190 - jacard_coef: 0.07428/9 [=========================>....] - ETA: 0s - loss: 0.1437 - accuracy: 0.9169 - jacard_coef: 0.07609/9 [==============================] - ETA: 0s - loss: 0.1437 - accuracy: 0.9173 - jacard_coef: 0.06859/9 [==============================] - 4s 389ms/step - loss: 0.1437 - accuracy: 0.9173 - jacard_coef: 0.0685 - val_loss: 0.1576 - val_accuracy: 0.9304 - val_jacard_coef: 0.0650 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0664 (epoch 10)
  Final Val Loss: 0.1576
  Training Time: 0:02:38.220425
  Stability (std): 0.0129

Results saved to: hyperparameter_optimization_20250926_123742/exp_32_Attention_ResUNet_lr1e-3_bs16/Attention_ResUNet_lr0.001_bs16_results.json

Experiment 32 completed in 194s
Progress: 32/36 completed
Estimated remaining time: 12 minutes

ðŸ”¬ EXPERIMENT 33/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 1e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.001, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758866595.273009 3364772 service.cc:145] XLA service 0x14cac18836b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758866595.273044 3364772 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758866595.652814 3364772 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 5:02 - loss: 0.3205 - accuracy: 0.4754 - jacard_coef: 0.08712/5 [===========>..................] - ETA: 46s - loss: 0.2764 - accuracy: 0.3624 - jacard_coef: 0.0849 3/5 [=================>............] - ETA: 16s - loss: 0.2570 - accuracy: 0.3323 - jacard_coef: 0.07584/5 [=======================>......] - ETA: 5s - loss: 0.2439 - accuracy: 0.2893 - jacard_coef: 0.0775 5/5 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.2882 - jacard_coef: 0.07445/5 [==============================] - 102s 7s/step - loss: 0.2435 - accuracy: 0.2882 - jacard_coef: 0.0744 - val_loss: 0.1400 - val_accuracy: 0.9299 - val_jacard_coef: 0.0629 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1965 - accuracy: 0.2142 - jacard_coef: 0.08632/5 [===========>..................] - ETA: 2s - loss: 0.1964 - accuracy: 0.2558 - jacard_coef: 0.08403/5 [=================>............] - ETA: 1s - loss: 0.1950 - accuracy: 0.2369 - jacard_coef: 0.07654/5 [=======================>......] - ETA: 0s - loss: 0.1944 - accuracy: 0.2244 - jacard_coef: 0.07735/5 [==============================] - 3s 667ms/step - loss: 0.1943 - accuracy: 0.2234 - jacard_coef: 0.0623 - val_loss: 0.4444 - val_accuracy: 0.0974 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1914 - accuracy: 0.1510 - jacard_coef: 0.08362/5 [===========>..................] - ETA: 2s - loss: 0.1906 - accuracy: 0.1393 - jacard_coef: 0.07803/5 [=================>............] - ETA: 1s - loss: 0.1907 - accuracy: 0.1352 - jacard_coef: 0.07704/5 [=======================>......] - ETA: 0s - loss: 0.1904 - accuracy: 0.1366 - jacard_coef: 0.07675/5 [==============================] - 3s 668ms/step - loss: 0.1904 - accuracy: 0.1367 - jacard_coef: 0.0738 - val_loss: 0.4188 - val_accuracy: 0.1092 - val_jacard_coef: 0.0682 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1879 - accuracy: 0.1835 - jacard_coef: 0.07342/5 [===========>..................] - ETA: 2s - loss: 0.1883 - accuracy: 0.2008 - jacard_coef: 0.07173/5 [=================>............] - ETA: 1s - loss: 0.1883 - accuracy: 0.2169 - jacard_coef: 0.07084/5 [=======================>......] - ETA: 0s - loss: 0.1883 - accuracy: 0.2418 - jacard_coef: 0.07625/5 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.2423 - jacard_coef: 0.09005/5 [==============================] - 3s 659ms/step - loss: 0.1883 - accuracy: 0.2423 - jacard_coef: 0.0900 - val_loss: 0.3088 - val_accuracy: 0.1075 - val_jacard_coef: 0.0670 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1873 - accuracy: 0.2954 - jacard_coef: 0.07772/5 [===========>..................] - ETA: 2s - loss: 0.1858 - accuracy: 0.3010 - jacard_coef: 0.07983/5 [=================>............] - ETA: 1s - loss: 0.1858 - accuracy: 0.3024 - jacard_coef: 0.07674/5 [=======================>......] - ETA: 0s - loss: 0.1860 - accuracy: 0.2973 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.2980 - jacard_coef: 0.08925/5 [==============================] - 3s 659ms/step - loss: 0.1860 - accuracy: 0.2980 - jacard_coef: 0.0892 - val_loss: 0.1933 - val_accuracy: 0.1973 - val_jacard_coef: 0.0653 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1837 - accuracy: 0.3137 - jacard_coef: 0.07102/5 [===========>..................] - ETA: 2s - loss: 0.1823 - accuracy: 0.3307 - jacard_coef: 0.07433/5 [=================>............] - ETA: 1s - loss: 0.1816 - accuracy: 0.3397 - jacard_coef: 0.07834/5 [=======================>......] - ETA: 0s - loss: 0.1812 - accuracy: 0.3467 - jacard_coef: 0.07635/5 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.3469 - jacard_coef: 0.08585/5 [==============================] - 3s 660ms/step - loss: 0.1812 - accuracy: 0.3469 - jacard_coef: 0.0858 - val_loss: 0.2372 - val_accuracy: 0.1155 - val_jacard_coef: 0.0663 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1764 - accuracy: 0.3822 - jacard_coef: 0.08142/5 [===========>..................] - ETA: 2s - loss: 0.1755 - accuracy: 0.3757 - jacard_coef: 0.07743/5 [=================>............] - ETA: 1s - loss: 0.1746 - accuracy: 0.3815 - jacard_coef: 0.07214/5 [=======================>......] - ETA: 0s - loss: 0.1739 - accuracy: 0.4161 - jacard_coef: 0.07665/5 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.4149 - jacard_coef: 0.06855/5 [==============================] - 3s 661ms/step - loss: 0.1741 - accuracy: 0.4149 - jacard_coef: 0.0685 - val_loss: 0.1525 - val_accuracy: 0.8618 - val_jacard_coef: 0.0643 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1790 - accuracy: 0.3280 - jacard_coef: 0.07302/5 [===========>..................] - ETA: 2s - loss: 0.1756 - accuracy: 0.4461 - jacard_coef: 0.07033/5 [=================>............] - ETA: 1s - loss: 0.1747 - accuracy: 0.4926 - jacard_coef: 0.07284/5 [=======================>......] - ETA: 0s - loss: 0.1745 - accuracy: 0.5086 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.5095 - jacard_coef: 0.09085/5 [==============================] - 3s 660ms/step - loss: 0.1745 - accuracy: 0.5095 - jacard_coef: 0.0908 - val_loss: 0.1095 - val_accuracy: 0.8870 - val_jacard_coef: 0.0617 - lr: 5.0000e-04
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1754 - accuracy: 0.5032 - jacard_coef: 0.08572/5 [===========>..................] - ETA: 2s - loss: 0.1755 - accuracy: 0.5025 - jacard_coef: 0.08563/5 [=================>............] - ETA: 1s - loss: 0.1754 - accuracy: 0.5029 - jacard_coef: 0.07994/5 [=======================>......] - ETA: 0s - loss: 0.1755 - accuracy: 0.5066 - jacard_coef: 0.07705/5 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.5065 - jacard_coef: 0.06205/5 [==============================] - 3s 661ms/step - loss: 0.1755 - accuracy: 0.5065 - jacard_coef: 0.0620 - val_loss: 0.0993 - val_accuracy: 0.9111 - val_jacard_coef: 0.0606 - lr: 5.0000e-04
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1734 - accuracy: 0.5536 - jacard_coef: 0.07992/5 [===========>..................] - ETA: 2s - loss: 0.1743 - accuracy: 0.5476 - jacard_coef: 0.07393/5 [=================>............] - ETA: 1s - loss: 0.1740 - accuracy: 0.5549 - jacard_coef: 0.07624/5 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.5627 - jacard_coef: 0.07645/5 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.5625 - jacard_coef: 0.08015/5 [==============================] - 3s 659ms/step - loss: 0.1741 - accuracy: 0.5625 - jacard_coef: 0.0801 - val_loss: 0.1496 - val_accuracy: 0.8604 - val_jacard_coef: 0.0645 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1713 - accuracy: 0.6273 - jacard_coef: 0.08942/5 [===========>..................] - ETA: 2s - loss: 0.1715 - accuracy: 0.6277 - jacard_coef: 0.08113/5 [=================>............] - ETA: 1s - loss: 0.1717 - accuracy: 0.6351 - jacard_coef: 0.07774/5 [=======================>......] - ETA: 0s - loss: 0.1716 - accuracy: 0.6401 - jacard_coef: 0.07705/5 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.6402 - jacard_coef: 0.06205/5 [==============================] - 3s 660ms/step - loss: 0.1716 - accuracy: 0.6402 - jacard_coef: 0.0620 - val_loss: 0.2286 - val_accuracy: 0.1294 - val_jacard_coef: 0.0665 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1699 - accuracy: 0.6780 - jacard_coef: 0.08302/5 [===========>..................] - ETA: 2s - loss: 0.1696 - accuracy: 0.6863 - jacard_coef: 0.08033/5 [=================>............] - ETA: 1s - loss: 0.1694 - accuracy: 0.6945 - jacard_coef: 0.07454/5 [=======================>......] - ETA: 0s - loss: 0.1694 - accuracy: 0.6931 - jacard_coef: 0.07585/5 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.6928 - jacard_coef: 0.09025/5 [==============================] - 3s 659ms/step - loss: 0.1694 - accuracy: 0.6928 - jacard_coef: 0.0902 - val_loss: 0.1509 - val_accuracy: 0.8601 - val_jacard_coef: 0.0644 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1678 - accuracy: 0.7154 - jacard_coef: 0.07412/5 [===========>..................] - ETA: 2s - loss: 0.1679 - accuracy: 0.7165 - jacard_coef: 0.07463/5 [=================>............] - ETA: 1s - loss: 0.1676 - accuracy: 0.7200 - jacard_coef: 0.07454/5 [=======================>......] - ETA: 0s - loss: 0.1673 - accuracy: 0.7194 - jacard_coef: 0.07665/5 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.7193 - jacard_coef: 0.07275/5 [==============================] - 3s 660ms/step - loss: 0.1673 - accuracy: 0.7193 - jacard_coef: 0.0727 - val_loss: 0.1373 - val_accuracy: 0.8885 - val_jacard_coef: 0.0641 - lr: 2.5000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0682 (epoch 3)
  Final Val Loss: 0.1373
  Training Time: 0:02:24.876700
  Stability (std): 0.0617

Results saved to: hyperparameter_optimization_20250926_123742/exp_33_Attention_ResUNet_lr1e-3_bs32/Attention_ResUNet_lr0.001_bs32_results.json

Experiment 33 completed in 181s
Progress: 33/36 completed
Estimated remaining time: 9 minutes

ðŸ”¬ EXPERIMENT 34/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-3
Batch Size: 8
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.005, Batch Size: 8, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758866766.158892 3372726 service.cc:145] XLA service 0x14ab91d63d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758866766.158950 3372726 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758866766.613744 3372726 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 1/17 [>.............................] - ETA: 16:51 - loss: 0.3471 - accuracy: 0.5247 - jacard_coef: 0.0496 2/17 [==>...........................] - ETA: 1:12 - loss: 0.3154 - accuracy: 0.5250 - jacard_coef: 0.0630  3/17 [====>.........................] - ETA: 35s - loss: 0.2929 - accuracy: 0.5632 - jacard_coef: 0.0683  4/17 [======>.......................] - ETA: 22s - loss: 0.2753 - accuracy: 0.5394 - jacard_coef: 0.0692 5/17 [=======>......................] - ETA: 16s - loss: 0.2638 - accuracy: 0.5318 - jacard_coef: 0.0687 6/17 [=========>....................] - ETA: 12s - loss: 0.2549 - accuracy: 0.5466 - jacard_coef: 0.0710 7/17 [===========>..................] - ETA: 9s - loss: 0.2479 - accuracy: 0.5496 - jacard_coef: 0.0689  8/17 [=============>................] - ETA: 7s - loss: 0.2416 - accuracy: 0.5630 - jacard_coef: 0.0698 9/17 [==============>...............] - ETA: 6s - loss: 0.2357 - accuracy: 0.5422 - jacard_coef: 0.072810/17 [================>.............] - ETA: 5s - loss: 0.2325 - accuracy: 0.5476 - jacard_coef: 0.072711/17 [==================>...........] - ETA: 4s - loss: 0.2287 - accuracy: 0.5469 - jacard_coef: 0.072812/17 [====================>.........] - ETA: 3s - loss: 0.2248 - accuracy: 0.5377 - jacard_coef: 0.073613/17 [=====================>........] - ETA: 2s - loss: 0.2212 - accuracy: 0.5307 - jacard_coef: 0.074814/17 [=======================>......] - ETA: 1s - loss: 0.2183 - accuracy: 0.5202 - jacard_coef: 0.075315/17 [=========================>....] - ETA: 1s - loss: 0.2156 - accuracy: 0.5188 - jacard_coef: 0.076216/17 [===========================>..] - ETA: 0s - loss: 0.2130 - accuracy: 0.5171 - jacard_coef: 0.076317/17 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.5164 - jacard_coef: 0.079017/17 [==============================] - 79s 1s/step - loss: 0.2131 - accuracy: 0.5164 - jacard_coef: 0.0790 - val_loss: 1.1758 - val_accuracy: 0.9270 - val_jacard_coef: 0.0034 - lr: 0.0010
Epoch 2/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1792 - accuracy: 0.6694 - jacard_coef: 0.0658 2/17 [==>...........................] - ETA: 3s - loss: 0.1793 - accuracy: 0.6344 - jacard_coef: 0.0681 3/17 [====>.........................] - ETA: 2s - loss: 0.1784 - accuracy: 0.5827 - jacard_coef: 0.0754 4/17 [======>.......................] - ETA: 2s - loss: 0.1774 - accuracy: 0.5498 - jacard_coef: 0.0821 5/17 [=======>......................] - ETA: 2s - loss: 0.1765 - accuracy: 0.5254 - jacard_coef: 0.0804 6/17 [=========>....................] - ETA: 2s - loss: 0.1761 - accuracy: 0.4984 - jacard_coef: 0.0782 7/17 [===========>..................] - ETA: 2s - loss: 0.1758 - accuracy: 0.4790 - jacard_coef: 0.0742 8/17 [=============>................] - ETA: 1s - loss: 0.1762 - accuracy: 0.4603 - jacard_coef: 0.0726 9/17 [==============>...............] - ETA: 1s - loss: 0.1761 - accuracy: 0.4542 - jacard_coef: 0.073010/17 [================>.............] - ETA: 1s - loss: 0.1763 - accuracy: 0.4422 - jacard_coef: 0.070111/17 [==================>...........] - ETA: 1s - loss: 0.1767 - accuracy: 0.4344 - jacard_coef: 0.071912/17 [====================>.........] - ETA: 1s - loss: 0.1768 - accuracy: 0.4321 - jacard_coef: 0.076013/17 [=====================>........] - ETA: 0s - loss: 0.1768 - accuracy: 0.4290 - jacard_coef: 0.075014/17 [=======================>......] - ETA: 0s - loss: 0.1768 - accuracy: 0.4222 - jacard_coef: 0.073115/17 [=========================>....] - ETA: 0s - loss: 0.1768 - accuracy: 0.4175 - jacard_coef: 0.075516/17 [===========================>..] - ETA: 0s - loss: 0.1768 - accuracy: 0.4101 - jacard_coef: 0.076717/17 [==============================] - 4s 209ms/step - loss: 0.1768 - accuracy: 0.4086 - jacard_coef: 0.0728 - val_loss: 0.9886 - val_accuracy: 0.9274 - val_jacard_coef: 0.0016 - lr: 0.0010
Epoch 3/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1733 - accuracy: 0.3880 - jacard_coef: 0.1023 2/17 [==>...........................] - ETA: 3s - loss: 0.1736 - accuracy: 0.3728 - jacard_coef: 0.1099 3/17 [====>.........................] - ETA: 2s - loss: 0.1730 - accuracy: 0.3868 - jacard_coef: 0.0986 4/17 [======>.......................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4098 - jacard_coef: 0.0927 5/17 [=======>......................] - ETA: 2s - loss: 0.1726 - accuracy: 0.4242 - jacard_coef: 0.0904 6/17 [=========>....................] - ETA: 2s - loss: 0.1725 - accuracy: 0.4442 - jacard_coef: 0.0921 7/17 [===========>..................] - ETA: 2s - loss: 0.1721 - accuracy: 0.4681 - jacard_coef: 0.0879 8/17 [=============>................] - ETA: 1s - loss: 0.1717 - accuracy: 0.4912 - jacard_coef: 0.0833 9/17 [==============>...............] - ETA: 1s - loss: 0.1714 - accuracy: 0.5020 - jacard_coef: 0.081510/17 [================>.............] - ETA: 1s - loss: 0.1712 - accuracy: 0.5218 - jacard_coef: 0.081811/17 [==================>...........] - ETA: 1s - loss: 0.1712 - accuracy: 0.5345 - jacard_coef: 0.081712/17 [====================>.........] - ETA: 1s - loss: 0.1712 - accuracy: 0.5398 - jacard_coef: 0.082813/17 [=====================>........] - ETA: 0s - loss: 0.1712 - accuracy: 0.5446 - jacard_coef: 0.078614/17 [=======================>......] - ETA: 0s - loss: 0.1714 - accuracy: 0.5449 - jacard_coef: 0.080115/17 [=========================>....] - ETA: 0s - loss: 0.1713 - accuracy: 0.5475 - jacard_coef: 0.077616/17 [===========================>..] - ETA: 0s - loss: 0.1713 - accuracy: 0.5533 - jacard_coef: 0.076317/17 [==============================] - 4s 214ms/step - loss: 0.1713 - accuracy: 0.5530 - jacard_coef: 0.0739 - val_loss: 0.0893 - val_accuracy: 0.9209 - val_jacard_coef: 0.0586 - lr: 0.0010
Epoch 4/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1682 - accuracy: 0.7366 - jacard_coef: 0.0720 2/17 [==>...........................] - ETA: 3s - loss: 0.1685 - accuracy: 0.7483 - jacard_coef: 0.0704 3/17 [====>.........................] - ETA: 2s - loss: 0.1680 - accuracy: 0.7457 - jacard_coef: 0.0699 4/17 [======>.......................] - ETA: 2s - loss: 0.1677 - accuracy: 0.7358 - jacard_coef: 0.0785 5/17 [=======>......................] - ETA: 2s - loss: 0.1677 - accuracy: 0.7316 - jacard_coef: 0.0706 6/17 [=========>....................] - ETA: 2s - loss: 0.1672 - accuracy: 0.7297 - jacard_coef: 0.0685 7/17 [===========>..................] - ETA: 2s - loss: 0.1668 - accuracy: 0.7269 - jacard_coef: 0.0687 8/17 [=============>................] - ETA: 1s - loss: 0.1666 - accuracy: 0.7226 - jacard_coef: 0.0700 9/17 [==============>...............] - ETA: 1s - loss: 0.1665 - accuracy: 0.7211 - jacard_coef: 0.072110/17 [================>.............] - ETA: 1s - loss: 0.1662 - accuracy: 0.7226 - jacard_coef: 0.074411/17 [==================>...........] - ETA: 1s - loss: 0.1660 - accuracy: 0.7239 - jacard_coef: 0.076612/17 [====================>.........] - ETA: 1s - loss: 0.1655 - accuracy: 0.7343 - jacard_coef: 0.073413/17 [=====================>........] - ETA: 0s - loss: 0.1652 - accuracy: 0.7389 - jacard_coef: 0.075214/17 [=======================>......] - ETA: 0s - loss: 0.1648 - accuracy: 0.7423 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 0s - loss: 0.1645 - accuracy: 0.7413 - jacard_coef: 0.075316/17 [===========================>..] - ETA: 0s - loss: 0.1644 - accuracy: 0.7383 - jacard_coef: 0.075517/17 [==============================] - 4s 214ms/step - loss: 0.1644 - accuracy: 0.7373 - jacard_coef: 0.0792 - val_loss: 0.1205 - val_accuracy: 0.9152 - val_jacard_coef: 0.0619 - lr: 0.0010
Epoch 5/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1712 - accuracy: 0.7294 - jacard_coef: 0.0970 2/17 [==>...........................] - ETA: 3s - loss: 0.1698 - accuracy: 0.7296 - jacard_coef: 0.0785 3/17 [====>.........................] - ETA: 2s - loss: 0.1715 - accuracy: 0.6592 - jacard_coef: 0.0695 4/17 [======>.......................] - ETA: 2s - loss: 0.1704 - accuracy: 0.6757 - jacard_coef: 0.0747 5/17 [=======>......................] - ETA: 2s - loss: 0.1688 - accuracy: 0.7160 - jacard_coef: 0.0709 6/17 [=========>....................] - ETA: 2s - loss: 0.1677 - accuracy: 0.7431 - jacard_coef: 0.0714 7/17 [===========>..................] - ETA: 2s - loss: 0.1668 - accuracy: 0.7654 - jacard_coef: 0.0691 8/17 [=============>................] - ETA: 1s - loss: 0.1661 - accuracy: 0.7813 - jacard_coef: 0.0675 9/17 [==============>...............] - ETA: 1s - loss: 0.1654 - accuracy: 0.7918 - jacard_coef: 0.069910/17 [================>.............] - ETA: 1s - loss: 0.1649 - accuracy: 0.8055 - jacard_coef: 0.068111/17 [==================>...........] - ETA: 1s - loss: 0.1645 - accuracy: 0.8118 - jacard_coef: 0.071112/17 [====================>.........] - ETA: 1s - loss: 0.1643 - accuracy: 0.8146 - jacard_coef: 0.074613/17 [=====================>........] - ETA: 0s - loss: 0.1638 - accuracy: 0.8232 - jacard_coef: 0.073714/17 [=======================>......] - ETA: 0s - loss: 0.1637 - accuracy: 0.8252 - jacard_coef: 0.077115/17 [=========================>....] - ETA: 0s - loss: 0.1633 - accuracy: 0.8320 - jacard_coef: 0.076316/17 [===========================>..] - ETA: 0s - loss: 0.1629 - accuracy: 0.8381 - jacard_coef: 0.074617/17 [==============================] - 4s 214ms/step - loss: 0.1629 - accuracy: 0.8374 - jacard_coef: 0.0801 - val_loss: 0.1363 - val_accuracy: 0.9207 - val_jacard_coef: 0.0627 - lr: 0.0010
Epoch 6/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1552 - accuracy: 0.9127 - jacard_coef: 0.0677 2/17 [==>...........................] - ETA: 3s - loss: 0.1566 - accuracy: 0.8894 - jacard_coef: 0.0856 3/17 [====>.........................] - ETA: 2s - loss: 0.1562 - accuracy: 0.9017 - jacard_coef: 0.0740 4/17 [======>.......................] - ETA: 2s - loss: 0.1646 - accuracy: 0.7916 - jacard_coef: 0.0778 5/17 [=======>......................] - ETA: 2s - loss: 0.1628 - accuracy: 0.8148 - jacard_coef: 0.0736 6/17 [=========>....................] - ETA: 2s - loss: 0.1617 - accuracy: 0.8251 - jacard_coef: 0.0741 7/17 [===========>..................] - ETA: 2s - loss: 0.1609 - accuracy: 0.8381 - jacard_coef: 0.0687 8/17 [=============>................] - ETA: 1s - loss: 0.1602 - accuracy: 0.8446 - jacard_coef: 0.0688 9/17 [==============>...............] - ETA: 1s - loss: 0.1599 - accuracy: 0.8461 - jacard_coef: 0.069510/17 [================>.............] - ETA: 1s - loss: 0.1598 - accuracy: 0.8439 - jacard_coef: 0.075111/17 [==================>...........] - ETA: 1s - loss: 0.1596 - accuracy: 0.8470 - jacard_coef: 0.075512/17 [====================>.........] - ETA: 1s - loss: 0.1592 - accuracy: 0.8513 - jacard_coef: 0.075613/17 [=====================>........] - ETA: 0s - loss: 0.1590 - accuracy: 0.8550 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1589 - accuracy: 0.8566 - jacard_coef: 0.078415/17 [=========================>....] - ETA: 0s - loss: 0.1585 - accuracy: 0.8633 - jacard_coef: 0.075616/17 [===========================>..] - ETA: 0s - loss: 0.1582 - accuracy: 0.8644 - jacard_coef: 0.075017/17 [==============================] - 4s 210ms/step - loss: 0.1582 - accuracy: 0.8640 - jacard_coef: 0.0795 - val_loss: 0.1169 - val_accuracy: 0.9304 - val_jacard_coef: 0.0623 - lr: 0.0010
Epoch 7/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1552 - accuracy: 0.9213 - jacard_coef: 0.0667 2/17 [==>...........................] - ETA: 3s - loss: 0.1557 - accuracy: 0.9036 - jacard_coef: 0.0803 3/17 [====>.........................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9149 - jacard_coef: 0.0703 4/17 [======>.......................] - ETA: 2s - loss: 0.1542 - accuracy: 0.9156 - jacard_coef: 0.0689 5/17 [=======>......................] - ETA: 2s - loss: 0.1541 - accuracy: 0.9129 - jacard_coef: 0.0711 6/17 [=========>....................] - ETA: 2s - loss: 0.1540 - accuracy: 0.9100 - jacard_coef: 0.0715 7/17 [===========>..................] - ETA: 2s - loss: 0.1538 - accuracy: 0.9091 - jacard_coef: 0.0692 8/17 [=============>................] - ETA: 1s - loss: 0.1541 - accuracy: 0.8985 - jacard_coef: 0.0753 9/17 [==============>...............] - ETA: 1s - loss: 0.1539 - accuracy: 0.8966 - jacard_coef: 0.075610/17 [================>.............] - ETA: 1s - loss: 0.1537 - accuracy: 0.8944 - jacard_coef: 0.075811/17 [==================>...........] - ETA: 1s - loss: 0.1536 - accuracy: 0.8940 - jacard_coef: 0.076512/17 [====================>.........] - ETA: 1s - loss: 0.1534 - accuracy: 0.8957 - jacard_coef: 0.075913/17 [=====================>........] - ETA: 0s - loss: 0.1533 - accuracy: 0.8951 - jacard_coef: 0.076214/17 [=======================>......] - ETA: 0s - loss: 0.1531 - accuracy: 0.8973 - jacard_coef: 0.075615/17 [=========================>....] - ETA: 0s - loss: 0.1530 - accuracy: 0.8976 - jacard_coef: 0.075516/17 [===========================>..] - ETA: 0s - loss: 0.1528 - accuracy: 0.8993 - jacard_coef: 0.075017/17 [==============================] - 4s 214ms/step - loss: 0.1528 - accuracy: 0.8989 - jacard_coef: 0.0783 - val_loss: 0.1399 - val_accuracy: 0.9304 - val_jacard_coef: 0.0633 - lr: 0.0010
Epoch 8/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1505 - accuracy: 0.9091 - jacard_coef: 0.0815 2/17 [==>...........................] - ETA: 3s - loss: 0.1490 - accuracy: 0.9244 - jacard_coef: 0.0689 3/17 [====>.........................] - ETA: 2s - loss: 0.1496 - accuracy: 0.9157 - jacard_coef: 0.0762 4/17 [======>.......................] - ETA: 2s - loss: 0.1491 - accuracy: 0.9187 - jacard_coef: 0.0738 5/17 [=======>......................] - ETA: 2s - loss: 0.1497 - accuracy: 0.9114 - jacard_coef: 0.0800 6/17 [=========>....................] - ETA: 2s - loss: 0.1497 - accuracy: 0.9043 - jacard_coef: 0.0798 7/17 [===========>..................] - ETA: 2s - loss: 0.1495 - accuracy: 0.9092 - jacard_coef: 0.0763 8/17 [=============>................] - ETA: 1s - loss: 0.1493 - accuracy: 0.9154 - jacard_coef: 0.0714 9/17 [==============>...............] - ETA: 1s - loss: 0.1495 - accuracy: 0.9127 - jacard_coef: 0.074110/17 [================>.............] - ETA: 1s - loss: 0.1496 - accuracy: 0.9120 - jacard_coef: 0.075011/17 [==================>...........] - ETA: 1s - loss: 0.1495 - accuracy: 0.9142 - jacard_coef: 0.073512/17 [====================>.........] - ETA: 1s - loss: 0.1495 - accuracy: 0.9133 - jacard_coef: 0.074513/17 [=====================>........] - ETA: 0s - loss: 0.1497 - accuracy: 0.9075 - jacard_coef: 0.079414/17 [=======================>......] - ETA: 0s - loss: 0.1495 - accuracy: 0.9101 - jacard_coef: 0.077515/17 [=========================>....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9131 - jacard_coef: 0.075116/17 [===========================>..] - ETA: 0s - loss: 0.1492 - accuracy: 0.9132 - jacard_coef: 0.075317/17 [==============================] - 4s 218ms/step - loss: 0.1492 - accuracy: 0.9133 - jacard_coef: 0.0748 - val_loss: 0.1556 - val_accuracy: 0.9304 - val_jacard_coef: 0.0636 - lr: 0.0010
Epoch 9/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1470 - accuracy: 0.9155 - jacard_coef: 0.0726 2/17 [==>...........................] - ETA: 3s - loss: 0.1473 - accuracy: 0.9085 - jacard_coef: 0.0802 3/17 [====>.........................] - ETA: 2s - loss: 0.1464 - accuracy: 0.9016 - jacard_coef: 0.0766 4/17 [======>.......................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9098 - jacard_coef: 0.0728 5/17 [=======>......................] - ETA: 2s - loss: 0.1462 - accuracy: 0.9164 - jacard_coef: 0.0689 6/17 [=========>....................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9192 - jacard_coef: 0.0665 7/17 [===========>..................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9187 - jacard_coef: 0.0677 8/17 [=============>................] - ETA: 1s - loss: 0.1464 - accuracy: 0.9101 - jacard_coef: 0.0710 9/17 [==============>...............] - ETA: 1s - loss: 0.1463 - accuracy: 0.9126 - jacard_coef: 0.070010/17 [================>.............] - ETA: 1s - loss: 0.1462 - accuracy: 0.9061 - jacard_coef: 0.071411/17 [==================>...........] - ETA: 1s - loss: 0.1460 - accuracy: 0.9074 - jacard_coef: 0.071412/17 [====================>.........] - ETA: 1s - loss: 0.1461 - accuracy: 0.9050 - jacard_coef: 0.074513/17 [=====================>........] - ETA: 0s - loss: 0.1483 - accuracy: 0.8826 - jacard_coef: 0.077014/17 [=======================>......] - ETA: 0s - loss: 0.1482 - accuracy: 0.8843 - jacard_coef: 0.077215/17 [=========================>....] - ETA: 0s - loss: 0.1484 - accuracy: 0.8843 - jacard_coef: 0.078416/17 [===========================>..] - ETA: 0s - loss: 0.1482 - accuracy: 0.8886 - jacard_coef: 0.075817/17 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.8889 - jacard_coef: 0.074917/17 [==============================] - 4s 215ms/step - loss: 0.1483 - accuracy: 0.8889 - jacard_coef: 0.0749 - val_loss: 0.1635 - val_accuracy: 0.9217 - val_jacard_coef: 0.0634 - lr: 0.0010
Epoch 10/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1497 - accuracy: 0.9096 - jacard_coef: 0.0753 2/17 [==>...........................] - ETA: 3s - loss: 0.1484 - accuracy: 0.9260 - jacard_coef: 0.0618 3/17 [====>.........................] - ETA: 2s - loss: 0.1487 - accuracy: 0.9183 - jacard_coef: 0.0666 4/17 [======>.......................] - ETA: 2s - loss: 0.1492 - accuracy: 0.9075 - jacard_coef: 0.0748 5/17 [=======>......................] - ETA: 2s - loss: 0.1499 - accuracy: 0.9054 - jacard_coef: 0.0772 6/17 [=========>....................] - ETA: 2s - loss: 0.1493 - accuracy: 0.9131 - jacard_coef: 0.0713 7/17 [===========>..................] - ETA: 2s - loss: 0.1492 - accuracy: 0.9146 - jacard_coef: 0.0708 8/17 [=============>................] - ETA: 1s - loss: 0.1494 - accuracy: 0.9122 - jacard_coef: 0.0732 9/17 [==============>...............] - ETA: 1s - loss: 0.1496 - accuracy: 0.9064 - jacard_coef: 0.078310/17 [================>.............] - ETA: 1s - loss: 0.1498 - accuracy: 0.9041 - jacard_coef: 0.080511/17 [==================>...........] - ETA: 1s - loss: 0.1497 - accuracy: 0.9040 - jacard_coef: 0.081012/17 [====================>.........] - ETA: 1s - loss: 0.1493 - accuracy: 0.9064 - jacard_coef: 0.079113/17 [=====================>........] - ETA: 0s - loss: 0.1491 - accuracy: 0.9067 - jacard_coef: 0.078514/17 [=======================>......] - ETA: 0s - loss: 0.1488 - accuracy: 0.9074 - jacard_coef: 0.078115/17 [=========================>....] - ETA: 0s - loss: 0.1486 - accuracy: 0.9084 - jacard_coef: 0.076316/17 [===========================>..] - ETA: 0s - loss: 0.1484 - accuracy: 0.9087 - jacard_coef: 0.075317/17 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9088 - jacard_coef: 0.074017/17 [==============================] - 4s 226ms/step - loss: 0.1484 - accuracy: 0.9088 - jacard_coef: 0.0740 - val_loss: 0.2470 - val_accuracy: 0.2309 - val_jacard_coef: 0.0657 - lr: 0.0010
Epoch 11/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1479 - accuracy: 0.8701 - jacard_coef: 0.0994 2/17 [==>...........................] - ETA: 3s - loss: 0.1475 - accuracy: 0.8830 - jacard_coef: 0.0889 3/17 [====>.........................] - ETA: 2s - loss: 0.1460 - accuracy: 0.8981 - jacard_coef: 0.0779 4/17 [======>.......................] - ETA: 2s - loss: 0.1456 - accuracy: 0.9045 - jacard_coef: 0.0757 5/17 [=======>......................] - ETA: 2s - loss: 0.1456 - accuracy: 0.9054 - jacard_coef: 0.0769 6/17 [=========>....................] - ETA: 2s - loss: 0.1456 - accuracy: 0.9049 - jacard_coef: 0.0786 7/17 [===========>..................] - ETA: 2s - loss: 0.1453 - accuracy: 0.9056 - jacard_coef: 0.0789 8/17 [=============>................] - ETA: 1s - loss: 0.1454 - accuracy: 0.9032 - jacard_coef: 0.0816 9/17 [==============>...............] - ETA: 1s - loss: 0.1452 - accuracy: 0.9045 - jacard_coef: 0.081110/17 [================>.............] - ETA: 1s - loss: 0.1453 - accuracy: 0.9019 - jacard_coef: 0.083611/17 [==================>...........] - ETA: 1s - loss: 0.1450 - accuracy: 0.9022 - jacard_coef: 0.083812/17 [====================>.........] - ETA: 1s - loss: 0.1445 - accuracy: 0.9077 - jacard_coef: 0.079313/17 [=====================>........] - ETA: 0s - loss: 0.1446 - accuracy: 0.9075 - jacard_coef: 0.079814/17 [=======================>......] - ETA: 0s - loss: 0.1442 - accuracy: 0.9098 - jacard_coef: 0.078115/17 [=========================>....] - ETA: 0s - loss: 0.1439 - accuracy: 0.9120 - jacard_coef: 0.076416/17 [===========================>..] - ETA: 0s - loss: 0.1434 - accuracy: 0.9142 - jacard_coef: 0.074717/17 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9137 - jacard_coef: 0.077817/17 [==============================] - 4s 228ms/step - loss: 0.1434 - accuracy: 0.9137 - jacard_coef: 0.0778 - val_loss: 0.2952 - val_accuracy: 0.1703 - val_jacard_coef: 0.0661 - lr: 0.0010
Epoch 12/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1434 - accuracy: 0.8810 - jacard_coef: 0.0993 2/17 [==>...........................] - ETA: 3s - loss: 0.1412 - accuracy: 0.9023 - jacard_coef: 0.0860 3/17 [====>.........................] - ETA: 2s - loss: 0.1414 - accuracy: 0.9059 - jacard_coef: 0.0818 4/17 [======>.......................] - ETA: 2s - loss: 0.1407 - accuracy: 0.9113 - jacard_coef: 0.0751 5/17 [=======>......................] - ETA: 2s - loss: 0.1403 - accuracy: 0.9108 - jacard_coef: 0.0731 6/17 [=========>....................] - ETA: 2s - loss: 0.1401 - accuracy: 0.9104 - jacard_coef: 0.0735 7/17 [===========>..................] - ETA: 2s - loss: 0.1396 - accuracy: 0.9114 - jacard_coef: 0.0721 8/17 [=============>................] - ETA: 1s - loss: 0.1397 - accuracy: 0.9096 - jacard_coef: 0.0732 9/17 [==============>...............] - ETA: 1s - loss: 0.1396 - accuracy: 0.9085 - jacard_coef: 0.074410/17 [================>.............] - ETA: 1s - loss: 0.1399 - accuracy: 0.9054 - jacard_coef: 0.077611/17 [==================>...........] - ETA: 1s - loss: 0.1397 - accuracy: 0.9056 - jacard_coef: 0.077712/17 [====================>.........] - ETA: 1s - loss: 0.1392 - accuracy: 0.9084 - jacard_coef: 0.075813/17 [=====================>........] - ETA: 0s - loss: 0.1390 - accuracy: 0.9100 - jacard_coef: 0.074814/17 [=======================>......] - ETA: 0s - loss: 0.1386 - accuracy: 0.9126 - jacard_coef: 0.072815/17 [=========================>....] - ETA: 0s - loss: 0.1385 - accuracy: 0.9119 - jacard_coef: 0.073516/17 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.9105 - jacard_coef: 0.074817/17 [==============================] - 4s 215ms/step - loss: 0.1386 - accuracy: 0.9098 - jacard_coef: 0.0790 - val_loss: 0.2203 - val_accuracy: 0.2431 - val_jacard_coef: 0.0651 - lr: 0.0010
Epoch 13/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1321 - accuracy: 0.9474 - jacard_coef: 0.0481 2/17 [==>...........................] - ETA: 3s - loss: 0.1359 - accuracy: 0.8792 - jacard_coef: 0.0809 3/17 [====>.........................] - ETA: 2s - loss: 0.1367 - accuracy: 0.8611 - jacard_coef: 0.0818 4/17 [======>.......................] - ETA: 2s - loss: 0.1369 - accuracy: 0.8682 - jacard_coef: 0.0860 5/17 [=======>......................] - ETA: 2s - loss: 0.1364 - accuracy: 0.8792 - jacard_coef: 0.0828 6/17 [=========>....................] - ETA: 2s - loss: 0.1365 - accuracy: 0.8825 - jacard_coef: 0.0839 7/17 [===========>..................] - ETA: 2s - loss: 0.1361 - accuracy: 0.8891 - jacard_coef: 0.0813 8/17 [=============>................] - ETA: 1s - loss: 0.1364 - accuracy: 0.8880 - jacard_coef: 0.0843 9/17 [==============>...............] - ETA: 1s - loss: 0.1359 - accuracy: 0.8940 - jacard_coef: 0.080910/17 [================>.............] - ETA: 1s - loss: 0.1357 - accuracy: 0.8964 - jacard_coef: 0.080311/17 [==================>...........] - ETA: 1s - loss: 0.1351 - accuracy: 0.9037 - jacard_coef: 0.075012/17 [====================>.........] - ETA: 1s - loss: 0.1353 - accuracy: 0.9035 - jacard_coef: 0.076113/17 [=====================>........] - ETA: 0s - loss: 0.1353 - accuracy: 0.9031 - jacard_coef: 0.077214/17 [=======================>......] - ETA: 0s - loss: 0.1350 - accuracy: 0.9050 - jacard_coef: 0.076315/17 [=========================>....] - ETA: 0s - loss: 0.1349 - accuracy: 0.9063 - jacard_coef: 0.075916/17 [===========================>..] - ETA: 0s - loss: 0.1347 - accuracy: 0.9076 - jacard_coef: 0.075317/17 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9082 - jacard_coef: 0.071917/17 [==============================] - 4s 215ms/step - loss: 0.1346 - accuracy: 0.9082 - jacard_coef: 0.0719 - val_loss: 0.1835 - val_accuracy: 0.3434 - val_jacard_coef: 0.0643 - lr: 0.0010
Epoch 14/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1335 - accuracy: 0.9119 - jacard_coef: 0.0797 2/17 [==>...........................] - ETA: 3s - loss: 0.1358 - accuracy: 0.8924 - jacard_coef: 0.0947 3/17 [====>.........................] - ETA: 2s - loss: 0.1357 - accuracy: 0.8906 - jacard_coef: 0.0911 4/17 [======>.......................] - ETA: 2s - loss: 0.1341 - accuracy: 0.9032 - jacard_coef: 0.0819 5/17 [=======>......................] - ETA: 2s - loss: 0.1335 - accuracy: 0.9074 - jacard_coef: 0.0793 6/17 [=========>....................] - ETA: 2s - loss: 0.1334 - accuracy: 0.9063 - jacard_coef: 0.0809 7/17 [===========>..................] - ETA: 2s - loss: 0.1331 - accuracy: 0.9079 - jacard_coef: 0.0800 8/17 [=============>................] - ETA: 1s - loss: 0.1332 - accuracy: 0.9068 - jacard_coef: 0.0812 9/17 [==============>...............] - ETA: 1s - loss: 0.1323 - accuracy: 0.9127 - jacard_coef: 0.076410/17 [================>.............] - ETA: 1s - loss: 0.1317 - accuracy: 0.9168 - jacard_coef: 0.073211/17 [==================>...........] - ETA: 1s - loss: 0.1313 - accuracy: 0.9195 - jacard_coef: 0.071012/17 [====================>.........] - ETA: 1s - loss: 0.1333 - accuracy: 0.8989 - jacard_coef: 0.073113/17 [=====================>........] - ETA: 0s - loss: 0.1333 - accuracy: 0.8993 - jacard_coef: 0.074114/17 [=======================>......] - ETA: 0s - loss: 0.1333 - accuracy: 0.9007 - jacard_coef: 0.074015/17 [=========================>....] - ETA: 0s - loss: 0.1332 - accuracy: 0.9022 - jacard_coef: 0.073716/17 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.9010 - jacard_coef: 0.075417/17 [==============================] - 4s 215ms/step - loss: 0.1334 - accuracy: 0.9014 - jacard_coef: 0.0731 - val_loss: 0.0843 - val_accuracy: 0.9304 - val_jacard_coef: 0.0575 - lr: 0.0010
Epoch 15/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1341 - accuracy: 0.9141 - jacard_coef: 0.0783 2/17 [==>...........................] - ETA: 3s - loss: 0.1358 - accuracy: 0.9073 - jacard_coef: 0.0833 3/17 [====>.........................] - ETA: 2s - loss: 0.1350 - accuracy: 0.9135 - jacard_coef: 0.0784 4/17 [======>.......................] - ETA: 2s - loss: 0.1357 - accuracy: 0.9097 - jacard_coef: 0.0816 5/17 [=======>......................] - ETA: 2s - loss: 0.1362 - accuracy: 0.9082 - jacard_coef: 0.0826 6/17 [=========>....................] - ETA: 2s - loss: 0.1360 - accuracy: 0.9084 - jacard_coef: 0.0825 7/17 [===========>..................] - ETA: 2s - loss: 0.1363 - accuracy: 0.9042 - jacard_coef: 0.0859 8/17 [=============>................] - ETA: 1s - loss: 0.1362 - accuracy: 0.9019 - jacard_coef: 0.0878 9/17 [==============>...............] - ETA: 1s - loss: 0.1353 - accuracy: 0.9083 - jacard_coef: 0.082410/17 [================>.............] - ETA: 1s - loss: 0.1349 - accuracy: 0.9101 - jacard_coef: 0.081011/17 [==================>...........] - ETA: 1s - loss: 0.1350 - accuracy: 0.9088 - jacard_coef: 0.082012/17 [====================>.........] - ETA: 1s - loss: 0.1348 - accuracy: 0.9104 - jacard_coef: 0.080713/17 [=====================>........] - ETA: 0s - loss: 0.1343 - accuracy: 0.9132 - jacard_coef: 0.078314/17 [=======================>......] - ETA: 0s - loss: 0.1340 - accuracy: 0.9153 - jacard_coef: 0.076415/17 [=========================>....] - ETA: 0s - loss: 0.1338 - accuracy: 0.9163 - jacard_coef: 0.075416/17 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.9174 - jacard_coef: 0.074417/17 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9170 - jacard_coef: 0.077217/17 [==============================] - 4s 215ms/step - loss: 0.1335 - accuracy: 0.9170 - jacard_coef: 0.0772 - val_loss: 0.1451 - val_accuracy: 0.9267 - val_jacard_coef: 0.0571 - lr: 0.0010
Epoch 16/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1329 - accuracy: 0.9092 - jacard_coef: 0.0806 2/17 [==>...........................] - ETA: 3s - loss: 0.1314 - accuracy: 0.9173 - jacard_coef: 0.0739 3/17 [====>.........................] - ETA: 2s - loss: 0.1298 - accuracy: 0.9287 - jacard_coef: 0.0643 4/17 [======>.......................] - ETA: 2s - loss: 0.1313 - accuracy: 0.9208 - jacard_coef: 0.0710 5/17 [=======>......................] - ETA: 2s - loss: 0.1310 - accuracy: 0.9212 - jacard_coef: 0.0709 6/17 [=========>....................] - ETA: 2s - loss: 0.1308 - accuracy: 0.9217 - jacard_coef: 0.0706 7/17 [===========>..................] - ETA: 2s - loss: 0.1313 - accuracy: 0.9163 - jacard_coef: 0.0750 8/17 [=============>................] - ETA: 1s - loss: 0.1310 - accuracy: 0.9185 - jacard_coef: 0.0733 9/17 [==============>...............] - ETA: 1s - loss: 0.1306 - accuracy: 0.9203 - jacard_coef: 0.071810/17 [================>.............] - ETA: 1s - loss: 0.1302 - accuracy: 0.9226 - jacard_coef: 0.069911/17 [==================>...........] - ETA: 1s - loss: 0.1305 - accuracy: 0.9194 - jacard_coef: 0.072612/17 [====================>.........] - ETA: 1s - loss: 0.1306 - accuracy: 0.9185 - jacard_coef: 0.073413/17 [=====================>........] - ETA: 0s - loss: 0.1306 - accuracy: 0.9185 - jacard_coef: 0.073414/17 [=======================>......] - ETA: 0s - loss: 0.1305 - accuracy: 0.9184 - jacard_coef: 0.073515/17 [=========================>....] - ETA: 0s - loss: 0.1307 - accuracy: 0.9165 - jacard_coef: 0.075116/17 [===========================>..] - ETA: 0s - loss: 0.1305 - accuracy: 0.9173 - jacard_coef: 0.074517/17 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9169 - jacard_coef: 0.076917/17 [==============================] - 4s 215ms/step - loss: 0.1305 - accuracy: 0.9169 - jacard_coef: 0.0769 - val_loss: 0.1001 - val_accuracy: 0.9279 - val_jacard_coef: 0.0612 - lr: 0.0010
Epoch 17/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1275 - accuracy: 0.9245 - jacard_coef: 0.0692 2/17 [==>...........................] - ETA: 3s - loss: 0.1278 - accuracy: 0.9229 - jacard_coef: 0.0704 3/17 [====>.........................] - ETA: 2s - loss: 0.1293 - accuracy: 0.9125 - jacard_coef: 0.0789 4/17 [======>.......................] - ETA: 2s - loss: 0.1286 - accuracy: 0.9198 - jacard_coef: 0.0728 5/17 [=======>......................] - ETA: 2s - loss: 0.1290 - accuracy: 0.9155 - jacard_coef: 0.0764 6/17 [=========>....................] - ETA: 2s - loss: 0.1296 - accuracy: 0.9118 - jacard_coef: 0.0794 7/17 [===========>..................] - ETA: 2s - loss: 0.1296 - accuracy: 0.9105 - jacard_coef: 0.0805 8/17 [=============>................] - ETA: 1s - loss: 0.1296 - accuracy: 0.9114 - jacard_coef: 0.0798 9/17 [==============>...............] - ETA: 1s - loss: 0.1291 - accuracy: 0.9143 - jacard_coef: 0.077410/17 [================>.............] - ETA: 1s - loss: 0.1289 - accuracy: 0.9152 - jacard_coef: 0.076611/17 [==================>...........] - ETA: 1s - loss: 0.1287 - accuracy: 0.9154 - jacard_coef: 0.076612/17 [====================>.........] - ETA: 1s - loss: 0.1283 - accuracy: 0.9183 - jacard_coef: 0.074113/17 [=====================>........] - ETA: 0s - loss: 0.1283 - accuracy: 0.9181 - jacard_coef: 0.074314/17 [=======================>......] - ETA: 0s - loss: 0.1283 - accuracy: 0.9177 - jacard_coef: 0.074615/17 [=========================>....] - ETA: 0s - loss: 0.1282 - accuracy: 0.9182 - jacard_coef: 0.074216/17 [===========================>..] - ETA: 0s - loss: 0.1284 - accuracy: 0.9166 - jacard_coef: 0.075517/17 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9172 - jacard_coef: 0.071217/17 [==============================] - 4s 215ms/step - loss: 0.1283 - accuracy: 0.9172 - jacard_coef: 0.0712 - val_loss: 0.1140 - val_accuracy: 0.9282 - val_jacard_coef: 0.0618 - lr: 5.0000e-04
Epoch 18/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1280 - accuracy: 0.9110 - jacard_coef: 0.0802 2/17 [==>...........................] - ETA: 3s - loss: 0.1292 - accuracy: 0.9063 - jacard_coef: 0.0840 3/17 [====>.........................] - ETA: 2s - loss: 0.1285 - accuracy: 0.9082 - jacard_coef: 0.0825 4/17 [======>.......................] - ETA: 2s - loss: 0.1280 - accuracy: 0.9101 - jacard_coef: 0.0811 5/17 [=======>......................] - ETA: 2s - loss: 0.1275 - accuracy: 0.9121 - jacard_coef: 0.0794 6/17 [=========>....................] - ETA: 2s - loss: 0.1277 - accuracy: 0.9118 - jacard_coef: 0.0796 7/17 [===========>..................] - ETA: 2s - loss: 0.1281 - accuracy: 0.9082 - jacard_coef: 0.0825 8/17 [=============>................] - ETA: 1s - loss: 0.1278 - accuracy: 0.9102 - jacard_coef: 0.0809 9/17 [==============>...............] - ETA: 1s - loss: 0.1275 - accuracy: 0.9125 - jacard_coef: 0.078910/17 [================>.............] - ETA: 1s - loss: 0.1276 - accuracy: 0.9115 - jacard_coef: 0.079811/17 [==================>...........] - ETA: 1s - loss: 0.1272 - accuracy: 0.9145 - jacard_coef: 0.077212/17 [====================>.........] - ETA: 1s - loss: 0.1276 - accuracy: 0.9120 - jacard_coef: 0.079213/17 [=====================>........] - ETA: 0s - loss: 0.1279 - accuracy: 0.9103 - jacard_coef: 0.080614/17 [=======================>......] - ETA: 0s - loss: 0.1273 - accuracy: 0.9143 - jacard_coef: 0.077115/17 [=========================>....] - ETA: 0s - loss: 0.1269 - accuracy: 0.9160 - jacard_coef: 0.075716/17 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9170 - jacard_coef: 0.074917/17 [==============================] - 4s 215ms/step - loss: 0.1269 - accuracy: 0.9172 - jacard_coef: 0.0738 - val_loss: 0.1178 - val_accuracy: 0.9277 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 19/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1235 - accuracy: 0.9396 - jacard_coef: 0.0562 2/17 [==>...........................] - ETA: 3s - loss: 0.1238 - accuracy: 0.9346 - jacard_coef: 0.0603 3/17 [====>.........................] - ETA: 3s - loss: 0.1265 - accuracy: 0.9152 - jacard_coef: 0.0760 4/17 [======>.......................] - ETA: 2s - loss: 0.1257 - accuracy: 0.9204 - jacard_coef: 0.0717 5/17 [=======>......................] - ETA: 2s - loss: 0.1257 - accuracy: 0.9208 - jacard_coef: 0.0715 6/17 [=========>....................] - ETA: 2s - loss: 0.1262 - accuracy: 0.9145 - jacard_coef: 0.0767 7/17 [===========>..................] - ETA: 2s - loss: 0.1271 - accuracy: 0.9113 - jacard_coef: 0.0793 8/17 [=============>................] - ETA: 1s - loss: 0.1274 - accuracy: 0.9081 - jacard_coef: 0.0819 9/17 [==============>...............] - ETA: 1s - loss: 0.1271 - accuracy: 0.9095 - jacard_coef: 0.080810/17 [================>.............] - ETA: 1s - loss: 0.1271 - accuracy: 0.9080 - jacard_coef: 0.082111/17 [==================>...........] - ETA: 1s - loss: 0.1267 - accuracy: 0.9106 - jacard_coef: 0.080012/17 [====================>.........] - ETA: 1s - loss: 0.1262 - accuracy: 0.9141 - jacard_coef: 0.077013/17 [=====================>........] - ETA: 0s - loss: 0.1259 - accuracy: 0.9150 - jacard_coef: 0.076414/17 [=======================>......] - ETA: 0s - loss: 0.1256 - accuracy: 0.9170 - jacard_coef: 0.074715/17 [=========================>....] - ETA: 0s - loss: 0.1254 - accuracy: 0.9179 - jacard_coef: 0.073916/17 [===========================>..] - ETA: 0s - loss: 0.1255 - accuracy: 0.9167 - jacard_coef: 0.075017/17 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9171 - jacard_coef: 0.072217/17 [==============================] - 4s 215ms/step - loss: 0.1254 - accuracy: 0.9171 - jacard_coef: 0.0722 - val_loss: 0.1212 - val_accuracy: 0.9273 - val_jacard_coef: 0.0625 - lr: 5.0000e-04
Epoch 20/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1248 - accuracy: 0.9269 - jacard_coef: 0.0669 2/17 [==>...........................] - ETA: 3s - loss: 0.1259 - accuracy: 0.9138 - jacard_coef: 0.0778 3/17 [====>.........................] - ETA: 2s - loss: 0.1246 - accuracy: 0.9187 - jacard_coef: 0.0739 4/17 [======>.......................] - ETA: 2s - loss: 0.1252 - accuracy: 0.9177 - jacard_coef: 0.0746 5/17 [=======>......................] - ETA: 2s - loss: 0.1244 - accuracy: 0.9210 - jacard_coef: 0.0718 6/17 [=========>....................] - ETA: 2s - loss: 0.1242 - accuracy: 0.9233 - jacard_coef: 0.0699 7/17 [===========>..................] - ETA: 2s - loss: 0.1243 - accuracy: 0.9207 - jacard_coef: 0.0721 8/17 [=============>................] - ETA: 1s - loss: 0.1241 - accuracy: 0.9216 - jacard_coef: 0.0713 9/17 [==============>...............] - ETA: 1s - loss: 0.1245 - accuracy: 0.9173 - jacard_coef: 0.074710/17 [================>.............] - ETA: 1s - loss: 0.1241 - accuracy: 0.9195 - jacard_coef: 0.072911/17 [==================>...........] - ETA: 1s - loss: 0.1242 - accuracy: 0.9181 - jacard_coef: 0.074212/17 [====================>.........] - ETA: 1s - loss: 0.1239 - accuracy: 0.9194 - jacard_coef: 0.073013/17 [=====================>........] - ETA: 0s - loss: 0.1236 - accuracy: 0.9204 - jacard_coef: 0.072314/17 [=======================>......] - ETA: 0s - loss: 0.1239 - accuracy: 0.9175 - jacard_coef: 0.074615/17 [=========================>....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9173 - jacard_coef: 0.074816/17 [===========================>..] - ETA: 0s - loss: 0.1238 - accuracy: 0.9179 - jacard_coef: 0.074317/17 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9172 - jacard_coef: 0.078517/17 [==============================] - 4s 216ms/step - loss: 0.1239 - accuracy: 0.9172 - jacard_coef: 0.0785 - val_loss: 0.1233 - val_accuracy: 0.9274 - val_jacard_coef: 0.0626 - lr: 5.0000e-04
Epoch 21/30
 1/17 [>.............................] - ETA: 3s - loss: 0.1267 - accuracy: 0.8876 - jacard_coef: 0.0994 2/17 [==>...........................] - ETA: 3s - loss: 0.1234 - accuracy: 0.9073 - jacard_coef: 0.0838 3/17 [====>.........................] - ETA: 3s - loss: 0.1227 - accuracy: 0.9136 - jacard_coef: 0.0786 4/17 [======>.......................] - ETA: 2s - loss: 0.1221 - accuracy: 0.9202 - jacard_coef: 0.0729 5/17 [=======>......................] - ETA: 2s - loss: 0.1209 - accuracy: 0.9293 - jacard_coef: 0.0649 6/17 [=========>....................] - ETA: 2s - loss: 0.1208 - accuracy: 0.9288 - jacard_coef: 0.0655 7/17 [===========>..................] - ETA: 2s - loss: 0.1211 - accuracy: 0.9285 - jacard_coef: 0.0658 8/17 [=============>................] - ETA: 1s - loss: 0.1213 - accuracy: 0.9271 - jacard_coef: 0.0669 9/17 [==============>...............] - ETA: 1s - loss: 0.1222 - accuracy: 0.9202 - jacard_coef: 0.072410/17 [================>.............] - ETA: 1s - loss: 0.1229 - accuracy: 0.9160 - jacard_coef: 0.075611/17 [==================>...........] - ETA: 1s - loss: 0.1226 - accuracy: 0.9181 - jacard_coef: 0.073912/17 [====================>.........] - ETA: 1s - loss: 0.1227 - accuracy: 0.9181 - jacard_coef: 0.073913/17 [=====================>........] - ETA: 0s - loss: 0.1222 - accuracy: 0.9204 - jacard_coef: 0.072014/17 [=======================>......] - ETA: 0s - loss: 0.1223 - accuracy: 0.9200 - jacard_coef: 0.072315/17 [=========================>....] - ETA: 0s - loss: 0.1226 - accuracy: 0.9176 - jacard_coef: 0.074316/17 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9175 - jacard_coef: 0.074517/17 [==============================] - ETA: 0s - loss: 0.1226 - accuracy: 0.9173 - jacard_coef: 0.075417/17 [==============================] - 4s 215ms/step - loss: 0.1226 - accuracy: 0.9173 - jacard_coef: 0.0754 - val_loss: 0.1241 - val_accuracy: 0.9272 - val_jacard_coef: 0.0627 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0661 (epoch 11)
  Final Val Loss: 0.1241
  Training Time: 0:02:33.642789
  Stability (std): 0.0383

Results saved to: hyperparameter_optimization_20250926_123742/exp_34_Attention_ResUNet_lr5e-3_bs8/Attention_ResUNet_lr0.005_bs8_results.json

Experiment 34 completed in 188s
Progress: 34/36 completed
Estimated remaining time: 6 minutes

ðŸ”¬ EXPERIMENT 35/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-3
Batch Size: 16
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.005, Batch Size: 16, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758866959.889523 3380612 service.cc:145] XLA service 0x14cebdee8f10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758866959.889564 3380612 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758866960.342263 3380612 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/9 [==>...........................] - ETA: 9:07 - loss: 0.3377 - accuracy: 0.4868 - jacard_coef: 0.07302/9 [=====>........................] - ETA: 59s - loss: 0.2915 - accuracy: 0.4271 - jacard_coef: 0.0745 3/9 [=========>....................] - ETA: 26s - loss: 0.2627 - accuracy: 0.3674 - jacard_coef: 0.07034/9 [============>.................] - ETA: 15s - loss: 0.2468 - accuracy: 0.3282 - jacard_coef: 0.07315/9 [===============>..............] - ETA: 9s - loss: 0.2370 - accuracy: 0.3084 - jacard_coef: 0.0747 6/9 [===================>..........] - ETA: 6s - loss: 0.2304 - accuracy: 0.2850 - jacard_coef: 0.07327/9 [======================>.......] - ETA: 3s - loss: 0.2255 - accuracy: 0.2643 - jacard_coef: 0.07408/9 [=========================>....] - ETA: 1s - loss: 0.2205 - accuracy: 0.2515 - jacard_coef: 0.07719/9 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.2509 - jacard_coef: 0.07449/9 [==============================] - 89s 3s/step - loss: 0.2202 - accuracy: 0.2509 - jacard_coef: 0.0744 - val_loss: 1.1188 - val_accuracy: 0.9244 - val_jacard_coef: 0.0031 - lr: 0.0010
Epoch 2/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1876 - accuracy: 0.1547 - jacard_coef: 0.08592/9 [=====>........................] - ETA: 2s - loss: 0.1888 - accuracy: 0.1468 - jacard_coef: 0.08223/9 [=========>....................] - ETA: 2s - loss: 0.1878 - accuracy: 0.1457 - jacard_coef: 0.08004/9 [============>.................] - ETA: 2s - loss: 0.1869 - accuracy: 0.1440 - jacard_coef: 0.07575/9 [===============>..............] - ETA: 1s - loss: 0.1860 - accuracy: 0.1568 - jacard_coef: 0.08226/9 [===================>..........] - ETA: 1s - loss: 0.1856 - accuracy: 0.1636 - jacard_coef: 0.08157/9 [======================>.......] - ETA: 0s - loss: 0.1850 - accuracy: 0.1721 - jacard_coef: 0.07838/9 [=========================>....] - ETA: 0s - loss: 0.1839 - accuracy: 0.1908 - jacard_coef: 0.07649/9 [==============================] - 4s 389ms/step - loss: 0.1840 - accuracy: 0.1917 - jacard_coef: 0.0787 - val_loss: 14.8358 - val_accuracy: 0.0770 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 3/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1838 - accuracy: 0.3773 - jacard_coef: 0.07702/9 [=====>........................] - ETA: 2s - loss: 0.1807 - accuracy: 0.3940 - jacard_coef: 0.07813/9 [=========>....................] - ETA: 2s - loss: 0.1790 - accuracy: 0.3928 - jacard_coef: 0.07604/9 [============>.................] - ETA: 2s - loss: 0.1781 - accuracy: 0.4029 - jacard_coef: 0.07495/9 [===============>..............] - ETA: 1s - loss: 0.1802 - accuracy: 0.4043 - jacard_coef: 0.08306/9 [===================>..........] - ETA: 1s - loss: 0.1793 - accuracy: 0.4351 - jacard_coef: 0.07777/9 [======================>.......] - ETA: 0s - loss: 0.1787 - accuracy: 0.4573 - jacard_coef: 0.07718/9 [=========================>....] - ETA: 0s - loss: 0.1784 - accuracy: 0.4662 - jacard_coef: 0.07649/9 [==============================] - 4s 389ms/step - loss: 0.1784 - accuracy: 0.4663 - jacard_coef: 0.0756 - val_loss: 6.4550 - val_accuracy: 0.1578 - val_jacard_coef: 0.0713 - lr: 0.0010
Epoch 4/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1774 - accuracy: 0.3674 - jacard_coef: 0.09582/9 [=====>........................] - ETA: 2s - loss: 0.1761 - accuracy: 0.3275 - jacard_coef: 0.09213/9 [=========>....................] - ETA: 2s - loss: 0.1759 - accuracy: 0.3037 - jacard_coef: 0.08824/9 [============>.................] - ETA: 2s - loss: 0.1755 - accuracy: 0.3064 - jacard_coef: 0.07895/9 [===============>..............] - ETA: 1s - loss: 0.1753 - accuracy: 0.3266 - jacard_coef: 0.07756/9 [===================>..........] - ETA: 1s - loss: 0.1749 - accuracy: 0.3620 - jacard_coef: 0.07697/9 [======================>.......] - ETA: 0s - loss: 0.1746 - accuracy: 0.3970 - jacard_coef: 0.07638/9 [=========================>....] - ETA: 0s - loss: 0.1743 - accuracy: 0.4260 - jacard_coef: 0.07579/9 [==============================] - 3s 379ms/step - loss: 0.1744 - accuracy: 0.4267 - jacard_coef: 0.0834 - val_loss: 12.3765 - val_accuracy: 0.0771 - val_jacard_coef: 0.0698 - lr: 0.0010
Epoch 5/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1701 - accuracy: 0.6625 - jacard_coef: 0.09912/9 [=====>........................] - ETA: 2s - loss: 0.1696 - accuracy: 0.6547 - jacard_coef: 0.06533/9 [=========>....................] - ETA: 2s - loss: 0.1751 - accuracy: 0.6091 - jacard_coef: 0.07014/9 [============>.................] - ETA: 2s - loss: 0.1745 - accuracy: 0.5782 - jacard_coef: 0.07065/9 [===============>..............] - ETA: 1s - loss: 0.1738 - accuracy: 0.5476 - jacard_coef: 0.07526/9 [===================>..........] - ETA: 1s - loss: 0.1730 - accuracy: 0.5375 - jacard_coef: 0.07727/9 [======================>.......] - ETA: 0s - loss: 0.1723 - accuracy: 0.5354 - jacard_coef: 0.07368/9 [=========================>....] - ETA: 0s - loss: 0.1718 - accuracy: 0.5379 - jacard_coef: 0.07549/9 [==============================] - 3s 379ms/step - loss: 0.1718 - accuracy: 0.5385 - jacard_coef: 0.0836 - val_loss: 0.1367 - val_accuracy: 0.9263 - val_jacard_coef: 0.0502 - lr: 0.0010
Epoch 6/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1658 - accuracy: 0.6576 - jacard_coef: 0.07722/9 [=====>........................] - ETA: 2s - loss: 0.1658 - accuracy: 0.6208 - jacard_coef: 0.07503/9 [=========>....................] - ETA: 2s - loss: 0.1656 - accuracy: 0.6468 - jacard_coef: 0.07944/9 [============>.................] - ETA: 2s - loss: 0.1652 - accuracy: 0.6642 - jacard_coef: 0.08335/9 [===============>..............] - ETA: 1s - loss: 0.1650 - accuracy: 0.6981 - jacard_coef: 0.07896/9 [===================>..........] - ETA: 1s - loss: 0.1647 - accuracy: 0.7309 - jacard_coef: 0.07717/9 [======================>.......] - ETA: 0s - loss: 0.1644 - accuracy: 0.7531 - jacard_coef: 0.07678/9 [=========================>....] - ETA: 0s - loss: 0.1642 - accuracy: 0.7725 - jacard_coef: 0.07679/9 [==============================] - 3s 381ms/step - loss: 0.1647 - accuracy: 0.7697 - jacard_coef: 0.0684 - val_loss: 0.1052 - val_accuracy: 0.9177 - val_jacard_coef: 0.0520 - lr: 0.0010
Epoch 7/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1631 - accuracy: 0.8979 - jacard_coef: 0.08772/9 [=====>........................] - ETA: 2s - loss: 0.1635 - accuracy: 0.9051 - jacard_coef: 0.07743/9 [=========>....................] - ETA: 2s - loss: 0.1648 - accuracy: 0.8896 - jacard_coef: 0.08294/9 [============>.................] - ETA: 2s - loss: 0.1654 - accuracy: 0.8799 - jacard_coef: 0.08185/9 [===============>..............] - ETA: 1s - loss: 0.1651 - accuracy: 0.8826 - jacard_coef: 0.08006/9 [===================>..........] - ETA: 1s - loss: 0.1647 - accuracy: 0.8835 - jacard_coef: 0.08047/9 [======================>.......] - ETA: 0s - loss: 0.1645 - accuracy: 0.8850 - jacard_coef: 0.08018/9 [=========================>....] - ETA: 0s - loss: 0.1641 - accuracy: 0.8916 - jacard_coef: 0.07549/9 [==============================] - 3s 380ms/step - loss: 0.1641 - accuracy: 0.8908 - jacard_coef: 0.0831 - val_loss: 0.8079 - val_accuracy: 0.9252 - val_jacard_coef: 0.0107 - lr: 0.0010
Epoch 8/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1621 - accuracy: 0.8873 - jacard_coef: 0.06972/9 [=====>........................] - ETA: 2s - loss: 0.1625 - accuracy: 0.8875 - jacard_coef: 0.06603/9 [=========>....................] - ETA: 2s - loss: 0.1625 - accuracy: 0.8825 - jacard_coef: 0.06934/9 [============>.................] - ETA: 2s - loss: 0.1620 - accuracy: 0.8860 - jacard_coef: 0.06775/9 [===============>..............] - ETA: 1s - loss: 0.1619 - accuracy: 0.8803 - jacard_coef: 0.07396/9 [===================>..........] - ETA: 1s - loss: 0.1615 - accuracy: 0.8850 - jacard_coef: 0.07227/9 [======================>.......] - ETA: 0s - loss: 0.1614 - accuracy: 0.8851 - jacard_coef: 0.07498/9 [=========================>....] - ETA: 0s - loss: 0.1612 - accuracy: 0.8866 - jacard_coef: 0.07559/9 [==============================] - 3s 379ms/step - loss: 0.1612 - accuracy: 0.8865 - jacard_coef: 0.0781 - val_loss: 0.5502 - val_accuracy: 0.9232 - val_jacard_coef: 0.0114 - lr: 0.0010
Epoch 9/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1620 - accuracy: 0.7262 - jacard_coef: 0.07932/9 [=====>........................] - ETA: 2s - loss: 0.1606 - accuracy: 0.8140 - jacard_coef: 0.08203/9 [=========>....................] - ETA: 2s - loss: 0.1601 - accuracy: 0.8472 - jacard_coef: 0.07964/9 [============>.................] - ETA: 2s - loss: 0.1597 - accuracy: 0.8652 - jacard_coef: 0.07735/9 [===============>..............] - ETA: 1s - loss: 0.1597 - accuracy: 0.8722 - jacard_coef: 0.07916/9 [===================>..........] - ETA: 1s - loss: 0.1594 - accuracy: 0.8823 - jacard_coef: 0.07557/9 [======================>.......] - ETA: 0s - loss: 0.1596 - accuracy: 0.8840 - jacard_coef: 0.07768/9 [=========================>....] - ETA: 0s - loss: 0.1597 - accuracy: 0.8885 - jacard_coef: 0.07659/9 [==============================] - 3s 379ms/step - loss: 0.1597 - accuracy: 0.8892 - jacard_coef: 0.0682 - val_loss: 0.1475 - val_accuracy: 0.8564 - val_jacard_coef: 0.0651 - lr: 5.0000e-04
Epoch 10/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1607 - accuracy: 0.9101 - jacard_coef: 0.07812/9 [=====>........................] - ETA: 2s - loss: 0.1600 - accuracy: 0.9130 - jacard_coef: 0.07603/9 [=========>....................] - ETA: 2s - loss: 0.1596 - accuracy: 0.9147 - jacard_coef: 0.07494/9 [============>.................] - ETA: 2s - loss: 0.1592 - accuracy: 0.9147 - jacard_coef: 0.07535/9 [===============>..............] - ETA: 1s - loss: 0.1590 - accuracy: 0.9146 - jacard_coef: 0.07566/9 [===================>..........] - ETA: 1s - loss: 0.1587 - accuracy: 0.9180 - jacard_coef: 0.07287/9 [======================>.......] - ETA: 0s - loss: 0.1586 - accuracy: 0.9161 - jacard_coef: 0.07478/9 [=========================>....] - ETA: 0s - loss: 0.1585 - accuracy: 0.9155 - jacard_coef: 0.07539/9 [==============================] - 3s 379ms/step - loss: 0.1585 - accuracy: 0.9148 - jacard_coef: 0.0826 - val_loss: 0.1597 - val_accuracy: 0.9304 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 11/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1567 - accuracy: 0.9264 - jacard_coef: 0.06712/9 [=====>........................] - ETA: 2s - loss: 0.1573 - accuracy: 0.9089 - jacard_coef: 0.08203/9 [=========>....................] - ETA: 2s - loss: 0.1570 - accuracy: 0.9157 - jacard_coef: 0.07634/9 [============>.................] - ETA: 2s - loss: 0.1570 - accuracy: 0.9160 - jacard_coef: 0.07625/9 [===============>..............] - ETA: 1s - loss: 0.1571 - accuracy: 0.9119 - jacard_coef: 0.07966/9 [===================>..........] - ETA: 1s - loss: 0.1569 - accuracy: 0.9169 - jacard_coef: 0.07537/9 [======================>.......] - ETA: 0s - loss: 0.1568 - accuracy: 0.9159 - jacard_coef: 0.07638/9 [=========================>....] - ETA: 0s - loss: 0.1567 - accuracy: 0.9170 - jacard_coef: 0.07549/9 [==============================] - 3s 379ms/step - loss: 0.1567 - accuracy: 0.9166 - jacard_coef: 0.0807 - val_loss: 0.1576 - val_accuracy: 0.9304 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 12/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1560 - accuracy: 0.9231 - jacard_coef: 0.07072/9 [=====>........................] - ETA: 2s - loss: 0.1565 - accuracy: 0.9091 - jacard_coef: 0.08253/9 [=========>....................] - ETA: 2s - loss: 0.1564 - accuracy: 0.9088 - jacard_coef: 0.08274/9 [============>.................] - ETA: 2s - loss: 0.1561 - accuracy: 0.9146 - jacard_coef: 0.07795/9 [===============>..............] - ETA: 1s - loss: 0.1558 - accuracy: 0.9212 - jacard_coef: 0.07226/9 [===================>..........] - ETA: 1s - loss: 0.1560 - accuracy: 0.9154 - jacard_coef: 0.07707/9 [======================>.......] - ETA: 0s - loss: 0.1559 - accuracy: 0.9161 - jacard_coef: 0.07658/9 [=========================>....] - ETA: 0s - loss: 0.1558 - accuracy: 0.9169 - jacard_coef: 0.07599/9 [==============================] - 3s 379ms/step - loss: 0.1558 - accuracy: 0.9171 - jacard_coef: 0.0733 - val_loss: 0.1576 - val_accuracy: 0.9304 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
Epoch 13/30
1/9 [==>...........................] - ETA: 3s - loss: 0.1550 - accuracy: 0.9221 - jacard_coef: 0.07182/9 [=====>........................] - ETA: 2s - loss: 0.1548 - accuracy: 0.9260 - jacard_coef: 0.06853/9 [=========>....................] - ETA: 2s - loss: 0.1546 - accuracy: 0.9294 - jacard_coef: 0.06544/9 [============>.................] - ETA: 2s - loss: 0.1549 - accuracy: 0.9214 - jacard_coef: 0.07225/9 [===============>..............] - ETA: 1s - loss: 0.1547 - accuracy: 0.9241 - jacard_coef: 0.06996/9 [===================>..........] - ETA: 1s - loss: 0.1546 - accuracy: 0.9252 - jacard_coef: 0.06907/9 [======================>.......] - ETA: 0s - loss: 0.1548 - accuracy: 0.9202 - jacard_coef: 0.07328/9 [=========================>....] - ETA: 0s - loss: 0.1548 - accuracy: 0.9177 - jacard_coef: 0.07539/9 [==============================] - 3s 380ms/step - loss: 0.1548 - accuracy: 0.9172 - jacard_coef: 0.0804 - val_loss: 0.1585 - val_accuracy: 0.9304 - val_jacard_coef: 0.0648 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0713 (epoch 3)
  Final Val Loss: 0.1585
  Training Time: 0:02:11.451091
  Stability (std): 3.6402

Results saved to: hyperparameter_optimization_20250926_123742/exp_35_Attention_ResUNet_lr5e-3_bs16/Attention_ResUNet_lr0.005_bs16_results.json

Experiment 35 completed in 167s
Progress: 35/36 completed
Estimated remaining time: 2 minutes

ðŸ”¬ EXPERIMENT 36/36
================================================
Architecture: Attention_ResUNet
Learning Rate: 5e-3
Batch Size: 32
Epochs: 30

Starting training...
âœ“ GPU memory growth enabled for 1 GPUs
Loading dataset...
Loaded 144 images and 144 masks
Training set: (129, 256, 256, 3), Validation set: (15, 256, 256, 3)

============================================================
TRAINING: Attention_ResUNet
Learning Rate: 0.005, Batch Size: 32, Epochs: 30
============================================================
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
WARNING:absl:`lr` is deprecated in TF-Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
None
âœ“ focal_loss imported successfully
Epoch 1/30
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1758867131.537429 3388192 service.cc:145] XLA service 0x1470b4caad40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1758867131.537465 3388192 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1758867131.920205 3388192 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/5 [=====>........................] - ETA: 5:00 - loss: 0.3428 - accuracy: 0.5329 - jacard_coef: 0.06732/5 [===========>..................] - ETA: 46s - loss: 0.3138 - accuracy: 0.4507 - jacard_coef: 0.0744 3/5 [=================>............] - ETA: 16s - loss: 0.2853 - accuracy: 0.3991 - jacard_coef: 0.07554/5 [=======================>......] - ETA: 5s - loss: 0.2655 - accuracy: 0.3536 - jacard_coef: 0.0768 5/5 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.3525 - jacard_coef: 0.08825/5 [==============================] - 102s 7s/step - loss: 0.2649 - accuracy: 0.3525 - jacard_coef: 0.0882 - val_loss: 0.2587 - val_accuracy: 0.9304 - val_jacard_coef: 0.0276 - lr: 0.0010
Epoch 2/30
1/5 [=====>........................] - ETA: 3s - loss: 0.2012 - accuracy: 0.1831 - jacard_coef: 0.06842/5 [===========>..................] - ETA: 2s - loss: 0.1983 - accuracy: 0.1932 - jacard_coef: 0.07463/5 [=================>............] - ETA: 1s - loss: 0.1961 - accuracy: 0.1995 - jacard_coef: 0.07634/5 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.2025 - jacard_coef: 0.07665/5 [==============================] - 3s 648ms/step - loss: 0.1943 - accuracy: 0.2031 - jacard_coef: 0.0817 - val_loss: 0.7679 - val_accuracy: 0.9254 - val_jacard_coef: 0.0031 - lr: 0.0010
Epoch 3/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1865 - accuracy: 0.2210 - jacard_coef: 0.08252/5 [===========>..................] - ETA: 2s - loss: 0.1860 - accuracy: 0.2786 - jacard_coef: 0.07383/5 [=================>............] - ETA: 1s - loss: 0.1850 - accuracy: 0.3213 - jacard_coef: 0.07444/5 [=======================>......] - ETA: 0s - loss: 0.1839 - accuracy: 0.3393 - jacard_coef: 0.07705/5 [==============================] - 3s 646ms/step - loss: 0.1839 - accuracy: 0.3389 - jacard_coef: 0.0630 - val_loss: 1.1414 - val_accuracy: 0.9162 - val_jacard_coef: 0.0070 - lr: 0.0010
Epoch 4/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1792 - accuracy: 0.3010 - jacard_coef: 0.07302/5 [===========>..................] - ETA: 2s - loss: 0.1778 - accuracy: 0.3318 - jacard_coef: 0.07623/5 [=================>............] - ETA: 1s - loss: 0.1768 - accuracy: 0.3860 - jacard_coef: 0.07304/5 [=======================>......] - ETA: 0s - loss: 0.1762 - accuracy: 0.4313 - jacard_coef: 0.07675/5 [==============================] - 3s 662ms/step - loss: 0.1761 - accuracy: 0.4321 - jacard_coef: 0.0676 - val_loss: 6.9929 - val_accuracy: 0.1526 - val_jacard_coef: 0.0699 - lr: 0.0010
Epoch 5/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1747 - accuracy: 0.5982 - jacard_coef: 0.08032/5 [===========>..................] - ETA: 2s - loss: 0.1752 - accuracy: 0.5175 - jacard_coef: 0.07843/5 [=================>............] - ETA: 1s - loss: 0.1754 - accuracy: 0.4569 - jacard_coef: 0.07754/5 [=======================>......] - ETA: 0s - loss: 0.1758 - accuracy: 0.4056 - jacard_coef: 0.07665/5 [==============================] - 3s 648ms/step - loss: 0.1758 - accuracy: 0.4047 - jacard_coef: 0.0717 - val_loss: 1.2144 - val_accuracy: 0.9152 - val_jacard_coef: 0.0042 - lr: 0.0010
Epoch 6/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1773 - accuracy: 0.3240 - jacard_coef: 0.07312/5 [===========>..................] - ETA: 2s - loss: 0.1771 - accuracy: 0.3981 - jacard_coef: 0.08153/5 [=================>............] - ETA: 1s - loss: 0.1770 - accuracy: 0.4488 - jacard_coef: 0.07764/5 [=======================>......] - ETA: 0s - loss: 0.1768 - accuracy: 0.4823 - jacard_coef: 0.07645/5 [==============================] - 3s 648ms/step - loss: 0.1768 - accuracy: 0.4822 - jacard_coef: 0.0688 - val_loss: 1.2488 - val_accuracy: 0.8605 - val_jacard_coef: 0.0511 - lr: 0.0010
Epoch 7/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1740 - accuracy: 0.5759 - jacard_coef: 0.07222/5 [===========>..................] - ETA: 2s - loss: 0.1737 - accuracy: 0.5538 - jacard_coef: 0.07483/5 [=================>............] - ETA: 1s - loss: 0.1735 - accuracy: 0.5410 - jacard_coef: 0.07944/5 [=======================>......] - ETA: 0s - loss: 0.1732 - accuracy: 0.5390 - jacard_coef: 0.07585/5 [==============================] - 3s 646ms/step - loss: 0.1732 - accuracy: 0.5378 - jacard_coef: 0.0893 - val_loss: 1.1084 - val_accuracy: 0.9302 - val_jacard_coef: 1.8650e-04 - lr: 0.0010
Epoch 8/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1752 - accuracy: 0.4278 - jacard_coef: 0.07492/5 [===========>..................] - ETA: 2s - loss: 0.1741 - accuracy: 0.4569 - jacard_coef: 0.07443/5 [=================>............] - ETA: 1s - loss: 0.1832 - accuracy: 0.4520 - jacard_coef: 0.07724/5 [=======================>......] - ETA: 0s - loss: 0.1855 - accuracy: 0.4616 - jacard_coef: 0.07765/5 [==============================] - 3s 647ms/step - loss: 0.1854 - accuracy: 0.4616 - jacard_coef: 0.0669 - val_loss: 0.9809 - val_accuracy: 0.9074 - val_jacard_coef: 0.0103 - lr: 0.0010
Epoch 9/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1782 - accuracy: 0.4506 - jacard_coef: 0.07892/5 [===========>..................] - ETA: 2s - loss: 0.1764 - accuracy: 0.4621 - jacard_coef: 0.07663/5 [=================>............] - ETA: 1s - loss: 0.1753 - accuracy: 0.4583 - jacard_coef: 0.07814/5 [=======================>......] - ETA: 0s - loss: 0.1745 - accuracy: 0.4693 - jacard_coef: 0.07655/5 [==============================] - 3s 649ms/step - loss: 0.1748 - accuracy: 0.4674 - jacard_coef: 0.0688 - val_loss: 1.1152 - val_accuracy: 0.9300 - val_jacard_coef: 2.0497e-05 - lr: 0.0010
Epoch 10/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1831 - accuracy: 0.4121 - jacard_coef: 0.07762/5 [===========>..................] - ETA: 2s - loss: 0.1774 - accuracy: 0.5093 - jacard_coef: 0.07623/5 [=================>............] - ETA: 1s - loss: 0.1756 - accuracy: 0.5503 - jacard_coef: 0.07824/5 [=======================>......] - ETA: 0s - loss: 0.1737 - accuracy: 0.5853 - jacard_coef: 0.07615/5 [==============================] - 3s 649ms/step - loss: 0.1739 - accuracy: 0.5838 - jacard_coef: 0.0656 - val_loss: 1.0301 - val_accuracy: 0.9303 - val_jacard_coef: 2.4800e-04 - lr: 5.0000e-04
Epoch 11/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1692 - accuracy: 0.7474 - jacard_coef: 0.07012/5 [===========>..................] - ETA: 2s - loss: 0.1689 - accuracy: 0.7651 - jacard_coef: 0.06503/5 [=================>............] - ETA: 1s - loss: 0.1689 - accuracy: 0.7561 - jacard_coef: 0.07424/5 [=======================>......] - ETA: 0s - loss: 0.1690 - accuracy: 0.7399 - jacard_coef: 0.07555/5 [==============================] - 3s 649ms/step - loss: 0.1692 - accuracy: 0.7365 - jacard_coef: 0.0864 - val_loss: 0.9156 - val_accuracy: 0.9270 - val_jacard_coef: 0.0063 - lr: 5.0000e-04
Epoch 12/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1682 - accuracy: 0.7295 - jacard_coef: 0.07322/5 [===========>..................] - ETA: 2s - loss: 0.1679 - accuracy: 0.7346 - jacard_coef: 0.07923/5 [=================>............] - ETA: 1s - loss: 0.1675 - accuracy: 0.7404 - jacard_coef: 0.07884/5 [=======================>......] - ETA: 0s - loss: 0.1675 - accuracy: 0.7455 - jacard_coef: 0.07675/5 [==============================] - 3s 652ms/step - loss: 0.1676 - accuracy: 0.7438 - jacard_coef: 0.0629 - val_loss: 0.2387 - val_accuracy: 0.9045 - val_jacard_coef: 0.0621 - lr: 5.0000e-04
Epoch 13/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1668 - accuracy: 0.7820 - jacard_coef: 0.07912/5 [===========>..................] - ETA: 2s - loss: 0.1666 - accuracy: 0.7905 - jacard_coef: 0.08283/5 [=================>............] - ETA: 1s - loss: 0.1664 - accuracy: 0.8043 - jacard_coef: 0.07784/5 [=======================>......] - ETA: 0s - loss: 0.1660 - accuracy: 0.8075 - jacard_coef: 0.07565/5 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.8068 - jacard_coef: 0.08715/5 [==============================] - 3s 659ms/step - loss: 0.1660 - accuracy: 0.8068 - jacard_coef: 0.0871 - val_loss: 0.1351 - val_accuracy: 0.9199 - val_jacard_coef: 0.0616 - lr: 5.0000e-04
Epoch 14/30
1/5 [=====>........................] - ETA: 3s - loss: 0.1648 - accuracy: 0.8240 - jacard_coef: 0.08462/5 [===========>..................] - ETA: 2s - loss: 0.1644 - accuracy: 0.8136 - jacard_coef: 0.08293/5 [=================>............] - ETA: 1s - loss: 0.1644 - accuracy: 0.8116 - jacard_coef: 0.08134/5 [=======================>......] - ETA: 0s - loss: 0.1644 - accuracy: 0.8034 - jacard_coef: 0.07625/5 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.8008 - jacard_coef: 0.07355/5 [==============================] - 3s 660ms/step - loss: 0.1647 - accuracy: 0.8008 - jacard_coef: 0.0735 - val_loss: 0.1457 - val_accuracy: 0.9235 - val_jacard_coef: 0.0646 - lr: 5.0000e-04
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(

âœ“ Training completed successfully!
  Best Val Jaccard: 0.0699 (epoch 4)
  Final Val Loss: 0.1457
  Training Time: 0:02:27.327660
  Stability (std): 0.4301

Results saved to: hyperparameter_optimization_20250926_123742/exp_36_Attention_ResUNet_lr5e-3_bs32/Attention_ResUNet_lr0.005_bs32_results.json

Experiment 36 completed in 183s
Progress: 36/36 completed
Estimated remaining time: 0 minutes

ðŸ“Š GENERATING RESULTS ANALYSIS
==============================
Running hyperparameter analysis...
No successful experiments found!

ðŸ“‹ HYPERPARAMETER OPTIMIZATION COMPLETE
=======================================
Job finished on Fri Sep 26 02:13:58 PM +08 2025

ðŸ“ ALL RESULTS SAVED IN: hyperparameter_optimization_20250926_123742

ðŸ“Š Generated Files:
  - hyperparameter_summary.csv (raw results)
  - hyperparameter_summary_report.txt (detailed analysis)
  - hyperparameter_heatmaps.png (performance heatmaps)
  - architecture_comparisons.png (comparative analysis)

ðŸ† QUICK RESULTS PREVIEW:
========================
No successful experiments found.

ðŸ” Next Steps:
  1. Review hyperparameter_summary_report.txt for detailed analysis
  2. Examine heatmaps for hyperparameter trends
  3. Select best configurations for production training
  4. Consider extended training with optimal hyperparameters

âœ… Hyperparameter optimization job complete!
total 1240
drwxr-x--- 38 phyzxi svuusers 1775 Sep 26 14:10 .
drwxr-xr-x  7 phyzxi svuusers 1129 Sep 26 14:13 ..
drwxr-x---  2 phyzxi svuusers  172 Sep 26 13:00 exp_10_UNet_lr5e-3_bs8
drwxr-x---  2 phyzxi svuusers  175 Sep 26 13:03 exp_11_UNet_lr5e-3_bs16
drwxr-x---  2 phyzxi svuusers  175 Sep 26 13:05 exp_12_UNet_lr5e-3_bs32
drwxr-x---  2 phyzxi svuusers  205 Sep 26 13:08 exp_13_Attention_UNet_lr1e-4_bs8
drwxr-x---  2 phyzxi svuusers  208 Sep 26 13:10 exp_14_Attention_UNet_lr1e-4_bs16
drwxr-x---  2 phyzxi svuusers  208 Sep 26 13:13 exp_15_Attention_UNet_lr1e-4_bs32
drwxr-x---  2 phyzxi svuusers  205 Sep 26 13:16 exp_16_Attention_UNet_lr5e-4_bs8
drwxr-x---  2 phyzxi svuusers  208 Sep 26 13:18 exp_17_Attention_UNet_lr5e-4_bs16
drwxr-x---  2 phyzxi svuusers  208 Sep 26 13:21 exp_18_Attention_UNet_lr5e-4_bs32
drwxr-x---  2 phyzxi svuusers  202 Sep 26 13:23 exp_19_Attention_UNet_lr1e-3_bs8
drwxr-x---  2 phyzxi svuusers  175 Sep 26 12:39 exp_1_UNet_lr1e-4_bs8
drwxr-x---  2 phyzxi svuusers  205 Sep 26 13:27 exp_20_Attention_UNet_lr1e-3_bs16
drwxr-x---  2 phyzxi svuusers  205 Sep 26 13:29 exp_21_Attention_UNet_lr1e-3_bs32
drwxr-x---  2 phyzxi svuusers  202 Sep 26 13:32 exp_22_Attention_UNet_lr5e-3_bs8
drwxr-x---  2 phyzxi svuusers  205 Sep 26 13:35 exp_23_Attention_UNet_lr5e-3_bs16
drwxr-x---  2 phyzxi svuusers  205 Sep 26 13:38 exp_24_Attention_UNet_lr5e-3_bs32
drwxr-x---  2 phyzxi svuusers  214 Sep 26 13:40 exp_25_Attention_ResUNet_lr1e-4_bs8
drwxr-x---  2 phyzxi svuusers  217 Sep 26 13:43 exp_26_Attention_ResUNet_lr1e-4_bs16
drwxr-x---  2 phyzxi svuusers  217 Sep 26 13:47 exp_27_Attention_ResUNet_lr1e-4_bs32
drwxr-x---  2 phyzxi svuusers  214 Sep 26 13:49 exp_28_Attention_ResUNet_lr5e-4_bs8
drwxr-x---  2 phyzxi svuusers  217 Sep 26 13:52 exp_29_Attention_ResUNet_lr5e-4_bs16
drwxr-x---  2 phyzxi svuusers  178 Sep 26 12:42 exp_2_UNet_lr1e-4_bs16
drwxr-x---  2 phyzxi svuusers  217 Sep 26 13:55 exp_30_Attention_ResUNet_lr5e-4_bs32
drwxr-x---  2 phyzxi svuusers  211 Sep 26 13:58 exp_31_Attention_ResUNet_lr1e-3_bs8
drwxr-x---  2 phyzxi svuusers  214 Sep 26 14:01 exp_32_Attention_ResUNet_lr1e-3_bs16
drwxr-x---  2 phyzxi svuusers  214 Sep 26 14:04 exp_33_Attention_ResUNet_lr1e-3_bs32
drwxr-x---  2 phyzxi svuusers  211 Sep 26 14:07 exp_34_Attention_ResUNet_lr5e-3_bs8
drwxr-x---  2 phyzxi svuusers  214 Sep 26 14:10 exp_35_Attention_ResUNet_lr5e-3_bs16
drwxr-x---  2 phyzxi svuusers  214 Sep 26 14:13 exp_36_Attention_ResUNet_lr5e-3_bs32
drwxr-x---  2 phyzxi svuusers  178 Sep 26 12:44 exp_3_UNet_lr1e-4_bs32
drwxr-x---  2 phyzxi svuusers  175 Sep 26 12:47 exp_4_UNet_lr5e-4_bs8
drwxr-x---  2 phyzxi svuusers  178 Sep 26 12:49 exp_5_UNet_lr5e-4_bs16
drwxr-x---  2 phyzxi svuusers  178 Sep 26 12:51 exp_6_UNet_lr5e-4_bs32
drwxr-x---  2 phyzxi svuusers  172 Sep 26 12:53 exp_7_UNet_lr1e-3_bs8
drwxr-x---  2 phyzxi svuusers  175 Sep 26 12:55 exp_8_UNet_lr1e-3_bs16
drwxr-x---  2 phyzxi svuusers  175 Sep 26 12:58 exp_9_UNet_lr1e-3_bs32
-rw-r-----  1 phyzxi svuusers 3017 Sep 26 14:13 hyperparameter_summary.csv

=======================================================================
HYPERPARAMETER OPTIMIZATION FINISHED
Results directory: hyperparameter_optimization_20250926_123742
=======================================================================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-09-26 14:14:02.979893:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 229134.stdct-mgmt-02
	Project: personal-phyzxi
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(36), Used(36)
	CPU Time Used: 01:36:51
	Memory: Requested(240gb), Used(22133640kb)
	Vmem Used: 64287916kb
	Walltime: Requested(48:00:00), Used(01:36:22)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (GN-A40-097[0]:ncpus=36:ngpus=1:mem=251658240kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	No GPU-related information available for this job.
