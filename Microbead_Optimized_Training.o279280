=======================================================================
OPTIMIZED MICROBEAD SEGMENTATION TRAINING
=======================================================================
Dataset analysis findings:
  - 73 images with 109.4 objects/image average (36√ó more than mitochondria)
  - Requires corrected hyperparameters for dense object segmentation

Job started on Thu Oct  9 03:31:11 PM +08 2025
Running on node: GN-A40-070
Job ID: 279280.stdct-mgmt-02
Available GPUs: GPU-5bc7106c-5ab8-98e5-a56b-3a12ff8c22e5
Memory: 503Gi, CPUs: 36

=== OPTIMIZED HYPERPARAMETERS ===
Comparison with mitochondria training:

Learning Rate:
  Mitochondria: 1e-3 (UNet), 1e-4 (Attention)
  Microbeads:   1e-4 (all models) ‚Üê 10√ó LOWER for dense objects

Batch Size:
  Mitochondria: 8-16
  Microbeads:   32 ‚Üê 2-4√ó LARGER for gradient stability

Dropout Regularization:
  Mitochondria: 0.0 (minimal)
  Microbeads:   0.3 ‚Üê Prevent overfitting uniform objects

Loss Function:
  Mitochondria: Binary Focal Loss (Œ≥=2)
  Microbeads:   Dice Loss ‚Üê Direct overlap optimization

Validation Split:
  Mitochondria: Random 90/10
  Microbeads:   Stratified 85/15 by object density

Expected Results:
  Previous (wrong params): Val Jaccard 0.14 ‚Üí 0.0 (collapsed)
  Expected (optimized):    Val Jaccard 0.50-0.70 (stable)
==================================

TensorFlow Container: /app1/common/singularity-img/hopper/tensorflow/tensorflow_2.16.1-cuda_12.5.0_24.06.sif

=== PRE-EXECUTION CHECKS ===
1. Checking dataset...
   ‚úì Dataset directories found
   ‚úì Images: 73 files
   ‚úì Masks: 73 files

2. Checking training script...
   ‚úì Optimized training script found

3. Checking model files...
   ‚úì Model definitions found

4. Checking dataset analysis...
   ‚úì Dataset analysis results found
   Analysis summary:
  "mean_positive_ratio": 0.11539009825824058,
  "mean_objects_per_image": 109.36986301369863,
============================

=== TENSORFLOW & GPU STATUS ===
Python version: 3.10.12
TensorFlow version: 2.16.1
CUDA built support: True
Physical GPUs found: 1
  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
‚úì GPU memory growth enabled

Checking dependencies:
  ‚úì cv2
  ‚úì PIL
  ‚úì matplotlib
  ‚úì numpy
  ‚úì sklearn
  ‚úì pandas

‚úì Environment ready
===============================

Created output directory: microbead_training_20251009_153124

üöÄ STARTING OPTIMIZED MICROBEAD TRAINING
==============================================
Training 3 models with corrected hyperparameters:
  1. UNet (LR=1e-4, BS=32, Dropout=0.3)
  2. Attention UNet (LR=1e-4, BS=32, Dropout=0.3)
  3. Attention ResUNet (LR=1e-4, BS=32, Dropout=0.3)

Expected training time: 6-10 hours (with early stopping)
Expected validation Jaccard: 0.50-0.70

This will REPLACE the previous failed training!
==============================================

================================================================================
OPTIMIZED MICROBEAD SEGMENTATION TRAINING
================================================================================

Configuration:
  Batch size: 32
  Learning rate: 0.0001
  Dropout rate: 0.3
  Loss function: dice
  Max epochs: 100

Loading dataset...
Loaded 73 images
Image dataset shape: (73, 256, 256, 3)
Mask dataset shape: (73, 256, 256, 1)

Creating stratified train/val split...
Stratification distribution:
  Stratum 1: 6 images
  Stratum 2: 7 images
  Stratum 3: 4 images
  Stratum 4: 15 images
  Stratum 5: 41 images
Training samples: 62
Validation samples: 11

Setting up data augmentation...
Augmentation settings:
  - Random rotation: 0-180¬∞
  - Random shifts: ¬±20%
  - Random flips: horizontal and vertical
  - Random zoom: ¬±15%
  - Random brightness: 80-120%

Input shape: (256, 256, 3)

Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
Output directory: microbead_training_20251009_073134

================================================================================
TRAINING MODEL 1/3: STANDARD U-NET
================================================================================
Hyperparameters: LR=0.0001, BS=32, Dropout=0.3

Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]        0         []                            
                                                                                                  
 conv2d_19 (Conv2D)          (None, 256, 256, 64)         1792      ['input_2[0][0]']             
                                                                                                  
 batch_normalization_19 (Ba  (None, 256, 256, 64)         256       ['conv2d_19[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_19 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_19[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_20 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_19[0][0]']       
                                                                                                  
 batch_normalization_20 (Ba  (None, 256, 256, 64)         256       ['conv2d_20[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_20 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_20[0][0]
                                                                    ']                            
                                                                                                  
 dropout (Dropout)           (None, 256, 256, 64)         0         ['activation_20[0][0]']       
                                                                                                  
 max_pooling2d_4 (MaxPoolin  (None, 128, 128, 64)         0         ['dropout[0][0]']             
 g2D)                                                                                             
                                                                                                  
 conv2d_21 (Conv2D)          (None, 128, 128, 128)        73856     ['max_pooling2d_4[0][0]']     
                                                                                                  
 batch_normalization_21 (Ba  (None, 128, 128, 128)        512       ['conv2d_21[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_21 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_21[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_22 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_21[0][0]']       
                                                                                                  
 batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['conv2d_22[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_22 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_22[0][0]
                                                                    ']                            
                                                                                                  
 dropout_1 (Dropout)         (None, 128, 128, 128)        0         ['activation_22[0][0]']       
                                                                                                  
 max_pooling2d_5 (MaxPoolin  (None, 64, 64, 128)          0         ['dropout_1[0][0]']           
 g2D)                                                                                             
                                                                                                  
 conv2d_23 (Conv2D)          (None, 64, 64, 256)          295168    ['max_pooling2d_5[0][0]']     
                                                                                                  
 batch_normalization_23 (Ba  (None, 64, 64, 256)          1024      ['conv2d_23[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_23 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_23[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_24 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_23[0][0]']       
                                                                                                  
 batch_normalization_24 (Ba  (None, 64, 64, 256)          1024      ['conv2d_24[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_24 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_24[0][0]
                                                                    ']                            
                                                                                                  
 dropout_2 (Dropout)         (None, 64, 64, 256)          0         ['activation_24[0][0]']       
                                                                                                  
 max_pooling2d_6 (MaxPoolin  (None, 32, 32, 256)          0         ['dropout_2[0][0]']           
 g2D)                                                                                             
                                                                                                  
 conv2d_25 (Conv2D)          (None, 32, 32, 512)          1180160   ['max_pooling2d_6[0][0]']     
                                                                                                  
 batch_normalization_25 (Ba  (None, 32, 32, 512)          2048      ['conv2d_25[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_25 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_25[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_26 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_25[0][0]']       
                                                                                                  
 batch_normalization_26 (Ba  (None, 32, 32, 512)          2048      ['conv2d_26[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_26 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_26[0][0]
                                                                    ']                            
                                                                                                  
 dropout_3 (Dropout)         (None, 32, 32, 512)          0         ['activation_26[0][0]']       
                                                                                                  
 max_pooling2d_7 (MaxPoolin  (None, 16, 16, 512)          0         ['dropout_3[0][0]']           
 g2D)                                                                                             
                                                                                                  
 conv2d_27 (Conv2D)          (None, 16, 16, 1024)         4719616   ['max_pooling2d_7[0][0]']     
                                                                                                  
 batch_normalization_27 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_27[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_27 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_27[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_28 (Conv2D)          (None, 16, 16, 1024)         9438208   ['activation_27[0][0]']       
                                                                                                  
 batch_normalization_28 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_28[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_28 (Activation)  (None, 16, 16, 1024)         0         ['batch_normalization_28[0][0]
                                                                    ']                            
                                                                                                  
 dropout_4 (Dropout)         (None, 16, 16, 1024)         0         ['activation_28[0][0]']       
                                                                                                  
 up_sampling2d_4 (UpSamplin  (None, 32, 32, 1024)         0         ['dropout_4[0][0]']           
 g2D)                                                                                             
                                                                                                  
 concatenate_4 (Concatenate  (None, 32, 32, 1536)         0         ['up_sampling2d_4[0][0]',     
 )                                                                   'dropout_3[0][0]']           
                                                                                                  
 conv2d_29 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate_4[0][0]']       
                                                                                                  
 batch_normalization_29 (Ba  (None, 32, 32, 512)          2048      ['conv2d_29[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_29 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_29[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_30 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_29[0][0]']       
                                                                                                  
 batch_normalization_30 (Ba  (None, 32, 32, 512)          2048      ['conv2d_30[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_30 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_30[0][0]
                                                                    ']                            
                                                                                                  
 dropout_5 (Dropout)         (None, 32, 32, 512)          0         ['activation_30[0][0]']       
                                                                                                  
 up_sampling2d_5 (UpSamplin  (None, 64, 64, 512)          0         ['dropout_5[0][0]']           
 g2D)                                                                                             
                                                                                                  
 concatenate_5 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_5[0][0]',     
 )                                                                   'dropout_2[0][0]']           
                                                                                                  
 conv2d_31 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_5[0][0]']       
                                                                                                  
 batch_normalization_31 (Ba  (None, 64, 64, 256)          1024      ['conv2d_31[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_31 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_31[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_32 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_31[0][0]']       
                                                                                                  
 batch_normalization_32 (Ba  (None, 64, 64, 256)          1024      ['conv2d_32[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_32 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_32[0][0]
                                                                    ']                            
                                                                                                  
 dropout_6 (Dropout)         (None, 64, 64, 256)          0         ['activation_32[0][0]']       
                                                                                                  
 up_sampling2d_6 (UpSamplin  (None, 128, 128, 256)        0         ['dropout_6[0][0]']           
 g2D)                                                                                             
                                                                                                  
 concatenate_6 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_6[0][0]',     
 )                                                                   'dropout_1[0][0]']           
                                                                                                  
 conv2d_33 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_6[0][0]']       
                                                                                                  
 batch_normalization_33 (Ba  (None, 128, 128, 128)        512       ['conv2d_33[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_33 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_33[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_34 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_33[0][0]']       
                                                                                                  
 batch_normalization_34 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_34 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_34[0][0]
                                                                    ']                            
                                                                                                  
 dropout_7 (Dropout)         (None, 128, 128, 128)        0         ['activation_34[0][0]']       
                                                                                                  
 up_sampling2d_7 (UpSamplin  (None, 256, 256, 128)        0         ['dropout_7[0][0]']           
 g2D)                                                                                             
                                                                                                  
 concatenate_7 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_7[0][0]',     
 )                                                                   'dropout[0][0]']             
                                                                                                  
 conv2d_35 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_7[0][0]']       
                                                                                                  
 batch_normalization_35 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_35 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_35[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_36 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_35[0][0]']       
                                                                                                  
 batch_normalization_36 (Ba  (None, 256, 256, 64)         256       ['conv2d_36[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_36 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_36[0][0]
                                                                    ']                            
                                                                                                  
 dropout_8 (Dropout)         (None, 256, 256, 64)         0         ['activation_36[0][0]']       
                                                                                                  
 conv2d_37 (Conv2D)          (None, 256, 256, 1)          65        ['dropout_8[0][0]']           
                                                                                                  
 batch_normalization_37 (Ba  (None, 256, 256, 1)          4         ['conv2d_37[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_37 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_37[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31402501 (119.79 MB)
Trainable params: 31390723 (119.75 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
Epoch 1/100
2025-10-09 07:31:53.359924: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inUNet/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1759995128.282188  465512 service.cc:145] XLA service 0x14d77f0aab50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1759995128.282212  465512 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
I0000 00:00:1759995128.433449  465512 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/1 [==============================] - ETA: 0s - loss: 0.8193 - accuracy: 0.8433 - jacard_coef: 4.6124e-13 - dice_coef: 0.1807
Epoch 1: val_jacard_coef improved from -inf to 0.00246, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
1/1 [==============================] - 54s 54s/step - loss: 0.8193 - accuracy: 0.8433 - jacard_coef: 4.6124e-13 - dice_coef: 0.1807 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 2/100
1/1 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.0796 - jacard_coef: 0.1231 - dice_coef: 0.1976
Epoch 2: val_jacard_coef did not improve from 0.00246
1/1 [==============================] - 18s 18s/step - loss: 0.8024 - accuracy: 0.0796 - jacard_coef: 0.1231 - dice_coef: 0.1976 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 3/100
1/1 [==============================] - ETA: 0s - loss: 0.8328 - accuracy: 0.0632 - jacard_coef: 0.1004 - dice_coef: 0.1672
Epoch 3: val_jacard_coef did not improve from 0.00246
1/1 [==============================] - 21s 21s/step - loss: 0.8328 - accuracy: 0.0632 - jacard_coef: 0.1004 - dice_coef: 0.1672 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 4/100
1/1 [==============================] - ETA: 0s - loss: 0.8062 - accuracy: 0.0768 - jacard_coef: 0.1202 - dice_coef: 0.1938
Epoch 4: val_jacard_coef improved from 0.00246 to 0.00248, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 10s 10s/step - loss: 0.8062 - accuracy: 0.0768 - jacard_coef: 0.1202 - dice_coef: 0.1938 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 5/100
1/1 [==============================] - ETA: 0s - loss: 0.8116 - accuracy: 0.0735 - jacard_coef: 0.1160 - dice_coef: 0.1884
Epoch 5: val_jacard_coef improved from 0.00248 to 0.00249, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 5s 5s/step - loss: 0.8116 - accuracy: 0.0735 - jacard_coef: 0.1160 - dice_coef: 0.1884 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 6/100
1/1 [==============================] - ETA: 0s - loss: 0.8108 - accuracy: 0.0741 - jacard_coef: 0.1167 - dice_coef: 0.1892
Epoch 6: val_jacard_coef improved from 0.00249 to 0.00252, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8108 - accuracy: 0.0741 - jacard_coef: 0.1167 - dice_coef: 0.1892 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 7/100
1/1 [==============================] - ETA: 0s - loss: 0.8263 - accuracy: 0.0665 - jacard_coef: 0.1051 - dice_coef: 0.1737
Epoch 7: val_jacard_coef did not improve from 0.00252
1/1 [==============================] - 1s 864ms/step - loss: 0.8263 - accuracy: 0.0665 - jacard_coef: 0.1051 - dice_coef: 0.1737 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 8/100
1/1 [==============================] - ETA: 0s - loss: 0.7775 - accuracy: 0.0917 - jacard_coef: 0.1431 - dice_coef: 0.2225
Epoch 8: val_jacard_coef improved from 0.00252 to 0.00255, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.7775 - accuracy: 0.0917 - jacard_coef: 0.1431 - dice_coef: 0.2225 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0025 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 9/100
1/1 [==============================] - ETA: 0s - loss: 0.7874 - accuracy: 0.0863 - jacard_coef: 0.1350 - dice_coef: 0.2126
Epoch 9: val_jacard_coef improved from 0.00255 to 0.00256, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.7874 - accuracy: 0.0863 - jacard_coef: 0.1350 - dice_coef: 0.2126 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0026 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 10/100
1/1 [==============================] - ETA: 0s - loss: 0.8404 - accuracy: 0.0603 - jacard_coef: 0.0949 - dice_coef: 0.1596
Epoch 10: val_jacard_coef improved from 0.00256 to 0.00256, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8404 - accuracy: 0.0603 - jacard_coef: 0.0949 - dice_coef: 0.1596 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0026 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 11/100
1/1 [==============================] - ETA: 0s - loss: 0.8295 - accuracy: 0.0660 - jacard_coef: 0.1028 - dice_coef: 0.1705
Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

Epoch 11: val_jacard_coef improved from 0.00256 to 0.00258, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8295 - accuracy: 0.0660 - jacard_coef: 0.1028 - dice_coef: 0.1705 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0026 - val_dice_coef: 0.1956 - lr: 1.0000e-04
Epoch 12/100
1/1 [==============================] - ETA: 0s - loss: 0.8376 - accuracy: 0.0615 - jacard_coef: 0.0969 - dice_coef: 0.1624
Epoch 12: val_jacard_coef improved from 0.00258 to 0.00260, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8376 - accuracy: 0.0615 - jacard_coef: 0.0969 - dice_coef: 0.1624 - val_loss: 0.8044 - val_accuracy: 0.8243 - val_jacard_coef: 0.0026 - val_dice_coef: 0.1956 - lr: 5.0000e-05
Epoch 13/100
1/1 [==============================] - ETA: 0s - loss: 0.8100 - accuracy: 0.0751 - jacard_coef: 0.1173 - dice_coef: 0.1900
Epoch 13: val_jacard_coef improved from 0.00260 to 0.00263, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8100 - accuracy: 0.0751 - jacard_coef: 0.1173 - dice_coef: 0.1900 - val_loss: 0.8043 - val_accuracy: 0.8243 - val_jacard_coef: 0.0026 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 14/100
1/1 [==============================] - ETA: 0s - loss: 0.8140 - accuracy: 0.0731 - jacard_coef: 0.1142 - dice_coef: 0.1860
Epoch 14: val_jacard_coef improved from 0.00263 to 0.00265, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8140 - accuracy: 0.0731 - jacard_coef: 0.1142 - dice_coef: 0.1860 - val_loss: 0.8043 - val_accuracy: 0.8243 - val_jacard_coef: 0.0027 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 15/100
1/1 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.0706 - jacard_coef: 0.1088 - dice_coef: 0.1787
Epoch 15: val_jacard_coef improved from 0.00265 to 0.00272, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8213 - accuracy: 0.0706 - jacard_coef: 0.1088 - dice_coef: 0.1787 - val_loss: 0.8043 - val_accuracy: 0.8243 - val_jacard_coef: 0.0027 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 16/100
1/1 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.0671 - jacard_coef: 0.1039 - dice_coef: 0.1720
Epoch 16: val_jacard_coef improved from 0.00272 to 0.00283, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8280 - accuracy: 0.0671 - jacard_coef: 0.1039 - dice_coef: 0.1720 - val_loss: 0.8043 - val_accuracy: 0.8243 - val_jacard_coef: 0.0028 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 17/100
1/1 [==============================] - ETA: 0s - loss: 0.8151 - accuracy: 0.0721 - jacard_coef: 0.1134 - dice_coef: 0.1849
Epoch 17: val_jacard_coef improved from 0.00283 to 0.00292, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8151 - accuracy: 0.0721 - jacard_coef: 0.1134 - dice_coef: 0.1849 - val_loss: 0.8043 - val_accuracy: 0.8243 - val_jacard_coef: 0.0029 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 18/100
1/1 [==============================] - ETA: 0s - loss: 0.8686 - accuracy: 0.0480 - jacard_coef: 0.0756 - dice_coef: 0.1314
Epoch 18: val_jacard_coef improved from 0.00292 to 0.00304, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8686 - accuracy: 0.0480 - jacard_coef: 0.0756 - dice_coef: 0.1314 - val_loss: 0.8043 - val_accuracy: 0.8243 - val_jacard_coef: 0.0030 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 19/100
1/1 [==============================] - ETA: 0s - loss: 0.8193 - accuracy: 0.0715 - jacard_coef: 0.1102 - dice_coef: 0.1807
Epoch 19: val_jacard_coef improved from 0.00304 to 0.00315, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8193 - accuracy: 0.0715 - jacard_coef: 0.1102 - dice_coef: 0.1807 - val_loss: 0.8043 - val_accuracy: 0.8242 - val_jacard_coef: 0.0031 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 20/100
1/1 [==============================] - ETA: 0s - loss: 0.8130 - accuracy: 0.0723 - jacard_coef: 0.1150 - dice_coef: 0.1870
Epoch 20: val_jacard_coef improved from 0.00315 to 0.00337, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8130 - accuracy: 0.0723 - jacard_coef: 0.1150 - dice_coef: 0.1870 - val_loss: 0.8043 - val_accuracy: 0.8243 - val_jacard_coef: 0.0034 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 21/100
1/1 [==============================] - ETA: 0s - loss: 0.8214 - accuracy: 0.0696 - jacard_coef: 0.1087 - dice_coef: 0.1786
Epoch 21: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

Epoch 21: val_jacard_coef improved from 0.00337 to 0.00354, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8214 - accuracy: 0.0696 - jacard_coef: 0.1087 - dice_coef: 0.1786 - val_loss: 0.8043 - val_accuracy: 0.8242 - val_jacard_coef: 0.0035 - val_dice_coef: 0.1957 - lr: 5.0000e-05
Epoch 22/100
1/1 [==============================] - ETA: 0s - loss: 0.8307 - accuracy: 0.0654 - jacard_coef: 0.1019 - dice_coef: 0.1693
Epoch 22: val_jacard_coef improved from 0.00354 to 0.00378, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8307 - accuracy: 0.0654 - jacard_coef: 0.1019 - dice_coef: 0.1693 - val_loss: 0.8043 - val_accuracy: 0.8242 - val_jacard_coef: 0.0038 - val_dice_coef: 0.1957 - lr: 2.5000e-05
Epoch 23/100
1/1 [==============================] - ETA: 0s - loss: 0.8483 - accuracy: 0.0569 - jacard_coef: 0.0894 - dice_coef: 0.1517
Epoch 23: val_jacard_coef improved from 0.00378 to 0.00417, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8483 - accuracy: 0.0569 - jacard_coef: 0.0894 - dice_coef: 0.1517 - val_loss: 0.8043 - val_accuracy: 0.8242 - val_jacard_coef: 0.0042 - val_dice_coef: 0.1957 - lr: 2.5000e-05
Epoch 24/100
1/1 [==============================] - ETA: 0s - loss: 0.8392 - accuracy: 0.0607 - jacard_coef: 0.0958 - dice_coef: 0.1608
Epoch 24: val_jacard_coef improved from 0.00417 to 0.00465, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8392 - accuracy: 0.0607 - jacard_coef: 0.0958 - dice_coef: 0.1608 - val_loss: 0.8043 - val_accuracy: 0.8242 - val_jacard_coef: 0.0046 - val_dice_coef: 0.1957 - lr: 2.5000e-05
Epoch 25/100
1/1 [==============================] - ETA: 0s - loss: 0.8039 - accuracy: 0.0782 - jacard_coef: 0.1220 - dice_coef: 0.1961
Epoch 25: val_jacard_coef improved from 0.00465 to 0.00519, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8039 - accuracy: 0.0782 - jacard_coef: 0.1220 - dice_coef: 0.1961 - val_loss: 0.8042 - val_accuracy: 0.8242 - val_jacard_coef: 0.0052 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 26/100
1/1 [==============================] - ETA: 0s - loss: 0.8004 - accuracy: 0.0797 - jacard_coef: 0.1247 - dice_coef: 0.1996
Epoch 26: val_jacard_coef improved from 0.00519 to 0.00603, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8004 - accuracy: 0.0797 - jacard_coef: 0.1247 - dice_coef: 0.1996 - val_loss: 0.8042 - val_accuracy: 0.8242 - val_jacard_coef: 0.0060 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 27/100
1/1 [==============================] - ETA: 0s - loss: 0.8318 - accuracy: 0.0655 - jacard_coef: 0.1011 - dice_coef: 0.1682
Epoch 27: val_jacard_coef improved from 0.00603 to 0.00706, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8318 - accuracy: 0.0655 - jacard_coef: 0.1011 - dice_coef: 0.1682 - val_loss: 0.8042 - val_accuracy: 0.8242 - val_jacard_coef: 0.0071 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 28/100
1/1 [==============================] - ETA: 0s - loss: 0.8084 - accuracy: 0.0761 - jacard_coef: 0.1185 - dice_coef: 0.1916
Epoch 28: val_jacard_coef improved from 0.00706 to 0.00834, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8084 - accuracy: 0.0761 - jacard_coef: 0.1185 - dice_coef: 0.1916 - val_loss: 0.8042 - val_accuracy: 0.8242 - val_jacard_coef: 0.0083 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 29/100
1/1 [==============================] - ETA: 0s - loss: 0.8368 - accuracy: 0.0625 - jacard_coef: 0.0975 - dice_coef: 0.1632
Epoch 29: val_jacard_coef improved from 0.00834 to 0.01003, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8368 - accuracy: 0.0625 - jacard_coef: 0.0975 - dice_coef: 0.1632 - val_loss: 0.8042 - val_accuracy: 0.8241 - val_jacard_coef: 0.0100 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 30/100
1/1 [==============================] - ETA: 0s - loss: 0.8097 - accuracy: 0.0763 - jacard_coef: 0.1175 - dice_coef: 0.1903
Epoch 30: val_jacard_coef improved from 0.01003 to 0.01213, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8097 - accuracy: 0.0763 - jacard_coef: 0.1175 - dice_coef: 0.1903 - val_loss: 0.8042 - val_accuracy: 0.8240 - val_jacard_coef: 0.0121 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 31/100
1/1 [==============================] - ETA: 0s - loss: 0.8487 - accuracy: 0.0567 - jacard_coef: 0.0891 - dice_coef: 0.1513
Epoch 31: val_jacard_coef improved from 0.01213 to 0.01455, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8487 - accuracy: 0.0567 - jacard_coef: 0.0891 - dice_coef: 0.1513 - val_loss: 0.8042 - val_accuracy: 0.8235 - val_jacard_coef: 0.0145 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 32/100
1/1 [==============================] - ETA: 0s - loss: 0.8163 - accuracy: 0.0711 - jacard_coef: 0.1125 - dice_coef: 0.1837
Epoch 32: val_jacard_coef improved from 0.01455 to 0.01782, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8163 - accuracy: 0.0711 - jacard_coef: 0.1125 - dice_coef: 0.1837 - val_loss: 0.8041 - val_accuracy: 0.8226 - val_jacard_coef: 0.0178 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 33/100
1/1 [==============================] - ETA: 0s - loss: 0.8166 - accuracy: 0.0723 - jacard_coef: 0.1123 - dice_coef: 0.1834
Epoch 33: val_jacard_coef improved from 0.01782 to 0.02187, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8166 - accuracy: 0.0723 - jacard_coef: 0.1123 - dice_coef: 0.1834 - val_loss: 0.8041 - val_accuracy: 0.8214 - val_jacard_coef: 0.0219 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 34/100
1/1 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.0660 - jacard_coef: 0.1022 - dice_coef: 0.1697
Epoch 34: val_jacard_coef improved from 0.02187 to 0.02728, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8303 - accuracy: 0.0660 - jacard_coef: 0.1022 - dice_coef: 0.1697 - val_loss: 0.8041 - val_accuracy: 0.8205 - val_jacard_coef: 0.0273 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 35/100
1/1 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.0765 - jacard_coef: 0.1196 - dice_coef: 0.1930
Epoch 35: val_jacard_coef improved from 0.02728 to 0.03477, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8070 - accuracy: 0.0765 - jacard_coef: 0.1196 - dice_coef: 0.1930 - val_loss: 0.8041 - val_accuracy: 0.8195 - val_jacard_coef: 0.0348 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 36/100
1/1 [==============================] - ETA: 0s - loss: 0.8141 - accuracy: 0.0726 - jacard_coef: 0.1142 - dice_coef: 0.1859
Epoch 36: val_jacard_coef improved from 0.03477 to 0.04449, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8141 - accuracy: 0.0726 - jacard_coef: 0.1142 - dice_coef: 0.1859 - val_loss: 0.8041 - val_accuracy: 0.8184 - val_jacard_coef: 0.0445 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 37/100
1/1 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.0746 - jacard_coef: 0.1184 - dice_coef: 0.1915
Epoch 37: val_jacard_coef improved from 0.04449 to 0.05702, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8085 - accuracy: 0.0746 - jacard_coef: 0.1184 - dice_coef: 0.1915 - val_loss: 0.8041 - val_accuracy: 0.8179 - val_jacard_coef: 0.0570 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 38/100
1/1 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.0844 - jacard_coef: 0.1320 - dice_coef: 0.2089
Epoch 38: val_jacard_coef improved from 0.05702 to 0.07599, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.7911 - accuracy: 0.0844 - jacard_coef: 0.1320 - dice_coef: 0.2089 - val_loss: 0.8041 - val_accuracy: 0.8178 - val_jacard_coef: 0.0760 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 39/100
1/1 [==============================] - ETA: 0s - loss: 0.8142 - accuracy: 0.0731 - jacard_coef: 0.1141 - dice_coef: 0.1858
Epoch 39: val_jacard_coef improved from 0.07599 to 0.10207, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8142 - accuracy: 0.0731 - jacard_coef: 0.1141 - dice_coef: 0.1858 - val_loss: 0.8040 - val_accuracy: 0.8182 - val_jacard_coef: 0.1021 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 40/100
1/1 [==============================] - ETA: 0s - loss: 0.7934 - accuracy: 0.0831 - jacard_coef: 0.1302 - dice_coef: 0.2066
Epoch 40: val_jacard_coef improved from 0.10207 to 0.13780, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7934 - accuracy: 0.0831 - jacard_coef: 0.1302 - dice_coef: 0.2066 - val_loss: 0.8040 - val_accuracy: 0.8179 - val_jacard_coef: 0.1378 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 41/100
1/1 [==============================] - ETA: 0s - loss: 0.8344 - accuracy: 0.0629 - jacard_coef: 0.0992 - dice_coef: 0.1656
Epoch 41: val_jacard_coef improved from 0.13780 to 0.17883, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8344 - accuracy: 0.0629 - jacard_coef: 0.0992 - dice_coef: 0.1656 - val_loss: 0.8040 - val_accuracy: 0.8158 - val_jacard_coef: 0.1788 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 42/100
1/1 [==============================] - ETA: 0s - loss: 0.8060 - accuracy: 0.0760 - jacard_coef: 0.1203 - dice_coef: 0.1940
Epoch 42: val_jacard_coef improved from 0.17883 to 0.22051, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8060 - accuracy: 0.0760 - jacard_coef: 0.1203 - dice_coef: 0.1940 - val_loss: 0.8040 - val_accuracy: 0.8106 - val_jacard_coef: 0.2205 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 43/100
1/1 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.0748 - jacard_coef: 0.1169 - dice_coef: 0.1895
Epoch 43: val_jacard_coef improved from 0.22051 to 0.24557, saving model to microbead_training_20251009_073134/best_unet_model.hdf5
1/1 [==============================] - 2s 2s/step - loss: 0.8105 - accuracy: 0.0748 - jacard_coef: 0.1169 - dice_coef: 0.1895 - val_loss: 0.8040 - val_accuracy: 0.7938 - val_jacard_coef: 0.2456 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 44/100
1/1 [==============================] - ETA: 0s - loss: 0.8136 - accuracy: 0.0729 - jacard_coef: 0.1145 - dice_coef: 0.1864
Epoch 44: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 856ms/step - loss: 0.8136 - accuracy: 0.0729 - jacard_coef: 0.1145 - dice_coef: 0.1864 - val_loss: 0.8039 - val_accuracy: 0.7511 - val_jacard_coef: 0.2391 - val_dice_coef: 0.1961 - lr: 2.5000e-05
Epoch 45/100
1/1 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.0761 - jacard_coef: 0.1184 - dice_coef: 0.1915
Epoch 45: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 913ms/step - loss: 0.8085 - accuracy: 0.0761 - jacard_coef: 0.1184 - dice_coef: 0.1915 - val_loss: 0.8039 - val_accuracy: 0.6663 - val_jacard_coef: 0.2049 - val_dice_coef: 0.1961 - lr: 2.5000e-05
Epoch 46/100
1/1 [==============================] - ETA: 0s - loss: 0.8361 - accuracy: 0.0617 - jacard_coef: 0.0980 - dice_coef: 0.1639
Epoch 46: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 906ms/step - loss: 0.8361 - accuracy: 0.0617 - jacard_coef: 0.0980 - dice_coef: 0.1639 - val_loss: 0.8039 - val_accuracy: 0.5368 - val_jacard_coef: 0.1672 - val_dice_coef: 0.1961 - lr: 2.5000e-05
Epoch 47/100
1/1 [==============================] - ETA: 0s - loss: 0.8118 - accuracy: 0.0745 - jacard_coef: 0.1159 - dice_coef: 0.1882
Epoch 47: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 918ms/step - loss: 0.8118 - accuracy: 0.0745 - jacard_coef: 0.1159 - dice_coef: 0.1882 - val_loss: 0.8038 - val_accuracy: 0.3780 - val_jacard_coef: 0.1388 - val_dice_coef: 0.1962 - lr: 2.5000e-05
Epoch 48/100
1/1 [==============================] - ETA: 0s - loss: 0.8276 - accuracy: 0.0654 - jacard_coef: 0.1041 - dice_coef: 0.1724
Epoch 48: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 866ms/step - loss: 0.8276 - accuracy: 0.0654 - jacard_coef: 0.1041 - dice_coef: 0.1724 - val_loss: 0.8037 - val_accuracy: 0.2476 - val_jacard_coef: 0.1249 - val_dice_coef: 0.1963 - lr: 2.5000e-05
Epoch 49/100
1/1 [==============================] - ETA: 0s - loss: 0.8107 - accuracy: 0.0751 - jacard_coef: 0.1167 - dice_coef: 0.1893
Epoch 49: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 914ms/step - loss: 0.8107 - accuracy: 0.0751 - jacard_coef: 0.1167 - dice_coef: 0.1893 - val_loss: 0.8036 - val_accuracy: 0.1879 - val_jacard_coef: 0.1224 - val_dice_coef: 0.1964 - lr: 2.5000e-05
Epoch 50/100
1/1 [==============================] - ETA: 0s - loss: 0.7950 - accuracy: 0.0826 - jacard_coef: 0.1289 - dice_coef: 0.2050
Epoch 50: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 917ms/step - loss: 0.7950 - accuracy: 0.0826 - jacard_coef: 0.1289 - dice_coef: 0.2050 - val_loss: 0.8036 - val_accuracy: 0.1551 - val_jacard_coef: 0.1216 - val_dice_coef: 0.1964 - lr: 2.5000e-05
Epoch 51/100
1/1 [==============================] - ETA: 0s - loss: 0.7990 - accuracy: 0.0805 - jacard_coef: 0.1258 - dice_coef: 0.2010
Epoch 51: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 908ms/step - loss: 0.7990 - accuracy: 0.0805 - jacard_coef: 0.1258 - dice_coef: 0.2010 - val_loss: 0.8035 - val_accuracy: 0.1366 - val_jacard_coef: 0.1212 - val_dice_coef: 0.1965 - lr: 2.5000e-05
Epoch 52/100
1/1 [==============================] - ETA: 0s - loss: 0.8200 - accuracy: 0.0701 - jacard_coef: 0.1097 - dice_coef: 0.1800
Epoch 52: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 851ms/step - loss: 0.8200 - accuracy: 0.0701 - jacard_coef: 0.1097 - dice_coef: 0.1800 - val_loss: 0.8035 - val_accuracy: 0.1270 - val_jacard_coef: 0.1212 - val_dice_coef: 0.1965 - lr: 2.5000e-05
Epoch 53/100
1/1 [==============================] - ETA: 0s - loss: 0.8049 - accuracy: 0.0771 - jacard_coef: 0.1211 - dice_coef: 0.1951
Epoch 53: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 909ms/step - loss: 0.8049 - accuracy: 0.0771 - jacard_coef: 0.1211 - dice_coef: 0.1951 - val_loss: 0.8034 - val_accuracy: 0.1215 - val_jacard_coef: 0.1213 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 54/100
1/1 [==============================] - ETA: 0s - loss: 0.8115 - accuracy: 0.0747 - jacard_coef: 0.1161 - dice_coef: 0.1885
Epoch 54: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 914ms/step - loss: 0.8115 - accuracy: 0.0747 - jacard_coef: 0.1161 - dice_coef: 0.1885 - val_loss: 0.8034 - val_accuracy: 0.1173 - val_jacard_coef: 0.1213 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 55/100
1/1 [==============================] - ETA: 0s - loss: 0.8299 - accuracy: 0.0652 - jacard_coef: 0.1024 - dice_coef: 0.1701
Epoch 55: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 860ms/step - loss: 0.8299 - accuracy: 0.0652 - jacard_coef: 0.1024 - dice_coef: 0.1701 - val_loss: 0.8034 - val_accuracy: 0.1140 - val_jacard_coef: 0.1212 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 56/100
1/1 [==============================] - ETA: 0s - loss: 0.8166 - accuracy: 0.0712 - jacard_coef: 0.1123 - dice_coef: 0.1834
Epoch 56: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 914ms/step - loss: 0.8166 - accuracy: 0.0712 - jacard_coef: 0.1123 - dice_coef: 0.1834 - val_loss: 0.8034 - val_accuracy: 0.1120 - val_jacard_coef: 0.1212 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 57/100
1/1 [==============================] - ETA: 0s - loss: 0.8324 - accuracy: 0.0637 - jacard_coef: 0.1007 - dice_coef: 0.1676
Epoch 57: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 864ms/step - loss: 0.8324 - accuracy: 0.0637 - jacard_coef: 0.1007 - dice_coef: 0.1676 - val_loss: 0.8034 - val_accuracy: 0.1108 - val_jacard_coef: 0.1212 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 58/100
1/1 [==============================] - ETA: 0s - loss: 0.8068 - accuracy: 0.0760 - jacard_coef: 0.1197 - dice_coef: 0.1932
Epoch 58: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 906ms/step - loss: 0.8068 - accuracy: 0.0760 - jacard_coef: 0.1197 - dice_coef: 0.1932 - val_loss: 0.8034 - val_accuracy: 0.1100 - val_jacard_coef: 0.1212 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 59/100
1/1 [==============================] - ETA: 0s - loss: 0.7816 - accuracy: 0.0888 - jacard_coef: 0.1396 - dice_coef: 0.2184
Epoch 59: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 854ms/step - loss: 0.7816 - accuracy: 0.0888 - jacard_coef: 0.1396 - dice_coef: 0.2184 - val_loss: 0.8034 - val_accuracy: 0.1091 - val_jacard_coef: 0.1211 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 60/100
1/1 [==============================] - ETA: 0s - loss: 0.7992 - accuracy: 0.0803 - jacard_coef: 0.1256 - dice_coef: 0.2008
Epoch 60: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 861ms/step - loss: 0.7992 - accuracy: 0.0803 - jacard_coef: 0.1256 - dice_coef: 0.2008 - val_loss: 0.8034 - val_accuracy: 0.1087 - val_jacard_coef: 0.1211 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 61/100
1/1 [==============================] - ETA: 0s - loss: 0.8256 - accuracy: 0.0662 - jacard_coef: 0.1056 - dice_coef: 0.1744
Epoch 61: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 850ms/step - loss: 0.8256 - accuracy: 0.0662 - jacard_coef: 0.1056 - dice_coef: 0.1744 - val_loss: 0.8034 - val_accuracy: 0.1089 - val_jacard_coef: 0.1211 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 62/100
1/1 [==============================] - ETA: 0s - loss: 0.7975 - accuracy: 0.0814 - jacard_coef: 0.1269 - dice_coef: 0.2025
Epoch 62: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 911ms/step - loss: 0.7975 - accuracy: 0.0814 - jacard_coef: 0.1269 - dice_coef: 0.2025 - val_loss: 0.8034 - val_accuracy: 0.1092 - val_jacard_coef: 0.1211 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 63/100
1/1 [==============================] - ETA: 0s - loss: 0.8098 - accuracy: 0.0751 - jacard_coef: 0.1174 - dice_coef: 0.1902
Epoch 63: val_jacard_coef did not improve from 0.24557
1/1 [==============================] - 1s 909ms/step - loss: 0.8098 - accuracy: 0.0751 - jacard_coef: 0.1174 - dice_coef: 0.1902 - val_loss: 0.8034 - val_accuracy: 0.1097 - val_jacard_coef: 0.1210 - val_dice_coef: 0.1966 - lr: 2.5000e-05
Epoch 63: early stopping
Restoring model weights from the end of the best epoch: 43.
‚úì UNet training completed in 0:03:35.913371
Best Val Jaccard: 0.2456

================================================================================
TRAINING MODEL 2/3: ATTENTION U-NET
================================================================================
Hyperparameters: LR=0.0001, BS=32, Dropout=0.3

Epoch 1/100
2025-10-09 07:35:30.422665: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inAttention_UNet/dropout_9/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
1/1 [==============================] - ETA: 0s - loss: 0.8121 - accuracy: 0.8345 - jacard_coef: 4.1211e-13 - dice_coef: 0.1879
Epoch 1: val_jacard_coef improved from -inf to 0.00681, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 31s 31s/step - loss: 0.8121 - accuracy: 0.8345 - jacard_coef: 4.1211e-13 - dice_coef: 0.1879 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0068 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 2/100
1/1 [==============================] - ETA: 0s - loss: 0.8048 - accuracy: 0.0774 - jacard_coef: 0.1213 - dice_coef: 0.1952
Epoch 2: val_jacard_coef improved from 0.00681 to 0.00703, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8048 - accuracy: 0.0774 - jacard_coef: 0.1213 - dice_coef: 0.1952 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0070 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 3/100
1/1 [==============================] - ETA: 0s - loss: 0.8175 - accuracy: 0.0719 - jacard_coef: 0.1116 - dice_coef: 0.1825
Epoch 3: val_jacard_coef improved from 0.00703 to 0.00737, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 5s 5s/step - loss: 0.8175 - accuracy: 0.0719 - jacard_coef: 0.1116 - dice_coef: 0.1825 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0074 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 4/100
1/1 [==============================] - ETA: 0s - loss: 0.8038 - accuracy: 0.0779 - jacard_coef: 0.1221 - dice_coef: 0.1962
Epoch 4: val_jacard_coef improved from 0.00737 to 0.00774, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8038 - accuracy: 0.0779 - jacard_coef: 0.1221 - dice_coef: 0.1962 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0077 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 5/100
1/1 [==============================] - ETA: 0s - loss: 0.8081 - accuracy: 0.0754 - jacard_coef: 0.1187 - dice_coef: 0.1919
Epoch 5: val_jacard_coef improved from 0.00774 to 0.00807, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8081 - accuracy: 0.0754 - jacard_coef: 0.1187 - dice_coef: 0.1919 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0081 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 6/100
1/1 [==============================] - ETA: 0s - loss: 0.8304 - accuracy: 0.0642 - jacard_coef: 0.1021 - dice_coef: 0.1696
Epoch 6: val_jacard_coef improved from 0.00807 to 0.00829, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8304 - accuracy: 0.0642 - jacard_coef: 0.1021 - dice_coef: 0.1696 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0083 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 7/100
1/1 [==============================] - ETA: 0s - loss: 0.8263 - accuracy: 0.0665 - jacard_coef: 0.1051 - dice_coef: 0.1737
Epoch 7: val_jacard_coef improved from 0.00829 to 0.00860, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8263 - accuracy: 0.0665 - jacard_coef: 0.1051 - dice_coef: 0.1737 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0086 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 8/100
1/1 [==============================] - ETA: 0s - loss: 0.8548 - accuracy: 0.0539 - jacard_coef: 0.0850 - dice_coef: 0.1452
Epoch 8: val_jacard_coef improved from 0.00860 to 0.00899, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8548 - accuracy: 0.0539 - jacard_coef: 0.0850 - dice_coef: 0.1452 - val_loss: 0.8042 - val_accuracy: 0.8210 - val_jacard_coef: 0.0090 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 9/100
1/1 [==============================] - ETA: 0s - loss: 0.8100 - accuracy: 0.0741 - jacard_coef: 0.1173 - dice_coef: 0.1900
Epoch 9: val_jacard_coef improved from 0.00899 to 0.00933, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8100 - accuracy: 0.0741 - jacard_coef: 0.1173 - dice_coef: 0.1900 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0093 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 10/100
1/1 [==============================] - ETA: 0s - loss: 0.7906 - accuracy: 0.0849 - jacard_coef: 0.1324 - dice_coef: 0.2094
Epoch 10: val_jacard_coef improved from 0.00933 to 0.00975, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7906 - accuracy: 0.0849 - jacard_coef: 0.1324 - dice_coef: 0.2094 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0097 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 11/100
1/1 [==============================] - ETA: 0s - loss: 0.8492 - accuracy: 0.0575 - jacard_coef: 0.0888 - dice_coef: 0.1508
Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

Epoch 11: val_jacard_coef improved from 0.00975 to 0.01030, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8492 - accuracy: 0.0575 - jacard_coef: 0.0888 - dice_coef: 0.1508 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0103 - val_dice_coef: 0.1958 - lr: 1.0000e-04
Epoch 12/100
1/1 [==============================] - ETA: 0s - loss: 0.7959 - accuracy: 0.0822 - jacard_coef: 0.1282 - dice_coef: 0.2041
Epoch 12: val_jacard_coef improved from 0.01030 to 0.01056, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7959 - accuracy: 0.0822 - jacard_coef: 0.1282 - dice_coef: 0.2041 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0106 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 13/100
1/1 [==============================] - ETA: 0s - loss: 0.8129 - accuracy: 0.0733 - jacard_coef: 0.1151 - dice_coef: 0.1871
Epoch 13: val_jacard_coef improved from 0.01056 to 0.01094, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8129 - accuracy: 0.0733 - jacard_coef: 0.1151 - dice_coef: 0.1871 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0109 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 14/100
1/1 [==============================] - ETA: 0s - loss: 0.7960 - accuracy: 0.0824 - jacard_coef: 0.1281 - dice_coef: 0.2040
Epoch 14: val_jacard_coef improved from 0.01094 to 0.01136, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7960 - accuracy: 0.0824 - jacard_coef: 0.1281 - dice_coef: 0.2040 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0114 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 15/100
1/1 [==============================] - ETA: 0s - loss: 0.8288 - accuracy: 0.0656 - jacard_coef: 0.1033 - dice_coef: 0.1712
Epoch 15: val_jacard_coef improved from 0.01136 to 0.01183, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8288 - accuracy: 0.0656 - jacard_coef: 0.1033 - dice_coef: 0.1712 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0118 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 16/100
1/1 [==============================] - ETA: 0s - loss: 0.7888 - accuracy: 0.0860 - jacard_coef: 0.1338 - dice_coef: 0.2112
Epoch 16: val_jacard_coef improved from 0.01183 to 0.01221, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7888 - accuracy: 0.0860 - jacard_coef: 0.1338 - dice_coef: 0.2112 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0122 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 17/100
1/1 [==============================] - ETA: 0s - loss: 0.8332 - accuracy: 0.0645 - jacard_coef: 0.1001 - dice_coef: 0.1668
Epoch 17: val_jacard_coef improved from 0.01221 to 0.01273, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8332 - accuracy: 0.0645 - jacard_coef: 0.1001 - dice_coef: 0.1668 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0127 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 18/100
1/1 [==============================] - ETA: 0s - loss: 0.8311 - accuracy: 0.0644 - jacard_coef: 0.1016 - dice_coef: 0.1689
Epoch 18: val_jacard_coef improved from 0.01273 to 0.01330, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8311 - accuracy: 0.0644 - jacard_coef: 0.1016 - dice_coef: 0.1689 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0133 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 19/100
1/1 [==============================] - ETA: 0s - loss: 0.8142 - accuracy: 0.0732 - jacard_coef: 0.1141 - dice_coef: 0.1858
Epoch 19: val_jacard_coef improved from 0.01330 to 0.01391, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8142 - accuracy: 0.0732 - jacard_coef: 0.1141 - dice_coef: 0.1858 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0139 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 20/100
1/1 [==============================] - ETA: 0s - loss: 0.8183 - accuracy: 0.0721 - jacard_coef: 0.1110 - dice_coef: 0.1817
Epoch 20: val_jacard_coef improved from 0.01391 to 0.01464, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8183 - accuracy: 0.0721 - jacard_coef: 0.1110 - dice_coef: 0.1817 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0146 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 21/100
1/1 [==============================] - ETA: 0s - loss: 0.8221 - accuracy: 0.0690 - jacard_coef: 0.1082 - dice_coef: 0.1779
Epoch 21: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

Epoch 21: val_jacard_coef improved from 0.01464 to 0.01518, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8221 - accuracy: 0.0690 - jacard_coef: 0.1082 - dice_coef: 0.1779 - val_loss: 0.8042 - val_accuracy: 0.8211 - val_jacard_coef: 0.0152 - val_dice_coef: 0.1958 - lr: 5.0000e-05
Epoch 22/100
1/1 [==============================] - ETA: 0s - loss: 0.8018 - accuracy: 0.0786 - jacard_coef: 0.1236 - dice_coef: 0.1982
Epoch 22: val_jacard_coef improved from 0.01518 to 0.01571, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8018 - accuracy: 0.0786 - jacard_coef: 0.1236 - dice_coef: 0.1982 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0157 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 23/100
1/1 [==============================] - ETA: 0s - loss: 0.8311 - accuracy: 0.0637 - jacard_coef: 0.1016 - dice_coef: 0.1689
Epoch 23: val_jacard_coef improved from 0.01571 to 0.01619, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8311 - accuracy: 0.0637 - jacard_coef: 0.1016 - dice_coef: 0.1689 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0162 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 24/100
1/1 [==============================] - ETA: 0s - loss: 0.7945 - accuracy: 0.0830 - jacard_coef: 0.1293 - dice_coef: 0.2055
Epoch 24: val_jacard_coef improved from 0.01619 to 0.01662, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7945 - accuracy: 0.0830 - jacard_coef: 0.1293 - dice_coef: 0.2055 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0166 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 25/100
1/1 [==============================] - ETA: 0s - loss: 0.7821 - accuracy: 0.0891 - jacard_coef: 0.1393 - dice_coef: 0.2179
Epoch 25: val_jacard_coef improved from 0.01662 to 0.01703, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7821 - accuracy: 0.0891 - jacard_coef: 0.1393 - dice_coef: 0.2179 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0170 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 26/100
1/1 [==============================] - ETA: 0s - loss: 0.8045 - accuracy: 0.0777 - jacard_coef: 0.1215 - dice_coef: 0.1955
Epoch 26: val_jacard_coef improved from 0.01703 to 0.01728, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8045 - accuracy: 0.0777 - jacard_coef: 0.1215 - dice_coef: 0.1955 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0173 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 27/100
1/1 [==============================] - ETA: 0s - loss: 0.8318 - accuracy: 0.0655 - jacard_coef: 0.1011 - dice_coef: 0.1682
Epoch 27: val_jacard_coef improved from 0.01728 to 0.01771, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8318 - accuracy: 0.0655 - jacard_coef: 0.1011 - dice_coef: 0.1682 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0177 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 28/100
1/1 [==============================] - ETA: 0s - loss: 0.8224 - accuracy: 0.0685 - jacard_coef: 0.1080 - dice_coef: 0.1776
Epoch 28: val_jacard_coef improved from 0.01771 to 0.01793, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8224 - accuracy: 0.0685 - jacard_coef: 0.1080 - dice_coef: 0.1776 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0179 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 29/100
1/1 [==============================] - ETA: 0s - loss: 0.8211 - accuracy: 0.0699 - jacard_coef: 0.1089 - dice_coef: 0.1789
Epoch 29: val_jacard_coef improved from 0.01793 to 0.01810, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8211 - accuracy: 0.0699 - jacard_coef: 0.1089 - dice_coef: 0.1789 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0181 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 30/100
1/1 [==============================] - ETA: 0s - loss: 0.8097 - accuracy: 0.0763 - jacard_coef: 0.1175 - dice_coef: 0.1903
Epoch 30: val_jacard_coef improved from 0.01810 to 0.01844, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8097 - accuracy: 0.0763 - jacard_coef: 0.1175 - dice_coef: 0.1903 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0184 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 31/100
1/1 [==============================] - ETA: 0s - loss: 0.8487 - accuracy: 0.0567 - jacard_coef: 0.0891 - dice_coef: 0.1513
Epoch 31: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.

Epoch 31: val_jacard_coef improved from 0.01844 to 0.01867, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8487 - accuracy: 0.0567 - jacard_coef: 0.0891 - dice_coef: 0.1513 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0187 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 32/100
1/1 [==============================] - ETA: 0s - loss: 0.8147 - accuracy: 0.0734 - jacard_coef: 0.1137 - dice_coef: 0.1853
Epoch 32: val_jacard_coef improved from 0.01867 to 0.01879, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8147 - accuracy: 0.0734 - jacard_coef: 0.1137 - dice_coef: 0.1853 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0188 - val_dice_coef: 0.1958 - lr: 1.2500e-05
Epoch 33/100
1/1 [==============================] - ETA: 0s - loss: 0.8199 - accuracy: 0.0711 - jacard_coef: 0.1098 - dice_coef: 0.1801
Epoch 33: val_jacard_coef improved from 0.01879 to 0.01900, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8199 - accuracy: 0.0711 - jacard_coef: 0.1098 - dice_coef: 0.1801 - val_loss: 0.8042 - val_accuracy: 0.8212 - val_jacard_coef: 0.0190 - val_dice_coef: 0.1958 - lr: 1.2500e-05
Epoch 34/100
1/1 [==============================] - ETA: 0s - loss: 0.8452 - accuracy: 0.0583 - jacard_coef: 0.0916 - dice_coef: 0.1548
Epoch 34: val_jacard_coef improved from 0.01900 to 0.01932, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8452 - accuracy: 0.0583 - jacard_coef: 0.0916 - dice_coef: 0.1548 - val_loss: 0.8041 - val_accuracy: 0.8212 - val_jacard_coef: 0.0193 - val_dice_coef: 0.1958 - lr: 1.2500e-05
Epoch 35/100
1/1 [==============================] - ETA: 0s - loss: 0.8147 - accuracy: 0.0718 - jacard_coef: 0.1137 - dice_coef: 0.1853
Epoch 35: val_jacard_coef improved from 0.01932 to 0.01965, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8147 - accuracy: 0.0718 - jacard_coef: 0.1137 - dice_coef: 0.1853 - val_loss: 0.8041 - val_accuracy: 0.8212 - val_jacard_coef: 0.0197 - val_dice_coef: 0.1959 - lr: 1.2500e-05
Epoch 36/100
1/1 [==============================] - ETA: 0s - loss: 0.8388 - accuracy: 0.0600 - jacard_coef: 0.0961 - dice_coef: 0.1612
Epoch 36: val_jacard_coef improved from 0.01965 to 0.02035, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8388 - accuracy: 0.0600 - jacard_coef: 0.0961 - dice_coef: 0.1612 - val_loss: 0.8041 - val_accuracy: 0.8212 - val_jacard_coef: 0.0204 - val_dice_coef: 0.1959 - lr: 1.2500e-05
Epoch 37/100
1/1 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.0746 - jacard_coef: 0.1184 - dice_coef: 0.1915
Epoch 37: val_jacard_coef improved from 0.02035 to 0.02130, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8085 - accuracy: 0.0746 - jacard_coef: 0.1184 - dice_coef: 0.1915 - val_loss: 0.8041 - val_accuracy: 0.8213 - val_jacard_coef: 0.0213 - val_dice_coef: 0.1959 - lr: 1.2500e-05
Epoch 38/100
1/1 [==============================] - ETA: 0s - loss: 0.8399 - accuracy: 0.0608 - jacard_coef: 0.0953 - dice_coef: 0.1601
Epoch 38: val_jacard_coef improved from 0.02130 to 0.02237, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8399 - accuracy: 0.0608 - jacard_coef: 0.0953 - dice_coef: 0.1601 - val_loss: 0.8041 - val_accuracy: 0.8213 - val_jacard_coef: 0.0224 - val_dice_coef: 0.1959 - lr: 1.2500e-05
Epoch 39/100
1/1 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.0751 - jacard_coef: 0.1178 - dice_coef: 0.1907
Epoch 39: val_jacard_coef improved from 0.02237 to 0.02388, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8093 - accuracy: 0.0751 - jacard_coef: 0.1178 - dice_coef: 0.1907 - val_loss: 0.8041 - val_accuracy: 0.8213 - val_jacard_coef: 0.0239 - val_dice_coef: 0.1959 - lr: 1.2500e-05
Epoch 40/100
1/1 [==============================] - ETA: 0s - loss: 0.8496 - accuracy: 0.0566 - jacard_coef: 0.0885 - dice_coef: 0.1504
Epoch 40: val_jacard_coef improved from 0.02388 to 0.02560, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8496 - accuracy: 0.0566 - jacard_coef: 0.0885 - dice_coef: 0.1504 - val_loss: 0.8041 - val_accuracy: 0.8213 - val_jacard_coef: 0.0256 - val_dice_coef: 0.1959 - lr: 1.2500e-05
Epoch 41/100
1/1 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.0737 - jacard_coef: 0.1160 - dice_coef: 0.1883
Epoch 41: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.

Epoch 41: val_jacard_coef improved from 0.02560 to 0.02790, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8117 - accuracy: 0.0737 - jacard_coef: 0.1160 - dice_coef: 0.1883 - val_loss: 0.8041 - val_accuracy: 0.8214 - val_jacard_coef: 0.0279 - val_dice_coef: 0.1959 - lr: 1.2500e-05
Epoch 42/100
1/1 [==============================] - ETA: 0s - loss: 0.7987 - accuracy: 0.0806 - jacard_coef: 0.1260 - dice_coef: 0.2013
Epoch 42: val_jacard_coef improved from 0.02790 to 0.03039, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7987 - accuracy: 0.0806 - jacard_coef: 0.1260 - dice_coef: 0.2013 - val_loss: 0.8041 - val_accuracy: 0.8215 - val_jacard_coef: 0.0304 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 43/100
1/1 [==============================] - ETA: 0s - loss: 0.8209 - accuracy: 0.0695 - jacard_coef: 0.1090 - dice_coef: 0.1791
Epoch 43: val_jacard_coef improved from 0.03039 to 0.03335, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8209 - accuracy: 0.0695 - jacard_coef: 0.1090 - dice_coef: 0.1791 - val_loss: 0.8041 - val_accuracy: 0.8215 - val_jacard_coef: 0.0334 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 44/100
1/1 [==============================] - ETA: 0s - loss: 0.8088 - accuracy: 0.0765 - jacard_coef: 0.1182 - dice_coef: 0.1912
Epoch 44: val_jacard_coef improved from 0.03335 to 0.03752, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8088 - accuracy: 0.0765 - jacard_coef: 0.1182 - dice_coef: 0.1912 - val_loss: 0.8041 - val_accuracy: 0.8216 - val_jacard_coef: 0.0375 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 45/100
1/1 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.0680 - jacard_coef: 0.1074 - dice_coef: 0.1769
Epoch 45: val_jacard_coef improved from 0.03752 to 0.04171, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8231 - accuracy: 0.0680 - jacard_coef: 0.1074 - dice_coef: 0.1769 - val_loss: 0.8041 - val_accuracy: 0.8216 - val_jacard_coef: 0.0417 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 46/100
1/1 [==============================] - ETA: 0s - loss: 0.8428 - accuracy: 0.0593 - jacard_coef: 0.0933 - dice_coef: 0.1572
Epoch 46: val_jacard_coef improved from 0.04171 to 0.04633, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8428 - accuracy: 0.0593 - jacard_coef: 0.0933 - dice_coef: 0.1572 - val_loss: 0.8041 - val_accuracy: 0.8214 - val_jacard_coef: 0.0463 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 47/100
1/1 [==============================] - ETA: 0s - loss: 0.8195 - accuracy: 0.0698 - jacard_coef: 0.1101 - dice_coef: 0.1805
Epoch 47: val_jacard_coef improved from 0.04633 to 0.05113, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8195 - accuracy: 0.0698 - jacard_coef: 0.1101 - dice_coef: 0.1805 - val_loss: 0.8041 - val_accuracy: 0.8208 - val_jacard_coef: 0.0511 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 48/100
1/1 [==============================] - ETA: 0s - loss: 0.8003 - accuracy: 0.0800 - jacard_coef: 0.1247 - dice_coef: 0.1997
Epoch 48: val_jacard_coef improved from 0.05113 to 0.05627, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8003 - accuracy: 0.0800 - jacard_coef: 0.1247 - dice_coef: 0.1997 - val_loss: 0.8041 - val_accuracy: 0.8200 - val_jacard_coef: 0.0563 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 49/100
1/1 [==============================] - ETA: 0s - loss: 0.8138 - accuracy: 0.0732 - jacard_coef: 0.1144 - dice_coef: 0.1862
Epoch 49: val_jacard_coef improved from 0.05627 to 0.06086, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8138 - accuracy: 0.0732 - jacard_coef: 0.1144 - dice_coef: 0.1862 - val_loss: 0.8041 - val_accuracy: 0.8196 - val_jacard_coef: 0.0609 - val_dice_coef: 0.1959 - lr: 6.2500e-06
Epoch 50/100
1/1 [==============================] - ETA: 0s - loss: 0.8361 - accuracy: 0.0621 - jacard_coef: 0.0980 - dice_coef: 0.1639
Epoch 50: val_jacard_coef improved from 0.06086 to 0.06569, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8361 - accuracy: 0.0621 - jacard_coef: 0.0980 - dice_coef: 0.1639 - val_loss: 0.8042 - val_accuracy: 0.8191 - val_jacard_coef: 0.0657 - val_dice_coef: 0.1958 - lr: 6.2500e-06
Epoch 51/100
1/1 [==============================] - ETA: 0s - loss: 0.7974 - accuracy: 0.0815 - jacard_coef: 0.1270 - dice_coef: 0.2026
Epoch 51: val_jacard_coef improved from 0.06569 to 0.06977, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7974 - accuracy: 0.0815 - jacard_coef: 0.1270 - dice_coef: 0.2026 - val_loss: 0.8042 - val_accuracy: 0.8176 - val_jacard_coef: 0.0698 - val_dice_coef: 0.1958 - lr: 6.2500e-06
Epoch 52/100
1/1 [==============================] - ETA: 0s - loss: 0.8114 - accuracy: 0.0741 - jacard_coef: 0.1162 - dice_coef: 0.1886
Epoch 52: val_jacard_coef improved from 0.06977 to 0.07349, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8114 - accuracy: 0.0741 - jacard_coef: 0.1162 - dice_coef: 0.1886 - val_loss: 0.8042 - val_accuracy: 0.8137 - val_jacard_coef: 0.0735 - val_dice_coef: 0.1958 - lr: 6.2500e-06
Epoch 53/100
1/1 [==============================] - ETA: 0s - loss: 0.7899 - accuracy: 0.0842 - jacard_coef: 0.1330 - dice_coef: 0.2101
Epoch 53: val_jacard_coef improved from 0.07349 to 0.07555, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7899 - accuracy: 0.0842 - jacard_coef: 0.1330 - dice_coef: 0.2101 - val_loss: 0.8043 - val_accuracy: 0.8055 - val_jacard_coef: 0.0756 - val_dice_coef: 0.1957 - lr: 6.2500e-06
Epoch 54/100
1/1 [==============================] - ETA: 0s - loss: 0.8240 - accuracy: 0.0688 - jacard_coef: 0.1068 - dice_coef: 0.1760
Epoch 54: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.

Epoch 54: val_jacard_coef improved from 0.07555 to 0.07594, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8240 - accuracy: 0.0688 - jacard_coef: 0.1068 - dice_coef: 0.1760 - val_loss: 0.8043 - val_accuracy: 0.7932 - val_jacard_coef: 0.0759 - val_dice_coef: 0.1957 - lr: 6.2500e-06
Epoch 55/100
1/1 [==============================] - ETA: 0s - loss: 0.7885 - accuracy: 0.0854 - jacard_coef: 0.1341 - dice_coef: 0.2115
Epoch 55: val_jacard_coef improved from 0.07594 to 0.07606, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7885 - accuracy: 0.0854 - jacard_coef: 0.1341 - dice_coef: 0.2115 - val_loss: 0.8043 - val_accuracy: 0.7805 - val_jacard_coef: 0.0761 - val_dice_coef: 0.1957 - lr: 3.1250e-06
Epoch 56/100
1/1 [==============================] - ETA: 0s - loss: 0.8144 - accuracy: 0.0732 - jacard_coef: 0.1139 - dice_coef: 0.1856
Epoch 56: val_jacard_coef improved from 0.07606 to 0.07631, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8144 - accuracy: 0.0732 - jacard_coef: 0.1139 - dice_coef: 0.1856 - val_loss: 0.8044 - val_accuracy: 0.7688 - val_jacard_coef: 0.0763 - val_dice_coef: 0.1956 - lr: 3.1250e-06
Epoch 57/100
1/1 [==============================] - ETA: 0s - loss: 0.8067 - accuracy: 0.0774 - jacard_coef: 0.1198 - dice_coef: 0.1933
Epoch 57: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8067 - accuracy: 0.0774 - jacard_coef: 0.1198 - dice_coef: 0.1933 - val_loss: 0.8044 - val_accuracy: 0.7569 - val_jacard_coef: 0.0759 - val_dice_coef: 0.1956 - lr: 3.1250e-06
Epoch 58/100
1/1 [==============================] - ETA: 0s - loss: 0.8146 - accuracy: 0.0727 - jacard_coef: 0.1138 - dice_coef: 0.1854
Epoch 58: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8146 - accuracy: 0.0727 - jacard_coef: 0.1138 - dice_coef: 0.1854 - val_loss: 0.8045 - val_accuracy: 0.7456 - val_jacard_coef: 0.0744 - val_dice_coef: 0.1955 - lr: 3.1250e-06
Epoch 59/100
1/1 [==============================] - ETA: 0s - loss: 0.7816 - accuracy: 0.0888 - jacard_coef: 0.1396 - dice_coef: 0.2184
Epoch 59: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.7816 - accuracy: 0.0888 - jacard_coef: 0.1396 - dice_coef: 0.2184 - val_loss: 0.8046 - val_accuracy: 0.7355 - val_jacard_coef: 0.0726 - val_dice_coef: 0.1954 - lr: 3.1250e-06
Epoch 60/100
1/1 [==============================] - ETA: 0s - loss: 0.7992 - accuracy: 0.0803 - jacard_coef: 0.1256 - dice_coef: 0.2008
Epoch 60: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.7992 - accuracy: 0.0803 - jacard_coef: 0.1256 - dice_coef: 0.2008 - val_loss: 0.8046 - val_accuracy: 0.7275 - val_jacard_coef: 0.0709 - val_dice_coef: 0.1954 - lr: 3.1250e-06
Epoch 61/100
1/1 [==============================] - ETA: 0s - loss: 0.8063 - accuracy: 0.0778 - jacard_coef: 0.1201 - dice_coef: 0.1937
Epoch 61: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8063 - accuracy: 0.0778 - jacard_coef: 0.1201 - dice_coef: 0.1937 - val_loss: 0.8047 - val_accuracy: 0.7207 - val_jacard_coef: 0.0693 - val_dice_coef: 0.1953 - lr: 3.1250e-06
Epoch 62/100
1/1 [==============================] - ETA: 0s - loss: 0.8188 - accuracy: 0.0706 - jacard_coef: 0.1106 - dice_coef: 0.1812
Epoch 62: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8188 - accuracy: 0.0706 - jacard_coef: 0.1106 - dice_coef: 0.1812 - val_loss: 0.8047 - val_accuracy: 0.7147 - val_jacard_coef: 0.0680 - val_dice_coef: 0.1953 - lr: 3.1250e-06
Epoch 63/100
1/1 [==============================] - ETA: 0s - loss: 0.8175 - accuracy: 0.0713 - jacard_coef: 0.1116 - dice_coef: 0.1825
Epoch 63: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8175 - accuracy: 0.0713 - jacard_coef: 0.1116 - dice_coef: 0.1825 - val_loss: 0.8048 - val_accuracy: 0.7096 - val_jacard_coef: 0.0668 - val_dice_coef: 0.1952 - lr: 3.1250e-06
Epoch 64/100
1/1 [==============================] - ETA: 0s - loss: 0.8113 - accuracy: 0.0740 - jacard_coef: 0.1163 - dice_coef: 0.1887
Epoch 64: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.

Epoch 64: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8113 - accuracy: 0.0740 - jacard_coef: 0.1163 - dice_coef: 0.1887 - val_loss: 0.8048 - val_accuracy: 0.7050 - val_jacard_coef: 0.0661 - val_dice_coef: 0.1952 - lr: 3.1250e-06
Epoch 65/100
1/1 [==============================] - ETA: 0s - loss: 0.8169 - accuracy: 0.0710 - jacard_coef: 0.1121 - dice_coef: 0.1831
Epoch 65: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8169 - accuracy: 0.0710 - jacard_coef: 0.1121 - dice_coef: 0.1831 - val_loss: 0.8048 - val_accuracy: 0.7030 - val_jacard_coef: 0.0663 - val_dice_coef: 0.1952 - lr: 1.5625e-06
Epoch 66/100
1/1 [==============================] - ETA: 0s - loss: 0.8157 - accuracy: 0.0721 - jacard_coef: 0.1129 - dice_coef: 0.1843
Epoch 66: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8157 - accuracy: 0.0721 - jacard_coef: 0.1129 - dice_coef: 0.1843 - val_loss: 0.8048 - val_accuracy: 0.7013 - val_jacard_coef: 0.0666 - val_dice_coef: 0.1952 - lr: 1.5625e-06
Epoch 67/100
1/1 [==============================] - ETA: 0s - loss: 0.8297 - accuracy: 0.0644 - jacard_coef: 0.1026 - dice_coef: 0.1703
Epoch 67: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8297 - accuracy: 0.0644 - jacard_coef: 0.1026 - dice_coef: 0.1703 - val_loss: 0.8049 - val_accuracy: 0.6996 - val_jacard_coef: 0.0670 - val_dice_coef: 0.1951 - lr: 1.5625e-06
Epoch 68/100
1/1 [==============================] - ETA: 0s - loss: 0.7842 - accuracy: 0.0875 - jacard_coef: 0.1375 - dice_coef: 0.2158
Epoch 68: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.7842 - accuracy: 0.0875 - jacard_coef: 0.1375 - dice_coef: 0.2158 - val_loss: 0.8049 - val_accuracy: 0.6978 - val_jacard_coef: 0.0678 - val_dice_coef: 0.1951 - lr: 1.5625e-06
Epoch 69/100
1/1 [==============================] - ETA: 0s - loss: 0.8056 - accuracy: 0.0776 - jacard_coef: 0.1206 - dice_coef: 0.1944
Epoch 69: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8056 - accuracy: 0.0776 - jacard_coef: 0.1206 - dice_coef: 0.1944 - val_loss: 0.8049 - val_accuracy: 0.6959 - val_jacard_coef: 0.0686 - val_dice_coef: 0.1951 - lr: 1.5625e-06
Epoch 70/100
1/1 [==============================] - ETA: 0s - loss: 0.8247 - accuracy: 0.0685 - jacard_coef: 0.1063 - dice_coef: 0.1753
Epoch 70: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8247 - accuracy: 0.0685 - jacard_coef: 0.1063 - dice_coef: 0.1753 - val_loss: 0.8049 - val_accuracy: 0.6940 - val_jacard_coef: 0.0698 - val_dice_coef: 0.1951 - lr: 1.5625e-06
Epoch 71/100
1/1 [==============================] - ETA: 0s - loss: 0.8114 - accuracy: 0.0744 - jacard_coef: 0.1162 - dice_coef: 0.1886
Epoch 71: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8114 - accuracy: 0.0744 - jacard_coef: 0.1162 - dice_coef: 0.1886 - val_loss: 0.8048 - val_accuracy: 0.6917 - val_jacard_coef: 0.0714 - val_dice_coef: 0.1952 - lr: 1.5625e-06
Epoch 72/100
1/1 [==============================] - ETA: 0s - loss: 0.8041 - accuracy: 0.0776 - jacard_coef: 0.1218 - dice_coef: 0.1959
Epoch 72: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.8041 - accuracy: 0.0776 - jacard_coef: 0.1218 - dice_coef: 0.1959 - val_loss: 0.8049 - val_accuracy: 0.6887 - val_jacard_coef: 0.0731 - val_dice_coef: 0.1951 - lr: 1.5625e-06
Epoch 73/100
1/1 [==============================] - ETA: 0s - loss: 0.7977 - accuracy: 0.0816 - jacard_coef: 0.1268 - dice_coef: 0.2023
Epoch 73: val_jacard_coef did not improve from 0.07631
1/1 [==============================] - 1s 1s/step - loss: 0.7977 - accuracy: 0.0816 - jacard_coef: 0.1268 - dice_coef: 0.2023 - val_loss: 0.8049 - val_accuracy: 0.6850 - val_jacard_coef: 0.0751 - val_dice_coef: 0.1951 - lr: 1.5625e-06
Epoch 74/100
1/1 [==============================] - ETA: 0s - loss: 0.8456 - accuracy: 0.0578 - jacard_coef: 0.0913 - dice_coef: 0.1544
Epoch 74: ReduceLROnPlateau reducing learning rate to 1e-06.

Epoch 74: val_jacard_coef improved from 0.07631 to 0.07704, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8456 - accuracy: 0.0578 - jacard_coef: 0.0913 - dice_coef: 0.1544 - val_loss: 0.8049 - val_accuracy: 0.6799 - val_jacard_coef: 0.0770 - val_dice_coef: 0.1951 - lr: 1.5625e-06
Epoch 75/100
1/1 [==============================] - ETA: 0s - loss: 0.8042 - accuracy: 0.0777 - jacard_coef: 0.1217 - dice_coef: 0.1958
Epoch 75: val_jacard_coef improved from 0.07704 to 0.07841, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8042 - accuracy: 0.0777 - jacard_coef: 0.1217 - dice_coef: 0.1958 - val_loss: 0.8050 - val_accuracy: 0.6736 - val_jacard_coef: 0.0784 - val_dice_coef: 0.1950 - lr: 1.0000e-06
Epoch 76/100
1/1 [==============================] - ETA: 0s - loss: 0.7959 - accuracy: 0.0811 - jacard_coef: 0.1282 - dice_coef: 0.2041
Epoch 76: val_jacard_coef improved from 0.07841 to 0.08023, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7959 - accuracy: 0.0811 - jacard_coef: 0.1282 - dice_coef: 0.2041 - val_loss: 0.8050 - val_accuracy: 0.6676 - val_jacard_coef: 0.0802 - val_dice_coef: 0.1950 - lr: 1.0000e-06
Epoch 77/100
1/1 [==============================] - ETA: 0s - loss: 0.8217 - accuracy: 0.0699 - jacard_coef: 0.1084 - dice_coef: 0.1783
Epoch 77: val_jacard_coef improved from 0.08023 to 0.08214, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8217 - accuracy: 0.0699 - jacard_coef: 0.1084 - dice_coef: 0.1783 - val_loss: 0.8050 - val_accuracy: 0.6613 - val_jacard_coef: 0.0821 - val_dice_coef: 0.1950 - lr: 1.0000e-06
Epoch 78/100
1/1 [==============================] - ETA: 0s - loss: 0.8200 - accuracy: 0.0705 - jacard_coef: 0.1097 - dice_coef: 0.1800
Epoch 78: val_jacard_coef improved from 0.08214 to 0.08414, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8200 - accuracy: 0.0705 - jacard_coef: 0.1097 - dice_coef: 0.1800 - val_loss: 0.8050 - val_accuracy: 0.6545 - val_jacard_coef: 0.0841 - val_dice_coef: 0.1950 - lr: 1.0000e-06
Epoch 79/100
1/1 [==============================] - ETA: 0s - loss: 0.8205 - accuracy: 0.0694 - jacard_coef: 0.1094 - dice_coef: 0.1795
Epoch 79: val_jacard_coef improved from 0.08414 to 0.08652, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8205 - accuracy: 0.0694 - jacard_coef: 0.1094 - dice_coef: 0.1795 - val_loss: 0.8049 - val_accuracy: 0.6477 - val_jacard_coef: 0.0865 - val_dice_coef: 0.1951 - lr: 1.0000e-06
Epoch 80/100
1/1 [==============================] - ETA: 0s - loss: 0.7864 - accuracy: 0.0875 - jacard_coef: 0.1358 - dice_coef: 0.2136
Epoch 80: val_jacard_coef improved from 0.08652 to 0.08912, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7864 - accuracy: 0.0875 - jacard_coef: 0.1358 - dice_coef: 0.2136 - val_loss: 0.8047 - val_accuracy: 0.6403 - val_jacard_coef: 0.0891 - val_dice_coef: 0.1953 - lr: 1.0000e-06
Epoch 81/100
1/1 [==============================] - ETA: 0s - loss: 0.8036 - accuracy: 0.0782 - jacard_coef: 0.1221 - dice_coef: 0.1964
Epoch 81: val_jacard_coef improved from 0.08912 to 0.09136, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8036 - accuracy: 0.0782 - jacard_coef: 0.1221 - dice_coef: 0.1964 - val_loss: 0.8044 - val_accuracy: 0.6316 - val_jacard_coef: 0.0914 - val_dice_coef: 0.1956 - lr: 1.0000e-06
Epoch 82/100
1/1 [==============================] - ETA: 0s - loss: 0.7948 - accuracy: 0.0833 - jacard_coef: 0.1291 - dice_coef: 0.2052
Epoch 82: val_jacard_coef improved from 0.09136 to 0.09403, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7948 - accuracy: 0.0833 - jacard_coef: 0.1291 - dice_coef: 0.2052 - val_loss: 0.8041 - val_accuracy: 0.6217 - val_jacard_coef: 0.0940 - val_dice_coef: 0.1959 - lr: 1.0000e-06
Epoch 83/100
1/1 [==============================] - ETA: 0s - loss: 0.8061 - accuracy: 0.0765 - jacard_coef: 0.1203 - dice_coef: 0.1939
Epoch 83: val_jacard_coef improved from 0.09403 to 0.09684, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8061 - accuracy: 0.0765 - jacard_coef: 0.1203 - dice_coef: 0.1939 - val_loss: 0.8039 - val_accuracy: 0.6106 - val_jacard_coef: 0.0968 - val_dice_coef: 0.1961 - lr: 1.0000e-06
Epoch 84/100
1/1 [==============================] - ETA: 0s - loss: 0.8137 - accuracy: 0.0728 - jacard_coef: 0.1144 - dice_coef: 0.1863
Epoch 84: val_jacard_coef improved from 0.09684 to 0.09994, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8137 - accuracy: 0.0728 - jacard_coef: 0.1144 - dice_coef: 0.1863 - val_loss: 0.8035 - val_accuracy: 0.5984 - val_jacard_coef: 0.0999 - val_dice_coef: 0.1965 - lr: 1.0000e-06
Epoch 85/100
1/1 [==============================] - ETA: 0s - loss: 0.7942 - accuracy: 0.0829 - jacard_coef: 0.1295 - dice_coef: 0.2058
Epoch 85: val_jacard_coef improved from 0.09994 to 0.10361, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7942 - accuracy: 0.0829 - jacard_coef: 0.1295 - dice_coef: 0.2058 - val_loss: 0.8032 - val_accuracy: 0.5854 - val_jacard_coef: 0.1036 - val_dice_coef: 0.1968 - lr: 1.0000e-06
Epoch 86/100
1/1 [==============================] - ETA: 0s - loss: 0.7966 - accuracy: 0.0824 - jacard_coef: 0.1276 - dice_coef: 0.2034
Epoch 86: val_jacard_coef improved from 0.10361 to 0.10797, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7966 - accuracy: 0.0824 - jacard_coef: 0.1276 - dice_coef: 0.2034 - val_loss: 0.8028 - val_accuracy: 0.5719 - val_jacard_coef: 0.1080 - val_dice_coef: 0.1972 - lr: 1.0000e-06
Epoch 87/100
1/1 [==============================] - ETA: 0s - loss: 0.8050 - accuracy: 0.0774 - jacard_coef: 0.1211 - dice_coef: 0.1950
Epoch 87: val_jacard_coef improved from 0.10797 to 0.11407, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8050 - accuracy: 0.0774 - jacard_coef: 0.1211 - dice_coef: 0.1950 - val_loss: 0.8023 - val_accuracy: 0.5574 - val_jacard_coef: 0.1141 - val_dice_coef: 0.1977 - lr: 1.0000e-06
Epoch 88/100
1/1 [==============================] - ETA: 0s - loss: 0.8204 - accuracy: 0.0697 - jacard_coef: 0.1094 - dice_coef: 0.1796
Epoch 88: val_jacard_coef improved from 0.11407 to 0.12189, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8204 - accuracy: 0.0697 - jacard_coef: 0.1094 - dice_coef: 0.1796 - val_loss: 0.8018 - val_accuracy: 0.5416 - val_jacard_coef: 0.1219 - val_dice_coef: 0.1982 - lr: 1.0000e-06
Epoch 89/100
1/1 [==============================] - ETA: 0s - loss: 0.8420 - accuracy: 0.0585 - jacard_coef: 0.0938 - dice_coef: 0.1580
Epoch 89: val_jacard_coef improved from 0.12189 to 0.13014, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8420 - accuracy: 0.0585 - jacard_coef: 0.0938 - dice_coef: 0.1580 - val_loss: 0.8013 - val_accuracy: 0.5217 - val_jacard_coef: 0.1301 - val_dice_coef: 0.1987 - lr: 1.0000e-06
Epoch 90/100
1/1 [==============================] - ETA: 0s - loss: 0.8068 - accuracy: 0.0759 - jacard_coef: 0.1197 - dice_coef: 0.1932
Epoch 90: val_jacard_coef improved from 0.13014 to 0.13809, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8068 - accuracy: 0.0759 - jacard_coef: 0.1197 - dice_coef: 0.1932 - val_loss: 0.8008 - val_accuracy: 0.4951 - val_jacard_coef: 0.1381 - val_dice_coef: 0.1992 - lr: 1.0000e-06
Epoch 91/100
1/1 [==============================] - ETA: 0s - loss: 0.8289 - accuracy: 0.0656 - jacard_coef: 0.1032 - dice_coef: 0.1711
Epoch 91: val_jacard_coef improved from 0.13809 to 0.14414, saving model to microbead_training_20251009_073134/best_attention_unet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8289 - accuracy: 0.0656 - jacard_coef: 0.1032 - dice_coef: 0.1711 - val_loss: 0.8002 - val_accuracy: 0.4566 - val_jacard_coef: 0.1441 - val_dice_coef: 0.1998 - lr: 1.0000e-06
Epoch 92/100
1/1 [==============================] - ETA: 0s - loss: 0.8339 - accuracy: 0.0634 - jacard_coef: 0.0996 - dice_coef: 0.1661
Epoch 92: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8339 - accuracy: 0.0634 - jacard_coef: 0.0996 - dice_coef: 0.1661 - val_loss: 0.7995 - val_accuracy: 0.3995 - val_jacard_coef: 0.1430 - val_dice_coef: 0.2005 - lr: 1.0000e-06
Epoch 93/100
1/1 [==============================] - ETA: 0s - loss: 0.8098 - accuracy: 0.0740 - jacard_coef: 0.1174 - dice_coef: 0.1902
Epoch 93: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8098 - accuracy: 0.0740 - jacard_coef: 0.1174 - dice_coef: 0.1902 - val_loss: 0.7989 - val_accuracy: 0.3175 - val_jacard_coef: 0.1343 - val_dice_coef: 0.2011 - lr: 1.0000e-06
Epoch 94/100
1/1 [==============================] - ETA: 0s - loss: 0.8010 - accuracy: 0.0796 - jacard_coef: 0.1242 - dice_coef: 0.1990
Epoch 94: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8010 - accuracy: 0.0796 - jacard_coef: 0.1242 - dice_coef: 0.1990 - val_loss: 0.7982 - val_accuracy: 0.2574 - val_jacard_coef: 0.1284 - val_dice_coef: 0.2018 - lr: 1.0000e-06
Epoch 95/100
1/1 [==============================] - ETA: 0s - loss: 0.8120 - accuracy: 0.0740 - jacard_coef: 0.1157 - dice_coef: 0.1880
Epoch 95: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8120 - accuracy: 0.0740 - jacard_coef: 0.1157 - dice_coef: 0.1880 - val_loss: 0.7974 - val_accuracy: 0.2252 - val_jacard_coef: 0.1254 - val_dice_coef: 0.2026 - lr: 1.0000e-06
Epoch 96/100
1/1 [==============================] - ETA: 0s - loss: 0.8162 - accuracy: 0.0709 - jacard_coef: 0.1126 - dice_coef: 0.1838
Epoch 96: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8162 - accuracy: 0.0709 - jacard_coef: 0.1126 - dice_coef: 0.1838 - val_loss: 0.7967 - val_accuracy: 0.2090 - val_jacard_coef: 0.1242 - val_dice_coef: 0.2033 - lr: 1.0000e-06
Epoch 97/100
1/1 [==============================] - ETA: 0s - loss: 0.8429 - accuracy: 0.0592 - jacard_coef: 0.0932 - dice_coef: 0.1571
Epoch 97: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8429 - accuracy: 0.0592 - jacard_coef: 0.0932 - dice_coef: 0.1571 - val_loss: 0.7959 - val_accuracy: 0.1990 - val_jacard_coef: 0.1237 - val_dice_coef: 0.2041 - lr: 1.0000e-06
Epoch 98/100
1/1 [==============================] - ETA: 0s - loss: 0.8191 - accuracy: 0.0704 - jacard_coef: 0.1104 - dice_coef: 0.1809
Epoch 98: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8191 - accuracy: 0.0704 - jacard_coef: 0.1104 - dice_coef: 0.1809 - val_loss: 0.7951 - val_accuracy: 0.1915 - val_jacard_coef: 0.1235 - val_dice_coef: 0.2049 - lr: 1.0000e-06
Epoch 99/100
1/1 [==============================] - ETA: 0s - loss: 0.8044 - accuracy: 0.0775 - jacard_coef: 0.1215 - dice_coef: 0.1956
Epoch 99: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8044 - accuracy: 0.0775 - jacard_coef: 0.1215 - dice_coef: 0.1956 - val_loss: 0.7942 - val_accuracy: 0.1856 - val_jacard_coef: 0.1232 - val_dice_coef: 0.2058 - lr: 1.0000e-06
Epoch 100/100
1/1 [==============================] - ETA: 0s - loss: 0.8345 - accuracy: 0.0636 - jacard_coef: 0.0992 - dice_coef: 0.1655
Epoch 100: val_jacard_coef did not improve from 0.14414
1/1 [==============================] - 1s 1s/step - loss: 0.8345 - accuracy: 0.0636 - jacard_coef: 0.0992 - dice_coef: 0.1655 - val_loss: 0.7933 - val_accuracy: 0.1802 - val_jacard_coef: 0.1231 - val_dice_coef: 0.2067 - lr: 1.0000e-06
Restoring model weights from the end of the best epoch: 91.
‚úì Attention UNet training completed in 0:04:40.594105
Best Val Jaccard: 0.1441

================================================================================
TRAINING MODEL 3/3: ATTENTION RESIDUAL U-NET
================================================================================
Hyperparameters: LR=0.0001, BS=32, Dropout=0.3

Epoch 1/100
2025-10-09 07:40:17.802941: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:1021] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inAttentionResUNet/dropout_18/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
1/1 [==============================] - ETA: 0s - loss: 0.8193 - accuracy: 0.8433 - jacard_coef: 4.6124e-13 - dice_coef: 0.1807
Epoch 1: val_jacard_coef improved from -inf to 0.02570, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 33s 33s/step - loss: 0.8193 - accuracy: 0.8433 - jacard_coef: 4.6124e-13 - dice_coef: 0.1807 - val_loss: 0.8047 - val_accuracy: 0.7838 - val_jacard_coef: 0.0257 - val_dice_coef: 0.1953 - lr: 1.0000e-04
Epoch 2/100
1/1 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.0796 - jacard_coef: 0.1231 - dice_coef: 0.1976
Epoch 2: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8024 - accuracy: 0.0796 - jacard_coef: 0.1231 - dice_coef: 0.1976 - val_loss: 0.8047 - val_accuracy: 0.7878 - val_jacard_coef: 0.0239 - val_dice_coef: 0.1953 - lr: 1.0000e-04
Epoch 3/100
1/1 [==============================] - ETA: 0s - loss: 0.7979 - accuracy: 0.0818 - jacard_coef: 0.1266 - dice_coef: 0.2021
Epoch 3: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.7979 - accuracy: 0.0818 - jacard_coef: 0.1266 - dice_coef: 0.2021 - val_loss: 0.8048 - val_accuracy: 0.7915 - val_jacard_coef: 0.0223 - val_dice_coef: 0.1952 - lr: 1.0000e-04
Epoch 4/100
1/1 [==============================] - ETA: 0s - loss: 0.8269 - accuracy: 0.0669 - jacard_coef: 0.1046 - dice_coef: 0.1731
Epoch 4: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8269 - accuracy: 0.0669 - jacard_coef: 0.1046 - dice_coef: 0.1731 - val_loss: 0.8048 - val_accuracy: 0.7949 - val_jacard_coef: 0.0209 - val_dice_coef: 0.1952 - lr: 1.0000e-04
Epoch 5/100
1/1 [==============================] - ETA: 0s - loss: 0.8116 - accuracy: 0.0735 - jacard_coef: 0.1160 - dice_coef: 0.1884
Epoch 5: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8116 - accuracy: 0.0735 - jacard_coef: 0.1160 - dice_coef: 0.1884 - val_loss: 0.8048 - val_accuracy: 0.7977 - val_jacard_coef: 0.0196 - val_dice_coef: 0.1952 - lr: 1.0000e-04
Epoch 6/100
1/1 [==============================] - ETA: 0s - loss: 0.8108 - accuracy: 0.0741 - jacard_coef: 0.1167 - dice_coef: 0.1892
Epoch 6: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8108 - accuracy: 0.0741 - jacard_coef: 0.1167 - dice_coef: 0.1892 - val_loss: 0.8048 - val_accuracy: 0.8003 - val_jacard_coef: 0.0186 - val_dice_coef: 0.1952 - lr: 1.0000e-04
Epoch 7/100
1/1 [==============================] - ETA: 0s - loss: 0.8263 - accuracy: 0.0665 - jacard_coef: 0.1051 - dice_coef: 0.1737
Epoch 7: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8263 - accuracy: 0.0665 - jacard_coef: 0.1051 - dice_coef: 0.1737 - val_loss: 0.8048 - val_accuracy: 0.8023 - val_jacard_coef: 0.0175 - val_dice_coef: 0.1952 - lr: 1.0000e-04
Epoch 8/100
1/1 [==============================] - ETA: 0s - loss: 0.7775 - accuracy: 0.0917 - jacard_coef: 0.1431 - dice_coef: 0.2225
Epoch 8: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.7775 - accuracy: 0.0917 - jacard_coef: 0.1431 - dice_coef: 0.2225 - val_loss: 0.8048 - val_accuracy: 0.8040 - val_jacard_coef: 0.0167 - val_dice_coef: 0.1952 - lr: 1.0000e-04
Epoch 9/100
1/1 [==============================] - ETA: 0s - loss: 0.7874 - accuracy: 0.0863 - jacard_coef: 0.1350 - dice_coef: 0.2126
Epoch 9: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.7874 - accuracy: 0.0863 - jacard_coef: 0.1350 - dice_coef: 0.2126 - val_loss: 0.8049 - val_accuracy: 0.8056 - val_jacard_coef: 0.0159 - val_dice_coef: 0.1951 - lr: 1.0000e-04
Epoch 10/100
1/1 [==============================] - ETA: 0s - loss: 0.8404 - accuracy: 0.0603 - jacard_coef: 0.0949 - dice_coef: 0.1596
Epoch 10: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8404 - accuracy: 0.0603 - jacard_coef: 0.0949 - dice_coef: 0.1596 - val_loss: 0.8049 - val_accuracy: 0.8072 - val_jacard_coef: 0.0152 - val_dice_coef: 0.1951 - lr: 1.0000e-04
Epoch 11/100
1/1 [==============================] - ETA: 0s - loss: 0.8029 - accuracy: 0.0780 - jacard_coef: 0.1227 - dice_coef: 0.1971
Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.

Epoch 11: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8029 - accuracy: 0.0780 - jacard_coef: 0.1227 - dice_coef: 0.1971 - val_loss: 0.8049 - val_accuracy: 0.8086 - val_jacard_coef: 0.0148 - val_dice_coef: 0.1951 - lr: 1.0000e-04
Epoch 12/100
1/1 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.0756 - jacard_coef: 0.1169 - dice_coef: 0.1895
Epoch 12: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8105 - accuracy: 0.0756 - jacard_coef: 0.1169 - dice_coef: 0.1895 - val_loss: 0.8049 - val_accuracy: 0.8098 - val_jacard_coef: 0.0145 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 13/100
1/1 [==============================] - ETA: 0s - loss: 0.8100 - accuracy: 0.0751 - jacard_coef: 0.1173 - dice_coef: 0.1900
Epoch 13: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8100 - accuracy: 0.0751 - jacard_coef: 0.1173 - dice_coef: 0.1900 - val_loss: 0.8049 - val_accuracy: 0.8106 - val_jacard_coef: 0.0141 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 14/100
1/1 [==============================] - ETA: 0s - loss: 0.8140 - accuracy: 0.0731 - jacard_coef: 0.1142 - dice_coef: 0.1860
Epoch 14: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8140 - accuracy: 0.0731 - jacard_coef: 0.1142 - dice_coef: 0.1860 - val_loss: 0.8049 - val_accuracy: 0.8112 - val_jacard_coef: 0.0142 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 15/100
1/1 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.0706 - jacard_coef: 0.1088 - dice_coef: 0.1787
Epoch 15: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8213 - accuracy: 0.0706 - jacard_coef: 0.1088 - dice_coef: 0.1787 - val_loss: 0.8049 - val_accuracy: 0.8113 - val_jacard_coef: 0.0147 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 16/100
1/1 [==============================] - ETA: 0s - loss: 0.8027 - accuracy: 0.0776 - jacard_coef: 0.1229 - dice_coef: 0.1973
Epoch 16: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8027 - accuracy: 0.0776 - jacard_coef: 0.1229 - dice_coef: 0.1973 - val_loss: 0.8049 - val_accuracy: 0.8106 - val_jacard_coef: 0.0156 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 17/100
1/1 [==============================] - ETA: 0s - loss: 0.7975 - accuracy: 0.0804 - jacard_coef: 0.1269 - dice_coef: 0.2025
Epoch 17: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.7975 - accuracy: 0.0804 - jacard_coef: 0.1269 - dice_coef: 0.2025 - val_loss: 0.8049 - val_accuracy: 0.8087 - val_jacard_coef: 0.0173 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 18/100
1/1 [==============================] - ETA: 0s - loss: 0.7713 - accuracy: 0.0949 - jacard_coef: 0.1482 - dice_coef: 0.2287
Epoch 18: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.7713 - accuracy: 0.0949 - jacard_coef: 0.1482 - dice_coef: 0.2287 - val_loss: 0.8049 - val_accuracy: 0.8066 - val_jacard_coef: 0.0196 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 19/100
1/1 [==============================] - ETA: 0s - loss: 0.8142 - accuracy: 0.0732 - jacard_coef: 0.1141 - dice_coef: 0.1858
Epoch 19: val_jacard_coef did not improve from 0.02570
1/1 [==============================] - 1s 1s/step - loss: 0.8142 - accuracy: 0.0732 - jacard_coef: 0.1141 - dice_coef: 0.1858 - val_loss: 0.8049 - val_accuracy: 0.8039 - val_jacard_coef: 0.0232 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 20/100
1/1 [==============================] - ETA: 0s - loss: 0.8130 - accuracy: 0.0723 - jacard_coef: 0.1150 - dice_coef: 0.1870
Epoch 20: val_jacard_coef improved from 0.02570 to 0.02804, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8130 - accuracy: 0.0723 - jacard_coef: 0.1150 - dice_coef: 0.1870 - val_loss: 0.8049 - val_accuracy: 0.8006 - val_jacard_coef: 0.0280 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 21/100
1/1 [==============================] - ETA: 0s - loss: 0.8094 - accuracy: 0.0750 - jacard_coef: 0.1177 - dice_coef: 0.1906
Epoch 21: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.

Epoch 21: val_jacard_coef improved from 0.02804 to 0.03521, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8094 - accuracy: 0.0750 - jacard_coef: 0.1177 - dice_coef: 0.1906 - val_loss: 0.8049 - val_accuracy: 0.7964 - val_jacard_coef: 0.0352 - val_dice_coef: 0.1951 - lr: 5.0000e-05
Epoch 22/100
1/1 [==============================] - ETA: 0s - loss: 0.7995 - accuracy: 0.0796 - jacard_coef: 0.1253 - dice_coef: 0.2005
Epoch 22: val_jacard_coef improved from 0.03521 to 0.04477, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7995 - accuracy: 0.0796 - jacard_coef: 0.1253 - dice_coef: 0.2005 - val_loss: 0.8049 - val_accuracy: 0.7909 - val_jacard_coef: 0.0448 - val_dice_coef: 0.1951 - lr: 2.5000e-05
Epoch 23/100
1/1 [==============================] - ETA: 0s - loss: 0.8014 - accuracy: 0.0802 - jacard_coef: 0.1238 - dice_coef: 0.1986
Epoch 23: val_jacard_coef improved from 0.04477 to 0.05803, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8014 - accuracy: 0.0802 - jacard_coef: 0.1238 - dice_coef: 0.1986 - val_loss: 0.8048 - val_accuracy: 0.7839 - val_jacard_coef: 0.0580 - val_dice_coef: 0.1952 - lr: 2.5000e-05
Epoch 24/100
1/1 [==============================] - ETA: 0s - loss: 0.8074 - accuracy: 0.0769 - jacard_coef: 0.1192 - dice_coef: 0.1926
Epoch 24: val_jacard_coef improved from 0.05803 to 0.07267, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8074 - accuracy: 0.0769 - jacard_coef: 0.1192 - dice_coef: 0.1926 - val_loss: 0.8048 - val_accuracy: 0.7745 - val_jacard_coef: 0.0727 - val_dice_coef: 0.1952 - lr: 2.5000e-05
Epoch 25/100
1/1 [==============================] - ETA: 0s - loss: 0.8545 - accuracy: 0.0541 - jacard_coef: 0.0851 - dice_coef: 0.1455
Epoch 25: val_jacard_coef improved from 0.07267 to 0.08695, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8545 - accuracy: 0.0541 - jacard_coef: 0.0851 - dice_coef: 0.1455 - val_loss: 0.8048 - val_accuracy: 0.7603 - val_jacard_coef: 0.0870 - val_dice_coef: 0.1952 - lr: 2.5000e-05
Epoch 26/100
1/1 [==============================] - ETA: 0s - loss: 0.8004 - accuracy: 0.0797 - jacard_coef: 0.1247 - dice_coef: 0.1996
Epoch 26: val_jacard_coef improved from 0.08695 to 0.09884, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8004 - accuracy: 0.0797 - jacard_coef: 0.1247 - dice_coef: 0.1996 - val_loss: 0.8048 - val_accuracy: 0.7388 - val_jacard_coef: 0.0988 - val_dice_coef: 0.1952 - lr: 2.5000e-05
Epoch 27/100
1/1 [==============================] - ETA: 0s - loss: 0.8009 - accuracy: 0.0785 - jacard_coef: 0.1243 - dice_coef: 0.1991
Epoch 27: val_jacard_coef improved from 0.09884 to 0.10610, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8009 - accuracy: 0.0785 - jacard_coef: 0.1243 - dice_coef: 0.1991 - val_loss: 0.8048 - val_accuracy: 0.7074 - val_jacard_coef: 0.1061 - val_dice_coef: 0.1952 - lr: 2.5000e-05
Epoch 28/100
1/1 [==============================] - ETA: 0s - loss: 0.7940 - accuracy: 0.0832 - jacard_coef: 0.1297 - dice_coef: 0.2060
Epoch 28: val_jacard_coef improved from 0.10610 to 0.10946, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7940 - accuracy: 0.0832 - jacard_coef: 0.1297 - dice_coef: 0.2060 - val_loss: 0.8048 - val_accuracy: 0.6661 - val_jacard_coef: 0.1095 - val_dice_coef: 0.1952 - lr: 2.5000e-05
Epoch 29/100
1/1 [==============================] - ETA: 0s - loss: 0.8103 - accuracy: 0.0743 - jacard_coef: 0.1170 - dice_coef: 0.1897
Epoch 29: val_jacard_coef improved from 0.10946 to 0.11011, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8103 - accuracy: 0.0743 - jacard_coef: 0.1170 - dice_coef: 0.1897 - val_loss: 0.8047 - val_accuracy: 0.6142 - val_jacard_coef: 0.1101 - val_dice_coef: 0.1953 - lr: 2.5000e-05
Epoch 30/100
1/1 [==============================] - ETA: 0s - loss: 0.8088 - accuracy: 0.0758 - jacard_coef: 0.1182 - dice_coef: 0.1912
Epoch 30: val_jacard_coef did not improve from 0.11011
1/1 [==============================] - 1s 1s/step - loss: 0.8088 - accuracy: 0.0758 - jacard_coef: 0.1182 - dice_coef: 0.1912 - val_loss: 0.8047 - val_accuracy: 0.5473 - val_jacard_coef: 0.1073 - val_dice_coef: 0.1953 - lr: 2.5000e-05
Epoch 31/100
1/1 [==============================] - ETA: 0s - loss: 0.8487 - accuracy: 0.0567 - jacard_coef: 0.0891 - dice_coef: 0.1513
Epoch 31: val_jacard_coef did not improve from 0.11011
1/1 [==============================] - 1s 1s/step - loss: 0.8487 - accuracy: 0.0567 - jacard_coef: 0.0891 - dice_coef: 0.1513 - val_loss: 0.8046 - val_accuracy: 0.4683 - val_jacard_coef: 0.1034 - val_dice_coef: 0.1954 - lr: 2.5000e-05
Epoch 32/100
1/1 [==============================] - ETA: 0s - loss: 0.8147 - accuracy: 0.0734 - jacard_coef: 0.1137 - dice_coef: 0.1853
Epoch 32: val_jacard_coef did not improve from 0.11011
1/1 [==============================] - 1s 1s/step - loss: 0.8147 - accuracy: 0.0734 - jacard_coef: 0.1137 - dice_coef: 0.1853 - val_loss: 0.8045 - val_accuracy: 0.3989 - val_jacard_coef: 0.1017 - val_dice_coef: 0.1955 - lr: 2.5000e-05
Epoch 33/100
1/1 [==============================] - ETA: 0s - loss: 0.8199 - accuracy: 0.0711 - jacard_coef: 0.1098 - dice_coef: 0.1801
Epoch 33: val_jacard_coef did not improve from 0.11011
1/1 [==============================] - 1s 1s/step - loss: 0.8199 - accuracy: 0.0711 - jacard_coef: 0.1098 - dice_coef: 0.1801 - val_loss: 0.8045 - val_accuracy: 0.3526 - val_jacard_coef: 0.1030 - val_dice_coef: 0.1955 - lr: 2.5000e-05
Epoch 34/100
1/1 [==============================] - ETA: 0s - loss: 0.7861 - accuracy: 0.0870 - jacard_coef: 0.1360 - dice_coef: 0.2139
Epoch 34: val_jacard_coef did not improve from 0.11011
1/1 [==============================] - 1s 1s/step - loss: 0.7861 - accuracy: 0.0870 - jacard_coef: 0.1360 - dice_coef: 0.2139 - val_loss: 0.8044 - val_accuracy: 0.3350 - val_jacard_coef: 0.1069 - val_dice_coef: 0.1956 - lr: 2.5000e-05
Epoch 35/100
1/1 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.0765 - jacard_coef: 0.1196 - dice_coef: 0.1930
Epoch 35: val_jacard_coef did not improve from 0.11011
1/1 [==============================] - 1s 1s/step - loss: 0.8070 - accuracy: 0.0765 - jacard_coef: 0.1196 - dice_coef: 0.1930 - val_loss: 0.8043 - val_accuracy: 0.3248 - val_jacard_coef: 0.1099 - val_dice_coef: 0.1957 - lr: 2.5000e-05
Epoch 36/100
1/1 [==============================] - ETA: 0s - loss: 0.8171 - accuracy: 0.0717 - jacard_coef: 0.1119 - dice_coef: 0.1829
Epoch 36: val_jacard_coef improved from 0.11011 to 0.11213, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8171 - accuracy: 0.0717 - jacard_coef: 0.1119 - dice_coef: 0.1829 - val_loss: 0.8042 - val_accuracy: 0.3192 - val_jacard_coef: 0.1121 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 37/100
1/1 [==============================] - ETA: 0s - loss: 0.8272 - accuracy: 0.0659 - jacard_coef: 0.1044 - dice_coef: 0.1728
Epoch 37: val_jacard_coef improved from 0.11213 to 0.11353, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8272 - accuracy: 0.0659 - jacard_coef: 0.1044 - dice_coef: 0.1728 - val_loss: 0.8042 - val_accuracy: 0.3157 - val_jacard_coef: 0.1135 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 38/100
1/1 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.0844 - jacard_coef: 0.1320 - dice_coef: 0.2089
Epoch 38: val_jacard_coef improved from 0.11353 to 0.11445, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7911 - accuracy: 0.0844 - jacard_coef: 0.1320 - dice_coef: 0.2089 - val_loss: 0.8041 - val_accuracy: 0.3131 - val_jacard_coef: 0.1144 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 39/100
1/1 [==============================] - ETA: 0s - loss: 0.8142 - accuracy: 0.0731 - jacard_coef: 0.1141 - dice_coef: 0.1858
Epoch 39: val_jacard_coef improved from 0.11445 to 0.11501, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8142 - accuracy: 0.0731 - jacard_coef: 0.1141 - dice_coef: 0.1858 - val_loss: 0.8041 - val_accuracy: 0.3109 - val_jacard_coef: 0.1150 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 40/100
1/1 [==============================] - ETA: 0s - loss: 0.7934 - accuracy: 0.0831 - jacard_coef: 0.1302 - dice_coef: 0.2066
Epoch 40: val_jacard_coef improved from 0.11501 to 0.11537, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.7934 - accuracy: 0.0831 - jacard_coef: 0.1302 - dice_coef: 0.2066 - val_loss: 0.8041 - val_accuracy: 0.3092 - val_jacard_coef: 0.1154 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 41/100
1/1 [==============================] - ETA: 0s - loss: 0.8344 - accuracy: 0.0629 - jacard_coef: 0.0992 - dice_coef: 0.1656
Epoch 41: val_jacard_coef improved from 0.11537 to 0.11572, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8344 - accuracy: 0.0629 - jacard_coef: 0.0992 - dice_coef: 0.1656 - val_loss: 0.8040 - val_accuracy: 0.3081 - val_jacard_coef: 0.1157 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 42/100
1/1 [==============================] - ETA: 0s - loss: 0.8060 - accuracy: 0.0760 - jacard_coef: 0.1203 - dice_coef: 0.1940
Epoch 42: val_jacard_coef improved from 0.11572 to 0.11600, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8060 - accuracy: 0.0760 - jacard_coef: 0.1203 - dice_coef: 0.1940 - val_loss: 0.8040 - val_accuracy: 0.3074 - val_jacard_coef: 0.1160 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 43/100
1/1 [==============================] - ETA: 0s - loss: 0.8209 - accuracy: 0.0695 - jacard_coef: 0.1090 - dice_coef: 0.1791
Epoch 43: val_jacard_coef improved from 0.11600 to 0.11619, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8209 - accuracy: 0.0695 - jacard_coef: 0.1090 - dice_coef: 0.1791 - val_loss: 0.8040 - val_accuracy: 0.3069 - val_jacard_coef: 0.1162 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 44/100
1/1 [==============================] - ETA: 0s - loss: 0.8088 - accuracy: 0.0765 - jacard_coef: 0.1182 - dice_coef: 0.1912
Epoch 44: val_jacard_coef improved from 0.11619 to 0.11631, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8088 - accuracy: 0.0765 - jacard_coef: 0.1182 - dice_coef: 0.1912 - val_loss: 0.8040 - val_accuracy: 0.3066 - val_jacard_coef: 0.1163 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 45/100
1/1 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.0761 - jacard_coef: 0.1184 - dice_coef: 0.1915
Epoch 45: val_jacard_coef improved from 0.11631 to 0.11640, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 4s 4s/step - loss: 0.8085 - accuracy: 0.0761 - jacard_coef: 0.1184 - dice_coef: 0.1915 - val_loss: 0.8040 - val_accuracy: 0.3064 - val_jacard_coef: 0.1164 - val_dice_coef: 0.1960 - lr: 2.5000e-05
Epoch 46/100
1/1 [==============================] - ETA: 0s - loss: 0.7946 - accuracy: 0.0834 - jacard_coef: 0.1292 - dice_coef: 0.2054
Epoch 46: val_jacard_coef improved from 0.11640 to 0.11649, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 4s 4s/step - loss: 0.7946 - accuracy: 0.0834 - jacard_coef: 0.1292 - dice_coef: 0.2054 - val_loss: 0.8041 - val_accuracy: 0.3065 - val_jacard_coef: 0.1165 - val_dice_coef: 0.1959 - lr: 2.5000e-05
Epoch 47/100
1/1 [==============================] - ETA: 0s - loss: 0.8202 - accuracy: 0.0692 - jacard_coef: 0.1096 - dice_coef: 0.1798
Epoch 47: val_jacard_coef improved from 0.11649 to 0.11649, saving model to microbead_training_20251009_073134/best_attention_resunet_model.hdf5
1/1 [==============================] - 3s 3s/step - loss: 0.8202 - accuracy: 0.0692 - jacard_coef: 0.1096 - dice_coef: 0.1798 - val_loss: 0.8042 - val_accuracy: 0.3067 - val_jacard_coef: 0.1165 - val_dice_coef: 0.1958 - lr: 2.5000e-05
Epoch 48/100
1/1 [==============================] - ETA: 0s - loss: 0.8276 - accuracy: 0.0654 - jacard_coef: 0.1041 - dice_coef: 0.1724
Epoch 48: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8276 - accuracy: 0.0654 - jacard_coef: 0.1041 - dice_coef: 0.1724 - val_loss: 0.8043 - val_accuracy: 0.3070 - val_jacard_coef: 0.1165 - val_dice_coef: 0.1957 - lr: 2.5000e-05
Epoch 49/100
1/1 [==============================] - ETA: 0s - loss: 0.8207 - accuracy: 0.0691 - jacard_coef: 0.1092 - dice_coef: 0.1793
Epoch 49: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8207 - accuracy: 0.0691 - jacard_coef: 0.1092 - dice_coef: 0.1793 - val_loss: 0.8044 - val_accuracy: 0.3074 - val_jacard_coef: 0.1164 - val_dice_coef: 0.1956 - lr: 2.5000e-05
Epoch 50/100
1/1 [==============================] - ETA: 0s - loss: 0.7972 - accuracy: 0.0817 - jacard_coef: 0.1272 - dice_coef: 0.2028
Epoch 50: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.7972 - accuracy: 0.0817 - jacard_coef: 0.1272 - dice_coef: 0.2028 - val_loss: 0.8045 - val_accuracy: 0.3079 - val_jacard_coef: 0.1163 - val_dice_coef: 0.1955 - lr: 2.5000e-05
Epoch 51/100
1/1 [==============================] - ETA: 0s - loss: 0.8340 - accuracy: 0.0633 - jacard_coef: 0.0995 - dice_coef: 0.1660
Epoch 51: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8340 - accuracy: 0.0633 - jacard_coef: 0.0995 - dice_coef: 0.1660 - val_loss: 0.8046 - val_accuracy: 0.3086 - val_jacard_coef: 0.1162 - val_dice_coef: 0.1954 - lr: 2.5000e-05
Epoch 52/100
1/1 [==============================] - ETA: 0s - loss: 0.8212 - accuracy: 0.0693 - jacard_coef: 0.1088 - dice_coef: 0.1788
Epoch 52: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8212 - accuracy: 0.0693 - jacard_coef: 0.1088 - dice_coef: 0.1788 - val_loss: 0.8047 - val_accuracy: 0.3097 - val_jacard_coef: 0.1160 - val_dice_coef: 0.1953 - lr: 2.5000e-05
Epoch 53/100
1/1 [==============================] - ETA: 0s - loss: 0.8271 - accuracy: 0.0670 - jacard_coef: 0.1045 - dice_coef: 0.1729
Epoch 53: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.

Epoch 53: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8271 - accuracy: 0.0670 - jacard_coef: 0.1045 - dice_coef: 0.1729 - val_loss: 0.8048 - val_accuracy: 0.3104 - val_jacard_coef: 0.1157 - val_dice_coef: 0.1952 - lr: 2.5000e-05
Epoch 54/100
1/1 [==============================] - ETA: 0s - loss: 0.8077 - accuracy: 0.0754 - jacard_coef: 0.1190 - dice_coef: 0.1923
Epoch 54: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8077 - accuracy: 0.0754 - jacard_coef: 0.1190 - dice_coef: 0.1923 - val_loss: 0.8048 - val_accuracy: 0.3112 - val_jacard_coef: 0.1154 - val_dice_coef: 0.1952 - lr: 1.2500e-05
Epoch 55/100
1/1 [==============================] - ETA: 0s - loss: 0.8299 - accuracy: 0.0652 - jacard_coef: 0.1024 - dice_coef: 0.1701
Epoch 55: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8299 - accuracy: 0.0652 - jacard_coef: 0.1024 - dice_coef: 0.1701 - val_loss: 0.8049 - val_accuracy: 0.3121 - val_jacard_coef: 0.1150 - val_dice_coef: 0.1951 - lr: 1.2500e-05
Epoch 56/100
1/1 [==============================] - ETA: 0s - loss: 0.8166 - accuracy: 0.0712 - jacard_coef: 0.1123 - dice_coef: 0.1834
Epoch 56: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8166 - accuracy: 0.0712 - jacard_coef: 0.1123 - dice_coef: 0.1834 - val_loss: 0.8050 - val_accuracy: 0.3131 - val_jacard_coef: 0.1146 - val_dice_coef: 0.1950 - lr: 1.2500e-05
Epoch 57/100
1/1 [==============================] - ETA: 0s - loss: 0.8324 - accuracy: 0.0637 - jacard_coef: 0.1007 - dice_coef: 0.1676
Epoch 57: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8324 - accuracy: 0.0637 - jacard_coef: 0.1007 - dice_coef: 0.1676 - val_loss: 0.8051 - val_accuracy: 0.3147 - val_jacard_coef: 0.1142 - val_dice_coef: 0.1949 - lr: 1.2500e-05
Epoch 58/100
1/1 [==============================] - ETA: 0s - loss: 0.8068 - accuracy: 0.0760 - jacard_coef: 0.1197 - dice_coef: 0.1932
Epoch 58: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8068 - accuracy: 0.0760 - jacard_coef: 0.1197 - dice_coef: 0.1932 - val_loss: 0.8053 - val_accuracy: 0.3170 - val_jacard_coef: 0.1139 - val_dice_coef: 0.1947 - lr: 1.2500e-05
Epoch 59/100
1/1 [==============================] - ETA: 0s - loss: 0.8501 - accuracy: 0.0566 - jacard_coef: 0.0882 - dice_coef: 0.1499
Epoch 59: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8501 - accuracy: 0.0566 - jacard_coef: 0.0882 - dice_coef: 0.1499 - val_loss: 0.8055 - val_accuracy: 0.3212 - val_jacard_coef: 0.1138 - val_dice_coef: 0.1945 - lr: 1.2500e-05
Epoch 60/100
1/1 [==============================] - ETA: 0s - loss: 0.8205 - accuracy: 0.0705 - jacard_coef: 0.1093 - dice_coef: 0.1795
Epoch 60: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8205 - accuracy: 0.0705 - jacard_coef: 0.1093 - dice_coef: 0.1795 - val_loss: 0.8056 - val_accuracy: 0.3255 - val_jacard_coef: 0.1136 - val_dice_coef: 0.1944 - lr: 1.2500e-05
Epoch 61/100
1/1 [==============================] - ETA: 0s - loss: 0.8256 - accuracy: 0.0662 - jacard_coef: 0.1056 - dice_coef: 0.1744
Epoch 61: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8256 - accuracy: 0.0662 - jacard_coef: 0.1056 - dice_coef: 0.1744 - val_loss: 0.8058 - val_accuracy: 0.3294 - val_jacard_coef: 0.1133 - val_dice_coef: 0.1942 - lr: 1.2500e-05
Epoch 62/100
1/1 [==============================] - ETA: 0s - loss: 0.7975 - accuracy: 0.0814 - jacard_coef: 0.1269 - dice_coef: 0.2025
Epoch 62: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.7975 - accuracy: 0.0814 - jacard_coef: 0.1269 - dice_coef: 0.2025 - val_loss: 0.8060 - val_accuracy: 0.3334 - val_jacard_coef: 0.1130 - val_dice_coef: 0.1940 - lr: 1.2500e-05
Epoch 63/100
1/1 [==============================] - ETA: 0s - loss: 0.8098 - accuracy: 0.0751 - jacard_coef: 0.1174 - dice_coef: 0.1902
Epoch 63: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.

Epoch 63: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8098 - accuracy: 0.0751 - jacard_coef: 0.1174 - dice_coef: 0.1902 - val_loss: 0.8061 - val_accuracy: 0.3374 - val_jacard_coef: 0.1128 - val_dice_coef: 0.1939 - lr: 1.2500e-05
Epoch 64/100
1/1 [==============================] - ETA: 0s - loss: 0.8113 - accuracy: 0.0740 - jacard_coef: 0.1163 - dice_coef: 0.1887
Epoch 64: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8113 - accuracy: 0.0740 - jacard_coef: 0.1163 - dice_coef: 0.1887 - val_loss: 0.8062 - val_accuracy: 0.3411 - val_jacard_coef: 0.1126 - val_dice_coef: 0.1938 - lr: 6.2500e-06
Epoch 65/100
1/1 [==============================] - ETA: 0s - loss: 0.8141 - accuracy: 0.0735 - jacard_coef: 0.1141 - dice_coef: 0.1859
Epoch 65: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8141 - accuracy: 0.0735 - jacard_coef: 0.1141 - dice_coef: 0.1859 - val_loss: 0.8063 - val_accuracy: 0.3448 - val_jacard_coef: 0.1124 - val_dice_coef: 0.1937 - lr: 6.2500e-06
Epoch 66/100
1/1 [==============================] - ETA: 0s - loss: 0.8205 - accuracy: 0.0700 - jacard_coef: 0.1093 - dice_coef: 0.1795
Epoch 66: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8205 - accuracy: 0.0700 - jacard_coef: 0.1093 - dice_coef: 0.1795 - val_loss: 0.8064 - val_accuracy: 0.3486 - val_jacard_coef: 0.1123 - val_dice_coef: 0.1936 - lr: 6.2500e-06
Epoch 67/100
1/1 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.0644 - jacard_coef: 0.1026 - dice_coef: 0.1704
Epoch 67: val_jacard_coef did not improve from 0.11649
1/1 [==============================] - 1s 1s/step - loss: 0.8296 - accuracy: 0.0644 - jacard_coef: 0.1026 - dice_coef: 0.1704 - val_loss: 0.8065 - val_accuracy: 0.3520 - val_jacard_coef: 0.1123 - val_dice_coef: 0.1935 - lr: 6.2500e-06
Epoch 67: early stopping
Restoring model weights from the end of the best epoch: 47.
‚úì Attention ResUNet training completed in 0:02:39.409344
Best Val Jaccard: 0.1165

================================================================================
TRAINING COMPLETE - SUMMARY
================================================================================

            model     lr  batch_size  dropout loss_type  best_val_jacard  training_time
             UNet 0.0001          32      0.3      dice         0.245566 0:03:35.913371
   Attention_UNet 0.0001          32      0.3      dice         0.144143 0:04:40.594105
Attention_ResUNet 0.0001          32      0.3      dice         0.116492 0:02:39.409344

üèÜ BEST MODEL:
  Architecture: UNet
  Best Val Jaccard: 0.2456
  Training Time: 0:03:35.913371

üìä COMPARISON WITH PREVIOUS RESULTS:
  Previous (mitochondria hyperparameters):
    Best Val Jaccard: 0.1427 (collapsed to ~0.0)
  Current (microbead hyperparameters):
    Best Val Jaccard: 0.2456
  ‚ö†Ô∏è  MODERATE: 1.7√ó improvement
  ‚Üí Check partial mask completion and data quality

================================================================================
All results saved in: microbead_training_20251009_073134/
================================================================================

=======================================================================
TRAINING COMPLETED
=======================================================================
Job finished on Thu Oct  9 03:42:43 PM +08 2025
Exit code: 0 ‚úì SUCCESS

‚úì Training completed successfully!

üìä TRAINING RESULTS:
===================
Performance summary:
model,lr,batch_size,dropout,loss_type,best_val_jacard,training_time
UNet,0.0001,32,0.3,dice,0.2455657422542572,0:03:35.913371
Attention_UNet,0.0001,32,0.3,dice,0.1441425234079361,0:04:40.594105
Attention_ResUNet,0.0001,32,0.3,dice,0.11649186909198761,0:02:39.409344

Generated files:
  - Model files (.hdf5): 6
  - Training histories (.csv): 4

üìÅ OUTPUT DIRECTORY: microbead_training_20251009_073134

üîç PERFORMANCE CHECK:
====================
Best model: UNet
Best validation Jaccard: 0.2456

Comparison with previous training:
  Previous (mitochondria params): 0.1427 ‚Üí 0.0 (collapsed)
  Current (microbead params):     0.2456

  ‚ö†  MODERATE: 1.7√ó improvement
  ‚Üí Some progress, check training curves

üì• TO DOWNLOAD RESULTS:
=======================
From your local machine:
  scp -r phyzxi@hpc:~/scratch/unet-HPC/microbead_training_20251009_073134 ./local_results/


üìù FULL LOGS:
=============
  Training console: microbead_training_20251009_153124/training_console.log
  PBS output: Microbead_Optimized_Training.ostdct-mgmt-02

=======================================================================
MICROBEAD OPTIMIZED TRAINING - JOB COMPLETE
=======================================================================
Models: UNet, Attention UNet, Attention ResUNet
Hyperparameters: Optimized for 109 objects/image (36√ó mitochondria)
Expected: Val Jaccard 0.50-0.70 (vs previous 0.14‚Üí0.0)
=======================================================================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-09 15:42:45.769773:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 279280.stdct-mgmt-02
	Project: personal-phyzxi
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(36), Used(36)
	CPU Time Used: 00:09:59
	Memory: Requested(240gb), Used(13033568kb)
	Vmem Used: 79713516kb
	Walltime: Requested(12:00:00), Used(00:11:36)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (GN-A40-070[0]:ncpus=36:ngpus=1:mem=251658240kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	No GPU-related information available for this job.
