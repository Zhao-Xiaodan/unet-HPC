=======================================================================
CONVNEXT-UNET DEDICATED TRAINING - MITOCHONDRIA SEGMENTATION
=======================================================================
Model: ConvNeXt-UNet (Modern CNN with improved efficiency)
Task: Mitochondria semantic segmentation
Framework: TensorFlow/Keras with enhanced dataset management
Expected Training Time: 12-18 hours (with optimizations and checkpointing)

Job started on Thu Oct  2 03:05:22 PM +08 2025
Running on node: GN-A40-008
Job ID: 246020.stdct-mgmt-02
Available GPUs: GPU-9fc434dc-34ca-0de5-be0e-47d91a51aae4
Memory: 503Gi, CPUs: 36

=== CONVNEXT-UNET TRAINING CONFIGURATION ===
Dataset Images: ./dataset_full_stack/images/ (1980 patches - REQUIRED)
Dataset Masks: ./dataset_full_stack/masks/ (1980 patches - REQUIRED)
Alternative: ./dataset/images/ and ./dataset/masks/
Image Size: 256x256x3
Batch Size: 6 (optimized for faster training)
Learning Rate: 2e-4 (Adam optimizer, optimized)
Epochs: 60 (reduced for completion within time limit)
Loss Function: Binary Focal Loss
Special Features: Enhanced dataset cache management + TF compatibility fixes
==============================================

TensorFlow Container: /app1/common/singularity-img/hopper/tensorflow/tensorflow_2.16.1-cuda_12.5.0_24.06.sif

=== AGGRESSIVE CACHE CLEARING ===
Performing comprehensive cache clearing to prevent dataset conflicts...
Clearing TensorFlow dataset caches...
Clearing Python cache...
Clearing previous model checkpoints...
Unique session ID: convnext_1759388723_2533395
✓ Aggressive cache clearing completed
==================================

=== PRE-EXECUTION CHECKS ===
1. Checking dataset structure...
   ✓ Full stack dataset directories found (PREFERRED)
   ✓ Images found: 1980 files
   ✓ Masks found: 1980 files

2. Checking Python files...
   ✓ convnext_unet_training.py found
   ✓ modern_unet_models.py found
==========================

=== TENSORFLOW & GPU STATUS ===
Python version: 3.10.12
TensorFlow version: 2.16.1
CUDA built support: True
Physical GPUs found: 1
  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
✓ GPU memory growth enabled
✓ GPU operation test successful

ConvNeXt-UNet memory requirements:
- Expected GPU memory: 8-12 GB
- Batch size: 4 (optimized)
- Model parameters: ~15-25M
===============================

=== CONVNEXT-UNET MODEL VALIDATION ===
Testing ConvNeXt-UNet creation to validate implementation...
2025-10-02 07:05:41.487183: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
Testing ConvNeXt-UNet model creation...
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
✓ ConvNeXt-UNet: 34,590,913 parameters
✓ Model building successful
✓ Forward pass successful: (1, 64, 64, 1)
✓ ConvNeXt-UNet validation completed successfully
✓ ConvNeXt-UNet validation passed
=====================================

🚀 STARTING CONVNEXT-UNET TRAINING
=============================================
Training ConvNeXt-UNet with enhanced dataset management

Training Configuration:
- Architecture: ConvNeXt-UNet (Modern CNN)
- Learning Rate: 1e-4 (Adam optimizer)
- Batch Size: 4 (memory optimized)
- Max Epochs: 100 (with early stopping)
- Loss: Binary Focal Loss (gamma=2)
- Special: No tf.data.Dataset caching

Expected timeline: 12-18 hours (complete training with checkpointing)
Expected performance: 93-95% Jaccard
Recent fixes: TensorFlow compatibility + time management optimizations
=============================================
Output directory: convnext_unet_training_20251002_150553

Starting ConvNeXt-UNet training execution...
======================================================================
CONVNEXT-UNET DEDICATED TRAINING FOR MITOCHONDRIA SEGMENTATION
======================================================================
Model: ConvNeXt-UNet (Modern CNN with improved efficiency)
Task: Mitochondria semantic segmentation
Framework: TensorFlow/Keras with enhanced dataset management

✓ GPU memory growth enabled for 1 GPUs

🔍 Checking for previous completed training...

🔍 Checking for previous checkpoints to resume from...

📁 Output directory: convnext_unet_training_20251002_070601
=== LOADING DATASET FOR CONVNEXT-UNET ===
Using dataset: dataset_full_stack/images/ and dataset_full_stack/masks/
✓ Loaded 1980 images and 1980 masks
Training set: 1782 samples
Validation set: 198 samples

⚙️ Training Configuration:
  Learning Rate: 0.0002
  Batch Size: 6
  Max Epochs: 60
  Input Shape: (256, 256, 3)
  Training Samples: 1782
  Validation Samples: 198
  Steps per Epoch: 297

⏱️ Time Estimates:
  Estimated time per epoch: ~20s
  Estimated total training time: ~99.0 hours
  Walltime limit: 24 hours
  ⚠️ Warning: Estimated time exceeds safe limit!
  Consider reducing epochs if training is slow

🚀 Starting training at: 2025-10-02 07:06:18

============================================================
TRAINING: ConvNeXt-UNet (Dedicated)
Learning Rate: 0.0002, Batch Size: 6, Max Epochs: 60
TensorFlow Version: 2.16.1
============================================================
🧹 Performing aggressive cache clearing for ConvNeXt-UNet...
✓ Aggressive cache clearing completed: 3 items cleared
✓ Unique session ID: 933f9617
Model: "UNet"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            
                                                                                                  
 conv2d (Conv2D)             (None, 256, 256, 64)         640       ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          
                                                                                                  
 batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        
 D)                                                                                               
                                                                                                  
 conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       
                                                                                                  
 batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        
                                                                                                  
 batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     
                                                                                                  
 batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        
                                                                                                  
 batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     
                                                                                                  
 batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        
                                                                                                  
 batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        
 g2D)                                                                                             
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        
                                                                                                  
 batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'
                                                                    ]                             
                                                                                                  
 up_sampling2d (UpSampling2  (None, 32, 32, 1024)         0         ['activation_9[0][0]']        
 D)                                                                                               
                                                                                                  
 concatenate (Concatenate)   (None, 32, 32, 1536)         0         ['up_sampling2d[0][0]',       
                                                                     'activation_7[0][0]']        
                                                                                                  
 conv2d_10 (Conv2D)          (None, 32, 32, 512)          7078400   ['concatenate[0][0]']         
                                                                                                  
 batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       
                                                                                                  
 batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_1 (UpSamplin  (None, 64, 64, 512)          0         ['activation_11[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_1 (Concatenate  (None, 64, 64, 768)          0         ['up_sampling2d_1[0][0]',     
 )                                                                   'activation_5[0][0]']        
                                                                                                  
 conv2d_12 (Conv2D)          (None, 64, 64, 256)          1769728   ['concatenate_1[0][0]']       
                                                                                                  
 batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       
                                                                                                  
 batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_2 (UpSamplin  (None, 128, 128, 256)        0         ['activation_13[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_2 (Concatenate  (None, 128, 128, 384)        0         ['up_sampling2d_2[0][0]',     
 )                                                                   'activation_3[0][0]']        
                                                                                                  
 conv2d_14 (Conv2D)          (None, 128, 128, 128)        442496    ['concatenate_2[0][0]']       
                                                                                                  
 batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       
                                                                                                  
 batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]
                                                                    ']                            
                                                                                                  
 up_sampling2d_3 (UpSamplin  (None, 256, 256, 128)        0         ['activation_15[0][0]']       
 g2D)                                                                                             
                                                                                                  
 concatenate_3 (Concatenate  (None, 256, 256, 192)        0         ['up_sampling2d_3[0][0]',     
 )                                                                   'activation_1[0][0]']        
                                                                                                  
 conv2d_16 (Conv2D)          (None, 256, 256, 64)         110656    ['concatenate_3[0][0]']       
                                                                                                  
 batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       
                                                                                                  
 batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]
                                                                    ']                            
                                                                                                  
 conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       
                                                                                                  
 batch_normalization_18 (Ba  (None, 256, 256, 1)          4         ['conv2d_18[0][0]']           
 tchNormalization)                                                                                
                                                                                                  
 activation_18 (Activation)  (None, 256, 256, 1)          0         ['batch_normalization_18[0][0]
                                                                    ']                            
                                                                                                  
==================================================================================================
Total params: 31401349 (119.79 MB)
Trainable params: 31389571 (119.74 MB)
Non-trainable params: 11778 (46.01 KB)
__________________________________________________________________________________________________
None
Creating ConvNeXt-UNet model...
Model parameters: 34,590,913
✓ Loss scaling optimizer enabled for mixed precision
Starting ConvNeXt-UNet training...
Applying ConvNeXt-specific optimizations...
✓ TF32 enabled
✓ Mixed precision (float16) enabled
✓ XLA JIT compilation enabled
🕐 Training started at: 2025-10-02 07:06:29
Epoch 1/60
WARNING:tensorflow:AutoGraph could not transform <function create_autocast_variable at 0x1504884e29e0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: <gast.gast.Expr object at 0x150032551bd0>
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1759388838.478003 2534384 service.cc:145] XLA service 0x1500f0001950 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1759388838.478044 2534384 service.cc:153]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1759388853.257643 2534384 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_81__6', 168 bytes spill stores, 168 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_74__6', 168 bytes spill stores, 168 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_69__6', 172 bytes spill stores, 172 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_66__8', 336 bytes spill stores, 336 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_62__4', 40 bytes spill stores, 40 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_reduce_transpose_fusion_2__1', 16 bytes spill stores, 16 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_60__6', 172 bytes spill stores, 172 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_55__6', 172 bytes spill stores, 172 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_50__6', 180 bytes spill stores, 180 bytes spill loads
ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_47__6', 168 bytes spill stores, 168 bytes spill loads

I0000 00:00:1759388853.291126 2534384 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  1/297 [..............................] - ETA: 6:25:05 - loss: 0.2210 - accuracy: 0.4768 - jacard_coef: 0.0305  2/297 [..............................] - ETA: 19s - loss: 0.1836 - accuracy: 0.5987 - jacard_coef: 0.0209      3/297 [..............................] - ETA: 17s - loss: 0.1566 - accuracy: 0.6962 - jacard_coef: 0.0234  4/297 [..............................] - ETA: 16s - loss: 0.1389 - accuracy: 0.7536 - jacard_coef: 0.0178  5/297 [..............................] - ETA: 16s - loss: 0.1263 - accuracy: 0.7884 - jacard_coef: 0.0142  6/297 [..............................] - ETA: 15s - loss: 0.1153 - accuracy: 0.8151 - jacard_coef: 0.0119  7/297 [..............................] - ETA: 15s - loss: 0.1068 - accuracy: 0.8353 - jacard_coef: 0.0102  9/297 [..............................] - ETA: 15s - loss: 0.0993 - accuracy: 0.8595 - jacard_coef: 0.0079 10/297 [>.............................] - ETA: 15s - loss: 0.0985 - accuracy: 0.8662 - jacard_coef: 0.0071 12/297 [>.............................] - ETA: 14s - loss: 0.0925 - accuracy: 0.8798 - jacard_coef: 0.0059 14/297 [>.............................] - ETA: 14s - loss: 0.0882 - accuracy: 0.8892 - jacard_coef: 0.0051 16/297 [>.............................] - ETA: 14s - loss: 0.0842 - accuracy: 0.8971 - jacard_coef: 0.0044 18/297 [>.............................] - ETA: 14s - loss: 0.0815 - accuracy: 0.9025 - jacard_coef: 0.0040 20/297 [=>............................] - ETA: 14s - loss: 0.0800 - accuracy: 0.9060 - jacard_coef: 0.0036 22/297 [=>............................] - ETA: 13s - loss: 0.0765 - accuracy: 0.9119 - jacard_coef: 0.0032 24/297 [=>............................] - ETA: 13s - loss: 0.0738 - accuracy: 0.9165 - jacard_coef: 0.0030 26/297 [=>............................] - ETA: 13s - loss: 0.0738 - accuracy: 0.9175 - jacard_coef: 0.0027 28/297 [=>............................] - ETA: 13s - loss: 0.0715 - accuracy: 0.9211 - jacard_coef: 0.0025 30/297 [==>...........................] - ETA: 13s - loss: 0.0707 - accuracy: 0.9229 - jacard_coef: 0.0024 32/297 [==>...........................] - ETA: 13s - loss: 0.0688 - accuracy: 0.9259 - jacard_coef: 0.0022 34/297 [==>...........................] - ETA: 13s - loss: 0.0695 - accuracy: 0.9255 - jacard_coef: 0.0021 36/297 [==>...........................] - ETA: 13s - loss: 0.0693 - accuracy: 0.9261 - jacard_coef: 0.0020 38/297 [==>...........................] - ETA: 13s - loss: 0.0686 - accuracy: 0.9275 - jacard_coef: 0.0019 40/297 [===>..........................] - ETA: 12s - loss: 0.0685 - accuracy: 0.9280 - jacard_coef: 0.0018 42/297 [===>..........................] - ETA: 12s - loss: 0.0681 - accuracy: 0.9287 - jacard_coef: 0.0017 44/297 [===>..........................] - ETA: 12s - loss: 0.0676 - accuracy: 0.9297 - jacard_coef: 0.0016 46/297 [===>..........................] - ETA: 12s - loss: 0.0673 - accuracy: 0.9304 - jacard_coef: 0.0015 48/297 [===>..........................] - ETA: 12s - loss: 0.0675 - accuracy: 0.9302 - jacard_coef: 0.0015 50/297 [====>.........................] - ETA: 12s - loss: 0.0670 - accuracy: 0.9312 - jacard_coef: 0.0014 52/297 [====>.........................] - ETA: 12s - loss: 0.0668 - accuracy: 0.9316 - jacard_coef: 0.0014 54/297 [====>.........................] - ETA: 12s - loss: 0.0661 - accuracy: 0.9326 - jacard_coef: 0.0013 56/297 [====>.........................] - ETA: 12s - loss: 0.0657 - accuracy: 0.9333 - jacard_coef: 0.0013 58/297 [====>.........................] - ETA: 11s - loss: 0.0657 - accuracy: 0.9335 - jacard_coef: 0.0012 60/297 [=====>........................] - ETA: 11s - loss: 0.0655 - accuracy: 0.9339 - jacard_coef: 0.0012 62/297 [=====>........................] - ETA: 11s - loss: 0.0652 - accuracy: 0.9343 - jacard_coef: 0.0011 64/297 [=====>........................] - ETA: 11s - loss: 0.0648 - accuracy: 0.9349 - jacard_coef: 0.0011 66/297 [=====>........................] - ETA: 11s - loss: 0.0642 - accuracy: 0.9358 - jacard_coef: 0.0011 68/297 [=====>........................] - ETA: 11s - loss: 0.0638 - accuracy: 0.9365 - jacard_coef: 0.0010 70/297 [======>.......................] - ETA: 11s - loss: 0.0643 - accuracy: 0.9360 - jacard_coef: 0.0010 72/297 [======>.......................] - ETA: 11s - loss: 0.0641 - accuracy: 0.9364 - jacard_coef: 9.8795e-04 74/297 [======>.......................] - ETA: 11s - loss: 0.0641 - accuracy: 0.9364 - jacard_coef: 9.6125e-04 76/297 [======>.......................] - ETA: 11s - loss: 0.0644 - accuracy: 0.9360 - jacard_coef: 9.3595e-04 78/297 [======>.......................] - ETA: 10s - loss: 0.0645 - accuracy: 0.9360 - jacard_coef: 9.1195e-04 80/297 [=======>......................] - ETA: 10s - loss: 0.0650 - accuracy: 0.9353 - jacard_coef: 8.8916e-04 82/297 [=======>......................] - ETA: 10s - loss: 0.0649 - accuracy: 0.9356 - jacard_coef: 8.6747e-04 84/297 [=======>......................] - ETA: 10s - loss: 0.0649 - accuracy: 0.9355 - jacard_coef: 8.4682e-04 86/297 [=======>......................] - ETA: 10s - loss: 0.0650 - accuracy: 0.9356 - jacard_coef: 8.2712e-04 88/297 [=======>......................] - ETA: 10s - loss: 0.0649 - accuracy: 0.9358 - jacard_coef: 8.0832e-04 90/297 [========>.....................] - ETA: 10s - loss: 0.0646 - accuracy: 0.9362 - jacard_coef: 7.9036e-04 92/297 [========>.....................] - ETA: 10s - loss: 0.0644 - accuracy: 0.9365 - jacard_coef: 7.7318e-04 94/297 [========>.....................] - ETA: 10s - loss: 0.0641 - accuracy: 0.9369 - jacard_coef: 7.5673e-04 96/297 [========>.....................] - ETA: 9s - loss: 0.0639 - accuracy: 0.9372 - jacard_coef: 7.4096e-04  98/297 [========>.....................] - ETA: 9s - loss: 0.0638 - accuracy: 0.9375 - jacard_coef: 7.2584e-04100/297 [=========>....................] - ETA: 9s - loss: 0.0641 - accuracy: 0.9370 - jacard_coef: 7.1132e-04102/297 [=========>....................] - ETA: 9s - loss: 0.0637 - accuracy: 0.9377 - jacard_coef: 6.9738e-04104/297 [=========>....................] - ETA: 9s - loss: 0.0633 - accuracy: 0.9383 - jacard_coef: 6.8397e-04106/297 [=========>....................] - ETA: 9s - loss: 0.0635 - accuracy: 0.9381 - jacard_coef: 6.7106e-04108/297 [=========>....................] - ETA: 9s - loss: 0.0633 - accuracy: 0.9383 - jacard_coef: 6.5863e-04110/297 [==========>...................] - ETA: 9s - loss: 0.0631 - accuracy: 0.9386 - jacard_coef: 6.4666e-04112/297 [==========>...................] - ETA: 9s - loss: 0.0631 - accuracy: 0.9387 - jacard_coef: 6.3511e-04114/297 [==========>...................] - ETA: 9s - loss: 0.0630 - accuracy: 0.9388 - jacard_coef: 6.2397e-04116/297 [==========>...................] - ETA: 8s - loss: 0.0629 - accuracy: 0.9390 - jacard_coef: 6.1321e-04118/297 [==========>...................] - ETA: 8s - loss: 0.0633 - accuracy: 0.9384 - jacard_coef: 6.0282e-04120/297 [===========>..................] - ETA: 8s - loss: 0.0634 - accuracy: 0.9383 - jacard_coef: 5.9277e-04122/297 [===========>..................] - ETA: 8s - loss: 0.0635 - accuracy: 0.9383 - jacard_coef: 5.8305e-04124/297 [===========>..................] - ETA: 8s - loss: 0.0634 - accuracy: 0.9385 - jacard_coef: 5.7365e-04126/297 [===========>..................] - ETA: 8s - loss: 0.0631 - accuracy: 0.9389 - jacard_coef: 5.6454e-04128/297 [===========>..................] - ETA: 8s - loss: 0.0631 - accuracy: 0.9389 - jacard_coef: 5.5572e-04130/297 [============>.................] - ETA: 8s - loss: 0.0636 - accuracy: 0.9384 - jacard_coef: 5.4717e-04132/297 [============>.................] - ETA: 8s - loss: 0.0633 - accuracy: 0.9387 - jacard_coef: 5.3888e-04134/297 [============>.................] - ETA: 8s - loss: 0.0636 - accuracy: 0.9383 - jacard_coef: 5.3084e-04136/297 [============>.................] - ETA: 7s - loss: 0.0636 - accuracy: 0.9383 - jacard_coef: 5.2303e-04138/297 [============>.................] - ETA: 7s - loss: 0.0636 - accuracy: 0.9384 - jacard_coef: 5.1545e-04140/297 [=============>................] - ETA: 7s - loss: 0.0638 - accuracy: 0.9381 - jacard_coef: 5.0809e-04142/297 [=============>................] - ETA: 7s - loss: 0.0635 - accuracy: 0.9385 - jacard_coef: 5.0093e-04144/297 [=============>................] - ETA: 7s - loss: 0.0635 - accuracy: 0.9385 - jacard_coef: 4.9398e-04146/297 [=============>................] - ETA: 7s - loss: 0.0635 - accuracy: 0.9385 - jacard_coef: 4.8721e-04148/297 [=============>................] - ETA: 7s - loss: 0.0635 - accuracy: 0.9386 - jacard_coef: 4.8062e-04150/297 [==============>...............] - ETA: 7s - loss: 0.0634 - accuracy: 0.9387 - jacard_coef: 4.7422e-04152/297 [==============>...............] - ETA: 7s - loss: 0.0634 - accuracy: 0.9387 - jacard_coef: 4.6798e-04154/297 [==============>...............] - ETA: 7s - loss: 0.0637 - accuracy: 0.9383 - jacard_coef: 4.6190e-04156/297 [==============>...............] - ETA: 6s - loss: 0.0639 - accuracy: 0.9381 - jacard_coef: 4.5598e-04158/297 [==============>...............] - ETA: 6s - loss: 0.0639 - accuracy: 0.9382 - jacard_coef: 4.5021e-04160/297 [===============>..............] - ETA: 6s - loss: 0.0640 - accuracy: 0.9380 - jacard_coef: 4.4458e-04162/297 [===============>..............] - ETA: 6s - loss: 0.0637 - accuracy: 0.9384 - jacard_coef: 4.3909e-04164/297 [===============>..............] - ETA: 6s - loss: 0.0636 - accuracy: 0.9386 - jacard_coef: 4.3373e-04166/297 [===============>..............] - ETA: 6s - loss: 0.0632 - accuracy: 0.9391 - jacard_coef: 4.2851e-04168/297 [===============>..............] - ETA: 6s - loss: 0.0633 - accuracy: 0.9390 - jacard_coef: 4.2341e-04170/297 [================>.............] - ETA: 6s - loss: 0.0633 - accuracy: 0.9391 - jacard_coef: 4.1843e-04172/297 [================>.............] - ETA: 6s - loss: 0.0634 - accuracy: 0.9391 - jacard_coef: 4.1356e-04174/297 [================>.............] - ETA: 6s - loss: 0.0636 - accuracy: 0.9389 - jacard_coef: 4.0881e-04176/297 [================>.............] - ETA: 5s - loss: 0.0638 - accuracy: 0.9386 - jacard_coef: 4.0416e-04178/297 [================>.............] - ETA: 5s - loss: 0.0637 - accuracy: 0.9387 - jacard_coef: 3.9962e-04180/297 [=================>............] - ETA: 5s - loss: 0.0638 - accuracy: 0.9386 - jacard_coef: 3.9518e-04182/297 [=================>............] - ETA: 5s - loss: 0.0639 - accuracy: 0.9385 - jacard_coef: 3.9084e-04184/297 [=================>............] - ETA: 5s - loss: 0.0639 - accuracy: 0.9386 - jacard_coef: 3.8659e-04186/297 [=================>............] - ETA: 5s - loss: 0.0639 - accuracy: 0.9385 - jacard_coef: 3.8243e-04188/297 [=================>............] - ETA: 5s - loss: 0.0638 - accuracy: 0.9387 - jacard_coef: 3.7836e-04190/297 [==================>...........] - ETA: 5s - loss: 0.0637 - accuracy: 0.9388 - jacard_coef: 3.7438e-04192/297 [==================>...........] - ETA: 5s - loss: 0.0636 - accuracy: 0.9390 - jacard_coef: 3.7048e-04194/297 [==================>...........] - ETA: 5s - loss: 0.0637 - accuracy: 0.9389 - jacard_coef: 3.6666e-04196/297 [==================>...........] - ETA: 4s - loss: 0.0635 - accuracy: 0.9391 - jacard_coef: 3.6292e-04198/297 [===================>..........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9393 - jacard_coef: 3.5925e-04200/297 [===================>..........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9394 - jacard_coef: 3.5566e-04202/297 [===================>..........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9394 - jacard_coef: 3.5214e-04204/297 [===================>..........] - ETA: 4s - loss: 0.0630 - accuracy: 0.9397 - jacard_coef: 3.4869e-04206/297 [===================>..........] - ETA: 4s - loss: 0.0631 - accuracy: 0.9396 - jacard_coef: 3.4530e-04208/297 [====================>.........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9394 - jacard_coef: 3.4198e-04210/297 [====================>.........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9394 - jacard_coef: 3.3873e-04212/297 [====================>.........] - ETA: 4s - loss: 0.0633 - accuracy: 0.9394 - jacard_coef: 3.3553e-04214/297 [====================>.........] - ETA: 4s - loss: 0.0634 - accuracy: 0.9392 - jacard_coef: 3.3239e-04216/297 [====================>.........] - ETA: 4s - loss: 0.0634 - accuracy: 0.9391 - jacard_coef: 3.2932e-04218/297 [=====================>........] - ETA: 3s - loss: 0.0633 - accuracy: 0.9394 - jacard_coef: 3.2630e-04220/297 [=====================>........] - ETA: 3s - loss: 0.0632 - accuracy: 0.9394 - jacard_coef: 3.2333e-04222/297 [=====================>........] - ETA: 3s - loss: 0.0633 - accuracy: 0.9393 - jacard_coef: 3.2042e-04224/297 [=====================>........] - ETA: 3s - loss: 0.0632 - accuracy: 0.9395 - jacard_coef: 3.1756e-04226/297 [=====================>........] - ETA: 3s - loss: 0.0631 - accuracy: 0.9396 - jacard_coef: 3.1475e-04228/297 [======================>.......] - ETA: 3s - loss: 0.0630 - accuracy: 0.9397 - jacard_coef: 3.1198e-04230/297 [======================>.......] - ETA: 3s - loss: 0.0629 - accuracy: 0.9398 - jacard_coef: 3.0927e-04232/297 [======================>.......] - ETA: 3s - loss: 0.0630 - accuracy: 0.9397 - jacard_coef: 3.0661e-04234/297 [======================>.......] - ETA: 3s - loss: 0.0632 - accuracy: 0.9394 - jacard_coef: 3.0398e-04236/297 [======================>.......] - ETA: 3s - loss: 0.0631 - accuracy: 0.9395 - jacard_coef: 3.0141e-04238/297 [=======================>......] - ETA: 2s - loss: 0.0632 - accuracy: 0.9393 - jacard_coef: 2.9888e-04240/297 [=======================>......] - ETA: 2s - loss: 0.0632 - accuracy: 0.9395 - jacard_coef: 2.9639e-04242/297 [=======================>......] - ETA: 2s - loss: 0.0631 - accuracy: 0.9396 - jacard_coef: 2.9394e-04244/297 [=======================>......] - ETA: 2s - loss: 0.0630 - accuracy: 0.9397 - jacard_coef: 2.9153e-04246/297 [=======================>......] - ETA: 2s - loss: 0.0630 - accuracy: 0.9397 - jacard_coef: 2.8916e-04247/297 [=======================>......] - ETA: 2s - loss: 0.0628 - accuracy: 0.9400 - jacard_coef: 2.8799e-04248/297 [========================>.....] - ETA: 2s - loss: 0.0627 - accuracy: 0.9400 - jacard_coef: 2.8682e-04249/297 [========================>.....] - ETA: 2s - loss: 0.0626 - accuracy: 0.9402 - jacard_coef: 2.8567e-04250/297 [========================>.....] - ETA: 2s - loss: 0.0627 - accuracy: 0.9401 - jacard_coef: 2.8453e-04251/297 [========================>.....] - ETA: 2s - loss: 0.0627 - accuracy: 0.9401 - jacard_coef: 2.8340e-04253/297 [========================>.....] - ETA: 2s - loss: 0.0627 - accuracy: 0.9402 - jacard_coef: 2.8116e-04255/297 [========================>.....] - ETA: 2s - loss: 0.0626 - accuracy: 0.9403 - jacard_coef: 2.7895e-04257/297 [========================>.....] - ETA: 1s - loss: 0.0625 - accuracy: 0.9403 - jacard_coef: 2.7678e-04259/297 [=========================>....] - ETA: 1s - loss: 0.0626 - accuracy: 0.9402 - jacard_coef: 2.7464e-04261/297 [=========================>....] - ETA: 1s - loss: 0.0626 - accuracy: 0.9401 - jacard_coef: 2.7254e-04263/297 [=========================>....] - ETA: 1s - loss: 0.0627 - accuracy: 0.9400 - jacard_coef: 2.7047e-04265/297 [=========================>....] - ETA: 1s - loss: 0.0627 - accuracy: 0.9400 - jacard_coef: 2.6842e-04267/297 [=========================>....] - ETA: 1s - loss: 0.0626 - accuracy: 0.9402 - jacard_coef: 2.6641e-04269/297 [==========================>...] - ETA: 1s - loss: 0.0626 - accuracy: 0.9402 - jacard_coef: 2.6443e-04271/297 [==========================>...] - ETA: 1s - loss: 0.0625 - accuracy: 0.9403 - jacard_coef: 2.6248e-04273/297 [==========================>...] - ETA: 1s - loss: 0.0625 - accuracy: 0.9403 - jacard_coef: 2.6056e-04275/297 [==========================>...] - ETA: 1s - loss: 0.0624 - accuracy: 0.9404 - jacard_coef: 2.5866e-04277/297 [==========================>...] - ETA: 0s - loss: 0.0622 - accuracy: 0.9406 - jacard_coef: 2.5680e-04279/297 [===========================>..] - ETA: 0s - loss: 0.0622 - accuracy: 0.9406 - jacard_coef: 2.5496e-04281/297 [===========================>..] - ETA: 0s - loss: 0.0623 - accuracy: 0.9405 - jacard_coef: 2.5314e-04283/297 [===========================>..] - ETA: 0s - loss: 0.0623 - accuracy: 0.9405 - jacard_coef: 2.5135e-04285/297 [===========================>..] - ETA: 0s - loss: 0.0622 - accuracy: 0.9406 - jacard_coef: 2.4959e-04287/297 [===========================>..] - ETA: 0s - loss: 0.0623 - accuracy: 0.9403 - jacard_coef: 2.4785e-04289/297 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9404 - jacard_coef: 2.4613e-04291/297 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9404 - jacard_coef: 2.4444e-04293/297 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9404 - jacard_coef: 2.4277e-04295/297 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9404 - jacard_coef: 2.4113e-04297/297 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9404 - jacard_coef: 2.3950e-04
📊 Progress Report (Epoch 1):
  ⏱️ Epoch time: 99.8s (avg: 99.8s)
  🕐 Elapsed: 0.0h, Remaining: 22.0h
  🎯 Val Jaccard: 0.0000, Val Loss: 0.0533

Epoch 1: val_jacard_coef improved from -inf to 0.00000, saving model to convnext_unet_training_20251002_070601/ConvNeXt_UNet_lr0.0002_bs6_933f9617_model.hdf5
/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
Traceback (most recent call last):
  File "/scratch/phyzxi/unet-HPC/convnext_unet_training.py", line 416, in train_convnext_unet
    history = model.fit(
  File "/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py", line 183, in create_dataset
    dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)
  File "/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py", line 163, in make_new_dset
    dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5d.pyx", line 137, in h5py.h5d.create
ValueError: Unable to synchronously create dataset (name already exists)

✗ ConvNeXt-UNet training failed: Unable to synchronously create dataset (name already exists)
🧹 Performing aggressive cache clearing for ConvNeXt-UNet...
✓ Aggressive cache clearing completed: 3 items cleared
✓ Unique session ID: 78709b11

✗ ConvNeXt-UNet training failed!

======================================================================
CONVNEXT-UNET DEDICATED TRAINING COMPLETED
======================================================================

=======================================================================
CONVNEXT-UNET TRAINING COMPLETED
=======================================================================
Job finished on Thu Oct  2 03:08:14 PM +08 2025
Exit code: 0 ✓ SUCCESS

✓ ConvNeXt-UNet training completed successfully!

Generated files:
📁 Training directory:
convnext_unet_training_20251001_153225/:
total 64
drwxr-x---  2 phyzxi svuusers    0 Oct  1 23:53 .
drwxr-xr-x 32 phyzxi svuusers 6186 Oct  2 15:06 ..

convnext_unet_training_20251001_155440/:
total 64
drwxr-x---  2 phyzxi svuusers    0 Oct  2 11:33 .
drwxr-xr-x 32 phyzxi svuusers 6186 Oct  2 15:06 ..

convnext_unet_training_20251002_033437/:
total 64
drwxr-x---  2 phyzxi svuusers    0 Oct  2 11:34 .
drwxr-xr-x 32 phyzxi svuusers 6186 Oct  2 15:06 ..

convnext_unet_training_20251002_052719/:
total 64
drwxr-x---  2 phyzxi svuusers    0 Oct  2 15:05 .
drwxr-xr-x 32 phyzxi svuusers 6186 Oct  2 15:06 ..

convnext_unet_training_20251002_070601/:
total 170776
drwxr-x---  2 phyzxi svuusers        64 Oct  2 15:08 .
drwxr-xr-x 32 phyzxi svuusers      6186 Oct  2 15:06 ..
-rw-r-----  1 phyzxi svuusers 139447040 Oct  2 15:08 ConvNeXt_UNet_lr0.0002_bs6_933f9617_model.hdf5

📊 Model and results:
-rw-r----- 1 phyzxi svuusers 139447040 Oct  2 15:08 convnext_unet_training_20251002_070601/ConvNeXt_UNet_lr0.0002_bs6_933f9617_model.hdf5

🎯 CONVNEXT-UNET PERFORMANCE SUMMARY:
======================================
No ConvNeXt-UNet results found.

📁 CONSOLE LOG SAVED: convnext_unet_training_20251002_150553_console.log

🔗 NEXT STEPS:
=============
1. Analyze ConvNeXt-UNet training results
2. Compare with Swin-UNet performance (93.57%)
3. Train CoAtNet-UNet separately if needed
4. Consider ensemble methods for best performance

=========================================
CONVNEXT-UNET TRAINING JOB COMPLETE
Model: ConvNeXt-UNet (Modern CNN)
Framework: TensorFlow/Keras + Enhanced Dataset Management
========================================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-10-02 15:08:17.271809:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 246020.stdct-mgmt-02
	Project: personal-phyzxi
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(36), Used(36)
	CPU Time Used: 00:03:05
	Memory: Requested(240gb), Used(15630632kb)
	Vmem Used: 46554604kb
	Walltime: Requested(24:00:00), Used(00:02:55)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (GN-A40-008[0]:ncpus=36:ngpus=1:mem=251658240kb)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	No GPU-related information available for this job.
